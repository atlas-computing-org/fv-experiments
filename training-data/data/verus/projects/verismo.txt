================
File: ./CODE_OF_CONDUCT.md
================

# Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns

================
File: ./LICENSE
================

    MIT License

    Copyright (c) Microsoft Corporation.

    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the "Software"), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE

================
File: ./source/Cargo.toml
================

[workspace]
members = [
    "verismo_main",
    "verismo",
    "verismo_macro",
    "vstd"
]

# the profile used for `cargo build`
[profile.dev]
panic = "abort" # disable stack unwinding on panic
incremental = true

# the profile used for `cargo build --release`
[profile.release]
panic = "abort" # disable stack unwinding on panic
incremental = true
debug = false

================
File: ./source/verismo_verus/Cargo.toml
================

[package]
name = "verismo_verus"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
proc-macro = true

[dependencies]
proc-macro2 = "1.0.39"
quote = "1.0"
synstructure = "0.12"
syn = "1.0"
syn_verus = { path="../../tools/verus/dependencies/syn", features = ["full", "visit-mut", "extra-traits"] }
prettyplease_verus = { path="../../tools/verus/dependencies/prettyplease" }

================
File: ./source/verismo_verus/src/struct_decl_inv.rs
================

use std::collections::{HashMap, HashSet};

use proc_macro2::{Span, TokenStream};
use quote::{quote, quote_spanned, ToTokens};
use syn_verus::buffer::Cursor;
use syn_verus::parse::{Parse, ParseStream};
use syn_verus::punctuated::Punctuated;
use syn_verus::spanned::Spanned;
use syn_verus::token::Comma;
use syn_verus::visit_mut::VisitMut;
use syn_verus::{
    braced, parenthesized, parse, parse_macro_input, token, visit_mut, Block, Error, Expr,
    Field, Fields, FnArg, FnArgKind, FnMode, GenericArgument, GenericParam, Ident, Index,
    ItemStruct, Lifetime, Member, Pat, PatIdent, PatType, PathArguments, Receiver, Signature,
    Token, Type, TypeInfer, TypePath, Visibility,
};

use crate::topological_sort::TopologicalSort;

pub fn struct_decl_inv(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let sdi: SDI = parse_macro_input!(input as SDI);
    match struct_decl_inv_main(sdi) {
        Ok(t) => t.into(),
        Err(err) => proc_macro::TokenStream::from(err.to_compile_error()),
    }
}

fn struct_decl_inv_main(sdi: SDI) -> parse::Result<TokenStream> {
    let main_name = sdi.item_struct.ident.to_string();

    let fields = get_fields(&sdi.item_struct.fields)?;
    check_dupe_field_names(&fields)?;
    check_dupe_param_names(&sdi)?;
    let partial_fields = get_partial_type_fields(&fields)?;
    check_invs_match_partial_types(&fields, &partial_fields, &sdi.invariant_decls)?;
    let ordering = check_deps_acyclic(&fields, &sdi.invariant_decls)?;
    let used_type_params = field_used_type_params(&sdi, &fields, &ordering);

    let mut stream = TokenStream::new();
    let mut wf_body_stream = quote! { true }; // to be extended with conjuncts

    let mut sdi = sdi;
    fill_in_item_struct(
        &main_name,
        &mut sdi.item_struct,
        &sdi.invariant_decls,
        &used_type_params,
    );
    sdi.item_struct.to_tokens(&mut stream);

    let fields_filled_in = get_fields(&sdi.item_struct.fields)?;
    for field in fields_filled_in.iter() {
        output_field_type_alias(
            &main_name,
            &sdi.item_struct.vis,
            &mut stream,
            field,
            &used_type_params,
        );
    }

    for inv in sdi.invariant_decls.iter() {
        output_invariant(
            &main_name,
            &sdi,
            &mut stream,
            &mut wf_body_stream,
            &partial_fields,
            inv,
            &used_type_params,
        );
    }

    output_wf(&sdi, &mut stream, wf_body_stream);

    Ok(quote! {
        ::builtin_macros::verus!{
            #stream
        }
    })
}

struct SDI {
    item_struct: ItemStruct,
    wf_vis: Visibility,
    wf_sig: Signature,
    invariant_decls: Vec<InvariantDecl>,
}

enum InvariantDecl {
    NormalExpr(Block),
    Invariant {
        field_name: Ident,
        depends_on: Vec<Ident>,
        quants: Vec<PatType>,
        condition: Option<Expr>,
        specifically: Option<Expr>,
        params: Vec<FnArg>,
        params_span: Span,
        predicate: Block,
    },
}

struct PartialField {
    name: Ident,
    partial_types: Vec<PartialType>,
}

struct PartialType {
    is_atomic_ghost: bool,
    concrete_args: Vec<Type>,
}

impl Parse for SDI {
    fn parse(input: ParseStream) -> parse::Result<SDI> {
        let item_struct: ItemStruct = input.parse()?;

        let wf_vis: Visibility = input.parse()?;
        let wf_sig: Signature = input.parse()?;
        check_wf_sig(&wf_sig)?;

        let brace_content;
        let _ = braced!(brace_content in input);

        let mut invariant_decls = vec![];
        while !brace_content.is_empty() {
            let invariant_decl: InvariantDecl = brace_content.parse()?;
            invariant_decls.push(invariant_decl);
        }

        Ok(SDI {
            item_struct,
            wf_vis,
            wf_sig,
            invariant_decls,
        })
    }
}

pub(crate) fn peek_keyword(cursor: Cursor, token: &str) -> bool {
    if let Some((ident, _rest)) = cursor.ident() {
        ident == token
    } else {
        false
    }
}

pub(crate) fn keyword(input: ParseStream, token: &str) -> parse::Result<Span> {
    input.step(|cursor| {
        if let Some((ident, rest)) = cursor.ident() {
            if ident == token {
                return Ok((ident.span(), rest));
            }
        }
        Err(cursor.error(format!("expected `{}`", token)))
    })
}

impl Parse for InvariantDecl {
    fn parse(input: ParseStream) -> parse::Result<InvariantDecl> {
        if peek_keyword(input.cursor(), "invariant") {
            let _ = keyword(input, "invariant");
            let _ = keyword(input, "on");

            let field_name: Ident = input.parse()?;

            let depends_on = if peek_keyword(input.cursor(), "with") {
                let _ = keyword(input, "with");
                let paren_content;
                let _ = parenthesized!(paren_content in input);
                let deps: Punctuated<Ident, token::Comma> =
                    paren_content.parse_terminated(Ident::parse)?;
                deps.into_iter().collect()
            } else {
                Vec::new()
            };

            let quants = if peek_keyword(input.cursor(), "forall") {
                let _ = keyword(input, "forall");
                parse_quants(input)?
            } else {
                Vec::new()
            };

            let condition = if peek_keyword(input.cursor(), "where") {
                let _ = keyword(input, "where");
                let paren_content;
                let _ = parenthesized!(paren_content in input);
                let expr: Expr = paren_content.parse()?;
                Some(expr)
            } else {
                None
            };

            let specifically = if peek_keyword(input.cursor(), "specifically") {
                let _ = keyword(input, "specifically");
                let paren_content;
                let _ = parenthesized!(paren_content in input);
                let expr: Expr = paren_content.parse()?;
                Some(expr)
            } else {
                None
            };

            let _ = keyword(input, "is");

            let (params_span, params) = {
                let paren_content;
                let ptoken = parenthesized!(paren_content in input);
                let params: Punctuated<FnArg, token::Comma> =
                    paren_content.parse_terminated(FnArg::parse)?;
                (ptoken.span, params.into_iter().collect())
            };

            let predicate: Block = input.parse()?;

            Ok(InvariantDecl::Invariant {
                field_name,
                depends_on,
                quants,
                condition,
                params,
                params_span,
                predicate,
                specifically,
            })
        } else if peek_keyword(input.cursor(), "predicate") {
            let _ = keyword(input, "predicate");
            let predicate: Block = input.parse()?;
            Ok(InvariantDecl::NormalExpr(predicate))
        } else {
            Err(input.error(format!(
                "expected an 'invariant' declaration or 'predicate' declaration"
            )))
        }
    }
}

fn parse_quants(input: ParseStream) -> parse::Result<Vec<PatType>> {
    let _or1_token: Token![|] = input.parse()?;

    let mut inputs = Vec::new();
    loop {
        if input.peek(Token![|]) {
            break;
        }
        let pat = Pat::parse(input)?;
        let colon_token: Token![:] = input.parse()?;
        let ty = Type::parse(input)?;
        let pat_type = PatType {
            attrs: vec![],
            pat: Box::new(pat),
            colon_token,
            ty: Box::new(ty),
        };
        inputs.push(pat_type);

        if input.peek(Token![|]) {
            break;
        }
        let _punct: Token![,] = input.parse()?;
    }

    let _or2_token: Token![|] = input.parse()?;

    Ok(inputs)
}

fn check_wf_sig(sig: &Signature) -> parse::Result<()> {
    if !is_spec(sig) {
        return Err(Error::new(
            sig.span(),
            "struct_with_invariants: this signature should be a `spec` function",
        ));
    }

    if !is_first_param_self(sig) {
        return Err(Error::new(
            sig.span(),
            "struct_with_invariants: the first param here should be `self`",
        ));
    }

    Ok(())
}

fn get_fields(f: &Fields) -> parse::Result<Vec<Field>> {
    match f {
        Fields::Named(fields_named) => Ok(fields_named.named.clone().into_iter().collect()),
        _ => {
            return Err(Error::new(
                f.span(),
                "struct_with_invariants: expected named fields",
            ));
        }
    }
}

fn check_dupe_field_names(fields: &Vec<Field>) -> parse::Result<()> {
    let mut names = HashSet::new();
    for field in fields {
        match &field.ident {
            None => {
                return Err(Error::new(
                    field.span(),
                    "struct_with_invariants: each field must have a name",
                ));
            }
            Some(id) => {
                let name = id.to_string();
                if names.contains(&name) {
                    return Err(Error::new(field.span(), "duplicate field name"));
                }
                names.insert(name.clone());
            }
        }
    }
    Ok(())
}

fn check_dupe_param_names(sdi: &SDI) -> parse::Result<()> {
    for inv_decl in sdi.invariant_decls.iter() {
        if let InvariantDecl::Invariant {
            depends_on,
            quants,
            params,
            ..
        } = inv_decl
        {
            let mut names = vec![];

            for dep in depends_on.iter() {
                names.push(dep.clone());
            }
            for quant in quants.iter() {
                match &*quant.pat {
                    Pat::Ident(PatIdent {
                        attrs: _,
                        by_ref: None,
                        mutability: None,
                        ident,
                        subpat: None,
                    }) => {
                        names.push(ident.clone());
                    }
                    _ => {
                        return Err(Error::new(
                            quant.pat.span(),
                            "declate_struct_with_invariants: expected identifier here",
                        ));
                    }
                }
            }
            for fn_arg in params.iter() {
                match &fn_arg.kind {
                    FnArgKind::Typed(pat_type) => match &*pat_type.pat {
                        Pat::Ident(PatIdent {
                            attrs: _,
                            by_ref: None,
                            mutability: None,
                            ident,
                            subpat: None,
                        }) => {
                            names.push(ident.clone());
                        }
                        _ => {
                            return Err(Error::new(
                                fn_arg.kind.span(),
                                "declate_struct_with_invariants: expected identifier here",
                            ));
                        }
                    },
                    _ => {
                        return Err(Error::new(
                            fn_arg.kind.span(),
                            "struct_with_invariants: this kind of argument not expected here",
                        ));
                    }
                }
            }

            let mut map: HashMap<String, usize> = HashMap::new();
            for (i, name) in names.iter().enumerate() {
                let s = name.to_string();
                if map.contains_key(&s) {
                    let mut er1 = Error::new(
                        names[map[&s]].span(),
                        "struct_with_invariants: duplicate identifier used in parameters to invariant",
                    );
                    let er2 = Error::new(
                        name.span(),
                        "struct_with_invariants: duplicate identifier used in parameters to invariant",
                    );
                    er1.combine(er2);
                    return Err(er1);
                }
                map.insert(s, i);
            }
        }
    }

    Ok(())
}

fn get_partial_type_fields(fields: &Vec<Field>) -> parse::Result<Vec<PartialField>> {
    let mut res = vec![];
    for field in fields {
        let partial_types = get_partial_types(&field.ty)?;
        let name = field.ident.clone().unwrap();
        res.push(PartialField {
            name,
            partial_types,
        });
    }
    Ok(res)
}

fn check_invs_match_partial_types(
    all_fields: &Vec<Field>,
    partial_fields: &Vec<PartialField>,
    invariant_decls: &Vec<InvariantDecl>,
) -> parse::Result<()> {
    let mut indices: HashMap<String, usize> = HashMap::new();

    for field in all_fields.iter() {
        let name = field.ident.as_ref().unwrap().to_string();
        indices.insert(name, 0);
    }

    for invariant_decl in invariant_decls.iter() {
        if let InvariantDecl::Invariant {
            field_name,
            depends_on,
            params,
            params_span,
            predicate: _,
            condition: _,
            quants: _,
            specifically: _,
        } = invariant_decl
        {
            let name = field_name.to_string();

            let pf = get_partial_field_by_name(partial_fields, &name);
            let pf = match pf {
                Some(pf) => pf,
                None => {
                    return Err(Error::new(
                        field_name.span(),
                        "struct_with_invariants: no field declared of this name",
                    ));
                }
            };

            for dep in depends_on.iter() {
                check_dep_in_fields(all_fields, dep)?;
            }

            let idx = indices[&name];
            if idx < pf.partial_types.len() {
                *indices.get_mut(&name).unwrap() = idx + 1;
                check_invdecl_params_match(params_span, params, &pf.partial_types[idx])?;
            } else {
                if pf.partial_types.len() == 0 {
                    return Err(Error::new(
                        field_name.span(),
                        "struct_with_invariants: the type for this field needs to be declared with wildcard placeholders in order to have an invariant",
                    ));
                } else {
                    return Err(Error::new(
                        field_name.span(),
                        "struct_with_invariants: too many invariants declared for this field",
                    ));
                }
            }
        }
    }

    for pf in partial_fields.iter() {
        let idx = indices[&pf.name.to_string()];
        if idx != pf.partial_types.len() {
            if idx == 0 {
                return Err(Error::new(
                    pf.name.span(),
                    "struct_with_invariants: no invariant declared for this field",
                ));
            } else {
                return Err(Error::new(
                    pf.name.span(),
                    "struct_with_invariants: not enough invariants declared for this field",
                ));
            }
        }
    }

    Ok(())
}

fn check_invdecl_params_match(
    params_span: &Span,
    params: &Vec<FnArg>,
    partial_type: &PartialType,
) -> parse::Result<()> {
    for (fn_arg, concrete_arg) in params.iter().zip(partial_type.concrete_args.iter()) {
        match &fn_arg.kind {
            FnArgKind::Typed(pat_type) => {
                let ty1 = &pat_type.ty;
                let ty2 = concrete_arg;
                if ty1.to_token_stream().to_string() != ty2.to_token_stream().to_string() {
                    return Err(Error::new(
                        ty1.span(),
                        format!(
                            "struct_with_invariants: this type is expected to be {:}",
                            ty2.to_token_stream().to_string()
                        ),
                    ));
                }
            }
            _ => {
                return Err(Error::new(
                    fn_arg.kind.span(),
                    "struct_with_invariants: this kind of argument not expected here",
                ));
            }
        }
    }

    if params.len() != partial_type.concrete_args.len() {
        return Err(Error::new(
            params_span.clone(),
            format!(
                "struct_with_invariants: expected {:} params here",
                partial_type.concrete_args.len(),
            ),
        ));
    }

    Ok(())
}

fn check_dep_in_fields(fields: &Vec<Field>, dep: &Ident) -> parse::Result<()> {
    if !fields_contains(fields, &dep.to_string()) {
        Err(Error::new(
            dep.span(),
            "struct_with_invariants: field not found as a member of the declared struct",
        ))
    } else {
        Ok(())
    }
}

fn check_deps_acyclic(
    fields: &Vec<Field>,
    invs: &Vec<InvariantDecl>,
) -> parse::Result<Vec<String>> {
    let mut ts = TopologicalSort::new();
    for f in fields.iter() {
        ts.add_node(f.ident.as_ref().unwrap().to_string());
    }
    for inv in invs.iter() {
        match inv {
            InvariantDecl::Invariant {
                field_name,
                depends_on,
                ..
            } => {
                let f = field_name.to_string();
                for dep in depends_on.iter() {
                    ts.add_edge(&f, &dep.to_string());
                }
            }
            InvariantDecl::NormalExpr(..) => {}
        }
    }

    match ts.compute_topological_sort() {
        None => Err(Error::new(
            Span::call_site(),
            "struct_with_invariants: dependencies between fields should be acyclic",
        )),
        Some(ordering) => Ok(ordering),
    }
}

// Manipulation and output

fn field_used_type_params(
    sdi: &SDI,
    fields: &Vec<Field>,
    ordering: &Vec<String>,
) -> HashMap<String, Vec<GenericParam>> {
    let mut hs = HashMap::<String, HashSet<String>>::new();
    let mut res = HashMap::<String, Vec<GenericParam>>::new();
    for field_name in ordering.iter().rev() {
        let field = get_field_by_name(fields, &field_name).unwrap();
        let mut used_params: HashSet<String> =
            get_params_used_in_type(&sdi.item_struct.generics.params, &field.ty);

        let invariant_decls = get_invariant_decls_by_name(&sdi.invariant_decls, &field_name);
        for invariant_decl in invariant_decls.iter() {
            if let InvariantDecl::Invariant { depends_on, .. } = invariant_decl {
                for dep in depends_on {
                    let k = hs
                        .get(&dep.to_string())
                        .expect("should exist because of top. sort");
                    for j in k.iter() {
                        used_params.insert(j.clone());
                    }
                }
            }
        }

        let p = sdi
            .item_struct
            .generics
            .params
            .clone()
            .into_iter()
            .filter(|generic_param| used_params.contains(&generic_param_to_string(generic_param)))
            .collect();

        hs.insert(field_name.clone(), used_params);
        res.insert(field_name.clone(), p);
    }

    res
}

fn fill_in_item_struct(
    main_name: &str,
    item_struct: &mut ItemStruct,
    invariant_decls: &Vec<InvariantDecl>,
    used_type_params: &HashMap<String, Vec<GenericParam>>,
) {
    match &mut item_struct.fields {
        Fields::Named(fields_named) => {
            for field in fields_named.named.iter_mut() {
                let name = field.ident.as_ref().unwrap().to_string();
                let invdecls = get_invariant_decls_by_name(invariant_decls, &name);
                field.ty = fill_in_type(&field.ty, main_name, invdecls, used_type_params);
            }
        }
        _ => {
            panic!("fill_in_item_struct expected Fields::Named");
        }
    }
}

fn output_invariant(
    main_name: &str,
    sdi: &SDI,
    stream: &mut TokenStream,
    wf_stream: &mut TokenStream,
    partial_fields: &Vec<PartialField>,
    invariant_decl: &InvariantDecl,
    used_type_params: &HashMap<String, Vec<GenericParam>>,
) {
    let mut indices: HashMap<String, usize> = HashMap::new();

    for field in partial_fields.iter() {
        let name = field.name.to_string();
        indices.insert(name, 0);
    }

    match invariant_decl {
        InvariantDecl::Invariant {
            field_name,
            depends_on,
            quants,
            condition,
            specifically,
            params,
            predicate,
            params_span: _,
        } => {
            let pf = get_partial_field_by_name(partial_fields, &field_name.to_string());
            let pf = pf.unwrap();

            let field_name_string = field_name.to_string();
            let idx = indices[&field_name_string];
            *indices.get_mut(&field_name_string).unwrap() = idx + 1;

            let partial_type = &pf.partial_types[idx];

            let predname = get_pred_typename(main_name, field_name);

            let k_type = get_constant_type(main_name, depends_on, quants, used_type_params);
            let tmp_k = Ident::new("declare_struct_with_invariants_tmp_k", Span::call_site());
            let tmp_v = Ident::new("declare_struct_with_invariants_tmp_v", Span::call_site());
            let tmp_g = Ident::new("declare_struct_with_invariants_tmp_g", Span::call_site());

            let quant_idents = quants.iter().map(|pat_type| match &*pat_type.pat {
                Pat::Ident(pat_ident) => &pat_ident.ident,
                _ => panic!("expect Pat::Ident"),
            });
            let all_idents: Vec<&Ident> = depends_on.iter().chain(quant_idents).collect();
            let k_pat = maybe_tuple(&all_idents);

            let v_pats: Vec<&Pat> = params
                .iter()
                .map(|fn_arg| match &fn_arg.kind {
                    FnArgKind::Typed(p) => &*p.pat,
                    _ => {
                        panic!("should have been ruled out by check_invdecl_params_match");
                    }
                })
                .collect();

            let type_params = &sdi.item_struct.generics.params;
            let where_clause = &sdi.item_struct.generics.where_clause;
            let vis = &sdi.item_struct.vis;

            let span = field_name.span();

            let mut e_stream_conjuncts = vec![];

            let specifically_expr = if let Some(specifically) = specifically {
                quote_spanned! { specifically.span() => (#specifically) }
            } else {
                quote_spanned! { field_name.span() => self.#field_name }
            };

            if partial_type.is_atomic_ghost {
                let v_type = &partial_type.concrete_args[0];
                let g_type = &partial_type.concrete_args[1];
                let v_pat = &v_pats[0];
                let g_pat = &v_pats[1];

                // TODO make it possible to configure open-ness?

                stream.extend(quote_spanned! { predicate.span() =>
                    #vis struct #predname { }
                    impl<#type_params> vstd::atomic_ghost::AtomicInvariantPredicate<#k_type, #v_type, #g_type> for #predname #where_clause {
                        open spec fn atomic_inv(#tmp_k: #k_type, #tmp_v: #v_type, #tmp_g: #g_type) -> bool {
                            let #k_pat = #tmp_k;
                            let #v_pat = #tmp_v;
                            let #g_pat = #tmp_g;
                            #predicate
                        }
                    }
                });

                e_stream_conjuncts.push(quote_spanned! { predicate.span() =>
                    #specifically_expr.well_formed()
                })
            } else {
                let v_type = maybe_tuple(&partial_type.concrete_args);
                let v_pat = maybe_tuple(&v_pats);
                stream.extend(quote_spanned! { predicate.span() =>
                    #vis struct #predname { }
                    impl<#type_params> vstd::invariant::InvariantPredicate<#k_type, #v_type> for #predname #where_clause {
                        open spec fn inv(#tmp_k: #k_type, #tmp_v: #v_type) -> bool {
                            let #k_pat = #tmp_k;
                            let #v_pat = #tmp_v;
                            #predicate
                        }
                    }
                });
            }

            for (i, dep) in depends_on.iter().enumerate() {
                let field_access = if depends_on.len() + quants.len() == 1 {
                    TokenStream::new()
                } else {
                    let j = Member::Unnamed(Index {
                        index: i as u32,
                        span: dep.span(),
                    });
                    quote_spanned! { dep.span() => .#j }
                };
                e_stream_conjuncts.push(quote_spanned! { dep.span() =>
                    ::builtin::equal(#specifically_expr.constant()#field_access, self.#dep)
                });
            }
            for (i, quant) in quants.iter().enumerate() {
                let field_access = if depends_on.len() + quants.len() == 1 {
                    TokenStream::new()
                } else {
                    let i = i + depends_on.len();
                    let j = Member::Unnamed(Index {
                        index: i as u32,
                        span: quant.span(),
                    });
                    quote_spanned! { quant.span() => .#j }
                };
                let quant_pat = &quant.pat;
                e_stream_conjuncts.push(quote_spanned! { quant.span() =>
                    ::builtin::equal(#specifically_expr.constant()#field_access, #quant_pat)
                });
            }

            if e_stream_conjuncts.len() > 0 {
                let mut e_stream = e_stream_conjuncts[0].clone();
                for e in e_stream_conjuncts.iter().skip(1) {
                    e_stream.extend(quote_spanned! { e.span() => && #e });
                }

                if let Some(cond) = condition {
                    e_stream = quote_spanned! { span =>
                        ::builtin::imply(
                            #cond,
                            #e_stream
                        )
                    }
                }
                if quants.len() > 0 {
                    e_stream = quote_spanned! { span =>
                        ::builtin::forall(|#(#quants),*|
                            ::builtin::with_triggers(
                                ((#specifically_expr,),),
                                #e_stream
                            )
                        )
                    };
                }

                wf_stream.extend(quote_spanned! { span =>
                    && #e_stream
                });
            }
        }
        InvariantDecl::NormalExpr(e) => {
            wf_stream.extend(quote_spanned! { e.span() =>
                && #e
            });
        }
    }
}

fn output_wf(sdi: &SDI, stream: &mut TokenStream, wf_body_stream: TokenStream) {
    let wf_sig = &sdi.wf_sig;
    let wf_vis = &sdi.wf_vis;
    let type_params = &sdi.item_struct.generics.params;
    let where_clause = &sdi.item_struct.generics.where_clause;
    let self_type = get_self_type(&sdi.item_struct);
    stream.extend(quote! {
        impl <#type_params> #self_type #where_clause {
            // Something like `pub open spec fn well_formed(&self) -> bool`
            #wf_vis #wf_sig {
                #wf_body_stream
            }
        }
    });
}

fn output_field_type_alias(
    main_name: &str,
    vis: &Visibility,
    stream: &mut TokenStream,
    field: &Field,
    used_type_params: &HashMap<String, Vec<GenericParam>>,
) {
    let field_ident = field.ident.as_ref().unwrap();
    let alias = get_type_alias(main_name, field_ident, used_type_params);
    let field_ty = &field.ty;

    stream.extend(quote! {
        #vis type #alias = #field_ty;
    });
}

// Defs

fn get_pred_typename(main_name: &str, field_name: &Ident) -> Ident {
    Ident::new(
        &format!(
            "InvariantPredicate_auto_{:}_{:}",
            main_name,
            field_name.to_string()
        ),
        Span::call_site(),
    )
}

fn get_type_alias(
    main_name: &str,
    field_ident: &Ident,
    used_type_params: &HashMap<String, Vec<GenericParam>>,
) -> TokenStream {
    let ident = Ident::new(
        &format!("FieldType_{:}_{:}", main_name, field_ident.to_string()),
        Span::call_site(),
    );
    let utp = used_type_params.get(&field_ident.to_string()).unwrap();
    if utp.len() == 0 {
        quote! { #ident }
    } else {
        let p = remove_bounds_vec(utp);
        quote! { #ident<#p> }
    }
}

fn get_constant_type(
    main_name: &str,
    depends_on: &Vec<Ident>,
    quants: &Vec<PatType>,
    used_type_params: &HashMap<String, Vec<GenericParam>>,
) -> Type {
    let t1 = depends_on
        .iter()
        .map(|dep| get_type_alias(main_name, dep, used_type_params));
    let t2 = quants.iter().map(|pat_type| {
        let ty = &pat_type.ty;
        quote! { #ty }
    });
    Type::Verbatim(maybe_tuple(&t1.chain(t2).collect()))
}

// Analyzing types, modifying them

fn get_partial_types(ty: &Type) -> parse::Result<Vec<PartialType>> {
    let mut fptv = FindPartialTypeVisitor {
        ptypes: vec![],
        errors: vec![],
    };
    fptv.visit_type(ty);
    let FindPartialTypeVisitor { errors, ptypes } = fptv;

    combine_errors_or_ok(errors)?;

    let c = count_infers(ty);
    let expected_c = 2 * ptypes.len();
    if c != expected_c {
        return Err(Error::new(
            ty.span(),
            "struct_with_invariants: cannot handle this type (expected 2 placeholder arguments)",
        ));
    }

    Ok(ptypes)
}

struct FindPartialTypeVisitor {
    ptypes: Vec<PartialType>,
    errors: Vec<Error>,
}

impl<'ast> Visit<'ast> for FindPartialTypeVisitor {
    fn visit_type_path(&mut self, type_path: &'ast TypePath) {
        let cargs = get_concrete_args(type_path);
        match cargs {
            Err(e) => {
                self.errors.push(e);
                return;
            }
            Ok(None) => {
                visit::visit_type_path(self, type_path);
            }
            Ok(Some(ptype)) => {
                self.ptypes.push(ptype);
            }
        };
    }

    fn visit_type_infer(&mut self, infer: &TypeInfer) {
        self.errors.push(Error::new(
            infer.span(),
            "struct_with_invariants: cannot handle placeholders here",
        ));
    }
}

fn get_concrete_args(type_path: &TypePath) -> parse::Result<Option<PartialType>> {
    match &type_path.path.segments.last().unwrap().arguments {
        PathArguments::AngleBracketed(abga) => {
            let mut infer_count = 0;
            let mut concrete_args: Vec<Type> = Vec::new();

            let last_ident = &type_path.path.segments.last().unwrap().ident;
            let is_atomic_ghost = match get_builtin_concrete_arg(&last_ident.to_string()) {
                Some(a) => {
                    concrete_args.push(a);
                    true
                }
                None => false,
            };

            for arg in abga.args.iter() {
                if let GenericArgument::Type(arg_type) = arg {
                    if let Type::Infer(_) = arg_type {
                        infer_count += 1;
                    } else {
                        concrete_args.push(arg_type.clone());
                    }
                }
            }

            if infer_count == 0 {
                Ok(None)
            } else if infer_count != 2 {
                Err(Error::new(
                    type_path.span(),
                    "struct_with_invariants: cannot handle this type",
                ))
            } else if is_atomic_ghost && concrete_args.len() != 2 {
                Err(Error::new(
                    type_path.span(),
                    "struct_with_invariants: cannot handle this type",
                ))
            } else {
                Ok(Some(PartialType {
                    concrete_args,
                    is_atomic_ghost,
                }))
            }
        }
        _ => Ok(None),
    }
}

fn get_builtin_concrete_arg(name: &str) -> Option<Type> {
    match name {
        "AtomicBool" => Some(Type::Verbatim(quote! { bool })),
        "AtomicU64" => Some(Type::Verbatim(quote! { u64 })),
        "AtomicU32" => Some(Type::Verbatim(quote! { u32 })),
        "AtomicU16" => Some(Type::Verbatim(quote! { u16 })),
        "AtomicU8" => Some(Type::Verbatim(quote! { u8 })),
        "AtomicUsize" => Some(Type::Verbatim(quote! { usize })),
        "AtomicI64" => Some(Type::Verbatim(quote! { i64 })),
        "AtomicI32" => Some(Type::Verbatim(quote! { i32 })),
        "AtomicI16" => Some(Type::Verbatim(quote! { i16 })),
        "AtomicI8" => Some(Type::Verbatim(quote! { i8 })),
        "AtomicIsize" => Some(Type::Verbatim(quote! { isize })),
        _ => None,
    }
}

fn fill_in_type(
    ty: &Type,
    main_name: &str,
    inv_decls: Vec<&InvariantDecl>,
    used_type_params: &HashMap<String, Vec<GenericParam>>,
) -> Type {
    let mut typs = vec![];

    for inv_decl in inv_decls {
        let (field_name, depends_on, quants) = match inv_decl {
            InvariantDecl::Invariant {
                field_name,
                depends_on,
                quants,
                ..
            } => (field_name, depends_on, quants),
            _ => {
                panic!("fill_in_type expected InvariantDecl::Invariant");
            }
        };
        let pred = get_pred_typename(main_name, field_name);
        typs.push(get_constant_type(
            main_name,
            depends_on,
            quants,
            used_type_params,
        ));
        typs.push(Type::Verbatim(quote! { #pred }));
    }

    fill_in_infers(ty, typs)
}

// Misc

fn get_partial_field_by_name<'a>(
    partial_fields: &'a Vec<PartialField>,
    name: &str,
) -> Option<&'a PartialField> {
    for pf in partial_fields.iter() {
        if pf.name.to_string() == name {
            return Some(pf);
        }
    }
    None
}

fn get_field_by_name<'a>(fields: &'a Vec<Field>, name: &str) -> Option<&'a Field> {
    for f in fields.iter() {
        if f.ident.as_ref().unwrap().to_string() == name {
            return Some(f);
        }
    }
    None
}

fn get_invariant_decls_by_name<'a>(
    invariant_decls: &'a Vec<InvariantDecl>,
    name: &str,
) -> Vec<&'a InvariantDecl> {
    let mut res = Vec::new();
    for invdecl in invariant_decls.iter() {
        if let InvariantDecl::Invariant { field_name, .. } = invdecl {
            if field_name.to_string() == name {
                res.push(invdecl);
            }
        }
    }
    res
}

fn maybe_tuple<T>(v: &Vec<T>) -> TokenStream
where
    T: ToTokens,
{
    if v.len() == 1 {
        let x = &v[0];
        quote! { #x }
    } else {
        quote! { ( #(#v),* ) }
    }
}

fn count_infers(ty: &Type) -> usize {
    let mut iv = InferVisitor { count: 0 };
    iv.visit_type(ty);
    iv.count
}

struct InferVisitor {
    count: usize,
}

impl<'ast> Visit<'ast> for InferVisitor {
    fn visit_type_infer(&mut self, _i: &'ast TypeInfer) {
        self.count += 1;
    }
}

fn fill_in_infers(ty: &Type, v: Vec<Type>) -> Type {
    let mut fiv = FillInferVisitor { v: v.into_iter() };
    let mut ty = ty.clone();
    fiv.visit_type_mut(&mut ty);
    assert!(fiv.v.next().is_none());
    ty
}

struct FillInferVisitor {
    v: std::vec::IntoIter<Type>,
}

impl VisitMut for FillInferVisitor {
    fn visit_type_mut(&mut self, ty: &mut Type) {
        let is_infer = match ty {
            Type::Infer(_) => true,
            _ => false,
        };
        if is_infer {
            *ty = self.v.next().unwrap();
        } else {
            visit_mut::visit_type_mut(self, ty);
        }
    }
}

fn fields_contains(fields: &Vec<Field>, name: &str) -> bool {
    for field in fields.iter() {
        if field.ident.as_ref().unwrap().to_string() == name {
            return true;
        }
    }
    false
}

fn get_self_type(item_struct: &ItemStruct) -> Type {
    let ident = &item_struct.ident;
    if item_struct.generics.params.len() > 0 {
        let p = remove_bounds(&item_struct.generics.params);
        Type::Verbatim(quote! {
            #ident<#p>
        })
    } else {
        Type::Verbatim(quote! {
            #ident
        })
    }
}

fn remove_bounds_vec(p: &Vec<GenericParam>) -> TokenStream {
    let mut toks = TokenStream::new();
    for (i, generic_param) in p.iter().enumerate() {
        match generic_param {
            GenericParam::Type(tp) => {
                tp.ident.to_tokens(&mut toks);
            }
            GenericParam::Lifetime(ld) => {
                ld.lifetime.to_tokens(&mut toks);
            }
            GenericParam::Const(c) => {
                c.ident.to_tokens(&mut toks);
            }
        }

        if i + 1 < p.len() {
            toks.extend(quote! { , });
        }
    }
    toks
}

fn remove_bounds(p: &Punctuated<GenericParam, Comma>) -> TokenStream {
    let mut toks = TokenStream::new();
    for (i, generic_param) in p.iter().enumerate() {
        match generic_param {
            GenericParam::Type(tp) => {
                tp.ident.to_tokens(&mut toks);
            }
            GenericParam::Lifetime(ld) => {
                ld.lifetime.to_tokens(&mut toks);
            }
            GenericParam::Const(c) => {
                c.ident.to_tokens(&mut toks);
            }
        }

        if i + 1 < p.len() {
            toks.extend(quote! { , });
        }
    }
    toks
}

fn is_first_param_self(sig: &Signature) -> bool {
    if sig.inputs.len() == 0 {
        return false;
    }
    match &sig.inputs[0].kind {
        FnArgKind::Receiver(Receiver {
            attrs: _,
            reference: _,
            mutability: None,
            self_token: _,
        }) => true,
        _ => false,
    }
}

fn is_spec(sig: &Signature) -> bool {
    match &sig.mode {
        FnMode::Spec(_) | FnMode::SpecChecked(_) => true,
        FnMode::Proof(_) | FnMode::Exec(_) | FnMode::Default => false,
    }
}

fn generic_param_to_string(gp: &GenericParam) -> String {
    match gp {
        GenericParam::Type(tp) => tp.ident.to_string(),
        GenericParam::Const(cp) => cp.ident.to_string(),
        GenericParam::Lifetime(l) => "'".to_string() + &l.lifetime.ident.to_string(),
    }
}

fn get_params_used_in_type(params: &Punctuated<GenericParam, Comma>, ty: &Type) -> HashSet<String> {
    let mut hs = HashSet::new();
    for p in params.iter() {
        hs.insert(generic_param_to_string(p));
    }
    let mut upv = UsedParamsVisitor {
        params: hs,
        result: HashSet::new(),
    };
    upv.visit_type(ty);
    upv.result
}

struct UsedParamsVisitor {
    params: HashSet<String>,
    result: HashSet<String>,
}

impl<'ast> Visit<'ast> for UsedParamsVisitor {
    fn visit_type_path(&mut self, type_path: &TypePath) {
        let TypePath { qself, path } = type_path;
        if qself.is_none()
            && path.leading_colon.is_none()
            && path.segments.len() == 1
            && path.segments[0].arguments == PathArguments::None
        {
            let id = path.segments[0].ident.to_string();
            if self.params.contains(&id) {
                self.result.insert(id);
            }
        }

        visit::visit_type_path(self, type_path);
    }

    fn visit_lifetime(&mut self, lt: &Lifetime) {
        let id = "'".to_string() + &lt.ident.to_string();
        self.result.insert(id);
    }
}

pub fn combine_errors_or_ok(errors: Vec<Error>) -> parse::Result<()> {
    let mut res = Ok(());
    for e in errors {
        match res {
            Ok(()) => {
                res = Err(e);
            }
            Err(e0) => {
                let mut e0 = e0;
                e0.combine(e);
                res = Err(e0);
            }
        }
    }
    res
}

================
File: ./source/verismo_verus/src/rustdoc.rs
================

// Here, we modify each function item by appending rustdoc
// containing information about the Verus signature that we want to appear
// in auto-generated rustdoc. For example, we add information about
// 'requires' and 'ensures' clauses, and we also add 'mode' information.
// This information would be absent if we tried to run rustdoc without any
// processing.
//
// The flow is:
//  - the verus! macro (this file) adds extra information into a rustdoc comment
//  - rustdoc (unmodified) generates the rustdoc HTML
//  - we run a postprocessor (verusdoc crate) to present the information in a nice way.
//
// Specifically, we add "attributes" in a kinda-silly format that the post-processing
// step can recognize.  The format is:
//
//     ```rust
//     // verusdoc_special_attr $ATTR_NAME
//     $ATTR_VALUE
//     ```
//
// The ATTR_NAME can be requires, ensures, recommends or modes.
//
// The reason we use a codeblock here is that so rustdoc will perform syntax highlighting
// on the value which is applicable if it's an expression. For example, if it's a
// requires, ensures, or recommends attribute then ATTR_VALUE will be a (pretty-printed)
// boolean expression.
//
// The other type, 'modes', is a bit more complicated: the value is a JSON blob with
// some data explaining the function mode, param modes, and return mode.

use std::iter::FromIterator;

use proc_macro2::{Span, TokenTree};
use syn_verus::punctuated::Punctuated;
use syn_verus::spanned::Spanned;
use syn_verus::{
    token, AttrStyle, Attribute, Block, Expr, ExprBlock, FnMode, Ident, ImplItemMethod, ItemFn,
    Meta, MetaList, NestedMeta, Path, PathArguments, PathSegment, Publish, ReturnType, Signature,
    TraitItemMethod,
};

/// Check if VERUSDOC=1.

pub fn env_rustdoc() -> bool {
    match proc_macro::tracked_env::var("VERUSDOC") {
        Err(_) => false, // VERUSDOC key not present in environment
        Ok(s) => s == "1",
    }
}

// Main hooks for the verus! macro to manipulate ItemFn, etc.

pub fn process_item_fn(item: &mut ItemFn) {
    match attr_for_sig(&mut item.attrs, &item.sig, Some(&item.block)) {
        Some(attr) => item.attrs.insert(0, attr),
        None => {}
    }
}

pub fn process_impl_item_method(item: &mut ImplItemMethod) {
    match attr_for_sig(&mut item.attrs, &item.sig, Some(&item.block)) {
        Some(attr) => item.attrs.insert(0, attr),
        None => {}
    }
}

pub fn process_trait_item_method(item: &mut TraitItemMethod) {
    match attr_for_sig(&mut item.attrs, &item.sig, item.default.as_ref()) {
        Some(attr) => item.attrs.insert(0, attr),
        None => {}
    }
}

/// Process a signature to get all the information, apply the codeblock
/// formatting tricks, and then package it all up into a #[doc = "..."] attribute
/// (as a syn_verus::Attribute object) that we can apply to the item.

fn attr_for_sig(
    attrs: &mut Vec<Attribute>,
    sig: &Signature,
    block: Option<&Block>,
) -> Option<Attribute> {
    let mut v = vec![];

    v.push(encoded_mode_info(sig));

    match &sig.requires {
        Some(es) => {
            for expr in es.exprs.exprs.iter() {
                v.push(encoded_expr("requires", expr));
            }
        }
        None => {}
    }
    match &sig.recommends {
        Some(es) => {
            for expr in es.exprs.exprs.iter() {
                v.push(encoded_expr("recommends", expr));
            }
        }
        None => {}
    }
    match &sig.ensures {
        Some(es) => {
            for expr in es.exprs.exprs.iter() {
                v.push(encoded_expr("ensures", expr));
            }
        }
        None => {}
    }

    match block {
        Some(block) => {
            if is_spec(&sig) {
                if show_body(attrs) {
                    let b = Expr::Block(ExprBlock {
                        attrs: vec![],
                        label: None,
                        block: block.clone(),
                    });
                    v.push(encoded_body("body", &b));
                }
            }
        }
        None => {}
    }

    if v.len() == 0 {
        None
    } else {
        Some(doc_attr_from_string(&v.join("\n\n"), sig.span()))
    }
}

fn is_spec(sig: &Signature) -> bool {
    match &sig.mode {
        FnMode::Spec(_) | FnMode::SpecChecked(_) => true,
        FnMode::Proof(_) | FnMode::Exec(_) | FnMode::Default => false,
    }
}

/// Check for:
///  #[doc(verus_show_body)]

fn show_body(attrs: &mut Vec<Attribute>) -> bool {
    for (i, attr) in attrs.iter().enumerate() {
        match attr.parse_meta() {
            Ok(Meta::List(MetaList { path, nested, .. })) => {
                if nested.len() == 1 && path.is_ident("doc") {
                    match nested.iter().next().unwrap() {
                        NestedMeta::Meta(Meta::Path(p)) => {
                            if p.is_ident("verus_show_body") {
                                // Remove the attribute; otherwise when rustdoc runs
                                // it will give an error about the unrecognized attribute.
                                attrs.remove(i);

                                return true;
                            }
                        }
                        _ => {}
                    }
                }
            }
            _ => {}
        }
    }
    false
}

fn fn_mode_to_string(mode: &FnMode, publish: &Publish) -> String {
    match mode {
        FnMode::Spec(_) | FnMode::SpecChecked(_) => match publish {
            Publish::Closed(_) => "closed spec".to_string(),
            Publish::Open(_) => "open spec".to_string(),
            Publish::OpenRestricted(res) => {
                "open(".to_string() + &module_path_to_string(&res.path) + ") spec"
            }
            Publish::Default => "spec".to_string(),
        },
        FnMode::Proof(_) => "proof".to_string(),
        FnMode::Exec(_) => "exec".to_string(),
        FnMode::Default => "exec".to_string(),
    }
}

fn module_path_to_string(p: &Path) -> String {
    // path is for a module; we can ignore type arguments

    let lead = if p.leading_colon.is_some() { "::" } else { "" };
    let main = p
        .segments
        .iter()
        .map(|path_seg| path_seg.ident.to_string())
        .collect::<Vec<String>>()
        .join("::");
    lead.to_string() + &main
}

fn encoded_mode_info(sig: &Signature) -> String {
    let fn_mode = fn_mode_to_string(&sig.mode, &sig.publish);
    let ret_mode = match &sig.output {
        ReturnType::Default => "Default",
        ReturnType::Type(_, tracked_token, _, _) => {
            if tracked_token.is_some() {
                "Tracked"
            } else {
                "Default"
            }
        }
    };

    let param_modes = sig
        .inputs
        .iter()
        .map(|fn_arg| {
            if fn_arg.tracked.is_some() {
                "\"Tracked\""
            } else {
                "\"Default\""
            }
        })
        .collect::<Vec<&str>>();
    let param_modes = param_modes.join(",");

    // JSON blob is parsed by the verusdoc post-processor into a `DocModeInfo` object.
    // I decided not to pull in serde as a dependency for builtin_macros,
    // but if serialization gets too complicated, we should probably do that instead.

    // We put it in a comment to avoid extra syntax highlighting or anything that would
    // complicate the post-processing.

    let info = format!(
        r#"// {{ "fn_mode": "{fn_mode:}", "ret_mode": "{ret_mode:}", "param_modes": [{param_modes:}] }}"#
    );

    encoded_str("modes", &info)
}

/// Pretty print the expression, then wrap in a code block.

fn encoded_expr(kind: &str, code: &Expr) -> String {
    let s = prettyplease_verus::unparse_expr(&code);
    let s = format!("{s:},");
    encoded_str(kind, &s)
}

fn encoded_body(kind: &str, code: &Expr) -> String {
    let s = prettyplease_verus::unparse_expr(&code);
    let s = format!("{s:}");
    encoded_str(kind, &s)
}

/// Wrap the given string into a code block,
/// into the format that the postprocessor will recognize.

fn encoded_str(kind: &str, data: &str) -> String {
    "```rust\n// verusdoc_special_attr ".to_string() + kind + "\n" + data + "\n```"
}

/// Create an attr that looks like #[doc = "doc_str"]

fn doc_attr_from_string(doc_str: &str, span: Span) -> Attribute {
    Attribute {
        pound_token: token::Pound { spans: [span] },
        style: AttrStyle::Outer,
        bracket_token: token::Bracket { span },
        path: Path {
            leading_colon: None,
            segments: Punctuated::from_iter(vec![PathSegment {
                ident: Ident::new("doc", span),
                arguments: PathArguments::None,
            }]),
        },
        tokens: proc_macro2::TokenStream::from_iter(vec![
            TokenTree::Punct(proc_macro2::Punct::new('=', proc_macro2::Spacing::Alone)),
            TokenTree::Literal(proc_macro2::Literal::string(doc_str)),
        ]),
    }
}

================
File: ./source/verismo_verus/src/lib.rs
================

#![feature(box_patterns)]
#![feature(proc_macro_span)]
#![feature(proc_macro_tracked_env)]
#![feature(proc_macro_quote)]
#![feature(proc_macro_expand)]
#![allow(unused)]

//mod atomic_ghost;
//mod fndecl;
//mod is_variant;
mod rustdoc;
//mod struct_decl_inv;
mod structural;
mod syntax;
mod topological_sort;

#[proc_macro]
pub fn verismo(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let ret = syntax::rewrite_items(input, cfg_erase(), true, false, true);
    //println!("ret = {}", ret);
    ret
}

#[proc_macro]
pub fn verismo_non_secret(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let ret = syntax::rewrite_items(input, cfg_erase(), true, true, true);
    //println!("ret = {}", ret);
    ret
}

#[proc_macro]
pub fn verismo_simple(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let ret = syntax::rewrite_items(input, cfg_erase(), true, true, false);
    //println!("ret = {}", ret);
    ret
}

#[proc_macro]
pub fn verismo_proof_expr(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    syntax::rewrite_expr(false, true, input, false)
}

#[proc_macro]
pub fn verismo_exec_expr_keep_ghost(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    syntax::rewrite_expr(false, false, input, false)
}

#[proc_macro]
pub fn verismo_exec_expr_erase_ghost(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    syntax::rewrite_expr(true, false, input, false)
}

#[proc_macro]
pub fn verismo_exec_expr(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    syntax::rewrite_expr(cfg_erase(), false, input, false)
}

pub(crate) fn cfg_erase() -> bool {
    let ts: proc_macro::TokenStream =
        quote::quote! { ::core::cfg!(verus_macro_erase_ghost) }.into();
    let bool_ts = ts.expand_expr();
    let bool_ts = match bool_ts {
        Ok(name) => name,
        Err(_) => {
            panic!("cfg_erase call failed");
        }
    };
    let bool_ts = bool_ts.to_string();
    assert!(bool_ts == "true" || bool_ts == "false");
    bool_ts == "true"
}

/// verismo_proof_macro_exprs!(f!(exprs)) applies verismo syntax to transform exprs into exprs',
/// then returns f!(exprs'),
/// where exprs is a sequence of expressions separated by ",", ";", and/or "=>".
#[proc_macro]
pub fn verismo_proof_macro_exprs(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    syntax::proof_macro_exprs(false, true, input, false)
}

#[proc_macro]
pub fn verismo_exec_macro_exprs(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    syntax::proof_macro_exprs(cfg_erase(), false, input, false)
}

#[proc_macro]
pub fn verismo_inv_macro_exprs(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    // Reads the first expression as proof; the second as exec
    syntax::inv_macro_exprs(cfg_erase(), input, false)
}

/// `verismo_proof_macro_explicit_exprs!(f!(tts))` applies verismo syntax to transform `tts` into
/// `tts'`, then returns `f!(tts')`, only applying the transform to any of the exprs within it that
/// are explicitly prefixed with `@@`, leaving the rest as-is. Contrast this to
/// [`verismo_proof_macro_exprs`] which is likely what you want to try first to see if it satisfies
/// your needs.
#[proc_macro]
pub fn verismo_proof_macro_explicit_exprs(
    input: proc_macro::TokenStream,
) -> proc_macro::TokenStream {
    syntax::proof_macro_explicit_exprs(false, true, input, false)
}

================
File: ./source/verismo_verus/src/topological_sort.rs
================

use std::collections::HashMap;

pub struct TopologicalSort<T> {
    values: Vec<T>,
    nodes: HashMap<T, usize>,
    edges_rev: Vec<Vec<usize>>,
}

impl<T> TopologicalSort<T>
where
    T: std::cmp::Eq + std::hash::Hash + Clone,
{
    pub fn new() -> Self {
        TopologicalSort {
            values: vec![],
            nodes: HashMap::new(),
            edges_rev: vec![],
        }
    }

    pub fn add_node(&mut self, v: T) {
        self.values.push(v.clone());
        let res = self.nodes.insert(v, self.nodes.len());
        assert!(res.is_none());
        self.edges_rev.push(vec![]);
    }

    pub fn add_edge(&mut self, a: &T, b: &T) {
        let i = *self.nodes.get(a).unwrap();
        let j = *self.nodes.get(b).unwrap();
        self.edges_rev[j].push(i);
    }

    pub fn compute_topological_sort(&mut self) -> Option<Vec<T>> {
        let mut res: Vec<T> = vec![];
        let mut visited: Vec<bool> = vec![false; self.values.len()];
        let mut in_stack: Vec<bool> = vec![false; self.values.len()];

        for i in 0..self.values.len() {
            if !self.visit(&mut res, &mut visited, &mut in_stack, i) {
                return None;
            }
        }

        Some(res)
    }

    fn visit(
        &mut self,
        res: &mut Vec<T>,
        visited: &mut Vec<bool>,
        in_stack: &mut Vec<bool>,
        i: usize,
    ) -> bool {
        if in_stack[i] {
            return false;
        }

        if visited[i] {
            return true;
        }

        visited[i] = true;
        in_stack[i] = true;

        for edge_idx in 0..self.edges_rev[i].len() {
            let j = self.edges_rev[i][edge_idx];
            if !self.visit(res, visited, in_stack, j) {
                return false;
            }
        }

        res.push(self.values[i].clone());
        in_stack[i] = false;

        return true;
    }
}

================
File: ./source/verismo_verus/src/atomic_ghost.rs
================

use proc_macro2::TokenStream;
use quote::quote;
use syn_verus::parse::{Parse, ParseStream};
use syn_verus::punctuated::Punctuated;
use syn_verus::spanned::Spanned;
use syn_verus::{
    parenthesized, parse, parse_macro_input, token, Block, Error, Expr, ExprBlock, Ident, Path,
    Token,
};

///! Helper proc-macro for the atomic_ghost lib
use crate::struct_decl_inv::keyword;
use crate::struct_decl_inv::peek_keyword;

pub fn atomic_ghost(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let ag: AG = parse_macro_input!(input as AG);
    match atomic_ghost_main(ag) {
        Ok(t) => t.into(),
        Err(err) => proc_macro::TokenStream::from(err.to_compile_error()),
    }
}

struct AG {
    pub inner_macro_path: Path,
    pub atomic: Expr,
    pub op_name: Ident,
    pub operands: Vec<Expr>,
    pub prev_next: Option<(Ident, Ident)>,
    pub ret: Option<Ident>,
    pub ghost_name: Ident,
    pub block: Block,
}

impl Parse for AG {
    fn parse(input: ParseStream) -> parse::Result<AG> {
        let inner_macro_path: Path = input.parse()?;
        let _: Token![,] = input.parse()?;

        let atomic: Expr = input.parse()?;
        let _: Token![=>] = input.parse()?;
        let op_name: Ident = input.parse()?;

        let paren_content;
        let _ = parenthesized!(paren_content in input);

        let operands: Punctuated<Expr, token::Comma> =
            paren_content.parse_terminated(Expr::parse)?;
        let operands: Vec<Expr> = operands.into_iter().collect();

        let _: Token![;] = input.parse()?;

        let prev_next = if peek_keyword(input.cursor(), "update") {
            let _ = keyword(input, "update");
            let prev: Ident = input.parse()?;
            let _: Token![->] = input.parse()?;
            let next: Ident = input.parse()?;
            let _: Token![;] = input.parse()?;
            Some((prev, next))
        } else {
            None
        };

        let ret = if peek_keyword(input.cursor(), "returning") {
            let _ = keyword(input, "returning");
            let ret: Ident = input.parse()?;
            let _: Token![;] = input.parse()?;
            Some(ret)
        } else {
            None
        };

        let _ = keyword(input, "ghost");

        let ghost_name: Ident = input.parse()?;
        let _: Token![=>] = input.parse()?;
        let block: Block = input.parse()?;

        Ok(AG {
            inner_macro_path,
            atomic,
            op_name,
            operands,
            prev_next,
            ret,
            ghost_name,
            block,
        })
    }
}

fn atomic_ghost_main(ag: AG) -> parse::Result<TokenStream> {
    // valid op names and the # of arguments they take
    // see the documentation in the verus atomic_ghost lib
    let ops = vec![
        ("load", 0),
        ("store", 1),
        ("swap", 1),
        ("compare_exchange", 2),
        ("compare_exchange_weak", 2),
        ("fetch_add", 1),
        ("fetch_add_wrapping", 1),
        ("fetch_sub", 1),
        ("fetch_sub_wrapping", 1),
        ("fetch_or", 1),
        ("fetch_and", 1),
        ("fetch_xor", 1),
        ("fetch_nand", 1),
        ("fetch_max", 1),
        ("fetch_min", 1),
        ("no_op", 0),
    ];

    let op_name = ag.op_name.to_string();
    match ops.iter().find(|p| &op_name == p.0) {
        None => {
            let valid_ops: Vec<&str> = ops.iter().map(|p| p.0).collect();
            let valid_ops = valid_ops.join(", ");

            Err(Error::new(
                op_name.span(),
                &format!(
                    "atomic_with_ghost: `{:}` is not a recognized operation (valid operations are: {:})",
                    op_name.to_string(),
                    valid_ops
                ),
            ))
        }
        Some((_, num_args)) if *num_args != ag.operands.len() => Err(Error::new(
            op_name.span(),
            &format!(
                "atomic_with_ghost: `{:}` expected {:} arguments (found {:})",
                op_name.to_string(),
                num_args,
                ag.operands.len()
            ),
        )),
        Some((_, _)) => {
            let AG {
                inner_macro_path,
                mut atomic,
                op_name,
                mut operands,
                prev_next,
                ret,
                ghost_name,
                mut block,
            } = ag;

            let (prev, next) = match prev_next {
                Some((p, n)) => (quote! { #p }, quote! { #n }),
                None => (quote! { _ }, quote! { _ }),
            };

            let ret = match ret {
                Some(r) => quote! { #r },
                None => quote! { _ },
            };

            let erase = crate::cfg_erase();
            crate::syntax::rewrite_expr_node(erase, false, &mut atomic, false);
            for operand in operands.iter_mut() {
                crate::syntax::rewrite_expr_node(erase, false, operand, false);
            }

            let mut block_expr = Expr::Block(ExprBlock {
                attrs: vec![],
                label: None,
                block,
            });
            crate::syntax::rewrite_expr_node(false, true, &mut block_expr, false);
            if let Expr::Block(expr_block) = block_expr {
                block = expr_block.block;
            } else {
                panic!("the Block should still be a Block");
            }

            Ok(quote! {
                #inner_macro_path!(
                    #op_name,
                    #atomic,
                    (#(#operands),*),
                    #prev,
                    #next,
                    #ret,
                    #ghost_name,
                    #block
                )
            })
        }
    }
}

================
File: ./source/verismo_verus/src/structural.rs
================

use quote::quote;

pub fn derive_structural(s: synstructure::Structure) -> proc_macro2::TokenStream {
    let assert_receiver_is_structural_body = s
        .variants()
        .iter()
        .flat_map(|v| v.ast().fields)
        .map(|f| {
            let ty = &f.ty;
            quote! {
                let _: ::builtin::AssertParamIsStructural<#ty>;
            }
        })
        .collect::<proc_macro2::TokenStream>();
    s.gen_impl(quote! {
        #[automatically_derived]
        gen impl ::builtin::Structural for @Self {
            #[inline]
            #[doc(hidden)]
            fn assert_receiver_is_structural(&self) -> () {
                #assert_receiver_is_structural_body
            }
        }
    })
}

================
File: ./source/verismo_verus/src/fndecl.rs
================

use proc_macro2::TokenStream;
use quote::quote;

#[inline(always)]
pub fn fndecl(input: TokenStream) -> TokenStream {
    quote! {
        #[verifier::spec] #[verifier::external_body] /* vattr */ #input { unimplemented!() }
    }
}

================
File: ./source/verismo_verus/src/is_variant.rs
================

use quote::quote;

pub fn attribute_is_variant(
    _attr: proc_macro2::TokenStream,
    s: synstructure::Structure,
) -> proc_macro2::TokenStream {
    let ast = &s.ast();
    match ast.data {
        syn::Data::Enum(_) => {}
        _ => return quote! { compile_error!("#[is_variant] is only allowed on enums"); },
    }
    let struct_name = &s.ast().ident;
    let vis = &ast.vis;
    let publish = if matches!(vis, syn::Visibility::Inherited) {
        quote! {}
    } else {
        quote! {
            #[verifier::publish]
        }
    };
    let is_impls =
        s.variants()
            .iter()
            .map(|v| {
                let variant_ident = v.ast().ident;
                let variant_ident_str = variant_ident.to_string();
                let fun_ident =
                    syn::Ident::new(&format!("is_{}", &variant_ident_str), v.ast().ident.span());
                let get_fns = match v.ast().fields {
                &syn::Fields::Named(syn::FieldsNamed { named: ref fields, .. }) => fields
                    .iter()
                    .map(|f| {
                        let field_ty = &f.ty;
                        let field_ident = f.ident.as_ref().expect("missing field ident");
                        let get_ident = syn::Ident::new(
                            &format!("get_{}_{}", variant_ident_str, field_ident.to_string()),
                            v.ast().ident.span(),
                        );
                        let field_str = field_ident.to_string();

                        quote! {
                            #[allow(non_snake_case)]
                            #[verus::internal(spec)]
                            #[verifier::inline]
                            #publish
                            #vis fn #get_ident(self) -> #field_ty {
                                ::builtin::get_variant_field(self, #variant_ident_str, #field_str)
                            }
                        }
                    })
                    .collect::<proc_macro2::TokenStream>(),
                &syn::Fields::Unnamed(syn::FieldsUnnamed { unnamed: ref fields, .. }) => fields
                    .iter()
                    .zip(0..)
                    .map(|(f, i)| {
                        let field_ty = &f.ty;
                        let field_lit = format!("{}", i);
                        let get_ident = syn::Ident::new(
                            &format!("get_{}_{}", variant_ident, i),
                            v.ast().ident.span(),
                        );

                        quote! {
                            #[allow(non_snake_case)]
                            #[verus::internal(spec)]
                            #[verifier::inline]
                            #publish
                            #vis fn #get_ident(self) -> #field_ty {
                                ::builtin::get_variant_field(self, #variant_ident_str, #field_lit)
                            }
                        }
                    })
                    .collect::<proc_macro2::TokenStream>(),
                &syn::Fields::Unit => quote! {},
            };

                quote! {
                    ::builtin_macros::verus! {
                        #[allow(non_snake_case)]
                        #[verus::internal(spec)]
                        #[verifier::inline]
                        #publish
                        #vis fn #fun_ident(&self) -> bool {
                            ::builtin::is_variant(self, #variant_ident_str)
                        }

                        #get_fns
                    }
                }
            })
            .collect::<proc_macro2::TokenStream>();

    let generics = &ast.generics;
    let (impl_generics, ty_generics, where_clause) = generics.split_for_impl();
    quote! {
        #ast

        #[automatically_derived]
        impl #impl_generics #struct_name #ty_generics #where_clause {
            #is_impls
        }
    }
}

================
File: ./source/verismo_verus/src/syntax.rs
================

use std::iter::FromIterator;

use proc_macro2::{Span, TokenStream, TokenTree};
use quote::{quote, quote_spanned, ToTokens};
use syn_verus::parse::{Parse, ParseStream};
use syn_verus::punctuated::Punctuated;
use syn_verus::spanned::Spanned;
use syn_verus::token::{Brace, Bracket, Paren, Semi};
use syn_verus::visit_mut::{
    visit_block_mut, visit_expr_loop_mut, visit_expr_mut, visit_expr_while_mut, visit_field_mut,
    visit_impl_item_method_mut, visit_item_const_mut, visit_item_enum_mut, visit_item_fn_mut,
    visit_item_struct_mut, visit_local_mut, visit_trait_item_method_mut, VisitMut,
};
use syn_verus::{
    braced, bracketed, parenthesized, parse_macro_input, token, AttrStyle, Attribute, BinOp, Block,
    DataMode, Decreases, Ensures, Expr, ExprBinary, ExprCall, ExprLit, ExprLoop, ExprTuple,
    ExprUnary, ExprWhile, Field, FnArgKind, FnMode, Generics, Ident, ImplItem, ImplItemMethod,
    Invariant, InvariantEnsures, Item, ItemConst, ItemEnum, ItemFn, ItemImpl, ItemMod, ItemStruct,
    ItemTrait, Lit, LitInt, Local, Pat, Path, PathArguments, PathSegment, Publish, Recommends,
    Requires, ReturnType, Signature, Specification, Stmt, Token, TraitItem, TraitItemMethod, Type,
    TypeArray, UnOp, Visibility,
};

use crate::rustdoc::env_rustdoc;

const VERUS_SPEC: &str = "VERUS_SPEC__";

fn take_expr(expr: &mut Expr) -> Expr {
    let dummy: Expr = Expr::Verbatim(TokenStream::new());
    std::mem::replace(expr, dummy)
}

fn take_type(expr: &mut Type) -> Type {
    let dummy: Type = Type::Verbatim(TokenStream::new());
    std::mem::replace(expr, dummy)
}

fn take_pat(pat: &mut Pat) -> Pat {
    let dummy: Pat = Pat::Verbatim(TokenStream::new());
    std::mem::replace(pat, dummy)
}

fn take_ghost<T: Default>(erase_ghost: bool, dest: &mut T) -> T {
    if erase_ghost {
        *dest = T::default();
        T::default()
    } else {
        std::mem::take(dest)
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum InsideArith {
    None,
    Widen,
    Fixed,
}

#[derive(Debug)]
struct Visitor {
    erase_ghost: bool,
    // TODO: this should always be true
    use_spec_traits: bool,
    // inside_ghost > 0 means we're currently visiting ghost code
    inside_ghost: u32,
    // inside_type > 0 means we're currently visiting a type
    inside_type: u32,
    // Widen means we're a direct subexpression in an arithmetic expression that will widen the result.
    // (e.g. "x" or "3" in x + 3 or in x < (3), but not in f(x) + g(3)).
    // When we see a constant in inside_arith, we preemptively give it type "int" rather than
    // asking Rust to infer an integer type, since the inference would usually fail.
    // We also use Widen inside "... as typ".
    // It is inherited through parentheses, if/else, match, and blocks.
    // Fixed is used for bitwise operations, where we use Rust's native integer literals
    // rather than an int literal.
    inside_arith: InsideArith,
    // assign_to == true means we're an expression being assigned to by Assign
    assign_to: bool,

    // Add extra verus signature information to the docstring
    rustdoc: bool,

    // if we are inside bit-vector assertion, warn users to use add/sub/mul for fixed-width operators,
    // rather than +/-/*, which will be promoted to integer operators
    inside_bitvector: bool,
    inside_external: bool,
    for_non_secret: bool,
    update_conditions: bool,
}

// For exec "let pat = init" declarations, recursively find Tracked(x), Ghost(x), x in pat
struct ExecGhostPatVisitor {
    inside_ghost: u32,
    tracked: Option<Token![tracked]>,
    ghost: Option<Token![ghost]>,
    x_decls: Vec<Stmt>,
    x_assigns: Vec<Stmt>,
}

fn data_mode_attrs(mode: &DataMode) -> Vec<Attribute> {
    match mode {
        DataMode::Default => vec![],
        DataMode::Ghost(token) => {
            vec![mk_verus_attr(token.ghost_token.span, quote! { spec })]
        }
        DataMode::Tracked(token) => {
            vec![mk_verus_attr(token.tracked_token.span, quote! { proof })]
        }
        DataMode::Exec(token) => {
            vec![mk_verus_attr(token.exec_token.span, quote! { exec })]
        }
    }
}

fn path_is_ident(path: &Path, s: &str) -> bool {
    let segments = &path.segments;
    segments.len() == 1 && segments.first().unwrap().ident.to_string() == s
}

macro_rules! stmt_with_semi {
    ($span:expr => $($tok:tt)*) => {
        Stmt::Semi(
            Expr::Verbatim(quote_spanned!{ $span => $($tok)* }),
            Semi { spans: [ $span ] },
        )
    };
}

macro_rules! quote_verbatim {
    ($span:expr, $attrs:tt => $($tok:tt)*) => {
        Expr::Verbatim(quote_spanned!{ $span => #(#$attrs)* $($tok)* })
    }
}

impl Visitor {
    fn take_ghost<T: Default>(&self, dest: &mut T) -> T {
        take_ghost(self.erase_ghost, dest)
    }

    fn maybe_erase_expr(&self, span: Span, e: Expr) -> Expr {
        if self.erase_ghost {
            Expr::Verbatim(quote_spanned!(span => {}))
        } else {
            e
        }
    }

    fn visit_fn(
        &mut self,
        attrs: &mut Vec<Attribute>,
        _vis: Option<&Visibility>,
        sig: &mut Signature,
        _semi_token: Option<Token![;]>,
        _is_trait: bool,
    ) -> Vec<Stmt> {
        let stmts: Vec<Stmt> = Vec::new();
        let _unwrap_ghost_tracked: Vec<Stmt> = Vec::new();
        let _call_external_fn = attr_is_call_external(attrs);
        let is_exe = is_exe(&sig);
        if !self.inside_external && is_exe {
            let mut varlist = vec![];
            for arg in &sig.inputs {
                let input = &arg.kind;
                match input {
                    FnArgKind::Receiver(receiver) => {
                        let (pre_varname, post_varname) = if receiver.mutability.is_some() {
                            (
                                Some(Expr::Verbatim(quote! {(*old(self))})),
                                Some(Expr::Verbatim(quote! {(*self)})),
                            )
                        } else {
                            (Some(Expr::Verbatim(quote! {self})), None)
                        };
                        varlist.push((pre_varname, post_varname));
                    }
                    FnArgKind::Typed(pat) => {
                        param_list(&pat.pat, &pat.ty, false, &mut varlist, &sig.generics, true)
                    }
                };
            }
            let output = sig.output.clone();
            match output {
                ReturnType::Default => {}
                ReturnType::Type(tk, tracked, ret_opt, ty) => {
                    let mut tmp = ty.clone();
                    self.replace_stype(&mut tmp, !self.inside_external);

                    if tracked.is_none() {
                        match &ret_opt {
                            None => {}
                            Some(box (_, pat, _)) => {
                                param_list(pat, &ty, false, &mut varlist, &sig.generics, false);
                            }
                        }
                    }
                    sig.output = ReturnType::Type(tk, tracked, ret_opt, tmp);
                }
            }
            if self.update_conditions {
                let mut pres = Punctuated::new();
                let mut posts = Punctuated::new();
                let mut constant_pre = quote! {true};
                let mut constant_post = quote! {true};
                for (pre, post) in &varlist {
                    if let Some(var) = pre {
                        constant_pre = quote! {
                        #constant_pre && #var.is_constant()};
                        if self.for_non_secret {
                            pres.push(Expr::Verbatim(quote! {#var.is_constant()}));
                        }
                        pres.push(Expr::Verbatim(quote! {#var.wf()}));
                    }

                    if let Some(var) = post {
                        constant_post = quote! {
                        #constant_post && #var.is_constant()};
                        if self.for_non_secret {
                            posts.push(Expr::Verbatim(quote! {#var.is_constant()}));
                        }
                        posts.push(Expr::Verbatim(quote! {#var.wf()}));
                    }
                }

                posts.push(Expr::Verbatim(
                    quote! {builtin::imply(#constant_pre, #constant_post)},
                ));

                //println!("self.inside_ghost = {}", self.inside_ghost);
                match &sig.requires {
                    Some(_requires) => {
                        sig.requires.as_mut().unwrap().exprs.exprs.extend(pres);
                    }
                    None => {
                        let _exprs: Punctuated<Expr, token::Comma> = Punctuated::new();
                        let r = Requires {
                            exprs: Specification { exprs: pres },
                            token: syn_verus::token::Requires {
                                span: sig.requires.span(),
                            },
                        };
                        sig.requires = Some(r);
                    }
                }

                match &sig.ensures {
                    Some(_ensures) => {
                        sig.ensures.as_mut().unwrap().exprs.exprs.extend(posts);
                    }
                    None => {
                        let e = Ensures {
                            exprs: Specification { exprs: posts },
                            token: syn_verus::token::Ensures {
                                span: sig.requires.span(),
                            },
                            attrs: vec![],
                        };
                        sig.ensures = Some(e);
                    }
                }
            }
        }
        /*self.inside_ghost += 1;
        if let Some(requires) = &mut sig.requires {
            for expr in &mut requires.exprs.exprs {
                //println!("requires self.inside_ghost = {}", self.inside_ghost);
                self.visit_expr_mut(expr);
            }
        }

        if let Some(ensures) = &mut sig.ensures {
            for expr in &mut ensures.exprs.exprs {
                //println!("ensures self.inside_ghost = {}", self.inside_ghost);
                self.visit_expr_mut(expr);
            }
        }
        self.inside_ghost -= 1;
        //println!("end self.inside_ghost = {}", self.inside_ghost);
        */
        self.inside_bitvector = false;
        stmts
    }

    fn visit_const(
        &mut self,
        span: proc_macro2::Span,
        attrs: &mut Vec<Attribute>,
        vis: Option<&Visibility>,
        publish: &mut Publish,
        mode: &mut FnMode,
    ) {
        attrs.push(mk_verus_attr(span, quote! { verus_macro }));

        let publish_attrs = match (vis, &publish) {
            (Some(Visibility::Inherited), _) => vec![],
            (_, Publish::Default) => vec![mk_verus_attr(span, quote! { publish })],
            (_, Publish::Closed(_)) => vec![],
            (_, Publish::Open(o)) => vec![mk_verus_attr(o.token.span, quote! { publish })],
            (_, Publish::OpenRestricted(_)) => {
                unimplemented!("TODO: support open(...)")
            }
        };

        let (inside_ghost, mode_attrs): (u32, Vec<Attribute>) = match &mode {
            FnMode::Default => (0, vec![]),
            FnMode::Spec(token) => (
                1,
                vec![mk_verus_attr(token.spec_token.span, quote! { spec })],
            ),
            FnMode::SpecChecked(token) => (
                1,
                vec![mk_verus_attr(
                    token.spec_token.span,
                    quote_spanned! { token.spec_token.span => spec(checked) },
                )],
            ),
            FnMode::Proof(token) => (
                1,
                vec![mk_verus_attr(token.proof_token.span, quote! { proof })],
            ),
            FnMode::Exec(token) => (
                0,
                vec![mk_verus_attr(token.exec_token.span, quote! { exec })],
            ),
        };
        self.inside_ghost = inside_ghost;
        *publish = Publish::Default;
        *mode = FnMode::Default;
        attrs.extend(publish_attrs);
        attrs.extend(mode_attrs);
    }
}

impl VisitMut for ExecGhostPatVisitor {
    // Recursive traverse pat, finding all Tracked(x), Ghost(x), and, for ghost/tracked, x.
    fn visit_pat_mut(&mut self, pat: &mut Pat) {
        // Replace
        //   pat[Tracked(x), Ghost(y), z]
        // with (for mode != exec and inside_ghost != 0):
        //   pat[tmp_x, tmp_y, z]
        //   x_decls: let tracked x = tmp_x.get(); let ghost y = tmp_y.view();
        //   x_assigns: []
        // with (for mode = exec):
        //   pat[tmp_x, tmp_y, z]
        //   x_decls: let tracked x; let ghost mut y;
        //   x_assigns: x = tmp_x.get(); y = tmp_y.view();
        // with (for mode != exec and inside_ghost == 0):
        //   pat[tmp_x, tmp_y, tmp_z]
        //   x_decls: let tracked x; let ghost mut y; let [mode] mut z;
        //   x_assigns: x = tmp_x.get(); y = tmp_y.view(); z = tmp_z;
        /*use syn_verus::parse_quote_spanned;
        let mk_ident_tmp = |x: &Ident| {
            Ident::new(
                &("verus_tmp_".to_string() + &x.to_string()),
                Span::mixed_site(),
            )
        };
        match pat {
            Pat::TupleStruct(pts)
                if pts.pat.elems.len() == 1
                    && (path_is_ident(&pts.path, "Tracked")
                        || path_is_ident(&pts.path, "Ghost")) =>
            {
                if let Pat::Ident(id) = &mut pts.pat.elems[0] {
                    if id.by_ref.is_some() || id.subpat.is_some() {
                        return;
                    }
                    let tmp_x = mk_ident_tmp(&id.ident);
                    let mut x = id.clone();
                    x.mutability = None;
                    let span = id.span();
                    let decl = if path_is_ident(&pts.path, "Tracked") {
                        if self.inside_ghost == 0 {
                            parse_quote_spanned!(span => #[verus::internal(proof)] let mut #x;)
                        } else if id.mutability.is_some() {
                            parse_quote_spanned!(span => #[verus::internal(proof)] let mut #x = #tmp_x.get();)
                        } else {
                            parse_quote_spanned!(span => #[verus::internal(proof)] let #x = #tmp_x.get();)
                        }
                    } else {
                        if self.inside_ghost == 0 {
                            parse_quote_spanned!(span => #[verus::internal(spec)] let mut #x;)
                        } else if id.mutability.is_some() {
                            parse_quote_spanned!(span => #[verus::internal(spec)] let mut #x = #tmp_x.view();)
                        } else {
                            parse_quote_spanned!(span => #[verus::internal(spec)] let #x = #tmp_x.view();)
                        }
                    };
                    self.x_decls.push(decl);
                    if self.inside_ghost == 0 {
                        let assign = if path_is_ident(&pts.path, "Tracked") {
                            quote_spanned!(span => #x = #tmp_x.get())
                        } else {
                            quote_spanned!(span => #x = #tmp_x.view())
                        };
                        let assign = Stmt::Semi(Expr::Verbatim(assign), Semi { spans: [span] });
                        self.x_assigns.push(assign);
                    }
                    *pat = parse_quote_spanned!(span => #tmp_x);
                    return;
                }
            }
            Pat::Struct(pat_struct) => {
                // When syn parses a struct pattern like `Foo { x }`,
                // it results in an AST similar to `Foo { x: x }`,
                // that is, with a separate node for the field and the expression.
                // The only difference is that one of the nodes has a 'colon' token
                // and one doesn't.
                // Since the transformation we're doing here might change
                // `x: x` to `x: verus_tmp_x`, we can't output it using the shorthand.
                // So we need to add the colon token in.
                for field_pat in pat_struct.fields.iter_mut() {
                    if field_pat.colon_token.is_none() {
                        let span = field_pat.member.span();
                        field_pat.colon_token = Some(token::Colon { spans: [span] });
                    }
                }
            }
            Pat::Ident(id)
                if (self.tracked.is_some() || self.ghost.is_some()) && self.inside_ghost == 0 =>
            {
                if id.by_ref.is_some() || id.subpat.is_some() {
                    return;
                }
                let tmp_x = mk_ident_tmp(&id.ident);
                let mut x = id.clone();
                x.mutability = None;
                let span = id.span();
                let decl = if self.ghost.is_some() {
                    parse_quote_spanned!(span => #[verus::internal(spec)] let mut #x;)
                } else if id.mutability.is_some() {
                    parse_quote_spanned!(span => #[verus::internal(proof)] let mut #x;)
                } else {
                    parse_quote_spanned!(span => #[verus::internal(proof)] let #x;)
                };
                let assign = quote_spanned!(span => #x = #tmp_x);
                id.ident = tmp_x;
                self.x_decls.push(decl);
                self.x_assigns
                    .push(Stmt::Semi(Expr::Verbatim(assign), Semi { spans: [span] }));
                return;
            }
            _ => {}
        }*/
        syn_verus::visit_mut::visit_pat_mut(self, pat);
    }
}

impl Visitor {
    fn visit_local_extend(&mut self, local: &mut Local) -> (bool, Vec<Stmt>) {
        if self.erase_ghost && (local.tracked.is_some() || local.ghost.is_some()) {
            return (false, vec![]);
        }
        if local.init.is_none() {
            return (false, vec![]);
        }

        // Replace
        //   let [mode] pat[Tracked(x), Ghost(y), z] = init;
        // with (for mode != exec and inside_ghost != 0):
        //   let pat[tmp_x, tmp_y, z] = init;
        //   let x = tmp_x.get();
        //   let y = tmp_y.view();
        // with (for mode = exec):
        //   let pat[tmp_x, tmp_y, z] = init;
        //   let tracked x;
        //   let ghost mut y;
        //   proof {
        //       x = tmp_x.get();
        //       y = tmp_y.view();
        //   }
        // with (for mode != exec and inside_ghost == 0):
        //   let [mode] mut tmp;
        //   proof { tmp = init; } // save init in tmp to guard against name conflicts with x, y, z
        //   let tracked x;
        //   let ghost mut y;
        //   let [mode] mut z;
        //   proof {
        //       let pat[tmp_x, tmp_y, tmp_z] = tmp;
        //       x = tmp_x.get();
        //       y = tmp_y.view();
        //       z = tmp_z;
        //   }

        let stmts: Vec<Stmt> = Vec::new();
        if local.tracked.is_some() || local.ghost.is_some() {
            self.inside_ghost += 1;
        }
        let mut visit_pat = ExecGhostPatVisitor {
            inside_ghost: self.inside_ghost,
            tracked: local.tracked.clone(),
            ghost: local.ghost.clone(),
            x_decls: Vec::new(),
            x_assigns: Vec::new(),
        };

        visit_pat.visit_pat_mut(&mut local.pat);
        if local.tracked.is_some() || local.ghost.is_some() {
            self.inside_ghost -= 1;
        }
        if visit_pat.x_decls.len() == 0 && local.tracked.is_none() && local.ghost.is_none() {
            assert!(visit_pat.x_assigns.len() == 0);
            return (false, vec![]);
        }
        if self.erase_ghost {
            return (false, vec![]);
        }

        let _span = local.span();
        // Make proof block that will be subsequently visited with inside_ghost > 0
        /*let mk_proof_block = |block: Block| {
            let expr_block = syn_verus::ExprBlock { attrs: vec![], label: None, block };
            let op = UnOp::Proof(token::Proof { span });
            Expr::Unary(ExprUnary { attrs: vec![], expr: Box::new(Expr::Block(expr_block)), op })
        };*/

        if self.inside_ghost != 0 {
            assert!(visit_pat.x_assigns.len() == 0);
            //stmts.extend(visit_pat.x_decls);
            (false, stmts)
        } else if local.tracked.is_none() && local.ghost.is_none() {
            /*stmts.extend(visit_pat.x_decls);
            let block = Block { brace_token: Brace(span), stmts: visit_pat.x_assigns };
            stmts.push(Stmt::Semi(mk_proof_block(block), Semi { spans: [span] }));
            */
            (false, stmts)
        } else {
            /*use syn_verus::parse_quote_spanned;
            let tmp = Ident::new("verus_tmp", Span::mixed_site().located_at(local.span()));
            let tmp_decl = if local.tracked.is_some() {
                parse_quote_spanned!(span => #[verus::internal(proof)] #[verus::internal(unwrapped_binding)] let #tmp;)
            } else {
                parse_quote_spanned!(span => #[verus::internal(spec)] #[verus::internal(unwrapped_binding)] let mut #tmp;)
            };
            stmts.push(tmp_decl);
            let pat = take_pat(&mut local.pat);
            let init = take_expr(&mut local.init.as_mut().expect("init").1);
            let block1 = parse_quote_spanned!(span => { #tmp = #init });
            stmts.push(Stmt::Semi(mk_proof_block(block1), Semi { spans: [span] }));
            stmts.extend(visit_pat.x_decls);
            let let_pat = if local.tracked.is_some() {
                parse_quote_spanned!(span => #[verus::internal(proof)] let #pat = #tmp;)
            } else {
                parse_quote_spanned!(span => #[verus::internal(spec)] let #pat = #tmp;)
            };
            let mut block_stmts = vec![let_pat];
            block_stmts.extend(visit_pat.x_assigns);
            let block2 = Block { brace_token: Brace(span), stmts: block_stmts };
            stmts.push(Stmt::Semi(mk_proof_block(block2), Semi { spans: [span] }));
            */
            (false, stmts)
        }
    }

    fn visit_stmt_extend(&mut self, stmt: &mut Stmt) -> (bool, Vec<Stmt>) {
        match stmt {
            Stmt::Local(local) => self.visit_local_extend(local),
            _ => (false, vec![]),
        }
    }

    fn visit_stream_expr(&mut self, stream: proc_macro::TokenStream) -> proc_macro::TokenStream {
        use quote::ToTokens;
        let mut expr: Expr = parse_macro_input!(stream as Expr);
        let mut new_stream = TokenStream::new();
        self.visit_expr_mut(&mut expr);
        expr.to_tokens(&mut new_stream);
        proc_macro::TokenStream::from(new_stream)
    }

    fn visit_items_prefilter(&mut self, items: &mut Vec<Item>) {
        let erase_ghost = self.erase_ghost;
        // We'd like to erase ghost items, but there may be dangling references to the ghost items:
        // - "use" declarations may refer to the items ("use m::f;" makes it hard to erase f)
        // - "impl" may refer to struct and enum items ("impl<A> S<A> { ... }" impedes erasing S)
        // Therefore, we leave arbitrary named stubs in the place of the erased ghost items:
        // - For erased pub spec or proof Fn item x, keep decl, replace body with panic!()
        // - For erased pub Const item x, keep as-is (REVIEW: it's not clear what expressions we can support)
        // - For erased non-pub Fn and Const item x, leave "use bool as x;"
        // - Leave Struct and Enum as-is (REVIEW: we could leave stubs with PhantomData fields)
        for item in items.iter_mut() {
            let span = item.span();
            match item {
                Item::Fn(fun) => match (&fun.vis, &fun.sig.mode) {
                    (
                        Visibility::Public(_),
                        FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_),
                    ) if erase_ghost => {
                        // replace body with panic!()
                        let expr: Expr = Expr::Verbatim(quote_spanned! {
                            span => { panic!() }
                        });
                        let stmt = Stmt::Expr(expr);
                        fun.block.stmts = vec![stmt];
                        fun.semi_token = None;
                        continue;
                    }
                    _ => {}
                },
                _ => {}
            }
            let erase_fn = match item {
                Item::Fn(fun) => match fun.sig.mode {
                    FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_) if erase_ghost => {
                        Some((fun.sig.ident.clone(), fun.vis.clone()))
                    }
                    _ => None,
                },
                Item::Const(c) => match (&c.vis, &c.mode) {
                    (Visibility::Public(_), _) => None,
                    (_, FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_))
                        if erase_ghost =>
                    {
                        Some((c.ident.clone(), c.vis.clone()))
                    }
                    _ => None,
                },
                /*
                Item::Struct(s) => match s.mode {
                    DataMode::Ghost(_) | DataMode::Tracked(_) if erase_ghost => {
                        ...
                    }
                    _ => None,
                },
                Item::Enum(e) => match e.mode {
                    DataMode::Ghost(_) | DataMode::Tracked(_) if erase_ghost => {
                        ...
                    }
                    _ => None,
                },
                */
                _ => None,
            };
            if let Some((name, vis)) = erase_fn {
                *item = Item::Verbatim(quote_spanned! {
                    span => #[allow(unused_imports)] #vis use bool as #name;
                });
            }
        }
    }

    fn visit_impl_items_prefilter(&mut self, items: &mut Vec<ImplItem>, for_trait: bool) {
        let erase_ghost = self.erase_ghost;
        // Unfortunately, we just have to assume that if for_trait == true,
        // the methods might be public
        items.retain(|item| match item {
            ImplItem::Method(fun) => match ((&fun.vis, for_trait), &fun.sig.mode) {
                (
                    (Visibility::Public(_), _) | (_, true),
                    FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_),
                ) => true,
                (_, FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_)) => !erase_ghost,
                (_, FnMode::Exec(_) | FnMode::Default) => true,
            },
            ImplItem::Const(c) => match (&c.vis, &c.mode) {
                (Visibility::Public(_), _) => true,
                (_, FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_)) => !erase_ghost,
                (_, FnMode::Exec(_) | FnMode::Default) => true,
            },
            _ => true,
        });
        for item in items.iter_mut() {
            let span = item.span();
            match item {
                ImplItem::Method(fun) => match ((&fun.vis, for_trait), &fun.sig.mode) {
                    (
                        (Visibility::Public(_), _) | (_, true),
                        FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_),
                    ) if erase_ghost => {
                        // replace body with panic!()
                        let expr: Expr = Expr::Verbatim(quote_spanned! {
                            span => { panic!() }
                        });
                        let stmt = Stmt::Expr(expr);
                        fun.block.stmts = vec![stmt];
                        fun.semi_token = None;
                        continue;
                    }
                    _ => {}
                },
                _ => {}
            }
        }
    }

    fn visit_trait_items_prefilter(&mut self, items: &mut Vec<TraitItem>) {
        // In addition to prefiltering ghost code, we also split methods declarations
        // into separate spec and implementation declarations.  For example:
        //   fn f() requires x;
        // becomes
        //   fn VERUS_SPEC__f() requires x;
        //   fn f();
        // In a later pass, this becomes:
        //   fn VERUS_SPEC__f() { requires(x); ... }
        //   fn f();
        // Note: we don't do this if there's a default body (although default bodies
        // aren't supported yet anyway), because it turns out that the parameter names
        // don't exactly match between fun and fun.clone() (they have different macro contexts),
        // which would cause the body and specs to mismatch.
        let erase_ghost = self.erase_ghost;
        let mut spec_items: Vec<TraitItem> = Vec::new();
        for item in items.iter_mut() {
            match item {
                TraitItem::Method(ref mut fun) => match fun.sig.mode {
                    FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_) if erase_ghost => {
                        fun.default = None;
                        continue;
                    }
                    _ if !erase_ghost && fun.default.is_none() => {
                        // Copy into separate spec method, then remove spec from original method
                        let mut spec_fun = fun.clone();
                        let x = &fun.sig.ident;
                        let span = x.span();
                        spec_fun.sig.ident = Ident::new(&format!("{VERUS_SPEC}{x}"), span);
                        spec_fun
                            .attrs
                            .push(mk_rust_attr(span, "doc", quote! { hidden }));
                        fun.sig.erase_spec_fields();
                        spec_items.push(TraitItem::Method(spec_fun));
                    }
                    _ => {}
                },
                _ => {}
            }
        }
        items.append(&mut spec_items);
    }
}

fn chain_count(expr: &Expr) -> u32 {
    if let Expr::Binary(binary) = expr {
        match binary.op {
            BinOp::Le(_) | BinOp::Lt(_) | BinOp::Ge(_) | BinOp::Gt(_) | BinOp::Eq(_) => {
                1 + chain_count(&binary.left)
            }
            _ => 0,
        }
    } else {
        0
    }
}

const ILLEGAL_CALLEES: &[&str] = &["forall", "exists", "choose"];

impl Visitor {
    /// Turn `forall|x| ...`
    /// into `::builtin::forall(|x| ...)`
    /// and similarly for `exists` and `choose`
    ///
    /// Also handle trigger attributes.
    ///
    /// Returns true if the transform is attempted, false if the transform is inapplicable.

    fn closure_quant_operators(&mut self, expr: &mut Expr) -> bool {
        let unary = match expr {
            Expr::Unary(
                u @ ExprUnary {
                    op: UnOp::Forall(..),
                    ..
                },
            ) => u,
            Expr::Unary(
                u @ ExprUnary {
                    op: UnOp::Exists(..),
                    ..
                },
            ) => u,
            Expr::Unary(
                u @ ExprUnary {
                    op: UnOp::Choose(..),
                    ..
                },
            ) => u,
            Expr::Call(ExprCall {
                attrs: _,
                func,
                paren_token: _,
                args: _,
            }) => {
                if let Expr::Path(syn_verus::ExprPath {
                    path,
                    qself: None,
                    attrs: _,
                }) = &**func
                {
                    if path.segments.len() == 1
                        && ILLEGAL_CALLEES.contains(&path.segments[0].ident.to_string().as_str())
                    {
                        let err = format!(
                            "forall, choose, and exists do not allow parentheses, use `{}|<vars>| expr` instead",
                            path.segments[0].ident.to_string()
                        );
                        *expr = Expr::Verbatim(quote_spanned!(expr.span() => compile_error!(#err)));
                        return true;
                    }
                }
                return false;
            }
            _ => {
                return false;
            }
        };

        // Recursively visit the closure expression, but *don't* call our
        // custom visitor fn on the closure node itself.
        visit_expr_mut(self, &mut unary.expr);

        let span = unary.span();

        let attrs = std::mem::take(&mut unary.attrs);

        let arg = &mut *unary.expr;
        let (inner_attrs, n_inputs) = match &mut *arg {
            Expr::Closure(closure) => {
                if closure.requires.is_some() || closure.ensures.is_some() {
                    let err = "quantifiers cannot have requires/ensures";
                    *expr = Expr::Verbatim(quote_spanned!(span => compile_error!(#err)));
                    return true;
                }
                (
                    std::mem::take(&mut closure.inner_attrs),
                    closure.inputs.len(),
                )
            }
            _ => panic!("expected closure for quantifier"),
        };

        match extract_quant_triggers(inner_attrs, span) {
            Ok(ExtractQuantTriggersFound::Auto) => match &mut *arg {
                Expr::Closure(closure) => {
                    let body = take_expr(&mut closure.body);
                    closure.body = Box::new(Expr::Verbatim(
                        quote_spanned!(span => #[verus::internal(auto_trigger)] (#body)),
                    ));
                }
                _ => panic!("expected closure for quantifier"),
            },
            Ok(ExtractQuantTriggersFound::Triggers(tuple)) => match &mut *arg {
                Expr::Closure(closure) => {
                    let body = take_expr(&mut closure.body);
                    closure.body = Box::new(Expr::Verbatim(
                        quote_spanned!(span => ::builtin::with_triggers(#tuple, #body)),
                    ));
                }
                _ => panic!("expected closure for quantifier"),
            },
            Ok(ExtractQuantTriggersFound::None) => {}
            Err(err_expr) => {
                *expr = err_expr;
                return true;
            }
        }

        match unary.op {
            UnOp::Forall(..) => {
                *expr = quote_verbatim!(span, attrs => ::builtin::forall(#arg));
            }
            UnOp::Exists(..) => {
                *expr = quote_verbatim!(span, attrs => ::builtin::exists(#arg));
            }
            UnOp::Choose(..) => {
                if n_inputs == 1 {
                    *expr = quote_verbatim!(span, attrs => ::builtin::choose(#arg));
                } else {
                    *expr = quote_verbatim!(span, attrs => ::builtin::choose_tuple(#arg));
                }
            }
            _ => panic!("unary= {}", quote! {unary}),
        }

        true
    }

    fn add_loop_specs(
        &mut self,
        stmts: &mut Vec<Stmt>,
        invariants: Option<Invariant>,
        invariant_ensures: Option<InvariantEnsures>,
        ensures: Option<Ensures>,
        decreases: Option<Decreases>,
    ) {
        // TODO: wrap specs inside ghost blocks
        self.inside_ghost += 1;
        if let Some(Invariant { token, mut exprs }) = invariants {
            if exprs.exprs.len() > 0 {
                for expr in exprs.exprs.iter_mut() {
                    self.visit_expr_mut(expr);
                }
                stmts.push(stmt_with_semi!(token.span => ::builtin::invariant([#exprs])));
            }
        }
        if let Some(InvariantEnsures { token, mut exprs }) = invariant_ensures {
            if exprs.exprs.len() > 0 {
                for expr in exprs.exprs.iter_mut() {
                    self.visit_expr_mut(expr);
                }
                stmts.push(stmt_with_semi!(token.span => ::builtin::invariant_ensures([#exprs])));
            }
        }
        if let Some(Ensures {
            attrs: _,
            token: _,
            mut exprs,
        }) = ensures
        {
            if exprs.exprs.len() > 0 {
                for expr in exprs.exprs.iter_mut() {
                    self.visit_expr_mut(expr);
                }
                //stmts.push(stmt_with_semi!(token.span => ::builtin::ensures([#exprs])));
            }
        }
        if let Some(Decreases { token, mut exprs }) = decreases {
            for expr in exprs.exprs.iter_mut() {
                self.visit_expr_mut(expr);
                if matches!(expr, Expr::Tuple(..)) {
                    let err = "decreases cannot be a tuple; use `decreases x, y` rather than `decreases (x, y)`";
                    let expr = Expr::Verbatim(quote_spanned!(token.span => compile_error!(#err)));
                    stmts.push(Stmt::Semi(
                        expr,
                        Semi {
                            spans: [token.span],
                        },
                    ));
                }
            }
            stmts.push(stmt_with_semi!(token.span => ::builtin::decreases((#exprs))));
        }
        //println!("end add_loop_spec inside_ghost = {}", self.inside_ghost);
        self.inside_ghost -= 1;
    }
}

enum ExtractQuantTriggersFound {
    Auto,
    Triggers(ExprTuple),
    None,
}

fn extract_quant_triggers(
    inner_attrs: Vec<Attribute>,
    span: Span,
) -> Result<ExtractQuantTriggersFound, Expr> {
    let mut triggers: Vec<Expr> = Vec::new();
    for attr in inner_attrs {
        let trigger: syn_verus::Result<syn_verus::Specification> =
            syn_verus::parse2(attr.tokens.clone());
        let path_segments_str = attr
            .path
            .segments
            .iter()
            .map(|x| x.ident.to_string())
            .collect::<Vec<_>>();
        let ident_str = match &path_segments_str[..] {
            [attr_name] => Some(attr_name),
            _ => None,
        };
        match (trigger, ident_str) {
            (Ok(trigger), Some(id)) if id == &"auto" && trigger.exprs.len() == 0 => {
                return Ok(ExtractQuantTriggersFound::Auto);
            }
            (Ok(trigger), Some(id)) if id == &"trigger" => {
                let tuple = ExprTuple {
                    attrs: vec![],
                    paren_token: Paren(span),
                    elems: trigger.exprs,
                };
                triggers.push(Expr::Tuple(tuple));
            }
            (Err(err), _) => {
                let span = attr.span();
                let err = err.to_string();

                return Err(Expr::Verbatim(quote_spanned!(span => compile_error!(#err))));
            }
            _ => {
                let span = attr.span();
                return Err(Expr::Verbatim(
                    quote_spanned!(span => compile_error!("expected trigger")),
                ));
            }
        }
    }

    Ok(if triggers.len() > 0 {
        let mut elems = Punctuated::new();
        for elem in triggers {
            elems.push(elem);
            elems.push_punct(Token![,](span));
        }
        ExtractQuantTriggersFound::Triggers(ExprTuple {
            attrs: vec![],
            paren_token: Paren(span),
            elems,
        })
    } else {
        ExtractQuantTriggersFound::None
    })
}

impl VisitMut for Visitor {
    fn visit_expr_mut(&mut self, expr: &mut Expr) {
        //println!("expr = {}, inside_ghot = {}", quote!{#expr}, self.inside_ghost);
        let is_inside_bitvector = match &expr {
            Expr::Assert(a) => match &a.prover {
                Some((_, id)) => {
                    if id.to_string() == "bit_vector" {
                        self.inside_bitvector = true;
                        true
                    } else {
                        false
                    }
                }
                None => false,
            },
            _ => false,
        };

        let is_auto_proof_block = if self.inside_ghost == 0 {
            match &expr {
                Expr::Assume(a) => Some(a.assume_token.span),
                Expr::Assert(a) => Some(a.assert_token.span),
                Expr::AssertForall(a) => Some(a.assert_token.span),
                _ => None,
            }
        } else {
            None
        };
        if let Some(_) = is_auto_proof_block {
            self.inside_ghost += 1;
        }

        let mode_block = match expr {
            Expr::Unary(ExprUnary {
                op: UnOp::Proof(..),
                ..
            }) => Some((false, false)),
            Expr::Call(ExprCall {
                func: box Expr::Path(path),
                args,
                ..
            }) if path.qself.is_none() && args.len() == 1 => {
                if path_is_ident(&path.path, "Ghost") {
                    Some((true, false))
                } else if path_is_ident(&path.path, "Tracked") {
                    Some((true, true))
                } else {
                    None
                }
            }
            _ => None,
        };

        let sub_inside_arith = match expr {
            Expr::Paren(..) | Expr::Block(..) | Expr::Group(..) => self.inside_arith,
            Expr::Cast(..) => InsideArith::Widen,
            Expr::Unary(unary) => match unary.op {
                UnOp::Neg(..) => InsideArith::Widen,
                UnOp::Not(..) => InsideArith::Fixed,
                _ => InsideArith::None,
            },
            Expr::Binary(binary) => match binary.op {
                BinOp::Add(..)
                | BinOp::Sub(..)
                | BinOp::Mul(..)
                | BinOp::Eq(..)
                | BinOp::Ne(..)
                | BinOp::Lt(..)
                | BinOp::Le(..)
                | BinOp::Gt(..)
                | BinOp::Ge(..) => InsideArith::Widen,
                BinOp::Div(..) | BinOp::Rem(..) => InsideArith::None,
                BinOp::BitXor(..)
                | BinOp::BitAnd(..)
                | BinOp::BitOr(..)
                | BinOp::Shl(..)
                | BinOp::Shr(..) => InsideArith::Fixed,
                _ => InsideArith::None,
            },
            _ => InsideArith::None,
        };
        let sub_assign_to = match expr {
            Expr::Field(..) => self.assign_to,
            _ => false,
        };

        // Recursively call visit_expr_mut
        let is_inside_ghost = self.inside_ghost > 0;
        let is_inside_arith = self.inside_arith;
        let is_assign_to = self.assign_to;
        let use_spec_traits = self.use_spec_traits && is_inside_ghost;
        if mode_block.is_some() {
            self.inside_ghost += 1;
        }
        self.inside_arith = sub_inside_arith;
        self.assign_to = sub_assign_to;
        let assign_left = if let Expr::Assign(assign) = expr {
            let mut left = take_expr(&mut assign.left);
            self.assign_to = true;
            self.visit_expr_mut(&mut left);
            self.assign_to = false;
            Some(left)
        } else {
            None
        };
        if !(is_inside_ghost && self.erase_ghost) {
            visit_expr_mut(self, expr);
        }
        if let Expr::Assign(assign) = expr {
            assign.left = Box::new(assign_left.expect("assign_left"));
        }
        if mode_block.is_some() {
            //println!("mode_block.is_some() inside_ghost -1 = {}", self.inside_ghost);
            self.inside_ghost -= 1;
        }
        self.inside_arith = is_inside_arith;
        self.assign_to = is_assign_to;
        let do_replace = match &expr {
            Expr::Lit(ExprLit {
                lit: Lit::Int(..), ..
            }) => true,
            Expr::Cast(..) => true,
            Expr::Index(..) if !self.inside_external => true,
            Expr::Unary(ExprUnary {
                op: UnOp::Neg(..), ..
            }) => true,
            Expr::Unary(ExprUnary {
                op: UnOp::Not(..), ..
            }) => true,
            Expr::Binary(ExprBinary {
                op:
                    BinOp::Eq(..)
                    | BinOp::Ne(..)
                    | BinOp::Le(..)
                    | BinOp::Lt(..)
                    | BinOp::Ge(..)
                    | BinOp::Gt(..)
                    | BinOp::Add(..)
                    | BinOp::Sub(..)
                    | BinOp::Mul(..)
                    | BinOp::Div(..)
                    | BinOp::Rem(..)
                    | BinOp::BitAnd(..)
                    | BinOp::BitOr(..)
                    | BinOp::BitXor(..)
                    | BinOp::Shl(..)
                    | BinOp::Shr(..),
                ..
            }) => true,
            Expr::Assume(..) | Expr::Assert(..) | Expr::AssertForall(..) => true,
            Expr::View(..) => true,
            Expr::Closure(..) => true,
            _ => false,
        };
        let use_sec_type = !self.inside_bitvector
            && !use_spec_traits
            && !self.inside_external
            && (self.inside_type == 0);

        let const_fn = if !use_spec_traits {
            quote! {constant}
        } else {
            quote! {spec_constant}
        };

        let replace_exe_op = !is_inside_ghost && (self.inside_type == 0);
        let sec_const = quote! {SecType::#const_fn};
        if do_replace && self.inside_type == 0 {
            let e = take_expr(expr);
            match e {
                Expr::Lit(ExprLit {
                    lit: Lit::Int(lit),
                    attrs,
                }) => {
                    ////println!("expr = {}", lit);
                    let span = lit.span();
                    let n = lit.base10_digits().to_string();
                    if lit.suffix() == "" {
                        match is_inside_arith {
                            InsideArith::None => {
                                // We don't know which integer type to use,
                                // so defer the decision to type inference.
                                // *expr = quote_verbatim!(span, attrs => ::builtin::spec_literal_integer(#n));
                                *expr = if !use_sec_type {
                                    quote_verbatim!(span, attrs => ::builtin::spec_literal_integer(#n))
                                } else {
                                    let lit = Expr::Lit(ExprLit {
                                        lit: Lit::Int(lit),
                                        attrs: vec![],
                                    });
                                    quote_verbatim! {span, attrs => #sec_const(#lit) }
                                };
                            }
                            InsideArith::Widen if n.starts_with("-") => {
                                // Use int inside +, -, etc., since these promote to int anyway
                                //*expr =
                                //    quote_verbatim!(span, attrs => ::builtin::spec_literal_int(#n));
                                *expr = if !use_sec_type {
                                    quote_verbatim! {span, attrs => (::builtin::spec_literal_int(#n)) }
                                } else {
                                    let lit = Expr::Lit(ExprLit {
                                        lit: Lit::Int(lit),
                                        attrs: vec![],
                                    });
                                    quote_verbatim! {span, attrs => #sec_const(#lit) }
                                };
                            }
                            InsideArith::Widen => {
                                // Use int inside +, -, etc., since these promote to int anyway
                                *expr = if !use_sec_type {
                                    quote_verbatim! {span, attrs => (::builtin::spec_literal_nat(#n)) }
                                } else {
                                    let lit = Expr::Lit(ExprLit {
                                        lit: Lit::Int(lit),
                                        attrs: vec![],
                                    });
                                    quote_verbatim! {span, attrs => #sec_const(#lit) }
                                };
                            }
                            InsideArith::Fixed => {
                                // We generally won't want int/nat literals for bitwise ops,
                                // so use Rust's native integer literals
                                let newexpr = Expr::Lit(ExprLit {
                                    lit: Lit::Int(lit),
                                    attrs: vec![],
                                });
                                *expr = if !use_sec_type {
                                    quote_verbatim! {span, attrs => (#newexpr) }
                                } else {
                                    quote_verbatim! {span, attrs => #sec_const(#newexpr) }
                                };
                            }
                        }
                    } else if lit.suffix() == "int" {
                        // *expr = quote_verbatim!(span, attrs => ::builtin::spec_literal_int(#n));
                        *expr = if use_spec_traits {
                            quote_verbatim! {span, attrs => (::builtin::spec_literal_int(#n)) }
                        } else {
                            panic!("No int in exe")
                        };
                    } else if lit.suffix() == "nat" {
                        //*expr = quote_verbatim!(span, attrs => ::builtin::spec_literal_nat(#n));
                        *expr = quote_verbatim! {span, attrs => (::builtin::spec_literal_nat(#n))};
                    } else if lit.suffix().ends_with("_s") {
                        let tmp = Expr::Lit(ExprLit {
                            lit: Lit::Int(LitInt::new(
                                &lit.to_string().as_str().replace("_s", ""),
                                lit.span(),
                            )),
                            attrs: vec![],
                        });
                        let ident = Ident::new(lit.suffix(), lit.span());

                        *expr = quote_verbatim! {span, attrs => #ident::#const_fn(#tmp)};
                    } else if lit.suffix().ends_with("_t") {
                        let tmp = Expr::Lit(ExprLit {
                            lit: Lit::Int(LitInt::new(
                                &lit.to_string().as_str().replace("_t", ""),
                                lit.span(),
                            )),
                            attrs: vec![],
                        });
                        let _ident = Ident::new(lit.suffix(), lit.span());

                        *expr = quote_verbatim! {span, attrs => #tmp};
                    } else {
                        // Has a native Rust integer suffix, so leave it as a secure literal
                        *expr = if !use_sec_type {
                            let tmp = Expr::Lit(ExprLit {
                                lit: Lit::Int(lit),
                                attrs: vec![],
                            });
                            quote_verbatim! {span, attrs => (#tmp) }
                        } else {
                            let ident =
                                Ident::new(format!("{}_s", lit.suffix()).as_str(), lit.span());
                            let tmp = Expr::Lit(ExprLit {
                                lit: Lit::Int(lit),
                                attrs: vec![],
                            });
                            quote_verbatim! {span, attrs => #ident::#const_fn(#tmp) }
                        };
                    }
                    ////println!("expr = {:?}", expr);
                }
                Expr::Cast(cast) => {
                    let span = cast.span();
                    let src = &cast.expr;
                    let attrs: Vec<Attribute> = cast.attrs.clone();
                    let mut ty = cast.ty.clone();
                    *expr = if self.inside_ghost > 0 {
                        quote_verbatim!(span, attrs => VTypeCast::<#ty>::vspec_cast_to(#src))
                    } else if !self.inside_external {
                        self.replace_stype(&mut ty, true);
                        quote_verbatim!(span, attrs => core::convert::Into::<#ty>::into(#src))
                    } else {
                        Expr::Cast(cast)
                    };
                }
                Expr::Index(idx) => {
                    let span = idx.span();
                    let src = idx.expr;
                    let attrs = idx.attrs;
                    let index = idx.index;
                    if use_spec_traits && is_inside_ghost {
                        *expr = quote_verbatim!(span, attrs => #src.spec_index(#index));
                    } else if replace_exe_op {
                        *expr = quote_verbatim!(span, attrs => #src.index(#index));
                    }
                }
                Expr::Unary(unary) => {
                    let span = unary.span();
                    let attrs = unary.attrs;
                    match unary.op {
                        UnOp::Neg(_neg) => {
                            let arg = unary.expr;
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => (#arg).spec_neg());
                            } else if replace_exe_op {
                                *expr = quote_verbatim!(span, attrs => (#arg).neg());
                            }
                        }
                        UnOp::Not(_neg) => {
                            let arg = unary.expr;
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => (#arg).spec_not());
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => (#arg).not());
                            }
                        }
                        _ => panic!("unary"),
                    }
                }
                Expr::Binary(binary) => {
                    let b = binary.clone();
                    let span = b.span();
                    let attrs = b.attrs;
                    let left = b.left;
                    let right = b.right;
                    match b.op {
                        BinOp::Eq(..) => {
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => (#left).spec_eq(#right));
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => (#left).eq(&#right));
                            }
                        }
                        BinOp::Ne(..) => {
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => !((#left).spec_eq(#right)));
                            } else if !is_inside_ghost {
                                *expr = Expr::Verbatim(quote! {!((#left).eq(&#right))});
                            }
                        }
                        BinOp::Le(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = Expr::Binary(binary);
                            } else if !is_inside_ghost {
                                //println!("{:?}", self);
                                *expr = quote_verbatim!(span, attrs => #left.le(&#right));
                            }
                        }
                        BinOp::Lt(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = Expr::Binary(binary);
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.lt(&#right));
                            }
                        }
                        BinOp::Ge(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = Expr::Binary(binary);
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.ge(&#right));
                            }
                        }
                        BinOp::Gt(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = Expr::Binary(binary);
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.gt(&#right));
                            }
                        }
                        BinOp::Add(..) if !self.inside_bitvector => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_add(#right));
                            } else if replace_exe_op {
                                *expr = quote_verbatim!(span, attrs => #left.add(#right));
                            }
                        }
                        BinOp::Sub(..) if !self.inside_bitvector => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_sub(#right));
                            } else if replace_exe_op {
                                *expr = quote_verbatim!(span, attrs => #left.sub(#right));
                            }
                        }
                        BinOp::Mul(..) if !self.inside_bitvector => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_mul(#right));
                            } else if replace_exe_op {
                                *expr = quote_verbatim!(span, attrs => #left.mul(#right));
                            }
                        }
                        BinOp::Add(..) | BinOp::Sub(..) | BinOp::Mul(..) => {
                            *expr = quote_verbatim!(span, attrs => compile_error!("Inside bit-vector assertion, use `add` `sub` `mul` for fixed-bit operators, instead of `+` `-` `*`. (see the functions builtin::add(left, right), builtin::sub(left, right), and builtin::mul(left, right))"));
                        }
                        BinOp::Div(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_euclidean_div(#right));
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.div(#right));
                            }
                        }
                        BinOp::Rem(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_euclidean_mod(#right));
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.rem(#right));
                            };
                        }
                        BinOp::BitAnd(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_bitand(#right));
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.bitand(#right));
                            }
                        }
                        BinOp::BitOr(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_bitor(#right));
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.bitor(#right));
                            }
                        }
                        BinOp::BitXor(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_bitxor(#right));
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.bitxor(#right));
                            }
                        }
                        BinOp::Shl(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_shl(#right));
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.shl(#right));
                            }
                        }
                        BinOp::Shr(..) => {
                            let left = quote_spanned! { left.span() => (#left) };
                            if use_spec_traits {
                                *expr = quote_verbatim!(span, attrs => #left.spec_shr(#right));
                            } else if !is_inside_ghost {
                                *expr = quote_verbatim!(span, attrs => #left.shr(#right));
                            }
                        }
                        _ => panic!("binary"),
                    }
                }
                _ => *expr = e,
            }
        }
        if is_inside_bitvector {
            self.inside_bitvector = false;
        }
        if let Some(_) = is_auto_proof_block {
            self.inside_ghost -= 1;
        }
    }

    fn visit_attribute_mut(&mut self, attr: &mut Attribute) {
        if let syn_verus::AttrStyle::Outer = attr.style {
            match &attr
                .path
                .segments
                .iter()
                .map(|x| &x.ident)
                .collect::<Vec<_>>()[..]
            {
                [attr_name] if attr_name.to_string() == "trigger" => {
                    *attr = mk_verus_attr(attr.span(), quote! { trigger });
                }
                [attr_name] if attr_name.to_string() == "via_fn" => {
                    *attr = mk_verus_attr(attr.span(), quote! { via });
                }
                [attr_name] if attr_name.to_string() == "verifier" => {
                    let parsed = attr.parse_meta().expect("failed to parse attribute");
                    match parsed {
                        syn_verus::Meta::List(meta_list) if meta_list.nested.len() == 1 => {
                            let span = attr.span();
                            let (second_segment, nested) = match &meta_list.nested[0] {
                                syn_verus::NestedMeta::Meta(syn_verus::Meta::List(meta_list)) => {
                                    let rest = &meta_list.nested[0];
                                    (&meta_list.path.segments[0], Some(quote! { (#rest) }))
                                }
                                syn_verus::NestedMeta::Meta(syn_verus::Meta::Path(meta_path)) => {
                                    (&meta_path.segments[0], None)
                                }
                                _ => {
                                    panic!("invalid verifier attribute (1)"); // TODO(main_new) use compile_error! if possible
                                }
                            };
                            let mut path_segments = Punctuated::new();
                            path_segments.push(PathSegment {
                                ident: Ident::new("verifier", span),
                                arguments: PathArguments::None,
                            });
                            path_segments.push(second_segment.clone());
                            *attr = Attribute {
                                pound_token: token::Pound { spans: [span] },
                                style: AttrStyle::Outer,
                                bracket_token: token::Bracket { span },
                                path: Path {
                                    leading_colon: None,
                                    segments: path_segments,
                                },
                                tokens: if let Some(nested) = nested {
                                    quote! { #nested }
                                } else {
                                    quote! {}
                                },
                            };
                        }
                        _ => panic!("invalid verifier attribute (2)"), // TODO(main_new) use compile_error! if possible
                    }
                }
                _ => (),
            }
        }

        if let syn_verus::AttrStyle::Inner(_) = attr.style {
            match &attr
                .path
                .segments
                .iter()
                .map(|x| &x.ident)
                .collect::<Vec<_>>()[..]
            {
                [attr_name] if attr_name.to_string() == "trigger" => {
                    // process something like: #![trigger f(a, b), g(c, d)]
                    // attr.tokens is f(a, b), g(c, d)
                    // turn this into a tuple (f(a, b), g(c, d)),
                    // parse it into an Expr, visit the Expr, turn the Expr back into tokens,
                    // remove the ( and ).
                    let old_stream = proc_macro::TokenStream::from(attr.tokens.clone());
                    let mut tuple_stream = proc_macro::TokenStream::new();
                    let group =
                        proc_macro::Group::new(proc_macro::Delimiter::Parenthesis, old_stream);
                    tuple_stream.extend(vec![proc_macro::TokenTree::Group(group)]);
                    let mut new_tuples = self.visit_stream_expr(tuple_stream).into_iter();
                    let new_tuple = new_tuples.next().expect("visited tuple");
                    assert!(new_tuples.next().is_none());
                    if let proc_macro::TokenTree::Group(group) = new_tuple {
                        assert!(group.delimiter() == proc_macro::Delimiter::Parenthesis);
                        attr.tokens = proc_macro2::TokenStream::from(group.stream());
                    } else {
                        panic!("expected tuple");
                    }
                }
                _ => (),
            }
        }
    }

    fn visit_expr_while_mut(&mut self, expr_while: &mut ExprWhile) {
        //let invariants = self.take_ghost(&mut expr_while.invariant);
        //let invariant_ensures = self.take_ghost(&mut expr_while.invariant_ensures);
        //let ensures = self.take_ghost(&mut expr_while.ensures);
        //let decreases = self.take_ghost(&mut expr_while.decreases);
        let stmts: Vec<Stmt> = Vec::new();
        if expr_while.invariant.is_some() {
            self.visit_invariant_mut(expr_while.invariant.as_mut().unwrap());
        }
        if expr_while.invariant_ensures.is_some() {
            self.visit_invariant_ensures_mut(&mut expr_while.invariant_ensures.as_mut().unwrap());
        }
        if expr_while.ensures.is_some() {
            self.visit_ensures_mut(&mut expr_while.ensures.as_mut().unwrap());
        }
        expr_while.body.stmts.splice(0..0, stmts);
        visit_expr_while_mut(self, expr_while);
    }

    fn visit_expr_loop_mut(&mut self, expr_loop: &mut ExprLoop) {
        //let invariants = self.take_ghost(&mut expr_loop.invariant);
        //let invariant_ensures = self.take_ghost(&mut expr_loop.invariant_ensures);
        //let ensures = self.take_ghost(&mut expr_loop.ensures);
        //let decreases = self.take_ghost(&mut expr_loop.decreases);
        let stmts: Vec<Stmt> = Vec::new();
        /*self.add_loop_specs(
            &mut stmts,
            invariants,
            invariant_ensures,
            ensures,
            decreases,
        );*/
        expr_loop.body.stmts.splice(0..0, stmts);
        visit_expr_loop_mut(self, expr_loop);
    }

    fn visit_local_mut(&mut self, local: &mut Local) {
        // Note: exec-mode "let ghost" and "let tracked" have already been transformed
        // into proof blocks by point, so we don't need to change inside_ghost here.
        let is_ghost = local.tracked.is_some() || local.ghost.is_some();
        if is_ghost {
            self.inside_ghost += 1;
        }
        visit_local_mut(self, local);
        if is_ghost {
            self.inside_ghost -= 1;
        }
    }

    fn visit_block_mut(&mut self, block: &mut Block) {
        /*let mut stmts: Vec<Stmt> = Vec::new();
        let block_stmts = std::mem::replace(&mut block.stmts, vec![]);
        for mut stmt in block_stmts {
            let (skip, extra_stmts) = self.visit_stmt_extend(&mut stmt);
            if !skip {
                stmts.push(stmt);
            }
            stmts.extend(extra_stmts);
        }
        block.stmts = stmts;*/
        visit_block_mut(self, block);
    }

    fn visit_requires_mut(&mut self, i: &mut Requires) {
        self.inside_ghost += 1;
        syn_verus::visit_mut::visit_requires_mut(self, i);
        self.inside_ghost -= 1;
    }

    fn visit_ensures_mut(&mut self, i: &mut Ensures) {
        self.inside_ghost += 1;
        syn_verus::visit_mut::visit_ensures_mut(self, i);
        self.inside_ghost -= 1;
    }

    fn visit_decreases_mut(&mut self, i: &mut Decreases) {
        self.inside_ghost += 1;
        syn_verus::visit_mut::visit_decreases_mut(self, i);
        self.inside_ghost -= 1;
    }

    fn visit_recommends_mut(&mut self, i: &mut Recommends) {
        self.inside_ghost += 1;
        syn_verus::visit_mut::visit_recommends_mut(self, i);
        self.inside_ghost -= 1;
    }

    fn visit_invariant_mut(&mut self, i: &mut Invariant) {
        self.inside_ghost += 1;
        syn_verus::visit_mut::visit_invariant_mut(self, i);
        self.inside_ghost -= 1;
    }

    fn visit_invariant_ensures_mut(&mut self, i: &mut InvariantEnsures) {
        self.inside_ghost += 1;
        syn_verus::visit_mut::visit_invariant_ensures_mut(self, i);
        self.inside_ghost -= 1;
    }

    fn visit_item_fn_mut(&mut self, fun: &mut ItemFn) {
        let is_external = self.inside_external;
        self.inside_external = attr_is_external(&fun.attrs);

        // Process rustdoc before processing the ItemFn itself.
        // That way, the generated rustdoc gets the prettier syntax instead of the
        // de-sugared syntax.
        if self.rustdoc {
            crate::rustdoc::process_item_fn(fun);
        }
        self.inside_ghost = if !is_exe(&fun.sig) { 1 } else { 0 };
        let _stmts = self.visit_fn(
            &mut fun.attrs,
            Some(&fun.vis),
            &mut fun.sig,
            fun.semi_token,
            false,
        );
        fun.semi_token = None;
        visit_item_fn_mut(self, fun);
        self.inside_ghost = 0;
        //println!("visit_item_fn_mut self.inside_ghost = 0");
        self.inside_external = is_external;
    }

    fn visit_impl_item_method_mut(&mut self, method: &mut ImplItemMethod) {
        let is_external = self.inside_external;
        self.inside_external = attr_is_external(&method.attrs);
        self.inside_ghost = if !is_exe(&method.sig) { 1 } else { 0 };
        if self.rustdoc {
            crate::rustdoc::process_impl_item_method(method);
        }

        let stmts = self.visit_fn(
            &mut method.attrs,
            Some(&method.vis),
            &mut method.sig,
            method.semi_token,
            false,
        );
        method.block.stmts.splice(0..0, stmts);
        method.semi_token = None;
        visit_impl_item_method_mut(self, method);
        self.inside_external = is_external;
        self.inside_ghost = 0;
        //println!("visit_impl_item_method_mut self.inside_ghost = 0");
    }

    fn visit_trait_item_method_mut(&mut self, method: &mut TraitItemMethod) {
        if self.rustdoc {
            crate::rustdoc::process_trait_item_method(method);
        }
        self.inside_ghost = if !is_exe(&method.sig) { 1 } else { 0 };
        let _is_spec_method = method.sig.ident.to_string().starts_with(VERUS_SPEC);
        let _stmts = self.visit_fn(
            &mut method.attrs,
            None,
            &mut method.sig,
            method.semi_token,
            true,
        );
        visit_trait_item_method_mut(self, method);
        self.inside_ghost = 0;
        //println!("visit_trait_item_method_mut self.inside_ghost = 0");
    }

    fn visit_item_const_mut(&mut self, con: &mut ItemConst) {
        let is_external = self.inside_external;
        self.inside_external = attr_is_external(&con.attrs);
        /*self.visit_const(
            con.const_token.span,
            &mut con.attrs,
            Some(&con.vis),
            &mut con.publish,
            &mut con.mode,
        );*/
        visit_item_const_mut(self, con);
        self.inside_external = is_external;
    }

    fn visit_field_mut(&mut self, field: &mut Field) {
        visit_field_mut(self, field);
        field.mode = DataMode::Default;
    }

    fn visit_item_enum_mut(&mut self, item: &mut ItemEnum) {
        let is_external = self.inside_external;
        self.inside_external = attr_is_external(&item.attrs);
        visit_item_enum_mut(self, item);
        item.mode = DataMode::Default;
        self.inside_external = is_external;
    }

    fn visit_item_struct_mut(&mut self, item: &mut ItemStruct) {
        let is_external = self.inside_external;
        self.inside_external = attr_is_external(&item.attrs);
        visit_item_struct_mut(self, item);
        item.attrs.extend(data_mode_attrs(&item.mode));
        item.attrs
            .extend(struct_data_mode_attrs(&item.mode, self.inside_external));

        item.mode = DataMode::Default;
        self.inside_external = is_external;
    }

    fn visit_type_mut(&mut self, ty: &mut Type) {
        self.inside_type += 1;
        syn_verus::visit_mut::visit_type_mut(self, ty);
        self.inside_type -= 1;

        let _span = ty.span();
        self.replace_stype(ty, false);
    }

    fn visit_path_mut(&mut self, path: &mut Path) {
        // generic type arguments can appear inside paths
        self.inside_type += 1;
        syn_verus::visit_mut::visit_path_mut(self, path);
        self.inside_type -= 1;
    }

    fn visit_generic_method_argument_mut(&mut self, arg: &mut syn_verus::GenericMethodArgument) {
        self.inside_type += 1;
        syn_verus::visit_mut::visit_generic_method_argument_mut(self, arg);
        self.inside_type -= 1;
    }

    fn visit_item_mod_mut(&mut self, item: &mut ItemMod) {
        let is_external = self.inside_external;
        self.inside_external = attr_is_external(&item.attrs);
        /*if let Some((_, items)) = &mut item.content {
            self.visit_items_prefilter(items);
        }*/
        syn_verus::visit_mut::visit_item_mod_mut(self, item);
        self.inside_external = is_external;
    }

    fn visit_item_impl_mut(&mut self, imp: &mut ItemImpl) {
        let is_external = self.inside_external;
        self.inside_external = attr_is_external(&imp.attrs);
        //self.visit_impl_items_prefilter(&mut imp.items, imp.trait_.is_some());
        syn_verus::visit_mut::visit_item_impl_mut(self, imp);
        self.inside_external = is_external;
    }

    fn visit_item_trait_mut(&mut self, tr: &mut ItemTrait) {
        let is_external = self.inside_external;
        self.inside_external = attr_is_external(&tr.attrs);
        //self.visit_trait_items_prefilter(&mut tr.items);
        syn_verus::visit_mut::visit_item_trait_mut(self, tr);
        self.inside_external = is_external;
    }
}

struct Items {
    items: Vec<Item>,
}

impl Parse for Items {
    fn parse(input: ParseStream) -> syn_verus::parse::Result<Items> {
        let mut items = Vec::new();
        while !input.is_empty() {
            items.push(input.parse()?);
        }
        Ok(Items { items })
    }
}

#[derive(Debug)]
enum MacroElement {
    Comma(Token![,]),
    Semi(Token![;]),
    FatArrow(Token![=>]),
    Expr(Expr),
}

#[derive(Debug)]
enum MacroElementExplicitExpr {
    Comma(Token![,]),
    Semi(Token![;]),
    FatArrow(Token![=>]),
    ExplicitExpr(Token![@], Token![@], Expr),
    TT(TokenTree),
}

#[derive(Debug)]
struct MacroElements {
    elements: Vec<MacroElement>,
}

#[derive(Debug)]
struct MacroElementsExplicitExpr {
    elements: Vec<MacroElementExplicitExpr>,
}

#[derive(Debug)]
enum Delimiter {
    Paren(Paren),
    Bracket(Bracket),
    Brace(Brace),
}

#[derive(Debug)]
struct MacroInvoke {
    path: Path,
    bang: Token![!],
    delimiter: Delimiter,
    elements: MacroElements,
}

#[derive(Debug)]
struct MacroInvokeExplicitExpr {
    path: Path,
    bang: Token![!],
    delimiter: Delimiter,
    elements: MacroElementsExplicitExpr,
}

impl Parse for MacroElement {
    fn parse(input: ParseStream) -> syn_verus::parse::Result<MacroElement> {
        if input.peek(Token![,]) {
            Ok(MacroElement::Comma(input.parse()?))
        } else if input.peek(Token![;]) {
            Ok(MacroElement::Semi(input.parse()?))
        } else if input.peek(Token![=>]) {
            Ok(MacroElement::FatArrow(input.parse()?))
        } else {
            Ok(MacroElement::Expr(input.parse()?))
        }
    }
}

impl Parse for MacroElementExplicitExpr {
    fn parse(input: ParseStream) -> syn_verus::parse::Result<MacroElementExplicitExpr> {
        if input.peek(Token![,]) {
            Ok(MacroElementExplicitExpr::Comma(input.parse()?))
        } else if input.peek(Token![;]) {
            Ok(MacroElementExplicitExpr::Semi(input.parse()?))
        } else if input.peek(Token![=>]) {
            Ok(MacroElementExplicitExpr::FatArrow(input.parse()?))
        } else if input.peek(Token![@]) && input.peek2(Token![@]) {
            let at1 = input.parse()?;
            let at2 = input.parse()?;
            let e = input.parse()?;
            Ok(MacroElementExplicitExpr::ExplicitExpr(at1, at2, e))
        } else {
            Ok(MacroElementExplicitExpr::TT(input.parse()?))
        }
    }
}

impl Parse for MacroElements {
    fn parse(input: ParseStream) -> syn_verus::parse::Result<MacroElements> {
        let mut elements = Vec::new();
        while !input.is_empty() {
            elements.push(input.parse()?);
        }
        Ok(MacroElements { elements })
    }
}

impl Parse for MacroElementsExplicitExpr {
    fn parse(input: ParseStream) -> syn_verus::parse::Result<MacroElementsExplicitExpr> {
        let mut elements = Vec::new();
        while !input.is_empty() {
            elements.push(input.parse()?);
        }
        Ok(MacroElementsExplicitExpr { elements })
    }
}

impl Parse for MacroInvoke {
    fn parse(input: ParseStream) -> syn_verus::parse::Result<MacroInvoke> {
        let path = input.parse()?;
        let bang = input.parse()?;
        let content;
        if input.peek(syn_verus::token::Paren) {
            let paren = parenthesized!(content in input);
            let elements = content.parse()?;
            Ok(MacroInvoke {
                path,
                bang,
                delimiter: Delimiter::Paren(paren),
                elements,
            })
        } else if input.peek(syn_verus::token::Bracket) {
            let bracket = bracketed!(content in input);
            let elements = content.parse()?;
            Ok(MacroInvoke {
                path,
                bang,
                delimiter: Delimiter::Bracket(bracket),
                elements,
            })
        } else {
            let brace = braced!(content in input);
            let elements = content.parse()?;
            Ok(MacroInvoke {
                path,
                bang,
                delimiter: Delimiter::Brace(brace),
                elements,
            })
        }
    }
}

impl Parse for MacroInvokeExplicitExpr {
    fn parse(input: ParseStream) -> syn_verus::parse::Result<MacroInvokeExplicitExpr> {
        let path = input.parse()?;
        let bang = input.parse()?;
        let content;
        if input.peek(syn_verus::token::Paren) {
            let paren = parenthesized!(content in input);
            let elements = content.parse()?;
            Ok(MacroInvokeExplicitExpr {
                path,
                bang,
                delimiter: Delimiter::Paren(paren),
                elements,
            })
        } else if input.peek(syn_verus::token::Bracket) {
            let bracket = bracketed!(content in input);
            let elements = content.parse()?;
            Ok(MacroInvokeExplicitExpr {
                path,
                bang,
                delimiter: Delimiter::Bracket(bracket),
                elements,
            })
        } else {
            let brace = braced!(content in input);
            let elements = content.parse()?;
            Ok(MacroInvokeExplicitExpr {
                path,
                bang,
                delimiter: Delimiter::Brace(brace),
                elements,
            })
        }
    }
}

impl quote::ToTokens for MacroElement {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        match self {
            MacroElement::Comma(e) => e.to_tokens(tokens),
            MacroElement::Semi(e) => e.to_tokens(tokens),
            MacroElement::FatArrow(e) => e.to_tokens(tokens),
            MacroElement::Expr(e) => e.to_tokens(tokens),
        }
    }
}

impl quote::ToTokens for MacroElementExplicitExpr {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        match self {
            MacroElementExplicitExpr::Comma(e) => e.to_tokens(tokens),
            MacroElementExplicitExpr::Semi(e) => e.to_tokens(tokens),
            MacroElementExplicitExpr::FatArrow(e) => e.to_tokens(tokens),
            MacroElementExplicitExpr::ExplicitExpr(_at1, _at2, e) => e.to_tokens(tokens),
            MacroElementExplicitExpr::TT(e) => e.to_tokens(tokens),
        }
    }
}

impl quote::ToTokens for MacroElements {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        for element in &self.elements {
            element.to_tokens(tokens);
        }
    }
}

impl quote::ToTokens for MacroElementsExplicitExpr {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        for element in &self.elements {
            element.to_tokens(tokens);
        }
    }
}

impl quote::ToTokens for MacroInvoke {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        self.path.to_tokens(tokens);
        self.bang.to_tokens(tokens);
        match self.delimiter {
            Delimiter::Paren(d) => {
                d.surround(tokens, |tokens| {
                    self.elements.to_tokens(tokens);
                });
            }
            Delimiter::Bracket(d) => {
                d.surround(tokens, |tokens| {
                    self.elements.to_tokens(tokens);
                });
            }
            Delimiter::Brace(d) => {
                d.surround(tokens, |tokens| {
                    self.elements.to_tokens(tokens);
                });
            }
        }
    }
}

impl quote::ToTokens for MacroInvokeExplicitExpr {
    fn to_tokens(&self, tokens: &mut TokenStream) {
        self.path.to_tokens(tokens);
        self.bang.to_tokens(tokens);
        match self.delimiter {
            Delimiter::Paren(d) => {
                d.surround(tokens, |tokens| {
                    self.elements.to_tokens(tokens);
                });
            }
            Delimiter::Bracket(d) => {
                d.surround(tokens, |tokens| {
                    self.elements.to_tokens(tokens);
                });
            }
            Delimiter::Brace(d) => {
                d.surround(tokens, |tokens| {
                    self.elements.to_tokens(tokens);
                });
            }
        }
    }
}

pub(crate) fn rewrite_items(
    stream: proc_macro::TokenStream,
    erase_ghost: bool,
    use_spec_traits: bool,
    for_non_secret: bool,
    update_conditions: bool,
) -> proc_macro::TokenStream {
    use quote::ToTokens;
    let stream = rejoin_tokens(stream);
    let items: Items = parse_macro_input!(stream as Items);
    let mut new_stream = TokenStream::new();
    let mut visitor = Visitor {
        erase_ghost,
        use_spec_traits,
        inside_ghost: 0,
        inside_type: 0,
        inside_arith: InsideArith::None,
        assign_to: false,
        rustdoc: env_rustdoc(),
        inside_bitvector: false,
        inside_external: false,
        for_non_secret,
        update_conditions,
    };
    for mut item in items.items {
        visitor.visit_item_mut(&mut item);
        visitor.inside_ghost = 0;
        //println!("new_item = {}", quote! {#item});
        //println!("rewrite_items self.inside_ghost = 0");
        visitor.inside_arith = InsideArith::None;
        item.to_tokens(&mut new_stream);
    }
    //println!("new_stream = {}", new_stream);
    proc_macro::TokenStream::from(quote! {verus!{#new_stream}})
}

pub(crate) fn rewrite_expr(
    erase_ghost: bool,
    inside_ghost: bool,
    stream: proc_macro::TokenStream,
    for_non_secret: bool,
) -> proc_macro::TokenStream {
    let stream = rejoin_tokens(stream);
    let mut expr: Expr = parse_macro_input!(stream as Expr);
    let mut new_stream = TokenStream::new();
    let mut visitor = Visitor {
        erase_ghost,
        use_spec_traits: true,
        inside_ghost: if inside_ghost { 1 } else { 0 },
        inside_type: 0,
        inside_arith: InsideArith::None,
        assign_to: false,
        rustdoc: env_rustdoc(),
        inside_bitvector: false,
        inside_external: false,
        for_non_secret,
        update_conditions: true,
    };
    visitor.visit_expr_mut(&mut expr);
    expr.to_tokens(&mut new_stream);
    proc_macro::TokenStream::from(new_stream)
}

pub(crate) fn rewrite_expr_node(
    erase_ghost: bool,
    inside_ghost: bool,
    expr: &mut Expr,
    for_non_secret: bool,
) {
    let mut visitor = Visitor {
        erase_ghost,
        use_spec_traits: true,
        inside_ghost: if inside_ghost { 1 } else { 0 },
        inside_type: 0,
        inside_arith: InsideArith::None,
        assign_to: false,
        rustdoc: env_rustdoc(),
        inside_bitvector: false,
        inside_external: false,
        for_non_secret,
        update_conditions: true,
    };
    visitor.visit_expr_mut(expr);
}

// Unfortunately, the macro_rules tt tokenizer breaks tokens like &&& and ==> into smaller tokens.
// Try to put the original tokens back together here.
fn rejoin_tokens(stream: proc_macro::TokenStream) -> proc_macro::TokenStream {
    use proc_macro::Spacing::*;
    use proc_macro::{Group, Punct, Span, TokenTree};
    let mut tokens: Vec<TokenTree> = stream.into_iter().collect();
    let pun = |t: &TokenTree| match t {
        TokenTree::Punct(p) => Some((p.as_char(), p.spacing(), p.span())),
        _ => None,
    };
    let adjacent = |s1: Span, s2: Span| {
        let l1 = s1.end();
        let l2 = s2.start();
        s1.source_file() == s2.source_file() && l1.eq(&l2)
    };
    for i in 0..(if tokens.len() >= 2 {
        tokens.len() - 2
    } else {
        0
    }) {
        let t0 = pun(&tokens[i]);
        let t1 = pun(&tokens[i + 1]);
        let t2 = pun(&tokens[i + 2]);
        let t3 = if i + 3 < tokens.len() {
            pun(&tokens[i + 3])
        } else {
            None
        };
        match (t0, t1, t2, t3) {
            (
                Some(('<', Joint, _)),
                Some(('=', Alone, s1)),
                Some(('=', Joint, s2)),
                Some(('>', Alone, _)),
            )
            | (Some(('=', Joint, _)), Some(('=', Alone, s1)), Some(('=', Alone, s2)), _)
            | (Some(('!', Joint, _)), Some(('=', Alone, s1)), Some(('=', Alone, s2)), _)
            | (Some(('=', Joint, _)), Some(('=', Alone, s1)), Some(('>', Alone, s2)), _)
            | (Some(('<', Joint, _)), Some(('=', Alone, s1)), Some(('=', Alone, s2)), _)
            | (Some(('&', Joint, _)), Some(('&', Alone, s1)), Some(('&', Alone, s2)), _)
            | (Some(('|', Joint, _)), Some(('|', Alone, s1)), Some(('|', Alone, s2)), _) => {
                if adjacent(s1, s2) {
                    let (op, _, span) = t1.unwrap();
                    let mut punct = Punct::new(op, Joint);
                    punct.set_span(span);
                    tokens[i + 1] = TokenTree::Punct(punct);
                }
            }
            _ => {}
        }
    }
    for tt in &mut tokens {
        match tt {
            TokenTree::Group(group) => {
                let mut new_group = Group::new(group.delimiter(), rejoin_tokens(group.stream()));
                new_group.set_span(group.span());
                *group = new_group;
            }
            _ => {}
        }
    }
    proc_macro::TokenStream::from_iter(tokens.into_iter())
}

pub(crate) fn proof_macro_exprs(
    erase_ghost: bool,
    inside_ghost: bool,
    stream: proc_macro::TokenStream,
    for_non_secret: bool,
) -> proc_macro::TokenStream {
    use quote::ToTokens;
    let stream = rejoin_tokens(stream);
    let mut invoke: MacroInvoke = parse_macro_input!(stream as MacroInvoke);
    let mut new_stream = TokenStream::new();
    let mut visitor = Visitor {
        erase_ghost,
        use_spec_traits: true,
        inside_ghost: if inside_ghost { 1 } else { 0 },
        inside_type: 0,
        inside_arith: InsideArith::None,
        assign_to: false,
        rustdoc: env_rustdoc(),
        inside_bitvector: false,
        inside_external: false,
        for_non_secret,
        update_conditions: true,
    };
    for element in &mut invoke.elements.elements {
        match element {
            MacroElement::Expr(expr) => visitor.visit_expr_mut(expr),
            _ => {}
        }
    }
    invoke.to_tokens(&mut new_stream);
    proc_macro::TokenStream::from(new_stream)
}

pub(crate) fn inv_macro_exprs(
    erase_ghost: bool,
    stream: proc_macro::TokenStream,
    for_non_secret: bool,
) -> proc_macro::TokenStream {
    use quote::ToTokens;
    let stream = rejoin_tokens(stream);
    let mut invoke: MacroInvoke = parse_macro_input!(stream as MacroInvoke);
    let mut new_stream = TokenStream::new();
    let mut visitor = Visitor {
        erase_ghost,
        use_spec_traits: true,
        inside_ghost: 1,
        inside_type: 0,
        inside_arith: InsideArith::None,
        assign_to: false,
        rustdoc: env_rustdoc(),
        inside_bitvector: false,
        inside_external: false,
        for_non_secret,
        update_conditions: true,
    };
    for element in &mut invoke.elements.elements {
        match element {
            MacroElement::Expr(expr) => visitor.visit_expr_mut(expr),
            _ => {}
        }
        // After the first element, parse as 'exec' expression
        visitor.inside_ghost = 0;
    }
    invoke.to_tokens(&mut new_stream);
    proc_macro::TokenStream::from(new_stream)
}

pub(crate) fn proof_macro_explicit_exprs(
    erase_ghost: bool,
    inside_ghost: bool,
    stream: proc_macro::TokenStream,
    for_non_secret: bool,
) -> proc_macro::TokenStream {
    use quote::ToTokens;
    let stream = rejoin_tokens(stream);
    let mut invoke: MacroInvokeExplicitExpr = parse_macro_input!(stream as MacroInvokeExplicitExpr);
    let mut new_stream = TokenStream::new();
    let mut visitor = Visitor {
        erase_ghost,
        use_spec_traits: true,
        inside_ghost: if inside_ghost { 1 } else { 0 },
        inside_type: 0,
        inside_arith: InsideArith::None,
        assign_to: false,
        rustdoc: env_rustdoc(),
        inside_bitvector: false,
        inside_external: false,
        for_non_secret,
        update_conditions: true,
    };
    for element in &mut invoke.elements.elements {
        match element {
            MacroElementExplicitExpr::ExplicitExpr(_at1, _at2, expr) => {
                visitor.visit_expr_mut(expr)
            }
            _ => {}
        }
    }
    invoke.to_tokens(&mut new_stream);
    proc_macro::TokenStream::from(new_stream)
}

/// Constructs #[name(tokens)]
fn mk_rust_attr(span: Span, name: &str, tokens: TokenStream) -> Attribute {
    let mut path_segments = Punctuated::new();
    path_segments.push(PathSegment {
        ident: Ident::new(name, span),
        arguments: PathArguments::None,
    });
    Attribute {
        pound_token: token::Pound { spans: [span] },
        style: AttrStyle::Outer,
        bracket_token: token::Bracket { span },
        path: Path {
            leading_colon: None,
            segments: path_segments,
        },
        tokens: quote! { (#tokens) },
    }
}

/// Constructs #[verus::internal(tokens)]
fn mk_verus_attr(span: Span, tokens: TokenStream) -> Attribute {
    let mut path_segments = Punctuated::new();
    path_segments.push(PathSegment {
        ident: Ident::new("verus", span),
        arguments: PathArguments::None,
    });
    path_segments.push(PathSegment {
        ident: Ident::new("internal", span),
        arguments: PathArguments::None,
    });
    Attribute {
        pound_token: token::Pound { spans: [span] },
        style: AttrStyle::Outer,
        bracket_token: token::Bracket { span },
        path: Path {
            leading_colon: None,
            segments: path_segments,
        },
        tokens: quote! { (#tokens) },
    }
}

fn struct_data_mode_attrs(mode: &DataMode, inside_external: bool) -> Vec<Attribute> {
    let tk = if inside_external {
        quote! { ExecStruct, NotPrimitive }
    } else {
        quote! { ExecStruct, NotPrimitive, VTypeCastSec, SpecSize, SpecOffset, WellFormed, IsConstant }
    };
    let ret = vec![mk_rust_attr(mode.span(), "derive", tk)];
    match mode {
        DataMode::Default => ret,
        DataMode::Exec(_token) => ret,
        _ => {
            vec![]
        }
    }
}

fn attr_is_external(attrs: &Vec<Attribute>) -> bool {
    for a in attrs {
        if a.tokens.to_string() == "(external_body)"
            || a.tokens.to_string() == "(external)"
            || a.tokens.to_string() == "(external_fn_specification)"
        {
            return true;
        }
    }
    false
}

fn attr_is_call_external(attrs: &Vec<Attribute>) -> bool {
    for a in attrs {
        if a.tokens.to_string() == "(external_fn_specification)" {
            //println!("external_fn_specification");
            return true;
        }
    }
    false
}

impl Visitor {
    fn replace_stype(&self, ty: &mut Type, must_replace: bool) {
        let span = ty.span();

        match ty {
            Type::Array(TypeArray {
                bracket_token: _,
                elem,
                semi_token: _,
                len,
            }) => {
                if !self.inside_external || must_replace {
                    *ty = Type::Verbatim(quote_spanned! { span =>
                        Array<#elem, #len>
                    });
                } else {
                    *ty = Type::Verbatim(quote_spanned! { span =>
                        [#elem; #len]
                    });
                }
            }
            Type::Path(patht)
                if (!self.inside_external && self.inside_ghost == 0 && !self.inside_bitvector)
                    || must_replace =>
            {
                let tpath = &patht.path;
                if path_is_ident(tpath, "u64") {
                    ////println!("{:?}", self);
                    *ty = Type::Verbatim(quote_spanned! { span =>
                        u64_s
                    });
                } else if path_is_ident(tpath, "u32") {
                    *ty = Type::Verbatim(quote_spanned! { span =>
                        u32_s
                    });
                } else if path_is_ident(tpath, "u16") {
                    *ty = Type::Verbatim(quote_spanned! { span =>
                        u16_s
                    });
                } else if path_is_ident(tpath, "u8") {
                    *ty = Type::Verbatim(quote_spanned! { span =>
                        u8_s
                    });
                } else if path_is_ident(tpath, "usize") {
                    ////println!("{:?}", self);
                    *ty = Type::Verbatim(quote_spanned! { span =>
                        usize_s
                    });
                }
            }
            _ => {}
        }
    }
}

fn param_list(
    pat: &Pat,
    ty: &Type,
    is_mut: bool,
    ret: &mut Vec<(Option<Expr>, Option<Expr>)>,
    generics: &Generics,
    is_param: bool,
) {
    let prefix = if is_mut {
        quote! {*old}
    } else {
        quote! {}
    };
    match &ty {
        Type::Array(_) | Type::Path(_) | Type::Slice(_) => {
            if is_ghost_or_tracked_type(&ty) {
                return;
            }
            if is_generic(&ty, generics) {
                return;
            }
            if is_param {
                ret.push((Some(Expr::Verbatim(quote! {(#prefix(#pat))})), None));
            } else {
                ret.push((None, Some(Expr::Verbatim(quote! {(#prefix(#pat))}))));
            }
        }
        Type::BareFn(_) => {}
        Type::Group(_) => todo!(),
        Type::ImplTrait(_) => todo!(),
        Type::Infer(_) => todo!(),
        Type::Macro(_) => todo!(),
        Type::Never(_) => {}
        Type::Paren(_) => todo!(),
        Type::Reference(r) => {
            let ty: &Box<Type> = &r.elem;
            param_list(&pat, &ty, r.mutability.is_some(), ret, generics, is_param);
        }
        Type::TraitObject(_) => todo!(),
        Type::Tuple(tup) => {
            let mut i = 0;
            for ty in &tup.elems {
                let index = LitInt::new(format! {"{}", i}.as_str(), tup.span());
                let p = Pat::Verbatim(quote! {(#prefix(#pat)).#index});
                param_list(&p, &ty, false, ret, generics, is_param);
                i = i + 1;
            }
        }
        Type::Verbatim(_) => {}
        Type::FnSpec(_) => {}
        _ => todo!(),
    }
}

fn is_exe(sig: &Signature) -> bool {
    match sig.mode {
        FnMode::Spec(_) | FnMode::SpecChecked(_) | FnMode::Proof(_) => false,
        FnMode::Exec(_) => true,
        FnMode::Default => true,
    }
}

pub fn is_ghost_or_tracked_type(ty: &Type) -> bool {
    if let Type::Path(tpath) = ty {
        let path_segments = tpath
            .path
            .segments
            .iter()
            .map(|segment| segment.ident.to_string())
            .collect::<Vec<_>>();
        if path_segments.last() == Some(&"Ghost".to_string()) {
            true
        } else if path_segments.last() == Some(&"Tracked".to_string()) {
            true
        } else {
            false
        }
    } else {
        false
    }
}

pub fn is_generic(ty: &Type, generics: &Generics) -> bool {
    for generic in &generics.params {
        match generic {
            syn_verus::GenericParam::Type(typaram) => {
                if let Type::Path(tpath) = ty {
                    if let Some(name) = tpath.path.segments.last() {
                        if name.ident.to_string() == typaram.ident.to_string() {
                            return true;
                        }
                    }
                }
            }
            syn_verus::GenericParam::Lifetime(_) => {}
            syn_verus::GenericParam::Const(_) => {}
        }
    }
    return false;
}

================
File: ./source/verismo/igvm.sh
================

#!/bin/sh
"python3" "/home/ziqiaozhou/verismo/source/verismo/../../igvm/igvm/igvmgen.py" "-k" "/home/ziqiaozhou/verismo/source/target/target/release/build/verismo-abee417d9afd8f47/out/../../../monitor" "-o" "/home/ziqiaozhou/verismo/source/target/target/release/build/verismo-abee417d9afd8f47/out/../../../verismo-rust.bin" "-vtl=2" "-append" "root=/dev/sda rw debugpat" "-inform" "verismo" "-boot_mode" "x64" "-pgtable_level" "4" "-vmpl2_kernel" "/root/snp/out/vmpl2/sm/arch/x86/boot/bzImage"

================
File: ./source/verismo/Cargo.toml
================

[package]
name = "verismo"
version = "0.1.0"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
# Verus deps
builtin = { path = "../../tools/verus/source/builtin"}
builtin_macros = { path = "../../tools/verus/source/builtin_macros"}
vstd = {path = "../vstd"}
# Trusted
hacl-sys = {path = "../../deps/hacl-packages/rust/hacl-sys", features = []}
# Useful macros
verismo_macro = {path = "../verismo_macro"}
verismo_verus = {path = "../verismo_verus"}
paste = "1.0"
seq-macro = "0.3"
vops = {path = "../vops"}

#memoffset = "0.8.0"

[features]
default = ["non_std", "no_global_allocator", "alloc", "test", "debug"]
non_std = []
no_global_allocator = []
test = []
debug = []
noverify = []
verifymodule = []
alloc = []

[package.metadata.rust-analyzer]
rustc_private=true
================
File: ./source/verismo/build.rs
================

use std::env;
use std::fs::File;
use std::io::Write;
use std::process::Command;

fn main() {
    // Environment vars during build.
    if cfg!(feature = "noverify") {
        println!("cargo:rustc-env=VERUS_ARGS=--no-verify");
    } else {
        println!("cargo:rustc-env=VERUS_ARGS=--rlimit=8000 --expand-errors --multiple-errors=5 --triggers-silent --time-expanded --no-auto-recommends-check --output-json --trace");
    }

    let module_path = env::var("CARGO_MANIFEST_DIR").unwrap();
    println!("cargo:rustc-env=MODULE_PATH={}", module_path);

    println!("cargo:rerun-if-changed=build.rs");
    println!("cargo:rerun-if-env-changed=MODULE");
}

================
File: ./source/verismo/src/addr_e/def_e.rs
================

use super::*;
use crate::debug::VPrint;

verus! {

pub open spec fn VM_MEM_RANGE() -> (int, nat) {
    (0, VM_MEM_SIZE as nat)
}

} // verus!
verismo! {
pub type VPage = usize;
pub type PPage = usize;
pub type VAddr = usize;
pub type PAddr = usize;

pub type OnePage = [u8; 0x1000];
}

================
File: ./source/verismo/src/addr_e/exe.rs
================

use super::*;
use crate::tspec::*;

verus! {

#[inline]
pub fn page_align_up(value: usize_t) -> (ret: usize_t)
    requires
        value <= VM_MEM_SIZE,
        value.is_constant(),
    ensures
        ret as int % PAGE_SIZE!() == 0,
        ret == spec_align_up(value as int, PAGE_SIZE!()),
        value as int <= ret as int,
        (ret as int) < (value as int) + PAGE_SIZE!(),
        ret.is_constant() == value.is_constant(),
{
    proof {
        bit_shl64_pow2_auto();
    }
    align_up_by(value as u64, PAGE_SIZE as u64) as usize
}

} // verus!
verismo_simple! {
    #[inline]
    pub fn page_align_down(value: usize_s) -> (ret: usize_s)
    requires
        value.wf(),
    ensures
        ret as int % PAGE_SIZE!() == 0,
        ret == spec_align_down(value as int, PAGE_SIZE!()),
        (value as int) - PAGE_SIZE!() <= ret as int,
        ret  as int <= value as int,
        value.is_constant() ==> ret.is_constant(),
    {
        proof {
            bit_shl64_pow2_auto();
        }
        let v: u64 = value.into();
        assert(v.wf());
        (align_down_by(v, PAGE_SIZE as u64) as usize)
    }
}

================
File: ./source/verismo/src/addr_e/addr_interface.rs
================

use super::*;

verus! {

pub const INVALID_ADDR: usize = VM_MEM_SIZE + 1;

pub open spec fn spec_pa_to_va(pa: int) -> int {
    pa
}

pub open spec fn spec_va_to_pa(pa: int) -> int {
    pa
}

pub open spec fn spec_pn_to_vn(pn: int) -> int {
    pn
}

pub open spec fn spec_vn_to_pn(vn: int) -> int {
    vn
}

} // verus!
verismo_simple! {
pub trait SpecAddrTrait {
    spec fn spec_valid_addr_with(&self, size: nat) -> bool;
    spec fn to_page(&self) -> int;
    spec fn is_page_aligned(&self) -> bool;
}

pub trait SpecPageTrait {
    spec fn spec_valid_pn_with(&self, size: nat) -> bool;
    spec fn to_addr(&self) -> int;
}

pub trait AddrTrait<PageT> {
    spec fn spec_to_page(&self) -> PageT;
    spec fn addrt_to_int(v: PageT) -> int;
    spec fn spec_ensures_to_page(&self, ret: PageT) -> bool;
    spec fn spec_valid_addr_with(&self, size: nat) -> bool;
    spec fn spec_check_valid_addr_requires(&self, size: PageT) -> bool;

    fn check_valid_addr(&self, size: PageT) -> (ret: bool)
    requires
        self.spec_check_valid_addr_requires(size),
    ensures
        ret == self.spec_valid_addr_with(Self::addrt_to_int(size) as nat);

    fn to_page(&self) -> (ret: PageT)
    requires
        self.spec_valid_addr_with(0)
    ensures
        ret === self.spec_to_page(),
        self.spec_ensures_to_page(ret);
}

pub trait PageTrait<AddrT> {
    spec fn spec_to_addr(&self) -> AddrT;
    spec fn spec_ensures_to_addr(&self, ret: AddrT) -> bool;
    spec fn spec_valid_pn_with(&self, size: nat) -> bool;
    spec fn paget_to_int(v: AddrT) -> int;
    spec fn spec_check_valid_pn_requires(&self, size: AddrT) -> bool;

    fn check_valid_pn(&self, size: AddrT) -> (ret: bool)
    requires
        self.spec_check_valid_pn_requires(size),
    ensures
        ret == self.spec_valid_pn_with(Self::paget_to_int(size) as nat);

    fn to_addr(&self) -> (ret: AddrT)
    requires
        self.spec_valid_pn_with(0)
    ensures
        ret === self.spec_to_addr(),
        self.spec_ensures_to_addr(ret);
}
}

#[macro_export]
macro_rules! impl_addr_interface {
    ($basetype: ty) => {
        verus! {
        impl AddrTrait<$basetype> for $basetype {
            open spec fn spec_to_page(&self) -> $basetype {
                let s: $basetype = PAGE_SIZE.vspec_cast_to();
                *self / s
            }

            open spec fn addrt_to_int(v: $basetype) -> int {
                v.vspec_cast_to()
            }

            open spec fn spec_ensures_to_page(&self, ret: $basetype) -> bool {
                &&& ret.wf()
                &&& self.is_constant() ==> ret.is_constant()
                &&& Self::addrt_to_int(ret) ==  Self::addrt_to_int(*self) / PAGE_SIZE!()
            }

            open spec fn spec_valid_addr_with(&self, size: nat) -> bool {
                let start: int = (*self).vspec_cast_to();
                &&& start.spec_valid_addr_with(size)
                &&& self.wf()
            }

            open spec fn spec_check_valid_addr_requires(&self, size: $basetype) -> bool{
                &&& self.is_constant()
                &&& size.is_constant()
            }

            fn check_valid_addr(&self, size: $basetype) -> (ret: bool)
            {
                let addr: usize = (*self) as usize;
                let size: usize = size as usize;
                (size <= VM_MEM_SIZE) && (addr <= VM_MEM_SIZE - size)
            }

            fn to_page(&self) -> (ret: $basetype) {
                *self / (PAGE_SIZE as $basetype)
            }
        }
        }
    };
}

#[macro_export]
macro_rules! impl_page_interface {
    ($basetype: ty) => {
        verus! {
        impl PageTrait<$basetype> for $basetype {
            open spec fn spec_to_addr(&self) -> $basetype {
                (*self * (PAGE_SIZE as $basetype)).vspec_cast_to()
            }

            open spec fn paget_to_int(v: $basetype) -> int {
                v.vspec_cast_to()
            }

            open spec fn spec_ensures_to_addr(&self, ret: $basetype) -> bool {
                &&& ret.wf()
                &&& self.is_constant() ==> ret.is_constant()
                &&& Self::paget_to_int(ret) ==  Self::paget_to_int(*self) * PAGE_SIZE!()
            }

            open spec fn spec_valid_pn_with(&self, size: nat) -> bool {
                let start: int = (*self).vspec_cast_to();
                &&& start.spec_valid_pn_with(size)
                &&& self.wf()
            }

            open spec fn spec_check_valid_pn_requires(&self, size: $basetype) -> bool {
                &&& self.is_constant()
                &&& size.is_constant()
            }

            fn check_valid_pn(&self, size: $basetype) -> (ret: bool) {
                let addr: usize = (*self) as usize;
                let size: usize = size as usize;
                (size <= VM_PAGE_NUM) && (addr <= VM_PAGE_NUM - size)
            }

            fn to_addr(&self) -> (ret: $basetype)
            {
                (*self) * 0x1000
            }
        }
        }
    };
}

#[macro_export]
macro_rules! impl_addr_safe_interface {
    ($basetype: ty) => {
        verus! {
        impl AddrTrait<$basetype> for $basetype {
            open spec fn spec_to_page(&self) -> $basetype {
                let s: $basetype = PAGE_SIZE.vspec_cast_to();
                *self / s
            }

            open spec fn addrt_to_int(v: $basetype) -> int {
                v.vspec_cast_to()
            }

            open spec fn spec_ensures_to_page(&self, ret: $basetype) -> bool {
                &&& ret.wf()
                &&& self.is_constant() ==> ret.is_constant()
                &&& Self::addrt_to_int(ret) ==  Self::addrt_to_int(*self) / PAGE_SIZE!()
            }

            open spec fn spec_valid_addr_with(&self, size: nat) -> bool {
                let start: int = (*self).vspec_cast_to();
                &&& start.spec_valid_addr_with(size)
                &&& self.wf()
            }

            open spec fn spec_check_valid_addr_requires(&self, size: $basetype) -> bool{
                &&& self.is_constant()
                &&& size.is_constant()
            }

            fn check_valid_addr(&self, size: $basetype) -> (ret: bool)
            {
                let addr: usize = (*self).into();
                let size: usize = size.into();
                (size <= VM_MEM_SIZE) && (addr <= VM_MEM_SIZE - size)
            }

            fn to_page(&self) -> (ret: $basetype) {
                let s: $basetype = PAGE_SIZE.into();
                (*self).div(s)
            }
        }
        }
    };
}

#[macro_export]
macro_rules! impl_page_safe_interface {
    ($basetype: ty, $ptype: ty) => {
        verus! {
        impl PageTrait<$basetype> for $basetype {
            open spec fn spec_to_addr(&self) -> $basetype {
                (*self * (PAGE_SIZE).vspec_cast_to()).vspec_cast_to()
            }

            open spec fn paget_to_int(v: $basetype) -> int {
                v.vspec_cast_to()
            }

            open spec fn spec_ensures_to_addr(&self, ret: $basetype) -> bool {
                &&& ret.wf()
                &&& self.is_constant() ==> ret.is_constant()
                &&& Self::paget_to_int(ret) ==  Self::paget_to_int(*self) * PAGE_SIZE!()
            }

            open spec fn spec_valid_pn_with(&self, size: nat) -> bool {
                let start: int = (*self).vspec_cast_to();
                &&& start.spec_valid_pn_with(size)
                &&& self.wf()
            }

            open spec fn spec_check_valid_pn_requires(&self, size: $basetype) -> bool {
                &&& self.is_constant()
                &&& size.is_constant()
            }

            fn check_valid_pn(&self, size: $basetype) -> (ret: bool) {
                let addr: usize = (*self).into();
                let size: usize = size.into();
                (size <= VM_PAGE_NUM) && (addr <= VM_PAGE_NUM - size)
            }

            fn to_addr(&self) -> (ret: $basetype)
            {
                (*self).mul(PAGE_SIZE as $ptype)
            }
        }
        }
    };
}

verus! {

impl SpecPageTrait for int {
    open spec fn spec_valid_pn_with(&self, size: nat) -> bool {
        &&& 0 <= *self as int + (size as int) <= VM_PAGE_NUM
        &&& 0 <= *self as int <= VM_PAGE_NUM
    }

    open spec fn to_addr(&self) -> int {
        *self * (PAGE_SIZE!())
    }
}

impl SpecAddrTrait for int {
    open spec fn spec_valid_addr_with(&self, size: nat) -> bool {
        &&& 0 <= *self as int + (size as int) <= VM_MEM_SIZE
        &&& 0 <= *self as int <= VM_MEM_SIZE
    }

    open spec fn to_page(&self) -> int {
        *self / (PAGE_SIZE!())
    }

    open spec fn is_page_aligned(&self) -> bool {
        *self === self.to_page().to_addr()
    }
}

} // verus!
impl_addr_interface! {u64_t}
impl_page_interface! {u64_t}
impl_addr_interface! {usize_t}
impl_page_interface! {usize_t}

impl_addr_safe_interface! {usize_s}
impl_page_safe_interface! {usize_s, usize_t}

impl_addr_safe_interface! {u64_s}
impl_page_safe_interface! {u64_s, u64_t}

================
File: ./source/verismo/src/addr_e/mod.rs
================

mod addr_interface;
mod def_e;
mod exe;
mod range_interface;

pub use addr_interface::*;
pub use def_e::*;
pub use exe::{page_align_down, page_align_up};
pub use range_interface::*;

use super::*;
use crate::arch::addr_s::*;
use crate::tspec::*;
use crate::tspec_e::*;

================
File: ./source/verismo/src/addr_e/range_interface.rs
================

use super::*;

verismo_simple! {

pub open spec fn spec_valid_range(real: (usize_s, usize_s), max: usize_s) -> (usize_s, usize_s) {
    let (start, size) = real;
    let valid_start = if start < max {
        start
    } else {
        max
    };

    let valid_size = if max <= start {
        usize_s::spec_constant(0)
    } else if size < (max - start) as usize_s {
        size
    } else {
        (max - start) as usize_s
    };
    (valid_start, valid_size)
}

pub trait MemRangeInterface {
    spec fn self_wf(&self) -> bool;

    spec fn real_wf(r: (usize_s, usize_s)) -> bool;

    proof fn proof_constant_real_wf(r: (usize_s, usize_s))
    ensures
        r.is_constant() ==> Self::real_wf(r);

    fn real_range(&self) -> (ret: (usize_s, usize_s))
    requires
        self.self_wf(),
    ensures
        ret === self.spec_real_range(),
        Self::real_wf(ret);

    fn end_max() -> (ret: usize_s)
    ensures
        ret == Self::spec_end_max(),
        ret.is_constant();

    spec fn spec_end_max() -> usize_t;

    spec fn spec_real_range(&self) -> (usize_s, usize_s);

    spec fn spec_set_range(self, r: (usize_s, usize_s)) -> Self where Self: core::marker::Sized;

    fn update_range(&mut self, r: (usize_s, usize_s)) where Self: core::marker::Sized
    requires
        Self::real_wf(r),
        old(self).self_wf(),
        r.0.wf(),
        r.1.wf(),
    ensures
        self.self_wf(),
        self.spec_real_range() === r,
        (*old(self)).spec_set_range(r) === *self;
}

impl MemRangeInterface for (usize_s, usize_s) {
    #[verifier(inline)]
    open spec fn self_wf(&self) -> bool {
        &&& self.0.wf()
        &&& self.1.wf()
    }

    open spec fn real_wf(r: (usize_s, usize_s)) -> bool {
        r.self_wf()
    }

    proof fn proof_constant_real_wf(r: (usize_s, usize_s)) {}

    #[inline]
    fn real_range(&self) -> (ret: (usize_s, usize_s))
    {
        *self
    }

    #[inline]
    fn end_max() -> (ret: usize_s)
    {
        VM_MEM_SIZE as usize_s
    }

    #[verifier(inline)]
    open spec fn spec_end_max() -> usize_t {
        VM_MEM_SIZE as usize_t
    }

    #[verifier(inline)]
    open spec fn spec_real_range(&self) -> (usize_s, usize_s) {
        *self
    }

    #[verifier(inline)]
    open spec fn spec_set_range(self, r: (usize_s, usize_s)) -> Self {
        r
    }

    #[inline]
    fn update_range(&mut self, r: (usize_s, usize_s))
    {
        *self = r;
    }
}

impl MemRangeInterface for (usize_t, usize_t) {
    #[verifier(inline)]
    open spec fn self_wf(&self) -> bool {
        true
    }

    open spec fn real_wf(r: (usize_s, usize_s)) -> bool {
        r.is_constant()
    }

    proof fn proof_constant_real_wf(r: (usize_s, usize_s)) {}

    #[inline]
    fn real_range(&self) -> (ret: (usize_s, usize_s))
    {
        (self.0.into(), self.1.into())
    }

    #[inline]
    fn end_max() -> (ret: usize_s)
    {
        VM_MEM_SIZE as usize_s
    }

    #[verifier(inline)]
    open spec fn spec_end_max() -> usize_t {
        VM_MEM_SIZE as usize_t
    }

    #[verifier(inline)]
    open spec fn spec_real_range(&self) -> (usize_s, usize_s) {
        (self.0.vspec_cast_to(), self.1.vspec_cast_to())
    }

    #[verifier(inline)]
    open spec fn spec_set_range(self, r: (usize_s, usize_s)) -> Self {
        (r.0.vspec_cast_to(), r.1.vspec_cast_to())
    }

    #[inline]
    fn update_range(&mut self, r: (usize_s, usize_s))
    {
        *self = (r.0.into(), r.1.into());
        proof{
            r.0@.proof_constant();
            r.1@.proof_constant();
            assert(self.spec_real_range().0@ === r.0@);
            assert(self.spec_real_range().1@ === r.1@);
        }
    }
}

pub open spec fn range_speclt<T: MemRangeInterface>() -> spec_fn(T, T) -> bool {
    |v1: T, v2: T|
    v1.spec_real_range().0 < v2.spec_real_range().0 ||
        ((v1.spec_real_range().0 == v2.spec_real_range().0 &&
        v1.spec_real_range().1 < v2.spec_real_range().1))
}

pub trait GeneratedMemRangeInterface {
    spec fn spec_max() -> nat;

    spec fn spec_sec_max() -> usize_s;

    spec fn spec_range(&self) -> (int, nat);

    spec fn spec_aligned_range(&self) -> (int, nat);

    spec fn spec_valid_range(&self) -> (usize_s, usize_s);

    spec fn spec_valid_end(&self) -> usize_s;

    spec fn spec_start(&self) -> int;

    spec fn spec_end(&self) -> int;

    spec fn spec_cmp_max_requires(&self) -> bool;

    spec fn spec_lt_requires(&self, other: &Self) -> bool;

    spec fn spec_check_disjoint_requires(&self, other: &Self) -> bool;

    spec fn wf_range(&self) -> bool;

    spec fn speclt() -> spec_fn(Self, Self) -> bool where Self: core::marker::Sized;

    fn less(&self, other: &Self) -> (ret: bool) where Self: core::marker::Sized
    requires
        self.spec_lt_requires(other)
    ensures
        ret == Self::speclt()(*self, *other);

    fn start(&self) -> (ret: usize_s)
    requires
        self.spec_cmp_max_requires()
    ensures
        ret == self.spec_range().0,
        ret.is_constant(),
        ret <= Self::spec_max(),
        ret.wf();

    fn end(&self) -> (ret: usize_s)
    requires
        self.spec_cmp_max_requires()
    ensures
        ret == self.spec_range().end(),
        ret.is_constant(),
        ret <= Self::spec_max(),
        ret.wf();

    fn aligned_start(&self) -> (ret: usize_s)
    requires
        self.spec_cmp_max_requires()
    ensures
        ret == self.spec_aligned_range().0,
        ret.is_constant(),
        ret <= Self::spec_max(),
        ret as int % PAGE_SIZE!() == 0,
        ret.wf();

    fn size(&self) -> (ret: usize_s)
    requires
        self.spec_cmp_max_requires()
    ensures
        ret == self.spec_range().1,
        ret.is_constant(),
        ret <= Self::spec_max(),
        ret.wf();

    fn check_disjoint(&self, other: &Self) -> (ret: bool)
    requires
        self.spec_check_disjoint_requires(other),
        self.spec_cmp_max_requires(),
        other.spec_cmp_max_requires(),
    ensures
        ret === range_disjoint_(self.spec_range(), other.spec_range());

    fn check_inside(&self, other: &Self) -> (ret: bool)
    requires
        self.spec_check_disjoint_requires(other),
        self.spec_cmp_max_requires(),
        other.spec_cmp_max_requires(),
    ensures
        ret === inside_range(self.spec_range(), other.spec_range());
}

impl<T: MemRangeInterface> GeneratedMemRangeInterface for T {
    open spec fn spec_max() -> nat {
        Self::spec_end_max() as nat
    }

    open spec fn spec_sec_max() -> usize_s {
        usize_s::spec_constant(Self::spec_end_max())
    }

    open spec fn spec_cmp_max_requires(&self) -> bool {
        &&& self.spec_real_range().0.is_constant()
        &&& self.spec_real_range().1.is_constant()
        &&& Self::spec_sec_max().is_constant()
        &&& self.self_wf()
    }

    #[verifier(inline)]
    open spec fn speclt() -> spec_fn(Self, Self) -> bool {
        range_speclt::<T>()
    }

    open spec fn spec_lt_requires(&self, other: &Self) -> bool {
        &&& self.spec_real_range().0.is_constant()
        &&& self.spec_real_range().1.is_constant()
        &&& other.spec_real_range().0.is_constant()
        &&& other.spec_real_range().1.is_constant()
        &&& Self::spec_sec_max().is_constant()
        &&& self.self_wf()
        &&& other.self_wf()
    }

    open spec fn spec_check_disjoint_requires(&self, other: &Self) -> bool {
        &&& self.spec_real_range().0.is_constant()
        &&& self.spec_real_range().1.is_constant()
        &&& other.spec_real_range().0.is_constant()
        &&& other.spec_real_range().1.is_constant()
        &&& Self::spec_sec_max().is_constant()
        &&& self.self_wf()
        &&& other.self_wf()
    }

    open spec fn spec_range(&self) -> (int, nat) {
        (self.spec_valid_range().0 as int, self.spec_valid_range().1 as nat)
    }

    open spec fn spec_aligned_range(&self) -> (int, nat) {
        let start = self.spec_range().0;
        let end  = self.spec_range().end();
        range(spec_align_down(start, PAGE_SIZE!()), spec_align_up(end, PAGE_SIZE!()))
    }

    open spec fn spec_valid_range(&self) -> (usize_s, usize_s) {
        spec_valid_range(self.spec_real_range(), Self::spec_sec_max())
    }

    open spec fn spec_valid_end(&self) -> usize_s {
        (self.spec_valid_range().0 + self.spec_valid_range().1) as usize_s
    }

    open spec fn wf_range(&self) -> bool {
        &&& self.spec_valid_range().0 == self.spec_real_range().0
        &&& self.spec_valid_range().1 == self.spec_real_range().1
        &&& self.spec_valid_range().1 != 0
        &&& self.spec_real_range().0.is_constant()
        &&& self.spec_real_range().1.is_constant()
        &&& self.spec_range().end() <= Self::spec_max()
    }

    open spec fn spec_start(&self) -> int {
        self.spec_range().0
    }

    open spec fn spec_end(&self) -> int {
        self.spec_range().end()
    }

    fn less(&self, other: &Self) -> bool {
        assert(self.spec_real_range().0.is_constant());
        assert(other.spec_real_range().0.is_constant());
        assert(self.spec_real_range().1.is_constant());
        assert(other.spec_real_range().1.is_constant());
        if self.real_range().0 < other.real_range().0 {
            true
        } else {
            if self.real_range().0 == other.real_range().0 {
                self.real_range().1 < other.real_range().1
            } else {
                false
            }
        }
    }

    fn start(&self) -> (ret: usize_s)
    {
        assert(self.spec_real_range().0.is_constant());
        if self.real_range().0 < Self::end_max() {
            self.real_range().0
        } else {
            Self::end_max()
        }
    }

    fn size(&self) -> (ret: usize_s)
    {
        let (start, size) = self.real_range();
        assert(start.is_constant());
        assert(size.is_constant());
        if !(start < Self::end_max()) {
            0
        } else if size < Self::end_max() - start {
            size
        } else {
            Self::end_max() - start
        }
    }

    #[inline]
    fn end(&self) -> (ret: usize_s)
    {
        self.start() + self.size()
    }

    fn aligned_start(&self) -> (ret: usize_s)
    {
        page_align_down(self.start())
    }

    fn check_disjoint(&self, other: &Self) -> (ret: bool)
    {
        assert(self.spec_real_range().0.is_constant());
        assert(other.spec_real_range().0.is_constant());
        assert(self.spec_real_range().1.is_constant());
        assert(other.spec_real_range().1.is_constant());
        !(self.start() < other.end()) || !(other.start() < self.end()) || self.size() == 0 || other.size() == 0
    }

    fn check_inside(&self, other: &Self) -> bool {
        &&& other.start() <= self.start()
        &&& self.start() < other.end()
        &&& self.end() <= other.end()
    }
}
}

verismo_simple! {
pub open spec fn to_range_fn<T: MemRangeInterface>() -> spec_fn(T) -> (int, nat) {
    |v: T| v.spec_range()
}

pub open spec fn to_page_aligned_range_fn() -> spec_fn((int, nat)) -> (int, nat) {
    |v: (int, nat)|
        range(spec_align_down(v.0, PAGE_SIZE!()), spec_align_up(v.end(), PAGE_SIZE!()))
}

pub trait MemRangeSeqInterface {
    spec fn to_valid_ranges(&self) -> Set<(int, nat)>;
    spec fn to_aligned_ranges(&self) -> Set<(int, nat)>;
    spec fn to_range_seq(&self) -> Seq<(int, nat)>;
    spec fn to_aligned_range_seq(&self) -> Seq<(int, nat)>;
    spec fn to_valid_ranges_internal(&self) -> Set<(int, nat)>;
    spec fn to_aligned_ranges_internal(&self) -> Set<(int, nat)>;
    spec fn to_aligned_ranges_internal2(&self) -> Set<(int, nat)>;
    spec fn has_aligned_ranges_internal(&self, r: (int, nat)) -> bool;
    proof fn lemma_align_ranges_reveal(&self)
    ensures
        self.to_aligned_ranges_internal() =~~= self.to_aligned_ranges(),
        self.to_aligned_ranges_internal2() =~~= self.to_aligned_ranges();

    proof fn lemma_valid_ranges_reveal(&self)
    ensures
        self.to_valid_ranges_internal() =~~= self.to_valid_ranges();
}

pub open spec fn empty_ranges() -> Set<(int, nat)> {
    Set::new(|r: (int, nat)| r.1 == 0)
}

impl<T: MemRangeInterface> MemRangeSeqInterface for Seq<T>
{
    open spec fn to_range_seq(&self) -> Seq<(int, nat)> {
        seq_uop(*self, to_range_fn())
    }

    open spec fn to_aligned_range_seq(&self) -> Seq<(int, nat)> {
        seq_uop(self.to_range_seq(), to_page_aligned_range_fn())
    }

    open spec fn to_valid_ranges(&self) -> Set<(int, nat)> {
        let s = self.to_range_seq();
        //set_uop(self.to_set(), to_range_fn()).difference(empty_ranges())
        s.to_set().difference(empty_ranges())
    }

    open spec fn to_aligned_ranges(&self) -> Set<(int, nat)>
    {
        /*let s = self.to_aligned_range_seq();
        //set_uop(self.to_set(), to_page_aligned_range_fn()).difference(empty_ranges())
        s.to_set().difference(empty_ranges())*/
        self.to_aligned_ranges_internal()
    }

    open spec fn to_valid_ranges_internal(&self) -> Set<(int, nat)> {
        //let s = self.to_range_seq();
        Set::new(|r: (int, nat)| r.1 != 0 &&
            exists |i: int| 0 <= i && i < self.len() &&
                (#[trigger]self[i]).spec_range() === r)
        //s.to_set().difference(empty_ranges())
    }

    open spec fn has_aligned_ranges_internal(&self, r: (int, nat)) -> bool {
        r.1 != 0 &&
        exists |i: int|
            0<= i && i < self.len() &&
            (#[trigger]self[i]).spec_aligned_range() === r &&
            self[i].spec_range().1 != 0
    }

    open spec fn to_aligned_ranges_internal(&self) -> Set<(int, nat)>
    {
        Set::new(|r: (int, nat)|
            self.has_aligned_ranges_internal(r)
        )
    }

    open spec fn to_aligned_ranges_internal2(&self) -> Set<(int, nat)>
    {
        set_uop(self.to_valid_ranges(), to_page_aligned_range_fn())
    }

    proof fn lemma_align_ranges_reveal(&self)
    {
        assert forall |v|
            self.to_aligned_ranges_internal().contains(v) == self.to_aligned_ranges().contains(v)
        by {
            if self.to_aligned_ranges_internal().contains(v) {
                assert (self.has_aligned_ranges_internal(v));
                let i = choose |i: int| 0 <= i && i < self.len() &&
                    self[i].spec_aligned_range() === v &&
                    self[i].spec_range().1 != 0;
                assert(self[i].spec_aligned_range() === v);
                assert(v.1 != 0);
                assert(self.to_aligned_ranges_internal().contains(self[i].spec_aligned_range()));
                assert(self.to_set().contains(self[i]));
                assert(self.to_aligned_range_seq()[i] === (v));
                assert(self.to_aligned_ranges().contains(v));
            }
            if self.to_aligned_ranges().contains(v) {
                assert(self.to_aligned_ranges_internal().contains(v));
            }
        }

        assert forall |v|
            self.to_aligned_ranges_internal().contains(v) == self.to_aligned_ranges_internal2().contains(v)
        by {
            if self.to_aligned_ranges_internal().contains(v) {
                assert (self.has_aligned_ranges_internal(v));
                let i = choose |i: int| 0 <= i && i < self.len() &&
                    self[i].spec_aligned_range() === v && self[i].spec_range().1 != 0;
                assert(self[i].spec_aligned_range() === v);
                assert(v.1 != 0);
                assert(0 <= i && i < self.len());
                assert(self.to_valid_ranges().contains(self[i].spec_range())) by {
                    self.lemma_valid_ranges_reveal();
                    assert(self[i].spec_range().1 != 0);
                    assert(self.to_valid_ranges_internal().contains(self[i].spec_range()));
                }
                assert(to_page_aligned_range_fn()(self[i].spec_range()) === v);
            }

            if self.to_aligned_ranges_internal2().contains(v) {
                assert (exists |i: int| 0<= i && i < self.len() &&
                to_page_aligned_range_fn()(self[i].spec_range()) === v);
                let i = choose |i: int| 0 <= i && i < self.len() &&
                to_page_aligned_range_fn()(self[i].spec_range()) === v;
                assert(to_page_aligned_range_fn()(self[i].spec_range()) === v);
                assert(to_page_aligned_range_fn()(self[i].spec_range()) === self[i].spec_aligned_range());
                assert(self.to_aligned_ranges_internal().contains(v));
            }
        }
    }

    proof fn lemma_valid_ranges_reveal(&self){
        assert forall |v|
            self.to_valid_ranges_internal().contains(v) == self.to_valid_ranges().contains(v)
        by {
            if self.to_valid_ranges_internal().contains(v) {
                let i = choose |i: int| 0 <= i && i < self.len() &&
                    self[i].spec_range() === v;
                assert(self[i].spec_range() === v);
                assert(v.1 != 0);
                assert(self.to_valid_ranges_internal().contains(self[i].spec_range()));
                assert(self.to_set().contains(self[i]));
                assert(self.to_range_seq()[i] === (v));
                assert(self.to_valid_ranges().contains(v));
            }
            if self.to_valid_ranges().contains(v) {
            }
        }
    }
}

proof fn lemma_to_valid_ranges_push(s: Seq<(int, nat)>, v: (int, nat))
ensures
    v.1 != 0 ==> s.push(v).to_set().difference(empty_ranges()) =~~= s.to_set().difference(empty_ranges()).insert(v),
    v.1 == 0 ==> s.push(v).to_set().difference(empty_ranges()) =~~= s.to_set().difference(empty_ranges())
{
    s.to_multiset_ensures();
    seq_to_multi_set_to_set(s);
    seq_to_multi_set_to_set(s.push(v));
    assert(s.push(v).to_multiset() === s.to_multiset().insert(v));
    assert(s.to_multiset().insert(v).dom() =~~= s.to_multiset().dom().insert(v));
    assert(s.push(v).to_set() =~~= s.to_set().insert(v));
    if v.1 == 0 {
        assert(empty_ranges().contains(v));
    } else {
        assert(!empty_ranges().contains(v));
    }
}

pub open spec fn mem_range_formatted<T: MemRangeInterface>(ret_seq: Seq<T>) -> bool {
    &&& seq_is_sorted(ret_seq, range_speclt())
    &&& forall |i: int| 0 <= i < ret_seq.len() ==> (#[trigger]ret_seq[i]).wf_range()
    &&& forall |i: int| 0 <= i < ret_seq.len() ==> ret_seq.to_valid_ranges().contains(#[trigger]ret_seq[i].spec_range())
    &&& forall |i: int, j: int| 0 <= i < j < ret_seq.len() ==>
        (#[trigger]ret_seq[i]).spec_range().0 <= (#[trigger]ret_seq[j]).spec_range().0 &&
        (ret_seq[i]).spec_range().end() <= (ret_seq[j]).spec_range().0
}

pub proof fn mem_range_formatted_is_disjoint<T: MemRangeInterface>(ret_seq: Seq<T>)
requires
    mem_range_formatted(ret_seq),
ensures
    forall |i: int, j: int| 0 <= i < j < ret_seq.len()
        ==> range_disjoint_((#[trigger]ret_seq[i]).spec_range(), (#[trigger]ret_seq[j]).spec_range())
{
    assert forall |i: int, j: int| 0 <= i < ret_seq.len() && 0 <= j < ret_seq.len() && i != j
    implies range_disjoint_((#[trigger]ret_seq[i]).spec_range(), (#[trigger]ret_seq[j]).spec_range())
    by {
        if i < j {
            assert((#[trigger]ret_seq[i]).spec_range().0 <= (#[trigger]ret_seq[j]).spec_range().0 &&
            (ret_seq[i]).spec_range().end() <= (ret_seq[j]).spec_range().0);
        } else {
            assert(j < i);
            assert((#[trigger]ret_seq[j]).spec_range().0 <= (#[trigger]ret_seq[i]).spec_range().0 &&
            (ret_seq[j]).spec_range().end() <= (ret_seq[i]).spec_range().0);
        }
    }
}

pub open spec fn format_range_ensures<T: MemRangeInterface>(ret_seq: Seq<T>, prev: Seq<T>, ri: nat) -> bool {
    let n = prev.len();
    let wi = ret_seq.len();
    &&& (wi <= ri <= n)
    &&& (ri == n) ==> (ret_seq.to_valid_ranges() === prev.to_valid_ranges())
    &&& mem_range_formatted(ret_seq)
    &&& forall |i| 0 <= i < wi as int ==> is_format_entry(ret_seq[i], prev)
}

pub open spec fn is_format_entry<T: MemRangeInterface>(entry: T, oldself: Seq<T>) -> bool {
    &&& (exists |j| entry === oldself[j].spec_set_range(entry.spec_real_range()) &&
                    0 <= j && j < oldself.len())
    //&&& entry.spec_real_range().0.is_constant()
    //&&& entry.spec_real_range().1.is_constant()
}
}
verus! {

impl<T: MemRangeInterface + Copy, const N: usize_t> Array<T, N> {
    pub fn format_range(&mut self, len: usize_t) -> (ret_lens: (usize_t, usize_t))
        requires
            (len).is_constant(),
            forall|i| 0 <= i < (len as int) ==> (#[trigger] old(self)@[i]).self_wf(),
            0 <= (len as int) <= old(self)@.len(),
            forall|i|
                0 <= i < (len as int) ==> (#[trigger] old(
                    self,
                )@[i]).spec_real_range().0.is_constant() && old(
                    self,
                )@[i].spec_real_range().1.is_constant(),
    //forall |e| old(self)@.take((len) as int).to_set().contains(e) ==> e.spec_real_range().0.is_constant() && e.spec_real_range().1.is_constant(),

        ensures
            ret_lens.0.is_constant(),
            ret_lens.1.is_constant(),
            old(self)@.len() == self@.len(),
            ret_lens.1 <= ret_lens.0,
            ret_lens.0 <= self@.len(),
            forall|i| 0 <= i < (len as int) ==> (#[trigger] self@[i]).self_wf(),
            format_range_ensures(
                self@.take(ret_lens.1 as int),
                old(self)@.take(len as int),
                ret_lens.0 as nat,
            ),
            forall|i: int| (ret_lens.1 as int) <= i < self@.len() ==> old(self)@.contains(self@[i]),
    {
        let n = len;
        if n == 0 {
            assert(seq_is_sorted(self@.take(n as int), range_speclt::<T>()));
            return (0, 0);
        }
        let ghost speclt = range_speclt::<T>();
        let ghost oldself = self@;
        let ghost oldseq = self@.take(n as int);
        let less = |v1: T, v2: T| -> (ret: bool)
            requires
                v1.spec_lt_requires(&v2),
            ensures
                ret == speclt(v1, v2),
            { v1.less(&v2) };
        proof {
            seq_to_multi_set_to_set(oldseq);
            assert forall|x, y|
                oldseq.to_set().contains(x) && oldseq.to_set().contains(
                    y,
                ) implies #[trigger] less.requires((x, y)) by {
                assert(x.spec_real_range().0.is_constant());
                assert(x.spec_real_range().1.is_constant());
                assert(y.spec_real_range().0.is_constant());
                assert(y.spec_real_range().1.is_constant());
                assert(T::spec_sec_max().is_constant());
                assert(x.spec_lt_requires(&y));
            }
        }
        self.sort(0, n, less, Ghost(speclt));
        proof {
            seq_to_multi_set_to_set(self@.take(n as int));
            assert(self@.take(n as int).to_set() =~~= oldseq.to_set());
            assert forall|e| self@.take(n as int).to_set().contains(e) implies (
            e.spec_real_range().0.is_constant() && e.spec_real_range().1.is_constant()) by {
                assert(oldseq.to_set().contains(e));
            }
            assert(oldself.take(n as int).to_range_seq().to_multiset() === self@.take(
                n as int,
            ).to_range_seq().to_multiset()) by {
                proof_seq_to_seq_eq_multiset(
                    oldself.take(n as int),
                    self@.take(n as int),
                    to_range_fn(),
                );
            }
            seq_to_multi_set_to_set(oldself.take(n as int).to_range_seq());
            seq_to_multi_set_to_set(self@.take(n as int).to_range_seq());
            assert(oldself.take(n as int).to_valid_ranges() === self@.take(
                n as int,
            ).to_valid_ranges());
            assert(self@.take(n as int) =~~= self@.take(n as int).take(n as int));
            self@.take(n as int).to_multiset_ensures();
            oldself.take(n as int).to_multiset_ensures();
            assert forall|i| 0 <= i < (n as int) implies (#[trigger] self@[i]).self_wf() by {
                let e = self@[i];
                assert(self@.take(n as int)[i] === e);
                assert(self@.take(n as int).to_multiset().count(e) > 0);
                assert(oldself.take(n as int).contains(e));
                let j = choose|j: int| 0 <= j < (n as int) && oldself.take(n as int)[j] === e;
                assert(0 <= j < (n as int));
                assert(oldself[j] === oldself.take(n as int)[j]);
            }
        }
        // read index
        let mut ri: usize = 0;
        // write index
        let mut wi: usize = 0;
        let ghost prev = self@.take(n as int);
        let ghost prevself = self@;
        let ghost remap: Seq<int> = Seq::empty();
        while ri < n
            invariant
                ri.is_constant(),
                wi.is_constant(),
                n.is_constant(),
                ri <= n,
                wi <= ri,
                n <= self@.len(),
                n == prev.len(),
                speclt === range_speclt::<T>(),
                forall|i| 0 <= i < (n as int) ==> (#[trigger] self@[i]).self_wf(),
                forall|i: int| 0 <= i < (n as int) ==> prev[i] === prevself[i],
                forall|i: int| (wi as int) <= i < self@.len() ==> self@[i] === prevself[i],
                forall|e|
                    self@.take(n as int).to_set().contains(e) ==> (
                    e.spec_real_range().0.is_constant() && e.spec_real_range().1.is_constant()),
                seq_is_sorted(prev, speclt),
                seq_is_sorted(self@.take(wi as int), speclt),
                forall|i: int| 0 <= i < (wi as int) ==> (#[trigger] self@[i]).wf_range(),
                self@.take(wi as int).to_valid_ranges() =~~= prev.take(ri as int).to_valid_ranges(),
                forall|i: int, j: int|
                    0 <= i < j < (wi as int) ==> (#[trigger] self@[i]).spec_range().0 <= (
                    #[trigger] self@[j]).spec_range().0 && (self@[i]).spec_range().end() <= (
                    self@[j]).spec_range().0,
                seq_is_sorted(remap, |v1: int, v2: int| v1 < v2),
                remap.len() == wi as int,
                forall|i| 0 <= i < remap.len() ==> 0 <= #[trigger] remap[i] < (ri as int),
                forall|i|
                    0 <= i < (wi as int) ==> self@[i] === prev[remap[i]].spec_set_range(
                        self@[i].spec_real_range(),
                    ),
            ensures
                wi.is_constant(),
                ri.is_constant(),
                forall|i| 0 <= i < (n as int) ==> (#[trigger] self@[i]).self_wf(),
                forall|e|
                    self@.take(n as int).to_set().contains(e) ==> (
                    e.spec_real_range().0.is_constant() && e.spec_real_range().1.is_constant()),
                (wi as int) <= (ri as int) <= (n as int),
                forall|i: int| (wi as int) <= i < (n as int) ==> self@[i] === prev[i],
                forall|i: int| (wi as int) <= i < self@.len() ==> self@[i] === prevself[i],
                self@.take(wi as int).to_valid_ranges() =~~= prev.take(ri as int).to_valid_ranges(),
                seq_is_sorted(self@.take(wi as int), speclt),
                forall|i: int| 0 <= i < (wi as int) ==> (#[trigger] self@[i]).wf_range(),
                forall|i: int, j: int|
                    0 <= i < j < (wi as int) ==> (#[trigger] self@[i]).spec_range().0 <= (
                    #[trigger] self@[j]).spec_range().0 && (self@[i]).spec_range().end() <= (
                    self@[j]).spec_range().0,
                ri != n ==> 0 < (wi as int) < (n as int) && self@[wi as int].spec_start()
                    < self@[wi as int - 1].spec_end(),
                seq_is_sorted(remap, |v1: int, v2: int| v1 < v2),
                remap.len() == wi as int,
                forall|i| 0 <= i < remap.len() ==> 0 <= #[trigger] remap[i] < (ri as int),
                forall|i|
                    0 <= i < wi as int ==> self@[i] === prev[remap[i]].spec_set_range(
                        self@[i].spec_real_range(),
                    ),
        {
            let ghost aset = self@.take(n as int).to_set();
            let ghost subs = self@.take(n as int);
            let ghost prev_self = self@;
            let ghost v = self@[ri as int];
            proof {
                let ghost u = self@[wi as int];
                assert(v === prev[ri as int]);
                assert(u === prev[wi as int]);
                let ghost prev_u = self@[wi as int - 1];
                let ghost next_u = self@[wi as int + 1];
                let ghost next_v = self@[ri as int + 1];
                assert(v === subs[ri as int]);
                assert(aset.contains(v));
                assert(u === subs[wi as int]);
                assert(aset.contains(u));
                assert(!range_speclt()(v, u));
                if ri as int + 1 < n as int {
                    assert(v === prev[ri as int]);
                    assert(next_v === prev[ri as int + 1]);
                    assert(!range_speclt()(next_v, v));
                    assert(!range_speclt()(next_v, u));
                }
                if wi > 0 {
                    assert(prev_u === subs[wi as int - 1]);
                    assert(aset.contains(prev_u));
                }
                let prev_sub = prev.take(ri as int);
                let prev_next_sub = prev.take(ri as int + 1);
                assert(prev_next_sub =~~= prev_sub.push(v));
                assert(prev_next_sub.to_range_seq() =~~= prev_sub.to_range_seq().push(
                    v.spec_range(),
                ));
                // prove valid range set when there is an empty range.
                if v.spec_range().1 == 0 {
                    assert(v.spec_range().1 == 0);
                    assert(prev_sub.to_valid_ranges() =~~= prev_next_sub.to_valid_ranges()) by {
                        //lemma_to_valid_ranges_push(prev_sub.to_range_seq(), v.spec_range());
                        prev_sub.lemma_valid_ranges_reveal();
                        prev_next_sub.lemma_valid_ranges_reveal();
                        assert(!prev_next_sub.to_valid_ranges_internal().contains(v.spec_range()));
                        assert(!prev_sub.to_valid_ranges_internal().contains(v.spec_range()));
                        assert(prev_sub.to_valid_ranges_internal()
                            =~~= prev_next_sub.to_valid_ranges_internal()) by {
                            let s1 = prev_sub.to_valid_ranges_internal();
                            let s2 = prev_next_sub.to_valid_ranges_internal();
                            assert forall|r: (int, nat)| s1.contains(r) == s2.contains(r) by {
                                if s1.contains(r) {
                                    assert(r.1 != 0);
                                    let i = choose|i|
                                        prev_sub[i].spec_range() === r && 0 <= i && i
                                            < prev_sub.len();
                                    assert(prev_sub[i].spec_range() === r);
                                    assert(0 <= i && i < prev_next_sub.len());
                                    assert(prev_next_sub[i].spec_range() === r);
                                    assert(s2.contains(r));
                                }
                                if s2.contains(r) {
                                    assert(r.1 != 0);
                                    let i = choose|i|
                                        prev_next_sub[i].spec_range() === r && 0 <= i && i
                                            < prev_next_sub.len();
                                    assert(prev_next_sub[i].spec_range() === r);
                                    assert(0 <= i && i < prev_next_sub.len());
                                    assert(i != ri as int);
                                    assert(0 <= i && i < prev_sub.len());
                                    assert(s1.contains(r));
                                }
                            }
                        }
                    }
                }
                // prove sorted subrange

                proof_sorted_subrange(prev, speclt, ri as int, n as int);
                proof_sorted_subrange(prev, speclt, ri as int + 1, n as int);
                // prove unchanged right seq
                if ri as int + 1 < n as int {
                    assert(prev_self.subrange(wi as int + 1, n as int) =~~= prev_self.subrange(
                        wi as int,
                        n as int,
                    ).subrange(1, n as int - wi as int));
                    assert(prev.subrange(wi as int + 1, n as int) =~~= prev.subrange(
                        wi as int,
                        n as int,
                    ).subrange(1, n as int - wi as int));
                    assert(self@.subrange(wi as int + 1, n as int) =~~= prev.subrange(
                        wi as int + 1,
                        n as int,
                    ));
                }
                assert forall|i| 0 <= i < wi as int implies self@[i]
                    === prev[remap[i]].spec_set_range(self@[i].spec_real_range()) by {
                    assert(self@[i] === prev_self[i]);
                }
            }
            let mut entry = *self.index(ri);
            assert(self@[ri as int].self_wf());
            if entry.size().reveal_value() == 0 {
                ri = ri + 1;
                continue ;
            }
            let start = entry.start();
            let size = entry.size();
            proof {
                T::proof_constant_real_wf((start, size));
            }
            entry.update_range((start, size));
            if wi > 0 {
                assert(self@[wi as int - 1].self_wf());
                if start.reveal_value() < self.index(wi - 1).end().reveal_value() {
                    break ;
                }
            }
            self.update(wi, entry);
            ri = ri + 1;
            wi = wi + 1;
            proof {
                remap = remap.push(ri as int - 1);
                assert(remap.len() == wi as int);
                assert(prev[remap[wi as int - 1]] === v);
                assert forall|i| 0 <= i < wi as int implies self@[i]
                    === prev[remap[i]].spec_set_range(self@[i].spec_real_range()) by {
                    if i < wi as int - 1 {
                        assert(self@[i] === prev_self[i]);
                    }
                    assert(prev[remap[wi as int - 1]] === v);
                }
                let newseq = self@;
                assert(n <= newseq.len());
                assert forall|e| newseq.take(n as int).to_set().contains(e) implies (
                e.spec_real_range().0.is_constant() && e.spec_real_range().1.is_constant()) by {
                    let newsub = newseq.take(n as int);
                    assert(newsub.to_set().contains(e));
                    assert(newsub.contains(e));
                    let i = choose|i: int| (newsub[i] === e && 0 <= i < (newsub.len() as int));
                    assert(newsub[i] === e);
                    assert(0 <= i < (newsub.len() as int));
                    assert(newseq[i] === newsub[i]);
                    if (subs[i] === e) {
                        assert(subs.to_set().contains(e));
                    } else {
                        assert(e === entry);
                    }
                }
                self@.take(wi as int).to_range_seq().to_multiset_ensures();
                seq_to_multi_set_to_set(self@.take(wi as int).to_range_seq());
                assert(self@.take(wi as int).to_valid_ranges() =~~= prev.take(
                    ri as int,
                ).to_valid_ranges()) by {
                    assert(self@.take(wi as int - 1) =~~= prev_self.take(wi as int - 1));
                    lemma_to_valid_ranges_push(
                        self@.take(wi as int - 1).to_range_seq(),
                        entry.spec_range(),
                    );
                    lemma_to_valid_ranges_push(
                        prev.take(ri as int - 1).to_range_seq(),
                        entry.spec_range(),
                    );
                    assert(self@.take(wi as int).to_range_seq() =~~= self@.take(
                        wi as int - 1,
                    ).to_range_seq().push(entry.spec_range()));
                    assert(prev.take(ri as int).to_range_seq() =~~= prev.take(
                        ri as int - 1,
                    ).to_range_seq().push(entry.spec_range()));
                }
                proof_sorted_subrange(prev, speclt, ri as int, n as int);
            }
        }
        proof {
            if ri == n {
                assert(self@.take(wi as int).to_valid_ranges() === prev.take(
                    n as int,
                ).to_valid_ranges());
                assert(oldself.take(n as int).to_valid_ranges() === prev.take(
                    n as int,
                ).to_valid_ranges());
                assert(self@.take(wi as int).to_valid_ranges() === oldself.take(
                    n as int,
                ).to_valid_ranges());
            }
            assert forall|i| 0 <= i < wi as int implies is_format_entry(
                self@[i],
                oldself.take(n as int),
            ) by {
                let k = remap[i];
                assert(self@[i] === prev[k].spec_set_range(self@[i].spec_real_range()));
                assert(k < ri as int);
                assert(oldself.subrange(0, n as int).to_multiset() =~~= prev.to_multiset());
                prev.to_multiset_ensures();
                assert(prev.to_multiset().contains(prev[k]));
                assert(oldself.subrange(0, n as int).to_multiset().contains(prev[k]));
                oldself.subrange(0, n as int).to_multiset_ensures();
                assert(exists|j| oldself[j] === prev[k] && 0 <= j < (n as int));
                let j = choose|j| oldself[j] === prev[k] && 0 <= j < (n as int);
                assert(oldself[j] === prev[k]);
                assert(exists|j|
                    0 <= j < (n as int) && self@[i] === oldself.take(n as int)[j].spec_set_range(
                        self@[i].spec_real_range(),
                    )) by {
                    assert(0 <= j < (n as int));
                    assert(self@[i] === oldself[j].spec_set_range(self@[i].spec_real_range()));
                }
                assert(is_format_entry(self@[i], oldself.take(n as int)));
            }
            assert forall|i: int| (wi as int) <= i < self@.len() implies oldself.contains(
                self@[i],
            ) by {
                assert(prevself[i] === self@[i]);
                assert(prevself.contains(self@[i]));
                /*prevself.take(n as int).to_multiset_ensures();
                oldself.take(n as int).to_multiset_ensures();
                assert(prevself.take(n as int).to_multiset().count(self@[i]) ===
                    oldself.take(n as int).to_multiset().count(self@[i]));*/
                prevself.to_multiset_ensures();
                oldself.to_multiset_ensures();
                assert(prevself.to_multiset().count(self@[i]) === oldself.to_multiset().count(
                    self@[i],
                ));
            }
        }
        proof {
            self@.take(wi as int).lemma_valid_ranges_reveal();
        }
        (ri, wi)
    }
}

} // verus!

================
File: ./source/verismo/src/vcell/vcell.rs
================

use super::*;
verismo_simple! {
pub struct VCell<T> {
    ucell: T,
}

impl<T> VCell<T> {
    pub const fn new(val: T) -> Self
    {
        VCell {
            ucell: val
        }
    }
}
}

================
File: ./source/verismo/src/vcell/mod.rs
================

mod vcell;

pub use vcell::*;

use crate::ptr::*;
use crate::tspec::*;
use crate::tspec_e::*;

================
File: ./source/verismo/src/boot/init/init_e.rs
================

use super::mshv_fmt::FmtHvParamCall;
use super::mshv_init::process_vm_mem;
use super::*;
use crate::allocator::VeriSMoAllocator;
use crate::arch::addr_s::VM_MEM_SIZE;
use crate::boot::init::e820_fmt::e820_format;
use crate::boot::monitor_params::MonitorParamGlobalPerms;
#[cfg(debug_assertions)]
use crate::boot::monitor_params::SimpleMonitorParams;
use crate::lock::{LockPermRaw, MapLockContains, MapRawLockTrait};
use crate::vbox::{MutFnTrait, MutFnWithCSTrait, VBox};

verus! {

spec fn init_prepare_e820_ensures(
    prev_mparam: MonitorParams,
    mparam: MonitorParams,
    cc: SnpCoreConsole,
    ret: &[E820Entry],
) -> bool {
    let prev_e820 = prev_mparam.e820();
    let e820 = ret@;
    &&& e820.to_valid_ranges() === prev_e820.to_valid_ranges()
    &&& ret@.to_aligned_ranges() === prev_e820.to_aligned_ranges()
    &&& mparam === prev_mparam.spec_set_validated_e820(
        mparam.validated_e820,
    ).spec_set_validated_entries(mparam.validated_entries)
    &&& mparam.validated_entries.spec_eq(ret@.len())
    &&& format_range_ensures(
        ret@,
        prev_mparam.validated_e820@.take(prev_mparam.validated_entries.vspec_cast_to()),
        prev_mparam.validated_entries.vspec_cast_to(),
    )
    &&& ret@.is_constant()
    &&& ret@ === mparam.e820()
    &&& cc.wf()
    &&& mparam.is_constant()
}

pub struct InitE820Fn;

pub type InitE820Params = InitE820Fn;

pub type InitE820Out<'a> = (&'a [E820Entry]);

impl<'a> MutFnWithCSTrait<'a, SnpCoreConsole, InitE820Fn, InitE820Out<'a>> for MonitorParams {
    closed spec fn spec_update_cs_requires(&self, params: InitE820Fn, cs: SnpCoreConsole) -> bool {
        &&& self.is_constant()
        &&& cs.wf()
    }

    closed spec fn spec_update_cs(
        &self,
        prev: &Self,
        params: InitE820Fn,
        oldcs: SnpCoreConsole,
        ret: InitE820Out<'a>,
        cs: SnpCoreConsole,
    ) -> bool {
        &&& init_prepare_e820_ensures(*prev, *self, cs, ret)
        &&& cs.wf_core(oldcs.snpcore.cpu())
        &&& cs.snpcore.only_reg_coremode_updated(oldcs.snpcore, set![GHCB_REGID()])
    }

    fn box_update_cs(
        &'a mut self,
        params: InitE820Fn,
        Tracked(cc): Tracked<&mut SnpCoreConsole>,
    ) -> (ret: InitE820Out<'a>) {
        let e820_entries = self.validated_entries.into();
        if (e820_entries >= ValidatedE820Table::const_len()) || (e820_entries == 0) {
            // Wrong e820 size
            new_strlit("Invalid e820 size\n").leak_debug();
            early_vc_terminate_debug(SM_TERM_INVALID_PARAM, Tracked(cc));
        }
        let ghost prev_e820 = self.validated_e820@.take(e820_entries as int);
        self.validated_e820.leak_debug();
        let e820 = e820_format(&mut self.validated_e820, e820_entries);
        if e820.is_none() {
            new_strlit("e820 format err\n").leak_debug();
            early_vc_terminate_debug(SM_TERM_INVALID_PARAM, Tracked(cc));
        }
        let e820 = e820.unwrap();
        self.validated_entries = e820.len().into();
        proof {
            assert(e820@.to_valid_ranges() === prev_e820.to_valid_ranges());
            assert(e820@.to_aligned_ranges() =~~= prev_e820.to_aligned_ranges()) by {
                e820@.lemma_align_ranges_reveal();
                e820@.lemma_valid_ranges_reveal();
                prev_e820.lemma_align_ranges_reveal();
                prev_e820.lemma_valid_ranges_reveal();
                assert(e820@.to_valid_ranges_internal() =~~= prev_e820.to_valid_ranges_internal());
                assert(e820@.to_aligned_ranges_internal2()
                    =~~= prev_e820.to_aligned_ranges_internal2());
            }
        }
        e820
    }
}

pub struct InitCpuCount;

type InitCpuCountParams = (InitCpuCount, u64_s);

impl<'a> MutFnTrait<'a, InitCpuCountParams, u64_s> for MonitorParams {
    closed spec fn spec_update_requires(&self, params: InitCpuCountParams) -> bool {
        &&& self.is_constant()
    }

    closed spec fn spec_update(&self, prev: &Self, params: InitCpuCountParams, ret: u64_s) -> bool {
        *self === prev.spec_set_cpu_count(params.1)
    }

    fn box_update(&'a mut self, params: InitCpuCountParams) -> (ret: u64_s) {
        self.cpu_count = params.1;
        params.1
    }
}

pub fn init_mem(
    mp_ptr: SnpPPtr<MonitorParams>,
    Tracked(mp_perms): Tracked<MonitorParamPerms>,
    Tracked(alloc_perm): Tracked<SnpPointsTo<VeriSMoAllocator>>,
    Tracked(alloc_lock): Tracked<&mut LockPermRaw>,
    Tracked(memcc): Tracked<SnpMemCoreConsole>,
    Tracked(unused_preval_memperm): Tracked<RawMemPerms>,
) -> (ret: (
    VBox<MonitorParams>,
    VBox<OnePage>,
    Tracked<SnpCoreConsole>,
    Tracked<MonitorParamGlobalPerms>,
))
    requires
        memcc.memperm.mem_perms_e820_invalid(mp_perms@.e820()),
        memcc.wf(),
        unused_preval_memperm.mem_perms_e820_valid(mp_perms@.e820()),
        unused_preval_memperm.wf(),
        mp_perms@.wf_at(mp_ptr),
        mp_ptr.is_constant(),
        is_alloc_perm(alloc_perm@),
        old(alloc_lock)@.is_clean_lock_for(spec_ALLOCATOR_range(), memcc.cc.snpcore.cpu()),
    ensures
        ret.2@.wf_core(memcc.cc.snpcore.cpu()),
        ret.3@.wf_value(ret.0@),
        spec_ALLOCATOR().lock_default_mem_requires(ret.2@.snpcore.cpu(), alloc_lock@),
        is_permof_ALLOCATOR(alloc_lock@),
        ret.2@.snpcore.only_reg_coremode_updated(memcc.cc.snpcore, set![GHCB_REGID()]),
        ret.0@.mp_wf(),
        ret.0.snp() === SwSnpMemAttr::spec_default(),
{
    let tracked mut memcc = memcc;
    let tracked mut mp_perms = mp_perms;
    let tracked SnpMemCoreConsole { memperm, cc } = memcc;
    let start_addr = verismo_start_addr();
    let end_addr = verismo_end_addr();
    proof {
        reveal_strlit("\nwelcome verismo:\n");
    }
    new_strlit("\nwelcome verismo:\n").info(Tracked(&mut cc));
    if !start_addr.check_valid_addr(1) || !end_addr.check_valid_addr(0) || start_addr >= end_addr {
        // bad address
        new_strlit("bad address\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cc.snpcore));
    }
    let start = start_addr.to_page();
    let end = page_align_up(end_addr).to_page();
    if !mp_ptr.check_valid() {
        // Wrong pointer value
        new_strlit("bad mp_ptr\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cc.snpcore));
    }
    let tracked MonitorParamPerms { mp: mpraw_perm, hvparampage: hvraw_perm, global_perms } =
        mp_perms;
    let tracked mut mp_perm = mpraw_perm.tracked_into();
    proof {
        let N: nat = 16;
        assert(N == 16 ==> N * spec_size::<u8_s>() == 16 * spec_size::<u8_s>());
        let N: nat = 0x80;
        assert(N == 0x80 ==> N * spec_size::<HyperVMemMapEntry>() == 0x80 * spec_size::<
            HyperVMemMapEntry,
        >());
        assert(spec_size::<HvParamTable>() < PAGE_SIZE!());
    }
    ;
    let tracked (hvparam_perm, hv_unused) = hvraw_perm.trusted_split(spec_size::<HvParamTable>());
    let tracked mut hvparam_perm: SnpPointsTo<HvParamTable> = hvparam_perm.tracked_into();
    let mparam = mp_ptr.borrow(Tracked(&mp_perm));
    if !mparam.check_valid() {
        new_strlit("\nInvalid mparam\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cc.snpcore));
    }
    #[cfg(debug_assertions)]
    {
        let mparam2 = SimpleMonitorParams {
            cpuid_page: mparam.cpuid_page,  // cpuid page gpa
            secret_page: mparam.secret_page,  // secret page gpa
            hv_param: mparam.hv_param,  // param page gpa
            validated_entries: mparam.validated_entries,
            acpi: mparam.acpi,
            acpi_size: mparam.acpi_size,
            richos_start: mparam.richos_start,
            richos_size: mparam.richos_size,
            richos_cmdline_len: mparam.richos_cmdline_len,
        };
        // Output parameters for debugging purpose.
        mparam2.debug(Tracked(&mut cc));
    }
    let hv_addr: usize = mparam.hv_param.into();
    let hvparam_ptr: SnpPPtr<HvParamTable> = SnpPPtr::from_usize(hv_addr);
    if !hvparam_ptr.check_valid() {
        new_strlit("bad hvparam_ptr\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cc.snpcore));
    }
    let mut mparam = VBox::from_raw(mp_ptr.to_usize(), Tracked(mp_perm));
    proof {
        assert(mparam@.mp_wf());
        assert(mparam.wf());
    }
    let mut hvparam: VBox<HvParamTable> = VBox::from_raw(
        hvparam_ptr.to_usize(),
        Tracked(hvparam_perm),
    );
    assert(hvparam.wf());
    mparam.box_update((InitCpuCount, hvparam.borrow().cpu_count));
    // format e820 params
    let e820 = mparam.box_update_cs(InitE820Fn, Tracked(&mut cc));
    SlicePrinter { s: e820 }.debug(Tracked(&mut cc));
    // format hv params
    let (hv_mem_slice) = hvparam.box_update_cs(FmtHvParamCall, Tracked(&mut cc));
    let cc = process_vm_mem(
        hv_mem_slice,
        e820,
        start_addr,
        end_addr,
        Tracked(SnpMemCoreConsole { memperm, cc }),
        Tracked(unused_preval_memperm),
        Tracked(alloc_perm),
        Tracked(alloc_lock),
    );
    let (_, Tracked(hvparam_perm)) = hvparam.into_raw();
    let tracked hvraw_perm = hvparam_perm.tracked_into_raw().trusted_join(hv_unused);
    (
        mparam,
        VBox::from_raw(hvparam_ptr.to_usize(), Tracked(hvraw_perm.tracked_into())),
        cc,
        Tracked(global_perms),
    )
}

} // verus!

================
File: ./source/verismo/src/boot/init/mshv_alloc.rs
================

use vstd::slice::slice_index_get;

use super::*;
use crate::allocator::VeriSMoAllocator;
use crate::boot::init::e820_init_alloc::init_allocator_e820;

verus! {

pub open spec fn init_allocator_requires(
    allocator: VeriSMoAllocator,
    hv_mem_tb: &[HyperVMemMapEntry],
    e820: &[E820Entry],
    static_start: usize_t,
    static_end: usize_t,
    memcc: SnpMemCoreConsole,
) -> bool {
    &&& allocator.is_constant()
    &&& allocator@.inv()
    &&& static_start.is_constant()
    &&& static_end.is_constant()
    &&& static_end.spec_valid_addr_with(0)
    &&& static_start.spec_valid_addr_with(1)
    &&& static_start < static_end
    &&& mem_range_formatted(hv_mem_tb@)
    &&& mem_range_formatted(e820@)
    &&& e820@.wf()
    &&& hv_mem_tb@.wf()
    &&& memcc.wf()
    &&& forall|i: int|
        0 <= i < hv_mem_tb@.len() ==> memcc.memperm.contains_default_except(
            (#[trigger] hv_mem_tb@[i]).range(),
            e820@.to_valid_ranges().insert(range(static_start as int, static_end as int)),
        )
}

fn init_allocator(
    allocator: &mut VeriSMoAllocator,
    hv_mem_tb: &[HyperVMemMapEntry],
    e820: &[E820Entry],
    static_start: usize_t,
    static_end: usize_t,
    Tracked(memcc): Tracked<SnpMemCoreConsole>,
) -> (newcc: Tracked<SnpCoreConsole>)
    requires
        init_allocator_requires(*old(allocator), hv_mem_tb, e820, static_start, static_end, memcc),
    ensures
        newcc@.wf_core(memcc.cc.snpcore.cpu()),
        allocator@.inv(),
        newcc@.snpcore.only_reg_coremode_updated(memcc.cc.snpcore, set![GHCB_REGID()]),
{
    let ghost oldmemcc = memcc;
    let tracked SnpMemCoreConsole { mut memperm, cc } = memcc;
    let ghost coreid = cc.snpcore.cpu();
    let ghost verismo_static = range(static_start as int, static_end as int);
    let ghost except_ranges = e820@.to_valid_ranges().insert(verismo_static);
    let ghost oldmem = memperm;
    let static_start = static_start;
    let static_end = static_end;
    let mut idx: usize = 0;
    let mut prev_end: usize = 0;
    let len = hv_mem_tb.len();
    if len == 0 {
        new_strlit("bad hv_mem len\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cc.snpcore));
    }
    let tracked mut memcc = SnpMemCoreConsole { memperm, cc };
    proof {
        assert forall|i: int|
            (idx as int) <= i < (len as int) implies memcc.memperm.contains_default_except(
            hv_mem_tb@[i].range(),
            except_ranges,
        ) by {
            assert(oldmem.contains_default_except(hv_mem_tb@[i].range(), except_ranges));
        }
    }
    while idx < len
        invariant
            idx <= len,
            static_end.spec_valid_addr_with(0),
            static_start.spec_valid_addr_with(1),
            static_start < static_end,
            len == hv_mem_tb@.len(),
            verismo_static === range(static_start as int, static_end as int),
            except_ranges === e820@.to_valid_ranges().insert(verismo_static),
            mem_range_formatted(hv_mem_tb@),
            mem_range_formatted(e820@),
            e820@.wf(),
            hv_mem_tb@.wf(),
            prev_end.spec_valid_addr_with(0),
            (allocator)@.inv(),
            idx == 0 ==> prev_end as int == 0,
            idx > 0 ==> prev_end as int == hv_mem_tb@[idx as int - 1].range().end(),
            memcc.wf_core(coreid),
            memcc.cc.snpcore.only_reg_coremode_updated(oldmemcc.cc.snpcore, set![GHCB_REGID()]),
            forall|i: int|
                (idx as int) <= i < (len as int) ==> memcc.memperm.contains_default_except(
                    (#[trigger] hv_mem_tb@[i]).range(),
                    except_ranges,
                ),
            forall|i: int|
                0 <= i < idx as int ==> prev_end as int >= (#[trigger] hv_mem_tb@[i]).range().end(),
            prev_end as int <= verismo_static.0 || prev_end as int >= verismo_static.end(),
    {
        let entry = slice_index_get(hv_mem_tb, idx as usize_t);
        let start_gpn = entry.start().reveal_value();
        let npages = entry.size().reveal_value();
        let tracked SnpMemCoreConsole { mut memperm, cc } = memcc;
        let ghost prev_memperm = memperm;
        proof {
            assert(npages != 0);
            assert(entry.wf_range());
        }
        if !start_gpn.check_valid_pn(npages) || prev_end > start_gpn.to_addr() {
            // HV reported overlapped memory
            new_strlit("zero or invalid or overlapped start pg").leak_debug();
            vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cc.snpcore));
        }
        let end_gpn = start_gpn + npages;
        let start = start_gpn.to_addr();
        let end = end_gpn.to_addr();
        let ghost g_current_range = range(start as int, end as int);
        let static_range = (static_start, static_end - static_start);
        let current_range = (start, end - start);
        let inside = static_range.check_inside(&current_range);
        let disjoint = static_range.check_disjoint(&current_range);
        if !disjoint && !inside {
            // the static mem is not fully covered in one range.
            new_strlit("the static mem is not fully covered in one range.\n").leak_debug();
            vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cc.snpcore));
        }
        let tracked mut cc = cc;
        if inside {
            proof {
                assert(inside_range(verismo_static, g_current_range));
                assert(start@ <= static_start@);
                assert(end@ >= static_end@);
                if static_end@ < end@ {
                    lemma_contains_except_remove(
                        memperm,
                        range(static_end as int, end as int),
                        g_current_range,
                        except_ranges,
                        verismo_static,
                    );
                }
                if start@ < static_start@ {
                    lemma_contains_except_remove(
                        memperm,
                        range(start as int, static_start as int),
                        g_current_range,
                        except_ranges,
                        verismo_static,
                    );
                }
            }
            let ghost mut remain_range = g_current_range;
            if start < static_start {
                proof {
                    assert(verismo_static.end() == static_end as int);
                    assert(end as int >= verismo_static.end());
                    let used_range = range(start as int, static_start as int);
                    assert(memperm.contains_default_except(used_range, except_ranges)) by {
                        memperm.lemma_with_except_sub(
                            used_range,
                            g_current_range,
                            SwSnpMemAttr::spec_default(),
                            except_ranges,
                        );
                    }
                    remain_range = range(static_start as int, end as int);
                    assert(memperm.contains_default_except(remain_range, except_ranges)) by {
                        memperm.lemma_with_except_sub(
                            remain_range,
                            g_current_range,
                            SwSnpMemAttr::spec_default(),
                            except_ranges,
                        );
                    }
                    memperm.proof_remove_range_ensures(used_range);
                    memperm.proof_remove_range_ensures_except(used_range);
                    let tracked hv_memperm = memperm.tracked_split(used_range);
                    assert(hv_memperm.contains_default_except(used_range, except_ranges));
                    assert(inside_range(used_range, g_current_range) && range_disjoint_(
                        verismo_static,
                        used_range,
                    ));
                    assert(hv_memperm.contains_default_except(used_range, except_ranges)) by {
                        assert forall|r|
                            inside_range(r, used_range) && ranges_disjoint(except_ranges, r) && r.1
                                != 0 implies #[trigger] hv_memperm.contains_range(r)
                            && hv_memperm[r].snp() === SwSnpMemAttr::spec_default() by {
                            assert(ranges_disjoint(except_ranges, r)) by {
                                assert(range_disjoint_(verismo_static, used_range));
                            }
                            assert(hv_memperm.contains_range(r));
                        }
                    }
                    assert(hv_memperm.wf());
                    memcc = SnpMemCoreConsole { memperm: hv_memperm, cc };
                }
                let Tracked(tmpcc) = init_allocator_e820(
                    allocator,
                    e820,
                    start,
                    static_start,
                    Tracked(memcc),
                );
                proof {
                    cc = tmpcc;
                }
            }
            if static_end < end {
                proof {
                    let used_range = range(static_end as int, end as int);
                    assert(memperm.contains_default_except(used_range, except_ranges)) by {
                        memperm.lemma_with_except_sub(
                            used_range,
                            remain_range,
                            SwSnpMemAttr::spec_default(),
                            except_ranges,
                        );
                    }
                    let memperm0 = memperm;
                    memperm.proof_remove_range_ensures(used_range);
                    memperm.proof_remove_range_ensures_except(used_range);
                    let tracked mut hv_memperm = memperm.tracked_split(used_range);
                    assert(hv_memperm.contains_default_except(used_range, except_ranges));
                    memcc = SnpMemCoreConsole { memperm: hv_memperm, cc };
                    assert(inside_range(used_range, g_current_range) && range_disjoint_(
                        verismo_static,
                        used_range,
                    ));
                    assert(hv_memperm.contains_default_except(used_range, e820@.to_valid_ranges()))
                        by {
                        assert forall|r|
                            (inside_range(r, used_range) && r.1 != 0 && ranges_disjoint(
                                e820@.to_valid_ranges(),
                                r,
                            )) implies (#[trigger] hv_memperm.contains_range(r)
                            && hv_memperm[r].snp() === SwSnpMemAttr::spec_default()) by {
                            assert(ranges_disjoint(except_ranges, r)) by {
                                assert(range_disjoint_(verismo_static, used_range));
                            }
                            assert(hv_memperm.contains_default_except(used_range, except_ranges));
                            assert(hv_memperm.contains_range(r));
                        }
                        assert(forall|r|
                            (inside_range(r, used_range) && r.1 != 0 && ranges_disjoint(
                                e820@.to_valid_ranges(),
                                r,
                            )) ==> (#[trigger] hv_memperm.contains_range(r) && hv_memperm[r].snp()
                                === SwSnpMemAttr::spec_default()));
                    }
                }
                let Tracked(tmpcc) = init_allocator_e820(
                    allocator,
                    e820,
                    static_end,
                    end,
                    Tracked(memcc),
                );
                proof {
                    cc = tmpcc;
                }
            }
        } else {
            proof {
                assert(range_disjoint_(verismo_static, g_current_range));
                let used_range = g_current_range;
                memperm.proof_remove_range_ensures(used_range);
                memperm.proof_remove_range_ensures_except(used_range);
                assert(memperm.contains_default_except(used_range, except_ranges));
                //lemma_contains_except_remove(memperm, used_range, g_current_range, except_ranges, verismo_static);
                let tracked mut hv_memperm = memperm.tracked_split(used_range);
                assert(hv_memperm.contains_default_except(used_range, except_ranges));
                assert(inside_range(used_range, g_current_range) && range_disjoint_(
                    verismo_static,
                    used_range,
                ));
                assert(hv_memperm.contains_default_except(used_range, e820@.to_valid_ranges())) by {
                    assert forall|r|
                        inside_range(r, used_range) && r.1 != 0 && ranges_disjoint(
                            e820@.to_valid_ranges(),
                            r,
                        ) implies hv_memperm.contains_range(r) && hv_memperm[r].snp()
                        === SwSnpMemAttr::spec_default() by {
                        assert(ranges_disjoint(except_ranges, r)) by {
                            assert(range_disjoint_(verismo_static, used_range));
                        }
                        assert(hv_memperm.contains_range(r));
                    }
                }
                memcc = SnpMemCoreConsole { memperm: hv_memperm, cc };
            }
            let Tracked(tmpcc) = init_allocator_e820(allocator, e820, start, end, Tracked(memcc));
            proof {
                cc = tmpcc;
            }
        }
        prev_end = end;
        idx = idx + 1;
        proof {
            memcc = SnpMemCoreConsole { memperm, cc };
            assert forall|i: int|
                (idx as int) <= i < (len as int) implies memcc.memperm.contains_default_except(
                hv_mem_tb@[i].range(),
                except_ranges,
            ) by {
                assert(prev_memperm.contains_default_except(hv_mem_tb@[i].range(), except_ranges));
                mem_range_formatted_is_disjoint(hv_mem_tb@);
                assert(range_disjoint_(hv_mem_tb@[i].range(), g_current_range));
                prev_memperm.proof_remove_range_ensures(g_current_range);
            }
        }
    }
    Tracked(memcc.cc)
}

} // verus!
verus! {

pub struct InitAllocFn;

pub type InitAllocParams<'a, 'b> = (
    InitAllocFn,
    &'a [HyperVMemMapEntry],
    &'b [E820Entry],
    usize,
    usize,
    Tracked<SnpMemCoreConsole>,
);

use crate::vbox::MutFnTrait;

impl<'a, 'b, 'c> MutFnTrait<
    'c,
    InitAllocParams<'a, 'b>,
    Tracked<SnpCoreConsole>,
> for VeriSMoAllocator {
    open spec fn spec_update_requires(&self, params: InitAllocParams<'a, 'b>) -> bool {
        init_allocator_requires(*self, params.1, params.2, params.3, params.4, params.5@)
    }

    open spec fn spec_update(
        &self,
        prev: &Self,
        params: InitAllocParams<'a, 'b>,
        ret: Tracked<SnpCoreConsole>,
    ) -> bool {
        &&& ret@.wf_core(params.5@.cc.snpcore.cpu())
        &&& self@.inv()
        &&& ret@.snpcore.only_reg_coremode_updated(params.5@.cc.snpcore, set![GHCB_REGID()])
    }

    fn box_update(&'c mut self, params: InitAllocParams<'a, 'b>) -> (ret: Tracked<SnpCoreConsole>) {
        init_allocator(self, params.1, params.2, params.3, params.4, params.5)
    }
}

} // verus!

================
File: ./source/verismo/src/boot/init/e820_init_alloc.rs
================

use vstd::slice::{slice_index_get, slice_subrange};

use super::*;
use crate::allocator::VeriSMoAllocator;
use crate::arch::addr_s::{PAGE_SIZE, VM_MEM_SIZE};
use crate::pgtable_e::pa_to_va;
use crate::ptr::rmp::*;

verus! {

pub fn init_allocator_e820(
    allocator: &mut VeriSMoAllocator,
    e820: &[E820Entry],
    start_addr: usize_t,
    end_addr: usize_t,
    Tracked(memcc): Tracked<SnpMemCoreConsole>,
) -> (cc: Tracked<SnpCoreConsole>)
    requires
        old(allocator)@.inv(),
        mem_range_formatted(e820@),
        e820@.wf(),
        start_addr.spec_valid_addr_with(1),
        start_addr.is_constant(),
        end_addr.is_constant(),
        end_addr.spec_valid_addr_with(0),
        start_addr <= end_addr,
        memcc.wf(),
        memcc.memperm.contains_default_except(
            range(start_addr as int, end_addr as int),
            e820@.to_valid_ranges(),
        ),
    ensures
        allocator@.inv(),
        cc@.wf_core(memcc.cc.snpcore.cpu()),
        cc@.snpcore.only_reg_coremode_updated(memcc.cc.snpcore, set![GHCB_REGID()]),
{
    let mut index: usize = 0;
    let mut prev_end: usize_t = start_addr;
    let n = e820.len();
    let ghost oldmemcc = memcc;
    let tracked SnpMemCoreConsole { mut memperm, mut cc } = memcc;
    let ghost mem_range = range(prev_end as int, end_addr as int);
    while prev_end < end_addr
        invariant
            prev_end >= start_addr,
            mem_range === range(prev_end as int, end_addr as int),
            n == e820@.len(),
            index.is_constant(),
            index <= n,
            mem_range_formatted(e820@),
            e820@.wf(),
            start_addr.spec_valid_addr_with(1),
            start_addr.is_constant(),
            end_addr.is_constant(),
            end_addr.spec_valid_addr_with(0),
            start_addr <= end_addr,
            prev_end.is_constant(),
            prev_end.spec_valid_addr_with(0),
            forall|i| 0 <= i < index as int ==> prev_end as int >= e820@[i].spec_end(),
            (prev_end < end_addr && prev_end > start_addr) ==> prev_end as int == e820@[index as int
                - 1].spec_end(),
            (index as int == 0 && index < n) ==> prev_end === start_addr,
            cc.wf_core(oldmemcc.cc.snpcore.cpu()),
            cc.snpcore.only_reg_coremode_updated(oldmemcc.cc.snpcore, set![GHCB_REGID()]),
            memperm.wf(),
            memperm.contains_default_except(mem_range, e820@.to_valid_ranges()),
            allocator@.inv(),
    {
        let to_add_start = prev_end;
        let ghost prev_memperm = memperm;
        let ghost prev_mem_range = mem_range;
        let mut to_add_end = prev_end;
        if index < n {
            let e = slice_index_get(e820, index);
            //let ghost e = e820@[index as int];
            let size = e.size().reveal_value();
            let paddr = e.start().reveal_value();  // 1:1 mapping
            assert(e.wf_range());
            if paddr > start_addr {
                if index > 0 {
                    assert(e820@[index as int - 1].spec_end() <= e.spec_range().0);
                    assert(to_add_start as int >= e820@[index as int - 1].spec_end());
                }
                assert(start_addr <= paddr);
                assert(prev_end <= paddr);
                assert(to_add_end <= paddr);
                to_add_end =
                if paddr < end_addr {
                    paddr
                } else {
                    end_addr
                };
                prev_end = paddr + size;
            } else {
                prev_end = paddr + size;
                prev_end =
                if prev_end < start_addr {
                    start_addr
                } else {
                    prev_end
                };
            }
            // else skip to next entry.

            index = index + 1;
        } else {
            to_add_end = end_addr;
            prev_end = end_addr;
        }
        if to_add_end > to_add_start {
            let tracked mut to_add_perm;
            proof {
                let ghost to_add_range = range(to_add_start as int, to_add_end as int);
                assert(ranges_disjoint(e820@.to_valid_ranges(), to_add_range)) by {
                    assert forall|r| #[trigger]
                        e820@.to_valid_ranges().contains(r) implies range_disjoint_(
                        r,
                        to_add_range,
                    ) by {
                        let ee = choose|ee| e820@.contains(ee) && ee.spec_range() === r;
                        assert(e820@.contains(ee));
                        let j = choose|j| e820@[j] === ee && 0 <= j < e820@.len();
                        assert(e820@[j] === ee);
                        assert(range_disjoint_(to_add_range, ee.spec_range()));
                    }
                    assert(ranges_disjoint(e820@.to_valid_ranges(), to_add_range));
                }
                assert(inside_range(to_add_range, prev_mem_range));
                memperm.proof_remove_range_ensures(to_add_range);
                to_add_perm = memperm.tracked_remove(to_add_range);
            }
            let mut add_start = to_add_start;
            let mut add_end = to_add_end;
            assert(add_start.is_constant());
            assert(add_end.is_constant());
            if add_start == 0 && add_end > PAGE_SIZE {
                let mut tmp_end = PAGE_SIZE;
                let tracked (tmp_perm, to_add_perm2) = to_add_perm.trusted_split(PAGE_SIZE as nat);
                proof {
                    to_add_perm = to_add_perm2;
                }
                let mut add_vstart = pa_to_va(add_start as u64, Tracked(&tmp_perm)) as usize;
                let mut add_vend = pa_to_va(tmp_end as u64, Tracked(&tmp_perm)) as usize;
                mem_set_zeros(add_vstart, add_vend - add_vstart, Tracked(&mut tmp_perm));
                allocator.add_mem(&mut add_vstart, &mut add_vend, Tracked(tmp_perm));
                add_start = tmp_end;
            }
            assert(add_start.is_constant());
            assert(add_end.is_constant());
            assert(add_end > add_start);
            if (add_end - add_start) >= VeriSMoAllocator::minsize() {
                let mut add_vstart = pa_to_va(add_start as u64, Tracked(&to_add_perm)) as usize;
                let mut add_vend = pa_to_va(add_end as u64, Tracked(&to_add_perm)) as usize;
                mem_set_zeros(add_vstart, add_vend - add_vstart, Tracked(&mut to_add_perm));
                allocator.add_mem(&mut add_vstart, &mut add_vend, Tracked(to_add_perm));
            }
        }
        proof {
            mem_range = range(prev_end as int, end_addr as int);
            assert forall|r|
                (inside_range(r, mem_range) && #[trigger] ranges_disjoint(
                    e820@.to_valid_ranges(),
                    r,
                ) && r.1 > 0) implies memperm.contains_default_mem(r) by {
                assert(inside_range(r, prev_mem_range));
                assert(prev_memperm.eq_at(memperm, r));
                assert(prev_memperm.contains_default_mem(r));
            }
        }
    }
    Tracked(cc)
}

} // verus!

================
File: ./source/verismo/src/boot/init/mshv_fmt.rs
================

use vstd::slice::slice_subrange;

use super::*;
use crate::arch::addr_s::VM_MEM_SIZE;
use crate::boot::init::e820_init::*;

verus! {

pub closed spec fn get_hv_mem_count_ensures(arr: Seq<HyperVMemMapEntry>, ret: nat) -> bool {
    &&& ret <= arr.len()
    &&& forall|i: int| 0 <= i < (ret as int) ==> arr[i].numpages@.val != 0
    &&& ret < arr.len() ==> arr[ret as int].numpages@.val == 0
}

pub fn get_hv_mem_count(arr: &HyperVMemMapTable) -> (ret: usize_t)
    requires
        arr.is_constant(),
    ensures
        ret <= arr@.len(),
        ret.is_constant(),
        forall|i: int| 0 <= i < (ret as int) ==> arr@[i].numpages@.val != 0,
        ret < arr@.len() ==> arr@[ret as int].numpages@.val == 0,
        get_hv_mem_count_ensures(arr@, ret as nat),
{
    let mut ret = 0usize;
    let len = arr.len();
    while ret < len
        invariant
            ret <= arr@.len(),
            len == arr@.len(),
            ret.is_constant(),
            len.is_constant(),
            arr.is_constant(),
            forall|i: int| 0 <= i < (ret as int) ==> arr@[i].numpages@.val != 0,
        ensures
            ret < arr@.len() ==> arr@[ret as int].numpages@.val == 0,
            ret <= arr@.len(),
            ret.is_constant(),
            forall|i: int| 0 <= i < (ret as int) ==> arr@[i].numpages@.val != 0,
    {
        if arr.index(ret).numpages.reveal_value() == 0 {
            break ;
        }
        ret = ret + 1usize;
    }
    ret
}

pub closed spec fn fmt_hvparam_ensures(
    prev_hvparam: HvParamTable,
    hvparam: HvParamTable,
    n: nat,
    ret: &[HyperVMemMapEntry],
) -> bool {
    let prev_memtab = prev_hvparam.mem_table@;
    let mem_table = ret@;
    &&& hvparam === prev_hvparam.spec_set_mem_table(hvparam.mem_table)
    &&& format_range_ensures(ret@, prev_hvparam.mem_table@.take(n as int), n)
    &&& ret@.is_constant()
    &&& ret@ === hvparam.mem_table@.take(ret@.len() as int)
    &&& ret@.len() <= n <= hvparam.mem_table@.len()
}

pub fn fmt_hvparam<'a>(hv_param: &'a mut HvParamTable, n: usize_t) -> (ret: Option<
    &'a [HyperVMemMapEntry],
>)
    requires
        old(hv_param).is_constant(),
        n <= old(hv_param).mem_table@.len(),
        n.is_constant(),
        forall|i: int| 0 <= i < (n as int) ==> old(hv_param).mem_table@[i].numpages@.val != 0,
        n < old(hv_param).mem_table@.len() ==> old(hv_param).mem_table@[n as int].numpages@.val
            == 0,
    ensures
        ret.is_Some() ==> fmt_hvparam_ensures(
            *old(hv_param),
            *hv_param,
            n as nat,
            ret.get_Some_0(),
        ),
{
    let ghost hvslice = hv_param.mem_table@.subrange(0, n as int);
    proof {
        let seq = hv_param.mem_table@;
        assert(hv_param.mem_table@.is_constant());
        assert forall|i| 0 <= i < (n as int) implies (
        #[trigger] seq[i]).spec_real_range().0.is_constant()
            && seq[i].spec_real_range().1.is_constant() by {
            assert(seq[i].is_constant());
            proof_into_is_constant::<_, usize_s>(seq[i].starting_gpn);
            proof_into_is_constant::<_, usize_s>(seq[i].numpages);
        }
    }
    let ghost prev_memtb = hv_param.mem_table@.take(n as int);
    let (visited, validn) = hv_param.mem_table.format_range(n);
    proof {
        assert(validn <= hv_param.mem_table@.len());
        let memtb = hv_param.mem_table@.take(validn as int);
        assert(format_range_ensures(memtb, prev_memtb, visited as nat));
        assert(forall|i| 0 <= i < memtb.len() ==> is_format_entry(#[trigger] memtb[i], prev_memtb));
        assert(memtb.is_constant()) by {
            assert forall|i| 0 <= i < memtb.len() implies memtb[i].is_constant() by {
                let entry = memtb[i];
                if i < validn as int {
                    assert(is_format_entry(entry, prev_memtb));
                    let j = choose|j|
                        entry === prev_memtb[j].spec_set_range(entry.spec_real_range()) && 0 <= j
                            && j < prev_memtb.len();
                    assert(entry === prev_memtb[j].spec_set_range(entry.spec_real_range()));
                    assert(entry.spec_real_range().0.is_constant());
                    assert(entry.spec_real_range().1.is_constant());
                    proof_into_is_constant::<_, u64_s>(entry.spec_real_range().0);
                    proof_into_is_constant::<_, u64_s>(entry.spec_real_range().1);
                    assert(prev_memtb[j].is_constant());
                    assert(entry.starting_gpn.is_constant());
                    assert(entry.numpages.is_constant());
                    assert(entry.is_constant());
                } else {
                    assert(prev_memtb.contains(entry));
                    let j = choose|j| entry === prev_memtb[j] && 0 <= j < prev_memtb.len();
                    assert(prev_memtb[j].is_constant());
                    assert(entry === prev_memtb[j]);
                    assert(entry.is_constant());
                }
            }
        }
    }
    if visited != n || validn == 0 {
        // HV reported memory is malformatted or is empty.
        return None
    }
    let ghost sorted_hvslice = hv_param.mem_table@.subrange(0, n as int);
    let hv_mem_slice = hv_param.mem_table.as_slice();
    let hv_mem_slice = slice_subrange(hv_mem_slice, 0, validn);
    Some(hv_mem_slice)
}

pub struct FmtHvParamCall;

pub type FmtHvParamOut<'a> = (&'a [HyperVMemMapEntry]);

use crate::vbox::MutFnWithCSTrait;

impl<'a> MutFnWithCSTrait<'a, SnpCoreConsole, FmtHvParamCall, FmtHvParamOut<'a>> for HvParamTable {
    open spec fn spec_update_cs_requires(
        &self,
        params: FmtHvParamCall,
        cs: SnpCoreConsole,
    ) -> bool {
        &&& cs.wf()
        &&& self.is_constant()
    }

    open spec fn spec_update_cs(
        &self,
        prev: &Self,
        params: FmtHvParamCall,
        oldcs: SnpCoreConsole,
        ret: FmtHvParamOut<'a>,
        cs: SnpCoreConsole,
    ) -> bool {
        &&& cs.wf_core(oldcs.snpcore.cpu())
        &&& cs.snpcore === oldcs.snpcore
        &&& ret@.len() > 0
        &&& mem_range_formatted(ret@)
        &&& ret@.is_constant()
        &&& exists|n: nat| #[trigger]
            get_hv_mem_count_ensures(prev.mem_table@, n) && #[trigger] fmt_hvparam_ensures(
                *prev,
                *self,
                n,
                ret,
            )
    }

    fn box_update_cs(
        &'a mut self,
        params: FmtHvParamCall,
        Tracked(cs): Tracked<&mut SnpCoreConsole>,
    ) -> (ret: FmtHvParamOut<'a>) {
        let n = get_hv_mem_count(&self.mem_table);
        let hv_mem_slice = fmt_hvparam(self, n);
        if hv_mem_slice.is_none() {
            early_vc_terminate_debug(SM_TERM_INVALID_PARAM, Tracked(cs));
        }
        let hv_mem_slice = hv_mem_slice.unwrap();
        if (hv_mem_slice.len() as usize) == 0 {
            early_vc_terminate_debug(SM_TERM_INVALID_PARAM, Tracked(cs));
        }
        hv_mem_slice
    }
}

} // verus!

================
File: ./source/verismo/src/boot/init/init_s.rs
================

use super::*;
use crate::arch::addr_s::VM_MEM_SIZE;
use crate::mem::{RawMemPerms, SnpMemCoreConsole};

verus! {

pub trait InitMem {
    spec fn mem_perms_e820_valid(&self, e820: Seq<E820Entry>) -> bool;

    spec fn mem_perms_e820_invalid(&self, e820: Seq<E820Entry>) -> bool;

    spec fn out_e820_invalidated(&self, e820: Seq<E820Entry>, bound: (int, nat)) -> bool;

    spec fn wf_initial(self, e820: Seq<E820Entry>) -> bool where Self: core::marker::Sized;

    spec fn initmem_wf_basic(self) -> bool where Self: core::marker::Sized;
}

impl InitMem for RawMemPerms {
    open spec fn mem_perms_e820_invalid(&self, e820: Seq<E820Entry>) -> bool {
        &&& self.contains_init_except((0, VM_MEM_SIZE as nat), e820.to_aligned_ranges())
    }

    open spec fn mem_perms_e820_valid(&self, e820: Seq<E820Entry>) -> bool {
        // e820 table records all validated memory.
        &&& forall|r|
            e820.to_aligned_ranges().contains(r) ==> self.contains_default_except(
                r,
                e820.to_valid_ranges(),
            )
    }

    open spec fn out_e820_invalidated(&self, e820: Seq<E820Entry>, bound: (int, nat)) -> bool {
        // contains all memory permission not validated
        forall|r|
            (inside_range(r, bound) && ranges_disjoint(e820.to_aligned_ranges(), r) && r.1 > 0)
                ==> #[trigger] self.contains_range(r) && self.contains_init_mem(r)
    }

    open spec fn wf_initial(self, e820: Seq<E820Entry>) -> bool {
        &&& self.wf()
        &&& self.mem_perms_e820_valid(e820)
    }

    #[verifier(inline)]
    open spec fn initmem_wf_basic(self) -> bool {
        self.wf()
    }
}

pub proof fn lemma_contains_except_remove(
    memperm: RawMemPerms,
    r: (int, nat),
    current_range: (int, nat),
    ranges: Set<(int, nat)>,
    toremove: (int, nat),
)
    requires
        memperm.contains_default_except(current_range, ranges),
        range_disjoint_(r, toremove),
        inside_range(r, current_range),
        ranges.contains(toremove),
    ensures
        memperm.contains_default_except(r, ranges.remove(toremove)),
{
    let newrange = ranges.remove(toremove);
    assert forall|r2|
        (inside_range(r2, r) && ranges_disjoint(newrange, r2) && r2.1
            > 0) implies memperm.contains_default_mem(r2) by {
        assert(ranges_disjoint(newrange.insert(toremove), r2));
        assert(memperm.contains_default_mem(r2));
    }
}

} // verus!

================
File: ./source/verismo/src/boot/init/e820_init.rs
================

use vstd::slice::slice_index_get;

use super::*;
use crate::arch::addr_s::VM_MEM_SIZE;
use crate::ptr::rmp::*;

verus! {

proof fn lemma_validate_e820_single(
    prev_memperm: RawMemPerms,
    memperm: RawMemPerms,
    pre_validated: Set<(int, nat)>,
    prev_validated_range: (int, nat),
    end_addr: int,
    toval_range: (int, nat),
    validated_range: (int, nat),
    entry_r: (int, nat),
)
    requires
        prev_memperm.contains_default_except(prev_validated_range, pre_validated),
        prev_memperm.contains_init_except(
            range(prev_validated_range.end(), VM_MEM_SIZE as int),
            pre_validated,
        ),
        prev_memperm.contains_init_except(
            range(end_addr as int, VM_MEM_SIZE as int),
            pre_validated,
        ),
        forall|r| range_disjoint_(r, toval_range) ==> memperm.eq_at(prev_memperm, r),
        (toval_range.1 > 0) ==> memperm.contains_default_mem(toval_range),
        ranges_disjoint(pre_validated, toval_range),
        (toval_range.1 > 0) ==> (toval_range.0 == prev_validated_range.end()),
        validated_range.0 == prev_validated_range.0,
        pre_validated.contains(entry_r),
        prev_validated_range.end() < entry_r.0 ==> toval_range.1 != 0,
        validated_range.end() == if entry_r.0 < end_addr as int {
            spec_max(prev_validated_range.0, entry_r.end())
        } else {
            end_addr
        },
        validated_range.end() >= prev_validated_range.end(),
        toval_range.end() <= end_addr,
        (toval_range.1 > 0) ==> toval_range.end() == spec_min(
            end_addr,
            spec_max(entry_r.0, prev_validated_range.0),
        ),
        prev_validated_range.end() <= end_addr,
    ensures
        memperm.contains_default_except(validated_range, pre_validated),
        memperm.contains_init_except(
            range(validated_range.end(), VM_MEM_SIZE as int),
            pre_validated,
        ),
        memperm.contains_init_except(range(end_addr as int, VM_MEM_SIZE as int), pre_validated),
        forall|r| range_disjoint_(r, toval_range) ==> memperm.eq_at(prev_memperm, r),
{
    let end_init_range = range(end_addr as int, VM_MEM_SIZE as int);
    assert forall|r: (int, nat)|
        r.1 != 0 && inside_range(r, validated_range) && ranges_disjoint(
            pre_validated,
            r,
        ) implies memperm.contains_range(r) && memperm.contains_default_mem(r) by {
        let snp = SwSnpMemAttr::spec_default();
        assert(memperm.contains_default_except(prev_validated_range, pre_validated));
        assert(memperm.contains_default_except(toval_range, pre_validated)) by {
            memperm.proof_contains_range_to_except(toval_range, pre_validated);
        }
        assert(memperm.contains_default_except(entry_r, pre_validated)) by {
            memperm.lemma_empty_contains_except(entry_r, snp, pre_validated)
        }
        let leftr = if toval_range.1 != 0 {
            let leftr = memperm.lemma_with_except_union(
                prev_validated_range,
                toval_range,
                snp,
                pre_validated,
            );
            leftr
        } else {
            prev_validated_range
        };
        let fullr = if leftr.0 <= entry_r.0 && leftr.end() >= entry_r.0 && leftr.end()
            <= entry_r.end() {
            memperm.lemma_with_except_union(leftr, entry_r, snp, pre_validated)
        } else {
            leftr
        };
        assert(inside_range(r, fullr));
        memperm.lemma_with_except_sub(r, fullr, snp, pre_validated);
        assert(memperm.contains_default_except(fullr, pre_validated));
        assert(memperm.contains_range(r));
        assert(memperm.contains_default_mem(r));
    }
    let prev_init_range = range(prev_validated_range.end(), VM_MEM_SIZE as int);
    let next_init_range = range(validated_range.end(), VM_MEM_SIZE as int);
    assert(memperm.contains_init_except(next_init_range, pre_validated)) by {
        if next_init_range.1 != 0 {
            assert(inside_range(next_init_range, prev_init_range));
        }
        prev_memperm.lemma_with_except_sub(
            next_init_range,
            prev_init_range,
            SwSnpMemAttr::init(),
            pre_validated,
        );
        assert forall|r: (int, nat)|
            r.1 != 0 && inside_range(r, next_init_range) && ranges_disjoint(
                pre_validated,
                r,
            ) implies memperm.contains_range(r) && memperm.contains_init_mem(r) by {
            assert(range_disjoint_(r, toval_range));
            assert(prev_memperm.contains_init_mem(r));
        }
    }
    assert forall|r: (int, nat)|
        r.1 != 0 && inside_range(r, end_init_range) && ranges_disjoint(
            pre_validated,
            r,
        ) implies #[trigger] memperm.contains_range(r) && memperm.contains_init_mem(r) by {
        assert(prev_memperm.contains_init_mem(r));
    }
}

spec fn validate_e820_loop_inv(
    e820: Seq<E820Entry>,
    n: usize,
    start_addr: usize,
    end_addr: usize,
    pre_validated: Set<(int, nat)>,
) -> bool {
    &&& n == e820.len()
    &&& pre_validated === e820.to_aligned_ranges()
    &&& e820.is_constant()
    &&& start_addr < end_addr <= VM_MEM_SIZE
    &&& end_addr as int % PAGE_SIZE!() == 0
    &&& start_addr as int % PAGE_SIZE!() == 0
    &&& mem_range_formatted(e820)
}

proof fn lemma_validated_range_disjoint_e820(
    e820: Seq<E820Entry>,
    i: int,
    start_addr: int,
    end_addr: int,
    toval_range: (int, nat),
)
    requires
        0 <= i < e820.len(),
        toval_range === validate_e820_iter_val_range(e820, i, start_addr, end_addr),
        mem_range_formatted(e820),
    ensures
        ranges_disjoint(e820.to_aligned_ranges(), toval_range),
{
    let pre_validated = e820.to_aligned_ranges();
    assert forall|r| #[trigger] pre_validated.contains(r) implies range_disjoint_(
        toval_range,
        r,
    ) by {
        let k = choose|k: int| e820[k].spec_aligned_range() === r && 0 <= k && k < e820.len();
        assert(e820[k].spec_aligned_range() === r);
        assert(pre_validated.contains(r));
    }
}

spec fn validate_e820_iter_val_start(e820: Seq<E820Entry>, i: int, start: int, end: int) -> int {
    if i <= 0 {
        start
    } else {
        if e820[i - 1].spec_aligned_range().0 < end {
            spec_max(start, e820[i - 1].spec_aligned_range().end())
        } else {
            end
        }
    }
}

spec fn validate_e820_iter_val_end(e820: Seq<E820Entry>, i: int, end: int) -> int {
    spec_min(e820[i].spec_aligned_range().0, end)
}

spec fn validate_e820_iter_val_range(e820: Seq<E820Entry>, i: int, start: int, end: int) -> (
    int,
    nat,
) {
    range(
        validate_e820_iter_val_start(e820, i, start, end),
        validate_e820_iter_val_end(e820, i, end),
    )
}

pub fn validate_e820(
    e820: &[E820Entry],
    start_addr: usize_t,
    end_addr: usize_t,
    Tracked(memcc): Tracked<SnpMemCoreConsole>,
) -> (newmemcc: Tracked<SnpMemCoreConsole>)
    requires
        e820@.is_constant(),
        mem_range_formatted(e820@),
        start_addr < end_addr <= VM_MEM_SIZE,
        start_addr as int % PAGE_SIZE!() == 0,
        end_addr as int % PAGE_SIZE!() == 0,
        memcc.wf(),
        memcc.memperm.contains_init_except(
            range(start_addr as int, VM_MEM_SIZE as int),
            e820@.to_aligned_ranges(),
        ),
    ensures
        newmemcc@.wf_core(memcc.cc.snpcore.cpu()),
        newmemcc@.cc.snpcore.only_reg_coremode_updated(memcc.cc.snpcore, set![GHCB_REGID()]),
        forall|r|
            range_disjoint_(r, range(start_addr as int, end_addr as int))
                ==> newmemcc@.memperm.eq_at(memcc.memperm, r),
        newmemcc@.memperm.contains_default_except(
            range(start_addr as int, end_addr as int),
            e820@.to_aligned_ranges(),
        ),
        newmemcc@.memperm.contains_init_except(
            range(end_addr as int, VM_MEM_SIZE as int),
            e820@.to_aligned_ranges(),
        ),
{
    let ghost pre_validated = e820@.to_aligned_ranges();
    let ghost oldmemcc = memcc;
    let ghost cpu = oldmemcc.cc.snpcore.cpu();
    let tracked SnpMemCoreConsole { mut memperm, mut cc } = memcc;
    let n = e820.len();
    let mut val_end = start_addr;
    let mut index = 0usize;
    proof {
        memperm.lemma_with_except_sub(
            range(end_addr as int, VM_MEM_SIZE as int),
            range(start_addr as int, VM_MEM_SIZE as int),
            SwSnpMemAttr::init(),
            pre_validated,
        );
        memperm.lemma_empty_contains_except(
            range(start_addr as int, val_end as int),
            SwSnpMemAttr::spec_default(),
            pre_validated,
        );
    }
    while val_end < end_addr && index < n
        invariant
            index <= e820@.len(),
            n == e820@.len(),
            validate_e820_loop_inv(e820@, n, start_addr, end_addr, pre_validated),
            memperm.wf(),
            cc.wf_core(cpu),
            cc.snpcore.only_reg_coremode_updated(oldmemcc.cc.snpcore, set![GHCB_REGID()]),
            memperm.contains_default_except(
                range(start_addr as int, val_end as int),
                pre_validated,
            ),
            memperm.contains_init_except(range(val_end as int, VM_MEM_SIZE as int), pre_validated),
            memperm.contains_init_except(range(end_addr as int, VM_MEM_SIZE as int), pre_validated),
            forall|r|
                range_disjoint_(r, range(start_addr as int, end_addr as int)) ==> memperm.eq_at(
                    oldmemcc.memperm,
                    r,
                ),
            val_end as int % PAGE_SIZE!() == 0,
            val_end >= start_addr,
            val_end == validate_e820_iter_val_start(
                e820@,
                index as int,
                start_addr as int,
                end_addr as int,
            ),
    {
        let ghost prev_end: int = val_end as int;
        let ghost prev_memperm = memperm;
        let ghost gstart = start_addr as int;
        let ghost prev_init_range = range(prev_end, VM_MEM_SIZE as int);
        let ghost prev_validated_range = range(gstart, prev_end);
        let mut next_start = val_end;
        let val_start = val_end;
        let entry = *slice_index_get(e820, index);
        let ghost entry_r = entry.spec_aligned_range();
        proof {
            assert(inside_ranges(entry_r, pre_validated) && pre_validated.contains(entry_r)) by {
                e820@.lemma_align_ranges_reveal();
                assert(pre_validated.contains(entry_r));
            }
            assert(entry.wf_range());
        }
        let cur_addr = entry.aligned_start().reveal_value();
        let cur_end = page_align_up(entry.end().reveal_value());
        assert(cur_end == entry_r.end());
        if cur_addr < end_addr {
            next_start =
            if cur_end > start_addr {
                cur_end
            } else {
                start_addr
            };
            val_end = cur_addr;
        } else {
            next_start = end_addr;
            val_end = end_addr;
        }
        assert(next_start as int == if entry_r.0 < end_addr as int {
            spec_max(start_addr as int, entry_r.end())
        } else {
            end_addr as int
        });
        let ghost toval_range = range(val_start as int, val_end as int);
        proof {
            assert(ranges_disjoint(pre_validated, toval_range)) by {
                lemma_validated_range_disjoint_e820(
                    e820@,
                    index as int,
                    start_addr as int,
                    end_addr as int,
                    toval_range,
                );
            }
        }
        proof {
            if toval_range.1 > 0 {
                assert(toval_range.0 == prev_end);
                memperm.lemma_with_except_sub(
                    toval_range,
                    prev_init_range,
                    SwSnpMemAttr::init(),
                    pre_validated,
                );
                assert(memperm.contains_init_except(toval_range, pre_validated));
                assert(memperm.contains_init_mem(toval_range)) by {
                    assert(ranges_disjoint(pre_validated, toval_range));
                    assert(inside_range(toval_range, toval_range));
                    assert(memperm.contains_range(toval_range));
                }
                memperm.proof_remove_range_ensures(toval_range);
            }
        }
        if val_end > val_start {
            let tracked pperm = memperm.tracked_remove(toval_range);
            let Tracked(pperm) = pvalmem(
                val_start as u64,
                val_end as u64,
                true,
                Tracked(pperm),
                Tracked(&mut cc.snpcore),
            );
            proof {
                assert(memperm.contains_default_except(prev_validated_range, pre_validated));
                memperm.tracked_insert(toval_range, pperm);
                assert(memperm.contains_default_mem(toval_range));
                memperm.proof_remove_range_ensures(toval_range);
                assert(range_disjoint_(toval_range, prev_validated_range));
                assert(memperm.contains_default_except(toval_range, pre_validated)) by {
                    memperm.proof_contains_range_to_except(toval_range, pre_validated);
                }
                assert(memperm.contains_default_except(prev_validated_range, pre_validated));
            }
        }
        proof {
            let validated_range = range(gstart, next_start as int);
            if toval_range.1 != 0 {
                assert(memperm.contains_default_mem(toval_range));
            }
            lemma_validate_e820_single(
                prev_memperm,
                memperm,
                pre_validated,
                prev_validated_range,
                end_addr as int,
                toval_range,
                validated_range,
                entry_r,
            );
            //assert(validate_e820_loop_inv(e820@, n, start_addr, end_addr, pre_validated));
        }
        index = index + 1;
        val_end = next_start;
        proof {
            assert(val_end as int % PAGE_SIZE!() == 0 && val_end >= start_addr && index == 0
                ==> val_end == start_addr && index > 0 ==> val_end == spec_max(
                start_addr as int,
                e820@[index as int - 1].spec_aligned_range().end(),
            ));
        }
    }
    let ghost validated_range = range(start_addr as int, end_addr as int);
    let ghost prev_memperm = memperm;
    let ghost end_init_range = range(end_addr as int, VM_MEM_SIZE!());
    if val_end < end_addr {
        let ghost toval_range = range(val_end as int, end_addr as int);
        let ghost prev_validated_range = range(start_addr as int, val_end as int);
        proof {
            memperm.proof_remove_range_ensures(toval_range);
        }
        let tracked pperm = memperm.tracked_remove(toval_range);
        let Tracked(pperm) = pvalmem(
            val_end as u64,
            end_addr as u64,
            true,
            Tracked(pperm),
            Tracked(&mut cc.snpcore),
        );
        proof {
            assert(memperm.contains_default_except(prev_validated_range, pre_validated));
            memperm.tracked_insert(toval_range, pperm);
            assert(memperm.contains_default_mem(toval_range));
            memperm.proof_remove_range_ensures(toval_range);
            assert(range_disjoint_(toval_range, prev_validated_range));
            assert(memperm.contains_default_except(toval_range, pre_validated)) by {
                memperm.proof_contains_range_to_except(toval_range, pre_validated);
            }
            assert(memperm.contains_default_except(validated_range, pre_validated)) by {
                memperm.lemma_with_except_union(
                    prev_validated_range,
                    toval_range,
                    SwSnpMemAttr::spec_default(),
                    pre_validated,
                );
            }
        }
    } else {
        proof {
            memperm.lemma_with_except_sub(
                validated_range,
                range(start_addr as int, val_end as int),
                SwSnpMemAttr::spec_default(),
                pre_validated,
            );
        }
    }
    proof {
        assert forall|r: (int, nat)|
            r.1 != 0 && inside_range(r, end_init_range) && ranges_disjoint(
                pre_validated,
                r,
            ) implies #[trigger] memperm.contains_range(r) && memperm.contains_init_mem(r) by {
            assert(prev_memperm.contains_init_mem(r));
        }
    }
    Tracked(SnpMemCoreConsole { memperm, cc })
}

} // verus!

================
File: ./source/verismo/src/boot/init/mod.rs
================

mod e820_fmt;
mod e820_init;
mod e820_init_alloc;
mod init_e;
mod init_s;
mod mshv_alloc;
mod mshv_fmt;
mod mshv_init;

pub use init_e::*;
pub use init_s::*;

use super::*;
use crate::addr_e::*;
use crate::boot::monitor_params::{
    MonitorParamPerms, MonitorParamPermsToData, MonitorParams, ValidatedE820Table,
    MAX_VALIDATED_E820,
};
use crate::boot::mshyper::*;
use crate::boot::params::*;
use crate::debug::{SlicePrinter, VEarlyPrintAtLevel, VPrint};
use crate::global::*;
use crate::mem::{RawMemPerms, SnpMemCoreConsole};
use crate::ptr::*;
use crate::registers::SnpCore;
use crate::snp::ghcb::*;
use crate::snp::SnpCoreConsole;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

def_asm_addr_for! {bootstack = boot_stack}
def_asm_addr_for! {bootstack_end = boot_stack_end}
def_asm_addr_for! {verismo_start_addr = verismo_start}
def_asm_addr_for! {verismo_end_addr = _monitor_end}
def_asm_addr_for! {ap_entry_addr = ap_entry}

================
File: ./source/verismo/src/boot/init/mshv_init.rs
================

use vstd::slice::{slice_index_get, slice_subrange};

use super::mshv_alloc::InitAllocFn;
use super::*;
use crate::allocator::VeriSMoAllocator;
use crate::arch::addr_s::VM_MEM_SIZE;
use crate::boot::init::e820_init::*;
use crate::lock::LockPermRaw;
use crate::vbox::{MutFnTrait, VBox};

verismo_simple! {
    pub open spec fn init_vm_mem_requires(
        e820: &[E820Entry],
        static_start: usize, static_end: usize,
        memcc: SnpMemCoreConsole,
        unused_preval_memperm: RawMemPerms
    ) -> bool {
        &&& e820@.is_constant()
        &&& mem_range_formatted(e820@)
        &&& memcc.wf()
        &&& static_start.spec_valid_addr_with(1)
        &&& static_end.spec_valid_addr_with(0)
        &&& static_start.is_constant()
        &&& static_end.is_constant()
        &&& static_start < static_end
        &&& unused_preval_memperm.wf()
        &&& forall |r| e820@.to_aligned_ranges().contains(r) ==>
            unused_preval_memperm.contains_default_except(r, e820@.to_valid_ranges())
        &&& memcc.memperm.contains_init_except((0, VM_MEM_SIZE as nat), e820@.to_aligned_ranges())
    }
}

verus! {

pub fn process_vm_mem(
    hv_mem_slice: &[HyperVMemMapEntry],
    e820: &[E820Entry],
    static_start: usize,
    static_end: usize,
    Tracked(memcc): Tracked<SnpMemCoreConsole>,
    Tracked(unused_preval_memperm): Tracked<RawMemPerms>,
    Tracked(alloc_perm): Tracked<SnpPointsTo<VeriSMoAllocator>>,
    Tracked(alloc_lock): Tracked<&mut LockPermRaw>,
) -> (newcc: Tracked<SnpCoreConsole>)
    requires
        is_alloc_perm(alloc_perm@),
        old(alloc_lock)@.is_clean_lock_for(spec_ALLOCATOR().ptr_range(), memcc.cc.snpcore.cpu()),
        init_vm_mem_requires(e820, static_start, static_end, memcc, unused_preval_memperm),
        hv_mem_slice@.is_constant(),
        mem_range_formatted(hv_mem_slice@),
        hv_mem_slice@.len() > 0,
    ensures
        newcc@.wf_core(memcc.cc.snpcore.cpu()),
        newcc@.snpcore.only_reg_coremode_updated(memcc.cc.snpcore, set![GHCB_REGID()]),
        spec_ALLOCATOR().lock_default_mem_requires(newcc@.snpcore.cpu(), alloc_lock@),
        is_permof_ALLOCATOR(alloc_lock@),
{
    let ghost verismo_range = range(static_start as int, static_end as int);
    let tracked mut memcc = memcc;
    let entry = slice_index_get(hv_mem_slice, 0);
    let tracked SnpMemCoreConsole { memperm, mut cc } = memcc;
    SlicePrinter { s: hv_mem_slice }.debug(Tracked(&mut cc));
    proof {
        memcc = SnpMemCoreConsole { memperm, cc };
        assert(cc.wf());
        assert(memperm.wf());
    }
    // initial 1:1 mapping
    if entry.start().to_addr().reveal_value() > static_start || entry.end().to_addr().reveal_value()
        < static_end {
        // verismo code and static vars are not covered by the first range?
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut memcc.cc.snpcore));
    }
    let Tracked(memcc) = validate_vm_mem(hv_mem_slice, e820, Tracked(memcc));
    let alloc_ref = ALLOCATOR();
    let alloc_ptr = alloc_ref.ptr();
    let mut allocator: VBox<VeriSMoAllocator> = VBox::from_raw(
        alloc_ptr.to_usize(),
        Tracked(alloc_perm),
    );
    proof {
        let tracked SnpMemCoreConsole { mut memperm, cc } = memcc;
        let ghost prev_memperm = memperm;
        memperm.tracked_union(unused_preval_memperm);
        assert forall|i: int| 0 <= i < hv_mem_slice@.len() implies memperm.contains_default_except(
            (#[trigger] hv_mem_slice@[i]).range(),
            e820@.to_valid_ranges(),
        ) by {
            RawMemPerms::lemma_union_propograte_except(
                prev_memperm,
                unused_preval_memperm,
                hv_mem_slice@[i].range(),
                SwSnpMemAttr::spec_default(),
                e820@.to_aligned_ranges(),
                e820@.to_valid_ranges(),
            );
        }
        let static_range = range(static_start as int, static_end as int);
        let first_range = hv_mem_slice@[0].range();
        assert(inside_range(static_range, first_range));
        let prev_memperm = memperm;
        memperm.proof_remove_range_ensures(verismo_range);
        memperm.tracked_split(verismo_range);
        assert forall|i: int| 0 <= i < hv_mem_slice@.len() implies memperm.contains_default_except(
            hv_mem_slice@[i].range(),
            e820@.to_valid_ranges().insert(verismo_range),
        ) by {
            let excepted = e820@.to_valid_ranges().insert(verismo_range);
            assert forall|r|
                inside_range(r, hv_mem_slice@[i].range()) && ranges_disjoint(excepted, r) && r.1
                    != 0 implies memperm.contains_range(r) && memperm.contains_default_mem(r) by {
                assert(excepted.contains(verismo_range));
                assert(range_disjoint_(verismo_range, r));
                assert(memperm.eq_at(prev_memperm, r));
                assert(ranges_disjoint(e820@.to_valid_ranges(), r)) by {
                    assert forall|rr| e820@.to_valid_ranges().contains(rr) implies range_disjoint_(
                        rr,
                        r,
                    ) by {
                        assert(excepted.contains(rr));
                    }
                }
                assert(prev_memperm.contains_default_except(
                    hv_mem_slice@[i].range(),
                    e820@.to_valid_ranges(),
                ));
                assert(prev_memperm.contains_range(r));
            }
        }
        memcc = SnpMemCoreConsole { memperm, cc };
    }
    let Tracked(cc) = allocator.box_update(
        (InitAllocFn, hv_mem_slice, e820, static_start, static_end, Tracked(memcc)),
    );
    let (_, Tracked(alloc_perm)) = allocator.into_raw();
    proof {
        assert(allocator@@.inv());
        alloc_lock.tracked_bind_new(VeriSMoAllocator::invfn(), alloc_perm);
        assert(alloc_lock@.invfn.inv(allocator@));
        assert(alloc_lock@.cpu == cc.snpcore.cpu());
        assert(alloc_lock@.is_unlocked(
            cc.snpcore.cpu(),
            spec_ALLOCATOR().lockid(),
            spec_ALLOCATOR().ptr_range(),
        ));
        assert(spec_ALLOCATOR().lock_requires(cc.snpcore.cpu(), alloc_lock@));
    }
    Tracked(cc)
}

} // verus!
verismo_simple! {
    fn validate_vm_mem(hv_mem_tb: &[HyperVMemMapEntry], e820: &[E820Entry], Tracked(memcc): Tracked<SnpMemCoreConsole>) -> (newmemcc: Tracked<SnpMemCoreConsole>)
    requires
        e820@.is_constant(),
        mem_range_formatted(e820@),
        memcc.wf(),
        memcc.memperm.contains_init_except((0, VM_MEM_SIZE as nat), e820@.to_aligned_ranges()),
        hv_mem_tb@.is_constant(),
        mem_range_formatted(hv_mem_tb@),
    ensures
        newmemcc@.wf(),
        newmemcc@.cc.snpcore.cpu() == memcc.cc.snpcore.cpu(),
        newmemcc@.cc.snpcore.only_reg_coremode_updated(
            memcc.cc.snpcore,
            set![GHCB_REGID()]),
        forall |i: int| 0 <= i < hv_mem_tb@.len() ==>
            newmemcc@.memperm.contains_default_except((#[trigger]hv_mem_tb@[i]).range(), e820@.to_aligned_ranges()),
    {
        let mut idx: usize = 0;
        let mut prev_end: usize = 0;
        let tracked mut memcc = memcc;
        let ghost oldmemcc = memcc;
        let ghost cpu = memcc.cc.snpcore.cpu();
        let len = usize_s::constant(hv_mem_tb.len());
        let ghost pre_validated = e820@.to_aligned_ranges();
        while idx < len
        invariant
            idx <= len,
            idx.is_constant(),
            len.is_constant(),
            e820@.is_constant(),
            pre_validated === e820@.to_aligned_ranges(),
            mem_range_formatted(e820@),
            len == hv_mem_tb@.len(),
            hv_mem_tb@.is_constant(),
            mem_range_formatted(hv_mem_tb@),
            prev_end.spec_valid_addr_with(0),
            prev_end.is_constant(),
            idx == 0 ==> prev_end as int == 0,
            idx > 0 ==> prev_end as int == hv_mem_tb@[idx as int - 1].range().end(),
            (memcc).wf_core(cpu),
            memcc.cc.snpcore.only_reg_coremode_updated(
                oldmemcc.cc.snpcore,
                set![GHCB_REGID()]),
            forall |i: int| 0 <= i < idx as int ==>
                memcc.memperm.contains_default_except(
                    (#[trigger]hv_mem_tb@[i]).range(), pre_validated),
            forall |i: int| 0 <= i < idx as int ==>
                prev_end as int >= (#[trigger]hv_mem_tb@[i]).range().end(),
            memcc.memperm.contains_init_except(range(prev_end as int, VM_MEM_SIZE!()), pre_validated),
        {
            let entry = slice_index_get(hv_mem_tb, idx as usize_t);
            let start_gpn = entry.starting_gpn as usize;
            let npages = entry.numpages as usize;
            let tracked SnpMemCoreConsole{mut memperm, mut cc} = memcc;
            let tracked mut cc: SnpCoreConsole = cc;
            let ghost prev_memperm = memperm;
            proof {
                assert(npages != 0);
                assert(cc.wf());
                assert(memperm.wf());
            }
            if !start_gpn.check_valid_pn(npages) ||
                prev_end > start_gpn.to_addr()
            {
                // HV reported overlapped memory
                vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cc.snpcore));
            }

            let end_gpn = start_gpn.add(npages);
            let start = start_gpn.to_addr();
            let end = end_gpn.to_addr();
            let ghost used_range = range(start as int, end as int);
            let ghost remain_init_range = range(start as int, VM_MEM_SIZE!());
            proof {reveal_strlit("range: ");}
            (new_strlit("range: "), (start as u64, end as u64)).debug(Tracked(&mut cc));
            proof {
                assert(used_range === hv_mem_tb@[idx as int].range());
                memperm.lemma_with_except_sub(remain_init_range, range(prev_end as int, VM_MEM_SIZE!()), SwSnpMemAttr::init(), pre_validated);
                //memcc.memperm.lemma_invalided_range_sub(
                //    e820@.to_set(), range(prev_end, VM_MEM_SIZE as int), range(start, VM_MEM_SIZE as int));
                assert(cc.wf());
                assert(memperm.wf());
                memcc = SnpMemCoreConsole{memperm, cc};
            }
            let Tracked(newmemcc) = validate_e820(e820, start.reveal_value(), end.reveal_value(), Tracked(memcc));
            prev_end = end;
            idx = idx + 1;
            proof {
                memcc = newmemcc;
                assert forall |i: int| 0 <= i < (idx as int)
                implies
                    memcc.memperm.contains_default_except(
                        (#[trigger]hv_mem_tb@[i]).range(), pre_validated)
                by {
                    if i < (idx as int - 1) {
                        assert(prev_memperm.contains_default_except(hv_mem_tb@[i].range(), pre_validated));
                        assert(range_disjoint_(hv_mem_tb@[i].range(), used_range));
                    }
                }
            }
        }
        Tracked(memcc)
    }
}

================
File: ./source/verismo/src/boot/init/e820_fmt.rs
================

use vstd::slice::{slice_index_get, slice_subrange};

use super::*;
use crate::arch::addr_s::{GVA, VM_MEM_SIZE};
use crate::ptr::rmp::*;

verismo_simple! {
    pub open spec fn e820_lt() -> spec_fn(E820Entry, E820Entry) -> bool {
        |v1: E820Entry, v2: E820Entry| v1.addr < v2.addr
    }
}

verus! {

pub fn e820_format<const N: usize_t>(
    e820tb: &mut Array<E820Entry, N>,
    e820_entries: usize_t,
) -> (ret: Option<&[E820Entry]>)
    requires
        old(e820tb).is_constant(),
        e820_entries.is_constant(),
        e820_entries < old(e820tb)@.len(),
    ensures
        ret.is_Some() ==> e820tb.is_constant(),
        ret.is_Some() ==> (ret.get_Some_0()@.is_constant() && ret.get_Some_0()@.len() <= (
        e820_entries as nat)),
        ret.is_Some() ==> ret.get_Some_0()@ === e820tb@.take(ret.get_Some_0()@.len() as int),
        ret.is_Some() ==> format_range_ensures(
            ret.get_Some_0()@,
            old(e820tb)@.take(e820_entries as int),
            e820_entries as nat,
        ),
{
    let ghost prev_e820tb = e820tb@;
    proof {
        assert(prev_e820tb.is_constant());
        assert forall|i| 0 <= i < (e820_entries as int) implies (
        #[trigger] prev_e820tb[i]).spec_real_range().0.is_constant()
            && prev_e820tb[i].spec_real_range().1.is_constant() by {
            proof_into_is_constant::<_, usize_s>(prev_e820tb[i].addr);
            proof_into_is_constant::<_, usize_s>(prev_e820tb[i].size);
        }
    }
    let ghost prev_e820 = e820tb@.take(e820_entries as int);
    let (visited, e820_len) = e820tb.format_range(e820_entries);
    if visited < e820_entries {
        // wrong format
        ((new_strlit("visited < e820_entries"), visited), e820_entries).leak_debug();
        return Option::None;
    }
    let e820 = e820tb.as_slice();
    let e820 = slice_subrange(e820, 0, e820_len);
    proof {
        let e820 = e820@;
        assert(e820.is_constant()) by {
            assert(prev_e820.is_constant());
            assert(format_range_ensures(e820, prev_e820, visited as nat));
            assert forall|i| 0 <= i < (e820_len as int) implies is_format_entry(
                e820[i],
                prev_e820,
            ) by {};
            assert forall|i| 0 <= i < e820.len() implies e820[i].is_constant() by {
                let entry = e820[i];
                assert(is_format_entry(entry, prev_e820));
                proof_into_is_constant::<_, u64_s>(entry.spec_real_range().0);
                proof_into_is_constant::<_, u64_s>(entry.spec_real_range().1);
                let j = choose|j|
                    entry === prev_e820[j].spec_set_range(entry.spec_real_range()) && (0 <= j) && j
                        < prev_e820.len();
                assert(prev_e820[j].is_constant());
                assert(entry.is_constant());
            }
        }
        assert(e820tb.is_constant()) by {
            assert forall|i| 0 <= i < e820tb@.len() implies e820tb@[i].is_constant() by {
                assert(prev_e820tb[i].is_constant());
                if i >= e820.len() {
                    assert(prev_e820tb.contains(e820tb@[i]));
                    let j = choose|j| prev_e820tb[j] === e820tb@[i] && 0 <= j < prev_e820tb.len();
                    assert(e820tb@[i] === prev_e820tb[j]);
                    assert(prev_e820tb[j].is_constant());
                } else {
                    assert(e820tb@[i] === e820[i]);
                }
            }
        }
    }
    Option::Some(e820)
}

} // verus!

================
File: ./source/verismo/src/boot/mshyper/mod.rs
================

use crate::addr_e::*;
use crate::ptr::*;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

mod param_e;
pub use param_e::*;

================
File: ./source/verismo/src/boot/mshyper/param_e.rs
================

use super::*;
use crate::arch::addr_s::VM_PAGE_NUM;
use crate::debug::VPrint;

verismo_simple! {
#[derive(VClone, Copy, VPrint)]
#[repr(C, align(1))]
pub struct HyperVMemMapEntry {
    pub starting_gpn: u64,
    pub numpages: u64,
    pub mem_type: u16,
    pub flags: u16,
    pub reserved: u32,
}

proof fn lemma_eq_hv_mementry(v1: HyperVMemMapEntry, v2: HyperVMemMapEntry)
requires
    v1.starting_gpn === v2.starting_gpn,
    v1.numpages === v2.numpages,
    v1.mem_type === v2.mem_type,
    v1.flags === v2.flags,
    v1.reserved === v2.reserved,
ensures
    v1 === v2
{}

impl MemRangeInterface for HyperVMemMapEntry {
    #[verifier(inline)]
    open spec fn self_wf(&self) -> bool {
        self.wf()
    }

    #[verifier(inline)]
    open spec fn real_wf(r: (usize_s, usize_s)) -> bool {
        r.self_wf()
    }

    proof fn proof_constant_real_wf(r: (usize_s, usize_s)) {}

    open spec fn spec_end_max() -> usize {
        VM_PAGE_NUM
    }

    open spec fn spec_real_range(&self) -> (usize_s, usize_s) {
        (self.starting_gpn.vspec_cast_to(), self.numpages.vspec_cast_to())
    }

    #[inline]
    fn real_range(&self) -> (usize_s, usize_s) {
        (self.starting_gpn.into(), self.numpages.into())
    }

    #[verifier(inline)]
    open spec fn spec_set_range(self, r: (usize_s, usize_s)) -> Self {
        HyperVMemMapEntry {
            starting_gpn: r.0.vspec_cast_to(),
            numpages: r.1.vspec_cast_to(),
            mem_type: self.mem_type,
            flags: self.flags,
            reserved: self.reserved,
        }
    }

    fn update_range(&mut self, r: (usize_s, usize_s)){
        let ghost oldself = *self;
        self.starting_gpn = r.0 as u64;
        self.numpages = r.1 as u64;
        proof {
            proof_sectype_cast_eq::<usize, u64, ()>(r.0);
            proof_sectype_cast_eq::<usize, u64, ()>(r.1);
            assert(r.0 as u64_s as usize_s === r.0);
            assert(r.1 as u64_s as usize_s === r.1);
            assert(self.starting_gpn === oldself.spec_set_range(r).starting_gpn);
            assert(self.numpages === oldself.spec_set_range(r).numpages);
            assert(self.flags === oldself.spec_set_range(r).flags);
            assert(self.reserved === oldself.spec_set_range(r).reserved);
            assert(self.mem_type === oldself.spec_set_range(r).mem_type);
            lemma_eq_hv_mementry(oldself.spec_set_range(r), *self);
            // BUG(verus):
            assume(oldself.spec_set_range(r) === *self);
        }
    }

    #[inline]
    fn end_max() -> usize_s {
        VM_PAGE_NUM as usize_s
    }
}


pub type HyperVMemMapTable = Array<HyperVMemMapEntry, 0x80>;

#[derive(SpecGetter, SpecSetter)]
#[repr(C, align(1))]
pub struct HvParamTable {
    pub cpu_count: u64,
    pub reserved: Array<u8, 16>,
    pub mem_table: HyperVMemMapTable,
}
}

verismo_simple! {
    impl HyperVMemMapEntry {
        #[verifier(inline)]
        pub open spec fn range(&self) -> (int, nat) {
            (self.starting_gpn as int * PAGE_SIZE!(),  ((self.numpages as nat) * PAGE_SIZE!()) as nat)
        }
    }
}

================
File: ./source/verismo/src/boot/params.rs
================

use vstd::slice::slice_index_get;

use super::*;
use crate::arch::addr_s::VM_MEM_SIZE;
use crate::debug::VPrint;
use crate::registers::SnpCore;
use crate::snp::SnpCoreConsole;
use crate::vbox::{BorrowFnTrait, MutFnTrait};

verus! {

pub const E820_TYPE_RAM: u32 = 1;

pub const E820_TYPE_RSVD: u32 = 2;

pub const E820_TYPE_ACPI: u32 = 3;

pub const E820_TYPE_READONLY: u32 = 3;

pub const E820_MAX_LEN: usize = 128;

pub const VERISMO_DYNAMIC_MEM_MAX: usize = 0x10000;

pub type E820Table = Array<E820Entry, E820_MAX_LEN>;

} // verus!
verismo_simple! {
#[repr(C, packed)]
#[derive(VClone, Copy)]
pub struct E820Entry {
    pub addr: u64,
    pub size: u64,
    pub memty: u32,
}
}

verus! {

impl VPrint for E820Entry {
    #[verifier(inline)]
    open spec fn early_print_requires(&self) -> bool {
        self.is_constant()
    }

    fn early_print2(
        &self,
        Tracked(snpcore): Tracked<&mut crate::registers::SnpCore>,
        Tracked(console): Tracked<SnpPointsToRaw>,
    ) -> (newconsole: Tracked<SnpPointsToRaw>) {
        let addr = self.addr;
        let size = self.size;
        let memty = self.memty;
        ((addr, size), memty).early_print2(Tracked(snpcore), Tracked(console))
    }
}

impl MemRangeInterface for E820Entry {
    #[verifier(inline)]
    open spec fn self_wf(&self) -> bool {
        self.wf()
    }

    #[verifier(inline)]
    open spec fn real_wf(r: (usize_s, usize_s)) -> bool {
        r.self_wf()
    }

    proof fn proof_constant_real_wf(r: (usize_s, usize_s)) {
    }

    open spec fn spec_real_range(&self) -> (usize_s, usize_s) {
        (self.addr.vspec_cast_to(), self.size.vspec_cast_to())
    }

    #[inline]
    fn real_range(&self) -> (usize_s, usize_s) {
        (self.addr.into(), self.size.into())
    }

    open spec fn spec_set_range(self, r: (usize_s, usize_s)) -> Self {
        E820Entry { addr: r.0.vspec_cast_to(), size: r.1.vspec_cast_to(), memty: self.memty }
    }

    fn update_range(&mut self, r: (usize_s, usize_s)) {
        let ghost oldself = *self;
        self.addr = r.0.into();
        self.size = r.1.into();
        proof {
            proof_sectype_cast_eq::<usize, u64, ()>(r.0);
            proof_sectype_cast_eq::<usize, u64, ()>(r.1);
            assert(oldself.spec_set_range(r) === E820Entry {
                addr: r.0.vspec_cast_to(),
                size: r.1.vspec_cast_to(),
                memty: self.memty,
            });
            // BUG(verus):
            assume(oldself.spec_set_range(r) === *self);
        }
    }

    open spec fn spec_end_max() -> usize {
        VM_MEM_SIZE
    }

    #[inline]
    fn end_max() -> usize_s {
        VM_MEM_SIZE.into()
    }
}

} // verus!
verismo_simple! {
impl E820Entry {
    #[verifier(inline)]
    pub open spec fn spec_aligned_end(&self) -> int {
        self.spec_aligned_range().end()
    }
}

#[verifier(inline)]
pub open spec fn e820_disjoint(e820: Seq<E820Entry>, r: (int, nat)) -> bool {
    ranges_disjoint(e820.to_valid_ranges(), r)
    //forall |e| e820.contains(e) ==> range_disjoint_(r, e.spec_range())
}

pub open spec fn e820_align_include(e820: Set<E820Entry>, r: (int, nat)) -> bool {
    exists |e| e820.contains(e) && inside_range(r, e.spec_aligned_range())
}

pub open spec fn e820_align_available_include(e820: Set<E820Entry>, r: (int, nat)) -> bool {
    exists |e| e820.contains(e) && inside_range(r, e.spec_aligned_range()) && e.memty != E820_TYPE_RSVD
}

#[repr(C, packed)]
#[derive(Copy, VClone)]
pub struct SetupHeader {
    pub setup_sects: u8,
    pub root_flags: u16,
    pub syssize: u32,
    pub ram_size: u16,
    pub vid_mode: u16,
    pub root_dev: u16,
    pub boot_flag: u16,
    pub jump: u16,
    pub header: u32,
    pub version: u16,
    pub realmode_swtch: u32,
    pub start_sys_seg: u16,
    pub kernel_version: u16,
    pub type_of_loader: u8,
    pub loadflags: u8,
    pub setup_move_size: u16,
    pub code32_start: u32,
    pub ramdisk_image: u32,
    pub ramdisk_size: u32,
    pub bootsect_kludge: u32,
    pub heap_end_ptr: u16,
    pub ext_loader_ver: u8,
    pub ext_loader_type: u8,
    pub cmd_line_ptr: u32,
    pub initrd_addr_max: u32,
    pub kernel_alignment: u32,
    pub relocatable_kernel: u8,
    pub min_alignment: u8,
    pub xloadflags: u16,
    pub cmdline_size: u32,
    pub hardware_subarch: u32,
    pub hardware_subarch_data: u64,
    pub payload_offset: u32,
    pub payload_length: u32,
    pub setup_data: u64,
    pub pref_address: u64,
    pub init_size: u32,
    pub handover_offset: u32,
    pub kernel_info_offset: u32,
}

// NOTE: Rust warns that 1. align(1) structure needs Copy, 2. Copy needs Clone;
#[repr(C, packed)]
#[derive(SpecGetter, SpecSetter, Copy, VClone)]
pub struct BootParams {
    pub _pad0: [u8; 0x70],
    pub acpi_rsdp_addr: u64, // 0x070
    pub _pad1: [u8; 0x50],
    pub _ext_cmd_line_ptr: u32, // 0xc8
    pub _pad2_0: Array<u8, { 0x13c - 0xc8 - 4 }>,
    pub cc_blob_addr: u32, // 0x13c
    pub _pad2_1: Array<u8, { 0x1e8 - 0x13c - 4 }>,
    #[def_offset]
    pub e820_entries: u8, // 0x1e8
    pub reserved_4: Array<u8, { 0x1f1 - 0x1e8 - 1 }>,
    pub hdr: SetupHeader, // 0x1f1
    pub reserved_5: Array<u8, { 0x2d0 - 0x1f1 - 123 }>,
    #[def_offset]
    pub e820: E820Table, // 0x2d0
    pub reserved_6: Array<u8, { 4096 - 0xcd0 }>,
}
}

================
File: ./source/verismo/src/boot/idt/idt_reg_t.rs
================

use super::*;

================
File: ./source/verismo/src/boot/idt/def.rs
================

use super::*;
use crate::debug::VPrint;

verus! {

pub const IDT_MAX_ENTRIES: usize_t = 256usize as usize_t;

pub const ENTRY_OPTION_MIN: u16_t = 0xe00;

// 0b1110_0000_0000;
pub const ENTRY_MIN_PRE: u16_t = 0x8e00;

//0b1000_1110_0000_0000;

} // verus!
verismo_simple! {
#[repr(C, align(1))]
#[derive(VPrint)]
pub struct InterruptStackFrame {
    pub reserved: u64,
    pub vector: u64,
    pub exception: u64,
    pub rip: u64,
    pub cs: u64,
    pub rflags: u64,
    pub rsp: u64,
    pub ss: u64,
}
pub type EntryOptions = u16;

#[derive(Copy, VClone)]
#[repr(C, align(1))]
pub struct IDTEntry {
    pub pointer_low: u16,
    pub gdt_selector: u16,
    pub options: u16,
    pub pointer_middle: u16,
    pub pointer_high: u32,
    pub reserved: u32,
}

verus! {

pub type InterruptDescriptorTable = Array<IDTEntry, IDT_MAX_ENTRIES>;

} // verus!
verismo_simple!{
    #[repr(C, packed)]
    #[derive(VDefault)]
    pub struct Idtr {
        /// Size of the DT.
        pub limit: u16,
        /// Pointer to the memory region containing the DT.
        pub base: u64,
    }
}

crate::impl_dpr!{
    IdtBaseLimit, Idtr, "idt", IdtrBaseLimit,
}
}

================
File: ./source/verismo/src/boot/idt/dummy.rs
================

use core::arch::global_asm;

use super::*;
use crate::arch::reg::RegName;
use crate::debug::VPrintAtLevel;
use crate::lock::MapLockContains;
use crate::snp::ghcb::GHCB_REGID;
use crate::snp::{SnpCoreConsole, SnpCoreSharedMem};
use crate::vbox::VBox;

#[cfg(target_os = "none")]
global_asm!(include_str!("isr.s"), options(att_syntax));

verus! {

// Requires the exception code and stackframe is not secret.
// This holds since we do not have secret-dependent control flows.
fn debug_handler(
    code: u64_s,
    stack_frame: &InterruptStackFrame,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        stack_frame.is_constant(),
        code.is_constant(),
        old(cs).inv_ac(),
    ensures
        cs.inv_ac(),
        cs.only_lock_reg_coremode_updated(
            *old(cs),
            set![GHCB_REGID()],
            set![spec_CONSOLE_lockid()],
        ),
{
    proof {
        reveal_strlit("idt handler for error:");
        reveal_strlit("\n");
        reveal_strlit("stack info: ");
        SnpCoreSharedMem::lemma_update_prop_auto();
    }
    (new_strlit("idt handler for error:"), code).err(Tracked(cs));
    new_strlit("\n").err(Tracked(cs));
    (new_strlit("stack info: ")).err(Tracked(cs));
    stack_frame.err(Tracked(cs));
    new_strlit("\n").err(Tracked(cs));
}

#[no_mangle]
pub extern "C" fn dummy_handler_fn(
    stack_frame: InterruptStackFrame,
    error: u64_s,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        stack_frame.is_constant(),
        error.is_constant(),
        old(cs).inv_ac(),
    ensures
        cs.inv_ac(),
{
    debug_handler(error, &stack_frame, Tracked(cs))
}

} // verus!
seq_macro::seq!(N in 0..21 {
    #(
        def_asm_addr_for! {
            isr_handler_addr_~N = isr_handler0
        }
    )*
});

seq_macro::seq!(N in 28..29 {
    def_asm_addr_for! {
        isr_handler_addr_~N = isr_handler~N
    }
});

verus! {

impl IDTEntry {
    verismo_simple! {
    #[verifier(inline)]
    pub open spec fn ensures_from_addr_selector(self, addr: u64_s, gdt_selector: u16_s) -> bool
    {
        &&& self.pointer_low === addr as u16_s
        &&& self.pointer_middle === (addr >> (16u64 as u64_s)) as u16_s
        &&& self.pointer_high ===  (addr >> (32u64 as u64_s)) as u32_s
        &&& self.gdt_selector === gdt_selector
        &&& self.options === ENTRY_MIN_PRE as EntryOptions
        &&& self.reserved === 0u32 as u32_s
    }
    }

    verus! {
    pub fn from_addr_selector(addr: u64, gdt_selector: u16) -> (ret: Self)
    requires
        gdt_selector.is_constant(),
        addr.is_constant(),
    ensures
        ret.is_constant(),
        ret.pointer_low@.val == addr as u16,
        ret.pointer_middle@.val == (addr >> 16u64) as u16,
        ret.pointer_high@.val ==  (addr >> 32u64) as u32,
        ret.gdt_selector@.val == gdt_selector,
        ret.options@.val == ENTRY_MIN_PRE,
        ret.reserved@.val == 0u32,
    {
        IDTEntry{
            pointer_low: addr.into(),
            pointer_middle: (addr >> 16u64).into(),
            pointer_high: (addr >> 32u64).into(),
            gdt_selector: gdt_selector.into(),
            options: ENTRY_MIN_PRE.into(),
            reserved: 0u32.into(),
        }
    }
    }
}

} // verus!
verus! {

pub fn init_idt_content(idt: &mut InterruptDescriptorTable, gdt_selector: u16)
    requires
        gdt_selector.is_constant(),
    ensures
        idt.wf(),
        idt.is_constant(),
{
    let dummy_handler = isr_handler_addr_0();
    let mut i: usize = 0;
    while i < idt.len()
        invariant
            i <= idt@.len(),
            dummy_handler == spec_isr_handler_addr_0(),
            dummy_handler.is_constant(),
            gdt_selector.is_constant(),
            forall|k: int| 0 <= k < (i as int) ==> idt@[k].is_constant(),
            i.is_constant(),
    {
        idt.update(i, IDTEntry::from_addr_selector(dummy_handler as u64, gdt_selector));
        i = i + 1;
    }/*assert(idt@.len() == 256);
    seq_macro::seq!(N in 0..21 {
        #(
            idt.update((N as usize_t).into(), IDTEntry::from_addr_selector(isr_handler_addr_~N().into(), gdt_selector));
        )*
    });
    seq_macro::seq!(N in 28..29 {
        #(
            idt.update((N as usize_t).into(), IDTEntry::from_addr_selector(isr_handler_addr_~N().into(), gdt_selector));
        )*
    });*/

}

} // verus!
verus! {

#[verifier(external_body)]
pub fn box_init_idt_content(idt: &mut VBox<InterruptDescriptorTable>, gdt_selector: u16) {
    init_idt_content(&mut *idt.b, gdt_selector);
}

} // verus!
verus! {

pub trait HasIDT {
    spec fn has_idt(&self) -> bool;
}

pub closed spec fn idt_wf(val: Idtr) -> bool {
    val.wf()
}

} // verus!
verus! {

impl HasIDT for Map<RegName, RegisterPerm> {
    // Reveal regs but hide bit ops.
    open spec fn has_idt(&self) -> bool {
        idt_wf(self[RegName::IdtrBaseLimit]@.value())
    }
}

} // verus!
verus! {

pub fn init_idt(Tracked(cs): Tracked<&mut SnpCoreSharedMem>)
    requires
        (*old(cs)).inv_ac(),
    ensures
        cs.inv_ac(),
        cs.snpcore.regs.has_idt(),
        cs.only_lock_reg_updated(
            (*old(cs)),
            set![RegName::IdtrBaseLimit],
            set![spec_ALLOCATOR_lockid()],
        ),
{
    let tracked mut cs_perm;
    let tracked mut idt_perm;
    let mut idt = VBox::new_uninit(Tracked(cs));
    proof {
        idt_perm = cs.snpcore.regs.tracked_remove(RegName::IdtrBaseLimit);
        cs_perm = cs.snpcore.regs.tracked_borrow(RegName::Cs);
    }
    let gdt_selector = CS.read(Tracked(cs_perm)).reveal_value();
    box_init_idt_content(&mut idt, gdt_selector);
    // convert vbox to raw mem.
    let (idt_addr, idt_memperm) = idt.into_raw();
    // TODO: adjust pte.
    assume(!idt_memperm@@.snp().pte().w);
    let dtp = Idtr { base: idt_addr.as_u64().into(), limit: 0xffffu64.into() };
    assert(dtp.is_constant());
    IdtBaseLimit.write(dtp, Tracked(&mut idt_perm));
    proof {
        cs.snpcore.regs.tracked_insert(RegName::IdtrBaseLimit, idt_perm);
    }
}

} // verus!

================
File: ./source/verismo/src/boot/idt/mod.rs
================

mod def;
mod dummy;
mod idt_reg_t;

pub use def::*;
pub use dummy::*;

use crate::arch::reg::RegName;
use crate::global::*;
use crate::ptr::*;
use crate::registers::*;
use crate::tspec::*;
use crate::tspec_e::*;

================
File: ./source/verismo/src/boot/idt/isr.s
================


.macro isr_handler ex:req err=0
.global isr_handler\ex
isr_handler\ex:
.if  \err
.else
    pushq $0 /* Dummy error code for this type */
.endif
    // Push exception code on the stack
    pushq $\ex
    // Ensure 16-byte stack pointer alignment
    // `reserved` in `ExceptionArguments`
    pushq $0x0
    callq dummy_handler_fn

    // We should not return form handle_generic_exception.
    // In case we do, cause a page-fault to ease debugging
    iretq

isr_early.loop\ex:
	hlt
	jmp isr_early.loop\ex
.endm

isr_handler 0
isr_handler 1
isr_handler 2
isr_handler 3
isr_handler 4
isr_handler 5
isr_handler 6
isr_handler 7
isr_handler 8
/* Double-fault is always going to isr_handler_early8 */
isr_handler 9
isr_handler 10,1
isr_handler 11,1
isr_handler 12,1
isr_handler 13,1
isr_handler 14,1
isr_handler 15,1
isr_handler 16
isr_handler 17
isr_handler 18,
/* Machine check is always going to isr_handler_early18 */
isr_handler 19
isr_handler 20
isr_handler 21
// HV
isr_handler 28,1
// VC
isr_handler 29,1

/* Classic PIC interrupts */
isr_handler 32
================
File: ./source/verismo/src/boot/monitor_params.rs
================

use super::*;
use crate::arch::addr_s::PAGE_SIZE;
use crate::boot::params::{E820Entry, E820_TYPE_RSVD};
use crate::debug::VPrint;
use crate::registers::SnpCore;
use crate::snp::SnpCoreConsole;

pub const MAX_VALIDATED_E820: usize = 16;

verismo_simple! {
pub type ValidatedE820Table = [E820Entry; MAX_VALIDATED_E820];

#[repr(C, align(1))]
#[derive(SpecSetter, SpecGetter)]
pub struct MonitorParams {
    pub cpu_count: u64,     // CPU count,
    pub cpuid_page: u64,    // cpuid page gpa
    pub secret_page: u64,   // secret page gpa
    pub hv_param: u64,      // param page gpa
    pub validated_entries: u64,
    pub validated_e820: ValidatedE820Table,
    pub acpi: u64,
    pub acpi_size: u64,
    pub richos_start: u64,
    pub richos_size: u64,
    pub richos_cmdline: [u8; 256],
    pub richos_cmdline_len: u64,
}

#[cfg(debug_assertions)]
#[repr(C, align(1))]
#[derive(SpecSetter, VPrint, SpecGetter)]
pub struct SimpleMonitorParams {
    pub cpuid_page: u64,         // cpuid page gpa
    pub secret_page: u64, // secret page gpa
    pub hv_param: u64,            // param page gpa
    pub validated_entries: u64,
    pub acpi: u64,
    pub acpi_size: u64,
    pub richos_start: u64,
    pub richos_size: u64,
    pub richos_cmdline_len: u64,
}

impl MonitorParams {
    pub open spec fn e820(&self) -> Seq<E820Entry> {
        let e820 = self.validated_e820@;
        let n = self.validated_entries;
        e820.take(n.vspec_cast_to())
    }

    pub open spec fn mp_wf(&self) -> bool {
        &&& self.validated_entries <= self.validated_e820@.len()
        &&& (self.acpi@.val as int).spec_valid_addr_with(self.acpi_size@.val as nat)
        &&& (self.richos_start@.val  as int).spec_valid_addr_with(self.richos_size@.val as nat)
        &&& self.is_constant()
    }
}

verus! {

impl MonitorParams {
    pub fn check_valid(&self) -> (ret: bool)
        requires
            self.is_constant(),
        ensures
            ret == self.mp_wf(),
    {
        if (self.validated_entries.reveal_value() as usize) > self.validated_e820.len() {
            return false;
        }
        if !self.acpi.reveal_value().check_valid_addr(self.acpi_size.reveal_value()) {
            return false;
        }
        if !self.richos_start.reveal_value().check_valid_addr(self.richos_size.reveal_value()) {
            return false;
        }
        return true
    }
}

} // verus!
pub ghost struct MonitorParamPermsToData {
    pub mp: SnpPointsToBytes,
    pub hvparampage: SnpPointsToBytes,
    pub global_perms: MonitorParamGlobalPerms,
}

pub tracked struct MonitorParamPerms {
    pub mp: SnpPointsToRaw,
    pub hvparampage: SnpPointsToRaw,
    pub global_perms: MonitorParamGlobalPerms,
}

pub tracked struct MonitorParamGlobalPerms {
    pub cpuid: SnpPointsToRaw,
    pub secret: SnpPointsToRaw,
    pub richos: SnpPointsToRaw,
    pub acpi: Map<int, SnpPointsToRaw>,
}

pub open spec fn spec_is_default_page_perms(page_perms: Map<int, SnpPointsToRaw>, start_page: int, npages: nat) -> bool {
    forall |i| start_page <= i < (start_page + npages) ==>
        #[trigger]page_perms.contains_key(i) &&
        page_perms[i]@.wf_const_default((i.to_addr(), PAGE_SIZE as nat))
}

impl MonitorParamGlobalPerms {
    pub open spec fn wf_value(&self, mp_value: MonitorParams) -> bool {
        &&& mp_value.is_constant()
        &&& self.cpuid@.wf_const_default((mp_value.cpuid_page as int, PAGE_SIZE!() as nat))
        &&& self.richos@.wf_const_default((mp_value.richos_start as int, mp_value.richos_size as nat))
        &&& self.secret@.wf_secret_default((mp_value.secret_page as int, PAGE_SIZE!() as nat))
        &&& spec_is_default_page_perms(self.acpi, (mp_value.acpi as int)/PAGE_SIZE!(), (mp_value.acpi_size as nat)/(PAGE_SIZE!() as nat))
    }
}

impl MonitorParamPerms {
    pub open spec fn view(self) -> MonitorParamPermsToData{
        MonitorParamPermsToData {
            mp: self.mp@,
            hvparampage: self.hvparampage@,
            global_perms: self.global_perms,
        }
    }
}

impl MonitorParamPermsToData {
    pub open spec fn wf(&self) -> bool {
        &&& self.mp.wf_const_default(self.mp.range())
        &&& self.mp.size() === spec_size::<MonitorParams>()
        &&& self.wf_param_value()
    }

    pub open spec fn e820(&self) -> Seq<E820Entry> {
        let mp: SnpPointsToData<MonitorParams> = self.mp.vspec_cast_to();
        let mp_value = mp.value().get_Some_0();
        let e820 = mp_value.validated_e820@;
        let n = mp_value.validated_entries;
        e820.take(n.vspec_cast_to())
    }

    pub open spec fn wf_param_value(&self) -> bool {
        let mp: SnpPointsToData<MonitorParams> = self.mp.vspec_cast_to();
        let mp_value = mp.value().get_Some_0();
        let n = mp_value.validated_entries;
        &&& self.global_perms.wf_value(mp_value)
        &&& self.hvparampage.wf_const_default((mp_value.hv_param as int, PAGE_SIZE!() as nat))
    }

    pub open spec fn wf_at(&self, mp_ptr: SnpPPtr<MonitorParams>) -> bool {
        &&& self.mp.range() === (mp_ptr.range_id())
        &&& self.wf()
    }
}
}

================
File: ./source/verismo/src/boot/mod.rs
================

pub mod idt;
pub mod init;
pub mod linux;
pub mod monitor_params;
mod mshyper;
pub mod params;

use crate::addr_e::*;
use crate::ptr::*;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

================
File: ./source/verismo/src/boot/linux/mod.rs
================

use alloc::vec::Vec;

use crate::addr_e::*;
use crate::arch::addr_s::PAGE_SIZE;
use crate::boot::monitor_params::*;
use crate::boot::params::*;
use crate::debug::VPrintAtLevel;
use crate::global::spec_ALLOCATOR_lockid;
use crate::pgtable_e::*;
use crate::ptr::*;
use crate::security::*;
use crate::snp::cpu::*;
use crate::snp::cpuid::SnpCpuidTable;
use crate::snp::ghcb::*;
use crate::snp::SnpCoreSharedMem;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::vbox::*;
use crate::*;

verismo_simple! {
#[repr(C, align(1))]
#[derive(SpecGetter, SpecSetter)]
pub struct BootInfo {
    #[def_offset]
    pub bp: BootParams,
    #[def_offset]
    pub secret: SnpSecretsPageLayout,
    #[def_offset]
    pub cpuid: SnpCpuidTable,
    #[def_offset]
    pub gdt: GDT,
    #[def_offset]
    pub cmdline: [u8; 256],
    #[def_offset]
    pub ccblob: CCBlobSevInfo, // 40
    pub reserved: [u8; {4096 - 48 - 256 - 256}], //4096 - 40 - 256 - 256 (32 * 8)
}

#[repr(C, align(1))]
#[derive(Copy, VClone)]
pub struct CCBlobSevInfo {
    pub reserved0: u32,
    pub reserved0_1: u16,
    pub reserved1: u16,
    pub secrets_phys: u64,
    pub secrets_len: u32,
    pub reserved2: u32,
    pub cpuid_phys: u64,
    pub cpuid_len: u32,
    pub reserved3: u32,
    pub shared_page: u64,
}
}

verus! {

proof fn lemma_boot_info_size()
    ensures
        spec_size::<BootInfo>() > PAGE_SIZE,
{
}

} // verus!
verus! {

pub struct BootUpdate<'a> {
    pub acpi_rsdp_addr: u64,
    pub cc_blob_addr: u64,
    pub cmd_line_addr: u64,
    pub cmdline_size: u64,
    pub e820_entries: u8,
    pub e820: &'a ValidatedE820Table,
    pub hdr: &'a SetupHeader,
}

use crate::vbox::MutFnTrait;

impl<'a, 'b> MutFnTrait<'a, BootUpdate<'b>, u8> for BootParams {
    open spec fn spec_update_requires(&self, params: BootUpdate<'b>) -> bool {
        &&& params.e820.is_constant()
        &&& params.hdr.is_constant()
        &&& params.e820@.len() >= params.e820_entries
        &&& self.is_constant()
    }

    open spec fn spec_update(&self, prev: &Self, params: BootUpdate<'b>, ret: u8) -> bool {
        &&& self.is_constant()
        &&& self.e820_entries.spec_eq(params.e820_entries)
        &&& self.e820@.take(params.e820_entries as int) =~~= params.e820@.take(
            params.e820_entries as int,
        )
    }

    fn box_update(&'a mut self, params: BootUpdate<'b>) -> (ret: u8) {
        let BootUpdate {
            acpi_rsdp_addr,
            cc_blob_addr,
            cmd_line_addr,
            cmdline_size,
            e820_entries,
            e820,
            hdr,
        } = params;
        self.acpi_rsdp_addr = acpi_rsdp_addr.into();
        self.cc_blob_addr = cc_blob_addr.into();
        self._ext_cmd_line_ptr = ((cmd_line_addr >> 32u64) as u32).into();
        self.hdr = hdr.clone();
        self.hdr.cmd_line_ptr = (cmd_line_addr as u32).into();
        self.hdr.cmdline_size = cmdline_size.into();
        self.e820_entries = e820_entries.into();
        let ghost oldself = *self;
        let mut i = 0;
        while i < e820_entries
            invariant
                0 <= i <= e820_entries,
                self.is_constant(),
                e820.is_constant(),
                e820_entries <= e820@.len(),
                self.e820@.take(i as int) =~~= e820@.take(i as int),
                *self === oldself.spec_set_e820(self.e820),
        {
            proof {
                assert(self.e820@.update(i as int, e820[i as int]).take(i as int + 1)
                    =~~= self.e820@.take(i as int).push(e820[i as int]));
                assert(e820@.take(i as int + 1) =~~= e820@.take(i as int).push(e820[i as int]));
            }
            self.e820.update(i as usize, e820.index(i as usize).clone());
            proof {
                assert(self.e820@.is_constant());
            }
            i = i + 1;
        }
        0
    }
}

} // verus!
verus! {

/// Use 32-bit boot protocol
/// https://www.kernel.org/doc/html/latest/x86/boot.html#bit-boot-protocol
pub struct SetBasicBootInfoParam<'a> {
    pub mparam: &'a MonitorParams,
    pub vmpl: u8,
    pub richos_boot: &'a BootParams,
    pub cc_blob_addr: u64,
    pub cmd_line_addr: u64,
}

impl<'a> MutFnTrait<'a, SetBasicBootInfoParam<'a>, u8> for BootInfo {
    open spec fn spec_update_requires(&self, params: SetBasicBootInfoParam) -> bool {
        &&& params.mparam.mp_wf()
        &&& 1 <= params.vmpl < 4
        &&& params.richos_boot.is_constant()
        &&& self.is_constant()
    }

    open spec fn spec_update(
        &self,
        prev: &Self,
        params: SetBasicBootInfoParam<'a>,
        ret: u8,
    ) -> bool {
        &&& self.is_constant()
        &&& *self === prev.spec_set_bp(self.bp).spec_set_gdt(self.gdt).spec_set_cmdline(
            self.cmdline,
        )
        &&& self.bp.e820_entries <= self.bp.e820@.len()
    }

    fn box_update(&'a mut self, params: SetBasicBootInfoParam<'a>) -> (ret: u8) {
        let SetBasicBootInfoParam { vmpl, mparam, richos_boot, cmd_line_addr, cc_blob_addr } =
            params;
        let richos_start: u64 = mparam.richos_start.into();
        let e820_entries: u8 = mparam.validated_entries.into();
        let cmdline_size: u64 = mparam.richos_cmdline_len.into();
        // Init GDT
        self.gdt.set(0, Descriptor::empty().value.into());
        self.gdt.set(1, Descriptor::entry_cs_sys().value.into());
        self.gdt.set(2, Descriptor::entry_cs_user().value.into());
        self.gdt.set(3, Descriptor::entry_ds_sys().value.into());
        self.gdt.set(4, Descriptor::entry_ds_user().value.into());
        self.bp.box_update(
            BootUpdate {
                acpi_rsdp_addr: mparam.acpi.into(),
                cc_blob_addr,
                cmd_line_addr,
                cmdline_size,
                e820_entries,
                e820: &mparam.validated_e820,
                hdr: &richos_boot.hdr,
            },
        );
        self.cmdline = mparam.richos_cmdline.clone();
        0
    }
}

pub struct SetSnpBootInfoParam<'a> {
    pub vmpl: u8,
    pub secret_addr: usize,
    pub master_secret: &'a SnpSecretsPageLayout,
    pub cpuid_addr: usize,
    pub early_shared: VBox<OnePage>,
    pub cpuid: &'a SnpCpuidTable,
}

impl<'a> MutFnTrait<'a, SetSnpBootInfoParam<'a>, u8> for BootInfo {
    open spec fn spec_update_requires(&self, params: SetSnpBootInfoParam<'a>) -> bool {
        &&& 1 <= params.vmpl < 4
        &&& params.early_shared.is_shared_page()
        &&& params.master_secret.wf_mastersecret()
        &&& self.secret.is_constant()
    }

    open spec fn spec_update(&self, prev: &Self, params: SetSnpBootInfoParam<'a>, ret: u8) -> bool {
        &&& self.secret.is_constant_to(params.vmpl as nat)
        &&& self.ccblob.is_constant()
        &&& self.cpuid === *params.cpuid
        &&& *self === prev.spec_set_secret(self.secret).spec_set_ccblob(self.ccblob).spec_set_cpuid(
            self.cpuid,
        )
    }

    fn box_update(&'a mut self, params: SetSnpBootInfoParam<'a>) -> (ret: u8) {
        let SetSnpBootInfoParam {
            master_secret,
            vmpl,
            secret_addr,
            cpuid_addr,
            cpuid,
            early_shared,
        } = params;
        self.ccblob = CCBlobSevInfo::new(secret_addr as u64, cpuid_addr as u64, early_shared);
        self.secret.box_update(FillSecretForVMPL { master_secret, vmpl });
        self.cpuid = cpuid.clone();
        0
    }
}

pub struct SetMemoryBootInfoParam {
    pub vmpl: u8,
}

impl<'a> MutFnWithCSTrait<'a, SnpCoreSharedMem, SetMemoryBootInfoParam, OSMem> for BootInfo {
    open spec fn spec_update_cs_requires(
        &self,
        params: SetMemoryBootInfoParam,
        cs: SnpCoreSharedMem,
    ) -> bool {
        &&& 1 <= params.vmpl < 4
        &&& self.bp.is_constant()
        &&& self.bp.e820_entries <= self.bp.e820@.len()
        &&& cs.inv()
    }

    open spec fn spec_update_cs(
        &self,
        prev: &Self,
        params: SetMemoryBootInfoParam,
        oldcs: SnpCoreSharedMem,
        ret: OSMem,
        cs: SnpCoreSharedMem,
    ) -> bool {
        &&& *self === prev.spec_set_bp(self.bp)
        &&& self.bp === prev.bp.spec_set_e820(self.bp.e820).spec_set_e820_entries(
            self.bp.e820_entries,
        )
        &&& osmem_wf(ret@)
        &&& self.bp.is_constant()
        &&& cs.inv()
        &&& cs.only_lock_reg_coremode_updated(oldcs, set![], set![spec_ALLOCATOR_lockid()])
    }

    fn box_update_cs(
        &'a mut self,
        params: SetMemoryBootInfoParam,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: OSMem) {
        let mut n = self.bp.e820_entries.into();
        let ret = add_ram_from_allocator(params.vmpl, &mut self.bp.e820, &mut n, Tracked(cs));
        self.bp.e820_entries = n.into();
        let mut osmem;
        match ret {
            Ok(tmposmem) => {
                osmem = tmposmem;
            },
            _ => {
                new_strlit("Err from osmem_add_ram_from_allocator\n").leak_debug();
                vc_terminate(SM_TERM_MEM, Tracked(&mut cs.snpcore));
            },
        }
        assert(self.bp.e820@.is_constant());
        assert(self.bp.e820_entries.is_constant());
        osmem
    }
}

} // verus!
verismo_simple! {
impl CCBlobSevInfo{
    fn new(secret_addr: u64_t, cpuid_addr: u64_t, shared: VBox<OnePage>) -> (retccblob: CCBlobSevInfo)
    requires
        shared.is_shared_page(),
    ensures
        retccblob.is_constant(),
    {
        let (shared_addr, _) = shared.into_raw();
        CCBlobSevInfo {
            reserved0: 0,
            reserved0_1: 0,
            reserved1: 0,
            secrets_phys: secret_addr.into(),
            secrets_len: PAGE_SIZE.into(),
            reserved2: 0,
            cpuid_phys: cpuid_addr.into(),
            cpuid_len: PAGE_SIZE.into(),
            reserved3: 0,
            shared_page: shared_addr.as_u64().into(),
        }
    }
}
}

verus! {

pub fn load_bzimage_to_vmsa(
    osmem: &mut Vec<OSMemEntry>,
    mparam: &MonitorParams,
    vmpl: u8,
    master_secret: &SnpSecretsPageLayout,
    cpuid: VBox<SnpCpuidTable>,
    hvparam: VBox<OnePage>,
    early_shared: VBox<OnePage>,
    Tracked(richos_perm): Tracked<SnpPointsToRaw>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (retbox: (VBox<VmsaPage>, VBox<SnpCpuidTable>))
    requires
        osmem_wf(old(osmem)@),
        early_shared.is_shared_page(),
        hvparam.is_default_page(),
        hvparam@.is_constant(),
        mparam.mp_wf(),
        mparam.richos_size > 0,
        vmpl == RICHOS_VMPL,
        master_secret.wf_mastersecret(),
        cpuid@.is_constant(),
        cpuid.snp().is_vmpl0_private(),
        richos_perm@.wf_const_default(
            (mparam.richos_start.vspec_cast_to(), mparam.richos_size.vspec_cast_to()),
        ),
        (*old(cs)).inv(),
    ensures
        osmem_wf(osmem@),
        (*cs).inv(),
        (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![spec_ALLOCATOR_lockid()]),
        retbox.1 === cpuid,
        retbox.0.is_default_page(),
        retbox.0@.is_constant(),
        retbox.0@.vmpl@.val == vmpl,
{
    let ghost cs1 = *cs;
    let mut vmsa = VBox::<VmsaPage>::new_aligned_uninit(PAGE_SIZE, Tracked(cs));
    let richos_size: usize = mparam.richos_size.into();
    let richos_start: usize = mparam.richos_start.into();
    if richos_start == 0 || richos_size <= size_of::<BootParams>() {
        new_strlit("Invalid richos load address or size\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cs.snpcore));
    }
    //verismo::snp::secret::measure(richos_start, richos_size, state);

    let tracked (boot_perm, richos_perm) = richos_perm.trusted_split(spec_size::<BootParams>());
    let richos_boot = VBox::<BootParams>::from_raw(richos_start, Tracked(boot_perm.tracked_into()));
    let ghost cs2 = *cs;
    let mut bootinfo = VBox::<BootInfo>::new_aligned_uninit(PAGE_SIZE, Tracked(cs));
    let (bootinfo_ptr, Tracked(bootinfo_perm)) = bootinfo.into_raw();
    let secret_addr = bootinfo_ptr.secret().to_usize();
    let cpuid_addr = bootinfo_ptr.cpuid().to_usize();
    let gdtr_addr: u64 = bootinfo_ptr.gdt().as_u64();
    let bp_addr: u64 = bootinfo_ptr.bp().as_u64();
    let cc_blob_addr: u64 = bootinfo_ptr.ccblob().as_u64();
    let cmd_line_addr: u64 = bootinfo_ptr.cmdline().as_u64();
    let mut bootinfo = VBox::<BootInfo>::from_raw(bootinfo_ptr.to_usize(), Tracked(bootinfo_perm));
    assert(bootinfo@.is_constant());
    bootinfo.box_update(
        SetBasicBootInfoParam {
            vmpl,
            mparam,
            cc_blob_addr,
            cmd_line_addr,
            richos_boot: richos_boot.borrow(),
        },
    );
    let (_, Tracked(boot_perm)) = richos_boot.into_raw();
    proof {
        richos_perm = boot_perm.tracked_into_raw().trusted_join(richos_perm);
    }
    bootinfo.box_update(
        SetSnpBootInfoParam {
            vmpl,
            early_shared,
            master_secret,
            secret_addr,
            cpuid_addr,
            cpuid: cpuid.borrow(),
        },
    );
    let ghost cs3 = *cs;
    let mut tmposmem = bootinfo.box_update_cs(SetMemoryBootInfoParam { vmpl }, Tracked(cs));
    proof {
        cs1.lemma_update_prop(
            cs2,
            cs3,
            set![],
            set![spec_ALLOCATOR_lockid()],
            set![],
            set![spec_ALLOCATOR_lockid()],
        );
        cs1.lemma_update_prop(
            cs3,
            *cs,
            set![],
            set![spec_ALLOCATOR_lockid()],
            set![],
            set![spec_ALLOCATOR_lockid()],
        );
    }
    osmem.append(&mut tmposmem);
    assert(osmem_wf(osmem@));
    let bi = bootinfo.borrow();
    bi.bp.e820.leak_debug();
    let prefer_vaddr: usize = bi.bp.hdr.pref_address.into();
    let setup_sects: usize = bi.bp.hdr.setup_sects.into();
    let start32_offset: usize = (1usize + setup_sects) * 512;
    let start32_addr: usize = start32_offset + richos_start;
    if start32_offset >= richos_size {
        new_strlit("Invalid richos start32_offset\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cs.snpcore));
    }
    if prefer_vaddr % PAGE_SIZE != 0 {
        new_strlit("Invalid richos prefer_vaddr\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cs.snpcore));
    }
    let kernel_size: usize = richos_size - start32_offset;
    if !prefer_vaddr.check_valid_addr(kernel_size) {
        new_strlit("Invalid richos prefer_vaddr\n").leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cs.snpcore));
    }
    let prefer_vaddr = prefer_vaddr.to_page().to_addr();
    let tracked (left, start32_perm) = richos_perm.trusted_split(start32_offset as nat);
    // Fixed #HV error due to uninitialized mem
    if let Some(i) = osmem_find(osmem, prefer_vaddr.to_page()) {
        proof {
            assert(osmem[i as int].wf());
        }
        let end_page = osmem[i].end();
        if prefer_vaddr + kernel_size > end_page.to_addr() {
            new_strlit("No enough Ram for kernel: ").leak_debug();
            vc_terminate(SM_TERM_MEM, Tracked(&mut cs.snpcore));
        }
        let mut entry = osmem.remove(i);
        let npages = if kernel_size % PAGE_SIZE == 0 {
            kernel_size.to_page()
        } else {
            kernel_size.to_page() + 1
        };
        let (left, mut entry, right) = osmem_entry_split(
            entry,
            (prefer_vaddr).to_page() as usize,
            npages as usize,
        );
        let Tracked(mut prefer_mem) = entry.page_perms;
        proof {
            assert(entry.wf());
            entry.proof_open_wf();
            let start = (prefer_vaddr as int).to_page();
            let end_page = ((prefer_vaddr + kernel_size) as int).to_page();
            assert forall|i: int| start <= i < end_page implies prefer_mem.contains_key(i) by {
                assert(entry.spec_start() <= i < entry.spec_end());
                entry.proof_contains(i);
            }
            assert forall|i: int| prefer_mem.contains_key(i) implies prefer_mem[i]@.wf_not_null(
                (i.to_addr(), PAGE_SIZE as nat),
            ) && entry.spec_start() <= i < entry.spec_end() by {
                entry.proof_contains(i);
            }
        }
        let ghost old_entry = entry;
        let ghost old_prefer_mem = prefer_mem;
        new_strlit("copy image\n").leak_debug();
        mem_copy_to_pages(
            start32_addr as usize,
            prefer_vaddr,
            kernel_size as usize,
            Tracked(&start32_perm),
            Tracked(&mut prefer_mem),
        );
        proof {
            assert forall|i| #[trigger] prefer_mem.contains_key(i) implies spec_contains_page_perm(
                prefer_mem,
                i,
                entry.spec_osperm(),
            ) by {
                old_entry.proof_contains(i);
                assert(old_prefer_mem.contains_key(i));
                let page_perm = prefer_mem[i]@;
                let old_page_perm = old_prefer_mem[i]@;
                assert(spec_contains_page_perm(old_prefer_mem, i, old_entry.spec_osperm()));
                assert(prefer_vaddr <= i.to_addr());
                let start = spec_mem_copy_onepage_start(i, prefer_vaddr as int, kernel_size as nat);
                let end = spec_mem_copy_onepage_end(i, prefer_vaddr as int, kernel_size as nat);
                let b1 = start32_perm@.bytes().subrange(start, end as int);
                assert(start32_perm@.bytes().len() == kernel_size);
                proof_subrange_is_constant_to(
                    start32_perm@.bytes(),
                    start,
                    kernel_size as int,
                    vmpl as nat,
                );
                let b2 = old_page_perm.bytes().skip(b1.len() as int);
                assert(b1.len() <= PAGE_SIZE);
                proof_subrange_is_constant_to(
                    old_page_perm.bytes(),
                    b1.len() as int,
                    PAGE_SIZE as int,
                    vmpl as nat,
                );
                proof_bytes_add_is_constant_to(b1, b2, vmpl as nat);
                assert(page_perm.bytes().is_constant_to(vmpl as nat));
            }
            assert(old_prefer_mem.dom() =~~= Set::new(
                |i: int| entry.spec_start() <= i < entry.spec_end(),
            ));
            assert(prefer_mem.dom() =~~= Set::new(
                |i: int| entry.spec_start() <= i < entry.spec_end(),
            ));
        }
        entry.page_perms = Tracked(prefer_mem);
        proof {
            entry.proof_open_wf();
            assert(entry.wf());
        }
        let entry = osmem_entry_merge(left, entry);
        let entry = osmem_entry_merge(entry, right);
        osmem.insert(i, entry);
        assert(osmem_wf(osmem@));
    } else {
        new_strlit("Bad prefer_vaddr\n").leak_debug();
        vc_terminate(SM_TERM_MEM, Tracked(&mut cs.snpcore));
    }
    vmsa.box_update(
        UpdateRichOSVmsa {
            vmpl,
            gdt: &bootinfo.borrow().gdt,
            gdtr_addr,
            bp_addr,
            kernel_addr: prefer_vaddr as u64,
        },
    );
    proof {
        assert(bootinfo@.is_constant_to(vmpl as nat));
    }
    let (bootinfo_ptr, Tracked(bootinfo_perm)) = bootinfo.into_raw();
    proof {
        proof_into_is_constant_to::<_, SecSeqByte>(bootinfo_perm@.value(), vmpl as nat);
    }
    let tracked raw_perm = bootinfo_perm.tracked_into_raw();
    assert(raw_perm@.bytes().is_constant_to(vmpl as nat));
    let start_page = bootinfo_ptr.to_usize().to_page();
    let npages = size_of::<BootInfo>() / PAGE_SIZE;
    let tracked page_perms = raw_perm.tracked_to_pages();
    proof {
        assert forall|i|
            start_page <= i < (start_page + npages) implies #[trigger] page_perms.contains_key(i)
            && page_perms[i]@.wf_default((i.to_addr(), PAGE_SIZE as nat))
            && page_perms[i]@.bytes().is_constant_to(vmpl as nat) by {
            assert(page_perms.contains_key(i));
            assert(page_perms[i]@.bytes() =~~= raw_perm@.bytes().subrange(
                (i - start_page).to_addr(),
                (i - start_page).to_addr() + PAGE_SIZE,
            ));
        }
    }
    osmem_add(
        osmem,
        bootinfo_ptr.to_usize().to_page(),
        size_of::<BootInfo>() / PAGE_SIZE,
        OSMemPerm::ram(),
        false,
        Tracked(page_perms),
        Tracked(&mut cs.snpcore),
    );
    // Add hvparam page into osmem
    let (hvparam_ptr, Tracked(hvparam_perm)) = hvparam.into_raw();
    let hvparam_page: usize = hvparam_ptr.to_usize().to_page();
    let tracked mut hvparam_perms = Map::tracked_empty();
    let tracked hvparam_perm = hvparam_perm.tracked_into_raw();
    proof {
        hvparam_perms.tracked_insert(hvparam_page as int, hvparam_perm);
    }
    proof {
        assert(hvparam_perm@.wf_default(((hvparam_page as int).to_addr(), PAGE_SIZE as nat)));
        assert(hvparam_perm@.bytes().is_constant_to(vmpl as nat));
        assert forall|i|
            hvparam_page <= i < (hvparam_page + 1) implies #[trigger] hvparam_perms.contains_key(i)
            && hvparam_perms[i]@.wf_default((i.to_addr(), PAGE_SIZE as nat))
            && hvparam_perms[i]@.bytes().is_constant_to(vmpl as nat) by {}
    }
    osmem_add(
        osmem,
        hvparam_page,
        1,
        OSMemPerm::readonly(),
        false,
        Tracked(hvparam_perms),
        Tracked(&mut cs.snpcore),
    );
    let mut i = 0;
    while i < osmem.len() {
        osmem[i].leak_debug();
        i = i + 1;
    }
    (vmsa, cpuid)
}

} // verus!

================
File: ./source/verismo/src/mshyper/wakeup.rs
================

use super::*;
use crate::pgtable_e::va_to_pa;
use crate::security::SnpSecretsPageLayout;
use crate::snp::cpu::{InitAPParams, InitApVmsa, PerCpuData, GDT};
use crate::snp::percpu::BSP;

verus! {

impl GhcbHyperPageHandle {
    /*
    EnableVtlProtection      : 0xb0;
    DefaultVtlProtectionMask : 0xb1111;
    ZeroMemoryOnReset        : 0xb0;
    DenyLowerVtlStartup      : 0xb0;
    InterceptAcceptance      : 1;
    InterceptEnableVtlProtection : 0;
    InterceptVpStartup       : 0;
    InterceptCpuidUnimplemented : 0;
    InterceptUnrecoverableException : 1;
    InterceptPage            : 0;
    InterceptRestorePartitionTime : 0;
    InterceptNotPresent      : 0;
    ReservedZ                : 0 */
    pub fn config_partition(self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (handle: Self)
        requires
            self.wf(),
            (*old(cs)).inv(),
        ensures
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            handle.wf(),
    {
        let ghost cs1 = (*cs);
        let handle = self.set_vp_reg(
            HV_REGISTER_VSM_PARTITION_CONFIG,
            0x89e,
            REG_NO_USE_VTL,
            Tracked(cs),
        );
        let ghost cs2 = (*cs);
        let (val, handle) = handle.get_vp_reg(
            HV_REGISTER_VSM_PARTITION_CONFIG,
            REG_NO_USE_VTL,
            Tracked(cs),
        );
        if val != 0x89e {
            (new_strlit("partition config = "), val).leak_debug();
            vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cs.snpcore));
        }
        proof {
            cs1.lemma_update_prop(cs2, (*cs), set![], set![], set![], set![]);
        }
        handle
    }

    fn config_percpu_intercept(self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (handle: Self)
        requires
            self.wf(),
            (*old(cs)).inv(),
        ensures
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            handle.wf(),
    {
        // set restricted interrupt policy at vtl0 0-3: max_vtl, 4-5: vtl0_int, 6-7: vtl1_int
        let ghost oldcs = (*cs);
        let (mut val, handle) = self.get_vp_reg(
            HV_REGISTER_GUEST_VSM_PARTITION_CONFIG,
            REG_NO_USE_VTL,
            Tracked(cs),
        );
        let ghost prevcs = (*cs);
        val = (val & 0xffff_ffff_ffff_ffcfu64) | 0x10u64;
        let handle = handle.set_vp_reg(
            HV_REGISTER_GUEST_VSM_PARTITION_CONFIG,
            val,
            REG_NO_USE_VTL,
            Tracked(cs),
        );
        proof {
            oldcs.lemma_update_prop(prevcs, (*cs), set![], set![], set![], set![]);
        }
        handle
    }

    pub fn register_vmsa(
        self,
        vmpl: u8,
        vmsa: VBox<VmsaPage>,
        Tracked(nextvmpl_id): Tracked<CoreIdPerm>,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: (Self, VBox<VmsaPage>))
        requires
            self.wf(),
            (*old(cs)).inv(),
            vmsa.is_vmpl0_private_page(),
            vmsa@.vmpl.spec_eq(vmpl as int),
            nextvmpl_id@.vmpl == vmpl,
            0 < vmpl < 4,
        ensures
            cs.inv(),
            cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            ret.0.wf(),
            ret.1.is_vmsa_page(),
            ret.1@ === vmsa@,
    {
        // install intercept
        new_strlit("Register page.").leak_debug();
        let ghost cs1 = (*cs);
        let GhcbHyperPageHandle(ghcb, hyperpage) = self;
        let ghcb = ghcb.ghcb_write_msr(HV_X64_MSR_SINT0, KeX64VectorSintIntercept, Tracked(cs));
        let ghost cs2 = (*cs);
        let handle = GhcbHyperPageHandle(ghcb, hyperpage);
        let vmsa_addr = vmsa.get_const_addr() as u64;
        proof {
            proof_cast_from_seq_unique(vmsa@);
        }
        let (vmsa_ptr, Tracked(vmsa_perm)) = vmsa.into_raw();
        let tracked mut vmsa_perm = vmsa_perm.tracked_into_raw();
        // register vmsa
        let vmsa_paddr = va_to_pa(vmsa_addr, Tracked(&vmsa_perm));
        let value = (vmsa_paddr as u64) | HV_X64_REGISTER_SEV_CONTROL_USE_SEV;
        let handle = handle.set_vp_reg(
            HV_X64_REGISTER_SEV_CONTROL,
            value,
            REG_USE_VTL | get_vtl(vmpl),
            Tracked(cs),
        );
        let ghost cs3 = (*cs);
        let rmp_attr = RmpAttr::empty().set_vmpl(vmpl as u64).set_vmsa(1).set_perms(0);
        assert(crate::arch::rmp::PagePerm::from_int(0) =~~= Set::empty());
        proof {
            let vmsa: VmsaPage = vmsa_perm@.bytes().vspec_cast_to();
            let vmpl = vmsa.vmpl;
            assert(vmpl.spec_eq(nextvmpl_id@.vmpl));
            assert(vmsa_perm@.snp().requires_rmpadjust_mem(
                vmsa_addr as int,
                0,
                rmp_attr@,
                Some(nextvmpl_id),
            ));
        }
        assert(vmsa_perm@.wf());
        let rc = rmpadjust(
            vmsa_addr,
            RMP_4K,
            rmp_attr,
            Tracked(&cs.snpcore),
            Tracked(Some(nextvmpl_id)),
            Tracked(&mut vmsa_perm),
        );
        if rc != 0 {
            // failed validation ==> possible attack
            vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cs.snpcore));
        }
        assert(vmsa_perm@.wf());
        let handle = handle.config_percpu_intercept(Tracked(cs));
        proof {
            cs1.lemma_update_prop(cs2, cs3, set![], set![], set![], set![]);
            cs1.lemma_update_prop(cs3, (*cs), set![], set![], set![], set![]);
        }
        assert(spec_size::<VmsaPage>() == PAGE_SIZE!());
        (handle, VBox::from_raw((vmsa_addr as usize), Tracked(vmsa_perm.tracked_into())))
    }

    pub fn snp_start_ap(
        self,
        cpu: VBox<PerCpuData>,
        gdt: &GDT,
        Tracked(ap_id): Tracked<CoreIdPerm>,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (handle: Self)
        requires
            self.wf(),
            old(cs).inv(),
            ap_id@.vmpl == 0,
            cpu@.cpu != BSP,
            cpu@.inv(),
            cpu.wf(),
            gdt.is_constant(),
            cpu.snp() === SwSnpMemAttr::spec_default(),
        ensures
            cs.inv(),
            cs.only_lock_reg_coremode_updated(
                *old(cs),
                set![],
                set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
            ),
            handle.wf(),
    {
        let ghost cs1 = *cs;
        let GhcbHyperPageHandle(bsp_ghcb_h, bsp_hyperpage_h) = self;
        let cpu_id = cpu.borrow().cpu;
        let mut vmsabox = VBox::<VmsaPage>::new_aligned_uninit(PAGE_SIZE, Tracked(cs));
        proof {
            assert(cs.inv_ac());
            assert(gdt.is_constant());
        }
        let ghost cs2 = *cs;
        vmsabox.box_update_cs(InitAPParams { fun: InitApVmsa, cpu, gdt }, Tracked(cs));
        let ghost cs3 = *cs;
        let (vmsa_ptr, Tracked(vmsa_perm)) = vmsabox.into_raw();
        let vmsa_vaddr = vmsa_ptr.as_u64();
        let tracked mut vmsa_perm = vmsa_perm.tracked_into_raw();
        let vmsa_paddr = va_to_pa(vmsa_vaddr, Tracked(&vmsa_perm));
        new_strlit("Wakeup ").leak_debug();
        // Setup hyperpage and enable VP
        let (ap_hyperv_h, bsp_ghcb_h) = VBox::<OnePage>::new_shared_page(
            PAGE_SIZE,
            bsp_ghcb_h,
            Tracked(cs),
        );
        let ghost cs4 = (*cs);
        // Enable VTL for the VP
        let (ap_hyperv_ptr, Tracked(ap_hyperv_perm)) = ap_hyperv_h.into_raw();
        let input = HvCallVpVtlInput::new(HV_PARTITION_ID_SELF, cpu_id, get_vtl(0), vmsa_paddr);
        let input_ptr: SnpPPtr<HvCallVpVtlInput> = ap_hyperv_ptr.to();
        let (_, Tracked(ap_hyperv_perm)) = input_ptr.replace_with::<OnePage>(
            input,
            Tracked(ap_hyperv_perm),
        );
        let ap_hyperv_h = HyperPageHandle::from_raw(
            ap_hyperv_ptr.to_usize(),
            Tracked(ap_hyperv_perm),
        );
        let bsp_for_ap_handle = GhcbHyperPageHandle(bsp_ghcb_h, ap_hyperv_h);
        let (ret, bsp_for_ap_handle) = bsp_for_ap_handle.hv_call_with_retry(
            hvcall_code(HVCALL_ENABLE_VP_VTL, 0),
            true,
            false,
            Tracked(cs),
        );
        let ghost cs5 = (*cs);
        match ret {
            Ok(status) if status != 0 => {
                (new_strlit("Failed"), status).leak_debug();
            },
            Err(code) => {
                vc_terminate_debug(SM_TERM_VMM_ERR, Tracked(cs));
            },
            _ => {},
        }
        // rmpadjust vmsa page to be a real vmsa page.
        // VMPL > 0 but is ignored when set vmsa.

        let mut rmp_attr = RmpAttr::empty().set_vmpl(VMPL::VMPL1.as_u64()).set_vmsa(1).set_perms(0);
        let rc = rmpadjust(
            vmsa_vaddr,
            0,
            rmp_attr,
            Tracked(&mut cs.snpcore),
            Tracked(Some(ap_id)),
            Tracked(&mut vmsa_perm),
        );
        if rc != 0 {
            vc_terminate(SM_TERM_VMM_ERR, Tracked(&mut cs.snpcore));
        }
        // Ask hypervisor to wakeup the processor.

        let (ret, bsp_for_ap_handle) = bsp_for_ap_handle.hv_call_with_retry(
            hvcall_code(HVCALL_START_VIRTUAL_PROCESSOR, 0),
            true,
            false,
            Tracked(cs),
        );
        match ret {
            Ok(status) if status == 0 => {
                (new_strlit("Boot AP"), cpu_id).leak_debug();
            },
            _ => {
                vc_terminate(SM_TERM_VMM_ERR, Tracked(&mut cs.snpcore));
            },
        }
        proof {
            assert(set![spec_ALLOCATOR_lockid()].union(set![spec_ALLOCATOR_lockid()])
                =~~= set![spec_ALLOCATOR_lockid()]);
            cs1.lemma_update_prop(
                cs2,
                cs3,
                set![],
                set![spec_ALLOCATOR_lockid()],
                set![],
                set![spec_ALLOCATOR_lockid()],
            );
            cs1.lemma_update_prop(
                cs3,
                cs4,
                set![],
                set![spec_ALLOCATOR_lockid()],
                set![],
                set![spec_ALLOCATOR_lockid()],
            );
            cs1.lemma_update_prop(
                cs4,
                cs5,
                set![],
                set![spec_ALLOCATOR_lockid()],
                set![],
                set![spec_PT_lockid()],
            );
            assert(set![spec_ALLOCATOR_lockid()].union(set![spec_PT_lockid()])
                =~~= set![spec_ALLOCATOR_lockid(), spec_PT_lockid()]);
            cs1.lemma_update_prop(
                cs5,
                *cs,
                set![],
                set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
                set![],
                set![],
            );
        }
        let GhcbHyperPageHandle(bsp_ghcb_h, ap_hyperv_h) = bsp_for_ap_handle;
        ap_hyperv_h.into_raw();  // TODO: pass to ap vmsa.
        GhcbHyperPageHandle(bsp_ghcb_h, bsp_hyperpage_h)
    }

    pub fn start_all_ap(
        self,
        cpu_count: u32,
        secret: &SnpSecretsPageLayout,
        gdt: &GDT,
        Tracked(ap_ids): Tracked<Map<int, CoreIdPerm>>,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (handle: Self)
        requires
            self.wf(),
            (*old(cs)).inv(),
            spec_ap_ids_wf(ap_ids, BSP as int),
            secret.wf_mastersecret(),
            gdt.is_constant(),
        ensures
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated(
                (*old(cs)),
                set![],
                set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
            ),
            handle.wf(),
    {
        if cpu_count <= 1 {
            return self;
        }
        let ghost oldcs = (*cs);
        let tracked mut ap_ids = ap_ids;
        let mut cpu = 0;
        let mut handle = self;
        while cpu < cpu_count
            invariant
                cs.inv(),
                cs.only_lock_reg_coremode_updated(
                    oldcs,
                    set![],
                    set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
                ),
                handle.wf(),
                spec_ap_ids_wf_lowerbound(ap_ids, BSP as int, cpu as int),
                secret.wf_mastersecret(),
                gdt.is_constant(),
        {
            if cpu != BSP as u32 {
                proof {
                    assert(spec_cpumap_contains_cpu(ap_ids, cpu as int, 0));
                }
                let ghost cs1 = (*cs);
                let ghost prev_ap_ids = ap_ids;
                let tracked ap_id = ap_ids.tracked_remove(cpu as int);
                let percpu = VBox::new(PerCpuData { cpu, resvd: 0, secret }, Tracked(cs));
                let ghost cs2 = (*cs);
                handle = handle.snp_start_ap(percpu, gdt, Tracked(ap_id), Tracked(cs));
                proof {
                    oldcs.lemma_update_prop(
                        cs1,
                        cs2,
                        set![],
                        set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
                        set![],
                        set![spec_ALLOCATOR_lockid()],
                    );
                    assert(set![spec_ALLOCATOR_lockid(), spec_PT_lockid()].union(
                        set![spec_ALLOCATOR_lockid()],
                    ) =~~= set![spec_ALLOCATOR_lockid(), spec_PT_lockid()]);
                    oldcs.lemma_update_prop(
                        cs2,
                        (*cs),
                        set![],
                        set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
                        set![],
                        set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
                    );
                    assert(set![spec_ALLOCATOR_lockid(), spec_PT_lockid()].union(
                        set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
                    ) =~~= set![spec_ALLOCATOR_lockid(), spec_PT_lockid()]);
                    assert forall|i: int|
                        (i != BSP) && i >= (cpu + 1) implies spec_cpumap_contains_cpu(
                        ap_ids,
                        i,
                        0,
                    ) by {
                        assert(spec_cpumap_contains_cpu(prev_ap_ids, i, 0));
                    }
                }
            }
            cpu = cpu + 1;
        }
        handle
    }
}

impl GhcbHandle {
    // Ask hypervisor to switch to a lower VTL
    pub fn switch_to_next_vmpl(
        self,
        vmsa: VBox<VmsaPage>,
        Tracked(snpcore): Tracked<&mut SnpCore>,
    ) -> (ret: (Self, VBox<VmsaPage>))
        requires
            self.ghcb_wf(),
            vmsa.is_vmsa_page(),
            (*old(snpcore)).inv(),
        ensures
            (*snpcore).inv(),
            (*snpcore).only_reg_coremode_updated((*old(snpcore)), set![GHCB_REGID()]),
            ret.0.ghcb_wf(),
            ret.1.only_val_updated(vmsa),
            ret.1.is_vmsa_page(),
    {
        let error_mask: u64 = 0xfff;
        let mut ghcb = self;
        let mut vmsa = vmsa;
        let ghost oldvmsa = vmsa;
        vmsa.set_guest_error_code(error_mask.into());
        let vmsa = vmsa;
        assert(vmsa.only_val_updated(oldvmsa));
        let ghost oldsnpcore = *snpcore;
        loop
            invariant
                ghcb.ghcb_wf(),
                snpcore.inv(),
                snpcore.only_reg_coremode_updated(oldsnpcore, set![GHCB_REGID()]),
                vmsa.is_vmsa_page(),
            ensures
                ghcb.ghcb_wf(),
                snpcore.inv(),
                snpcore.only_reg_coremode_updated(oldsnpcore, set![GHCB_REGID()]),
                vmsa.is_vmsa_page(),
        {
            let mut check_error = vmsa.copy_guest_error_code();
            check_error.declassify();
            if error_mask != check_error.reveal_value() {
                break ;
            }
            ghcb.box_update(GhcbClear);
            let ghost prev_ghcb = ghcb@;
            ghcb.set_usage_ext(GHCB_VTL_RETURN_USAGE.into());
            assert(ghcb.ghcb_wf());
            let (ghcb_ptr, Tracked(mut ghcbpage_perm)) = ghcb.into_raw();
            assert(ghcbpage_perm@.snp() === SwSnpMemAttr::shared()) by {
                ghcb.proof_ghcb_wf();
            }
            let tracked mut ghcb_msr_perm = snpcore.regs.tracked_remove(GHCB_REGID());
            let tracked mut op_ghcbpage_perm = Some(ghcbpage_perm.tracked_into_raw());
            vmgexit(
                Tracked(&mut ghcb_msr_perm),
                Tracked(&mut snpcore.coreid),
                Tracked(&mut op_ghcbpage_perm),
            );
            let tracked ghcbpage_perm = op_ghcbpage_perm.tracked_unwrap().tracked_into();
            ghcb = VBox::from_raw(ghcb_ptr.to_usize(), Tracked(ghcbpage_perm));
            proof {
                snpcore.regs.tracked_insert(GHCB_REGID(), ghcb_msr_perm);
            }
        }
        (ghcb, vmsa)
    }
}

} // verus!

================
File: ./source/verismo/src/mshyper/mod.rs
================

mod hypercall;
mod wakeup;

use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::arch::entities::VMPL;
use crate::debug::VPrintAtLevel;
use crate::global::*;
use crate::pgtable_e::va_to_pa;
use crate::ptr::*;
use crate::registers::*;
use crate::snp::cpu::VmsaPage;
use crate::snp::ghcb::*;
use crate::snp::SnpCoreSharedMem;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::vbox::*;
use crate::*;

verus! {

pub type HvCallStatus = u64;

pub const HV_STATUS_TIMEOUT: u64 = 0x78u64;

pub const HV_MAX_RETRY: usize = 10;

// Hypercalls
pub const HVCALL_SET_VP_REGISTERS: u32 = 0x0051;

pub const HVCALL_GET_VP_REGISTERS: u32 = 0x0050;

pub const HVCALL_ENABLE_VP_VTL: u32 = 0x000f;

pub const HVCALL_START_VIRTUAL_PROCESSOR: u32 = 0x0099;

// Hyper-V registers
pub const HV_REGISTER_VSM_PARTITION_CONFIG: u32 = 0x000d0007;

/* SEV control register */

pub const HV_X64_REGISTER_SEV_CONTROL: u32 = 0x00090040;

pub const HV_X64_REGISTER_SEV_CONTROL_USE_SEV: u64 = 0x1;

/* MSR used to identify the guest OS. */

pub const SECURITY_MONITOR_GUEST_ID: u64 = 0x123;

// Any non-zero values work
pub const HV_X64_MSR_GUEST_OS_ID: u32 = 0x40000000;

/* intercept MSR */

pub const HV_X64_MSR_SINT0: u32 = 0x40000090;

pub const KeX64VectorSintIntercept: u64 = 0x30;

pub const HV_REGISTER_GUEST_VSM_PARTITION_CONFIG: u32 = 0x000D0008;

pub const HV_PARTITION_ID_SELF: u64 = 0xffff_ffff_ffff_ffffu64;

pub const HV_VP_INDEX_SELF: u32 = 0xffff_fffeu32;

pub const REG_NO_USE_VTL: u8 = 0;

pub const REG_USE_VTL: u8 = 0x10;

pub type HyperPageHandle = VBox<OnePage>;

pub struct GhcbHyperPageHandle(pub GhcbHandle, pub HyperPageHandle);

} // verus!
verus! {

pub closed spec fn spec_vtl_vmpl_map(vtl: int, vmpl: u8) -> bool {
    ||| (vtl == 2 && vmpl == 0)
    ||| (vtl == 0 && vmpl != 0)
}

pub fn get_vtl(vmpl: u8) -> (vtl: u8)
    ensures
        spec_vtl_vmpl_map(vtl as int, vmpl),
{
    if vmpl == 0 {
        2
    } else {
        0
    }
}

pub open spec fn _hyperpage_wf(id: int, snp: SwSnpMemAttr) -> bool {
    &&& snp === SwSnpMemAttr::shared()
    &&& id % PAGE_SIZE!() == 0
}

impl VBox<OnePage> {
    pub open spec fn hyperpage_wf(&self) -> bool {
        &&& self@.is_constant()
        &&& _hyperpage_wf(self.id(), self.snp())
    }
}

// Register OD ID and mark hyperpage as shared.
pub fn hyperv_register(
    handle: GhcbHyperPageHandle,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (handles: GhcbHyperPageHandle)
    requires
        handle.wf(),
        old(cs).inv(),
    ensures
        handles.wf(),
        cs.inv(),
        cs.only_lock_reg_coremode_updated(
            *old(cs),
            set![GHCB_REGID()],
            set![spec_PT().lockid(), spec_CONSOLE().lockid()],
        ),
{
    let GhcbHyperPageHandle(ghcb_handle, hyperpage_handle) = handle;
    let ghost cs1 = *cs;
    let ghcb_handle = ghcb_handle.ghcb_write_msr(
        HV_X64_MSR_GUEST_OS_ID,
        SECURITY_MONITOR_GUEST_ID,
        Tracked(cs),
    );
    let ghost cs2 = *cs;
    let (read, ghcb_handle) = ghcb_handle.ghcb_read_msr(HV_X64_MSR_GUEST_OS_ID, Tracked(cs));
    let ghost cs3 = *cs;
    proof {
        cs1.lemma_update_prop(cs2, *cs, set![], set![spec_PT().lockid()], set![], set![]);
        assert(set![spec_PT().lockid()].union(set![]) =~~= set![spec_PT().lockid()]);
        reveal_strlit("Register OS ID: ");
    }
    (new_strlit("Register OS ID: "), read).debug(Tracked(cs));
    proof {
        cs1.lemma_update_prop(
            cs3,
            *cs,
            set![],
            set![spec_PT().lockid()],
            set![GHCB_REGID()],
            set![spec_CONSOLE().lockid()],
        );
        assert(set![spec_PT().lockid()].union(set![spec_CONSOLE().lockid()])
            =~~= set![spec_PT().lockid(), spec_CONSOLE().lockid()]);
        assert(set![].union(set![GHCB_REGID()]) =~~= set![GHCB_REGID()]);
    }
    GhcbHyperPageHandle(ghcb_handle, hyperpage_handle)
}

} // verus!
verismo_simple! {
#[repr(C, packed)]
pub struct RegSetEntry {
    pub name: u32,
    pub reserved_2: u32,
    pub reserved_3: u64,
    pub value_low: u64,
    pub value_high: u64,
}

#[repr(C, packed)]
pub struct HvCallInputSetReg {
    pub ptid: u64,
    pub vpid: u32,
    pub vtl: u8,
    pub reserved: [u8; 3],
    pub element: RegSetEntry,
}

#[repr(C, packed)]
pub struct RegGetEntry {
    pub name0: u32,
    pub name1: u32,
}

#[repr(C, packed)]
pub struct HvCallInputGetReg {
    pub ptid: u64,
    pub vpid: u32,
    pub vtl: u8,
    pub reserved: [u8; 3],
    pub element: RegGetEntry,
}

#[repr(C, packed)]
#[derive(Copy, VClone)]
pub struct HvCallOutputGetReg {
    pub low: u64,
    //pub high: u64,
}

use crate::debug::VPrint;
#[repr(C, align(1))]
#[derive(VPrint)]
pub struct HvCallVpVtlInput {
    pub ptid: u64,
    pub vpid: u32,
    pub vtl: u32,
    pub vmsa_addr: u64,
    pub reserved_ctx: [u64; 27],
}
}

verus! {

impl HvCallVpVtlInput {
    pub fn new(ptid: u64, vpid: u32, vtl: u8, vmsa_addr: u64) -> (ret: Self)
        ensures
            ret.ptid.spec_eq(ptid),
            ret.vpid.spec_eq(vpid),
            ret.vtl.spec_eq(vtl),
            ret.vmsa_addr.spec_eq((vmsa_addr | HV_X64_REGISTER_SEV_CONTROL_USE_SEV)),
            ret.is_constant(),
    {
        let vmsa_addr = vmsa_addr | HV_X64_REGISTER_SEV_CONTROL_USE_SEV;
        HvCallVpVtlInput {
            ptid: ptid.into(),
            vpid: vpid.into(),
            vtl: vtl.into(),
            vmsa_addr: vmsa_addr.into(),
            reserved_ctx: Array::new(u64_s::new(0)),
        }
    }
}

} // verus!
verus! {

#[inline]
pub fn hvcall_code(lower: u32, upper: u32) -> (ret: u64)
    ensures
        ret == add(lower as u64, (upper as u64) << 32u64),
{
    let ghost upper64 = upper as u64;
    assert((upper64 << 32u64) <= 0xffff_ffff_0000_0000) by (bit_vector)
        requires
            upper64 == upper as u64,
    ;
    ((upper as u64) << 32u64) + (lower as u64)
}

impl GhcbHyperPageHandle {
    pub open spec fn wf(&self) -> bool {
        &&& self.0.ghcb_wf()
        &&& self.1.hyperpage_wf()
    }
}

} // verus!

================
File: ./source/verismo/src/mshyper/hypercall.rs
================

use super::*;

verus! {

impl GhcbHyperPageHandle {
    // ret.0: Ok(hypercall status) if GHCB call succeeds; else GHCB error code
    pub fn hv_call(
        self,
        control: u64,
        has_input: bool,
        has_output: bool,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: (Result<HvCallStatus, SvmStatus>, GhcbHyperPageHandle))
        requires
            self.wf(),
            (*old(cs)).inv(),
        ensures
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            ret.1.wf(),
    {
        let ret = SvmStatus::Ok;
        let GhcbHyperPageHandle(mut ghcb, mut hyperpage) = self;
        let hyperpage_addr: u64 = hyperpage.get_const_addr() as u64;
        let in_addr = if has_input {
            hyperpage_addr
        } else {
            0
        };
        let out_addr = if has_output {
            hyperpage_addr
        } else {
            0
        };
        assert(ghcb.ghcb_wf());
        ghcb.box_update(GhcbClear);
        assert(ghcb.ghcb_wf());
        ghcb.box_update((GhcbSetCplFn, 0));
        ghcb.box_update((GhcbSetRaxFn, 0));
        ghcb.box_update((GhcbSetRcxFn, control));
        ghcb.box_update((GhcbSetRdxFn, in_addr));  // input addr
        ghcb.box_update((GhcbSetR8Fn, out_addr));  // output addr
        let mut exit_code = SVM_EXIT_VMMCALL;
        let mut exit_info1 = 0;
        let mut exit_info2 = 0;
        let (resp, mut ghcb) = ghcb.ghcb_page_proto(
            &mut exit_code,
            &mut exit_info1,
            &mut exit_info2,
            Tracked(cs),
        );
        // Cannot borrow Hv-shared memory. Copy to private.
        let ret: Result<HvCallStatus, SvmStatus> = match resp {
            SvmStatus::Ok => {
                if ghcb.box_borrow(GhcbCheckRax) {
                    let (rax, newghcb) = ghcb.rax();
                    ghcb = newghcb;
                    Ok(rax)
                } else {
                    new_strlit("resp rax is invalid").leak_debug();
                    Err(SvmStatus::VmmError)
                }
            },
            _ => Err(ret),
        };
        (ret, GhcbHyperPageHandle(ghcb, hyperpage))
    }

    pub fn hv_call_with_retry(
        self,
        control: u64,
        has_input: bool,
        has_output: bool,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: (Result<HvCallStatus, SvmStatus>, GhcbHyperPageHandle))
        requires
            self.wf(),
            (*old(cs)).inv(),
        ensures
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            ret.1.wf(),
    {
        (new_strlit("hvcall start: "), control).leak_debug();
        let mut i = 0;
        let mut ret: Result<HvCallStatus, SvmStatus> = Err(SvmStatus::Unsupported);
        let mut handle = self;
        let ghost oldcs = (*cs);
        while i < HV_MAX_RETRY
            invariant
                0 <= i <= HV_MAX_RETRY,
                handle.wf(),
                (*cs).inv(),
                (*cs).only_lock_reg_coremode_updated(oldcs, set![], set![]),
            ensures
                handle.wf(),
                (*cs).inv(),
                (*cs).only_lock_reg_coremode_updated(oldcs, set![], set![]),
        {
            let ghost prevcs = (*cs);
            let (tmpret, tmphandle) = handle.hv_call(control, has_input, has_output, Tracked(cs));
            proof {
                oldcs.lemma_update_prop(prevcs, (*cs), set![], set![], set![], set![]);
            }
            ret = tmpret;
            handle = tmphandle;
            i = i + 1;
            match &ret {
                Ok(hvcall_code) => {
                    let hvcall_code = (*hvcall_code) as u32 as u64;
                    ((new_strlit("status: "), hvcall_code), new_strlit("\n")).leak_debug();
                    if hvcall_code != HV_STATUS_TIMEOUT {
                        break ;
                    }
                    continue ;
                },
                Err(code) => {
                    (new_strlit("err: "), code.as_u64()).leak_debug();
                    break ;
                },
            }
        }
        (ret, handle)
    }

    //pub const USE_VTL: u8 = 0x10;
    /// Set VP register
    /// HVCALL_SET_VP_REGISTERS
    ///  vtl[0:4]: vtl value
    ///  vtl[5]: use vtl
    pub fn set_vp_reg(
        self,
        reg: u32,
        val: u64,
        use_vtl: u8,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (handle: Self)
        requires
            self.wf(),
            (*old(cs)).inv(),
        ensures
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            handle.wf(),
    {
        ((new_strlit("set_vp_reg:"), reg), val).leak_debug();
        let control = hvcall_code(HVCALL_SET_VP_REGISTERS, 1);
        let reserved = Array::new(u8_s::new(0));
        let input = HvCallInputSetReg {
            ptid: HV_PARTITION_ID_SELF.into(),
            vpid: HV_VP_INDEX_SELF.into(),
            element: RegSetEntry {
                name: reg.into(),
                value_low: val.into(),
                value_high: (0u64).into(),
                reserved_2: 0u32.into(),
                reserved_3: 0u64.into(),
            },
            vtl: use_vtl.into(),
            reserved,
        };
        let GhcbHyperPageHandle(mut ghcb, mut hyperpage) = self;
        let (hyper_ptr, Tracked(hyperperm)) = hyperpage.into_raw();
        let input_ptr = hyper_ptr.to();
        let (_, Tracked(hyperperm)) = input_ptr.replace_with::<OnePage>(input, Tracked(hyperperm));
        let hyperpage = VBox::from_raw(hyper_ptr.to_usize(), Tracked(hyperperm));
        let mut handle = GhcbHyperPageHandle(ghcb, hyperpage);
        let (status, handle) = handle.hv_call_with_retry(control, true, false, Tracked(cs));
        match status {
            Ok(hvcall_code) if (hvcall_code as u32) != 0 => {
                vc_terminate(SM_TERM_VMM_ERR, Tracked(&mut cs.snpcore));
            },
            _ => {},
        }
        handle
    }

    pub fn get_vp_reg(
        self,
        reg: u32,
        use_vtl: u8,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret_handle: (u64, Self))
        requires
            self.wf(),
            (*old(cs)).inv(),
        ensures
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            ret_handle.1.wf(),
    {
        let control = hvcall_code(HVCALL_GET_VP_REGISTERS, 1);
        let reserved = Array::new(u8_s::new(0));
        let input = HvCallInputGetReg {
            ptid: HV_PARTITION_ID_SELF.into(),
            vpid: HV_VP_INDEX_SELF.into(),
            element: RegGetEntry { name0: reg.into(), name1: 0u32.into() },
            vtl: use_vtl.into(),
            reserved,
        };
        let GhcbHyperPageHandle(mut ghcb, mut hyperpage) = self;
        let (hyper_ptr, Tracked(hyperperm)) = hyperpage.into_raw();
        let input_ptr = hyper_ptr.to();
        let (_, Tracked(hyperperm)) = input_ptr.replace_with::<OnePage>(input, Tracked(hyperperm));
        let hyperpage = VBox::from_raw(hyper_ptr.to_usize(), Tracked(hyperperm));
        let mut handle = GhcbHyperPageHandle(ghcb, hyperpage);
        let (status, handle) = handle.hv_call_with_retry(control, true, true, Tracked(cs));
        match status {
            Ok(hvcall_code) if hvcall_code as u32 != 0 => {
                vc_terminate(SM_TERM_VMM_ERR, Tracked(&mut cs.snpcore));
            },
            _ => {},
        }
        let GhcbHyperPageHandle(ghcb, hyperpage) = handle;
        let (hyper_ptr, Tracked(hyperperm)) = hyperpage.into_raw();
        let output_ptr = hyper_ptr.to::<HvCallOutputGetReg>();
        let (output, Tracked(hyperperm)) = output_ptr.copy_with::<OnePage>(Tracked(hyperperm));
        let hyperpage = VBox::from_raw(hyper_ptr.to_usize(), Tracked(hyperperm));
        (output.low.into(), GhcbHyperPageHandle(ghcb, hyperpage))
    }
}

} // verus!

================
File: ./source/verismo/src/bsp.rs
================

use crate::allocator::VeriSMoAllocator;
use crate::arch::addr_s::PAGE_SIZE;
use crate::arch::reg::RegName;
use crate::boot::idt::init_idt;
use crate::boot::init::{init_mem, InitMem};
use crate::boot::monitor_params::{MonitorParamGlobalPerms, MonitorParamPerms, MonitorParams};
use crate::debug::Console;
use crate::global::*;
use crate::lock::{LockPermRaw, MapLockContains, MapRawLockTrait};
use crate::mem::{RawMemPerms, SnpMemCoreConsole};
use crate::mshyper::{hyperv_register, GhcbHyperPageHandle, HyperPageHandle};
use crate::pgtable_e::TrackedPTEPerms;
use crate::ptr::{SnpMemAttrTrait, SnpPPtr, SnpPointsTo, SwSnpMemAttr};
use crate::registers::*;
use crate::security::{SnpGuestChannel, SnpSecretsPageLayout, RICHOS_VMPL};
use crate::snp::cpu::{PerCpuData, *};
use crate::snp::cpuid::init_cpu_for_crypto;
use crate::snp::ghcb::GhcbHandle;
use crate::snp::percpu::BSP;
use crate::snp::{SnpCoreConsole, SnpCoreSharedMem};
use crate::tspec::*;
use crate::tspec_e::*;
use crate::vbox::VBox;

mod ap {
    use super::*;
    use crate::debug::VPrintAtLevel;
    verus! {

/// AP entry
#[no_mangle]
pub extern "C" fn ap_call(
    cpu: &PerCpuData,
    Tracked(cs): Tracked<SnpCoreSharedMem>,
    Tracked(nextvmpl_id): Tracked<CoreIdPerm>,
)
    requires
        nextvmpl_id@.vmpl == RICHOS_VMPL as nat,
        cs.inv_stage_ap_wait(),
        cpu.inv(),
{
    let tracked mut cs = cs;
    let cpu_id = cpu.cpu as usize;
    (new_strlit("ap call "), cpu_id).leak_debug();
    new_strlit("ap alloc_ghcb_handle").leak_debug();
    let ghcb = GhcbHandle::alloc_ghcb_handle(Tracked(&mut cs));
    let (hyperv, ghcb) = HyperPageHandle::new_shared_page(PAGE_SIZE, ghcb, Tracked(&mut cs));
    let (guest_channel, ghcb) = SnpGuestChannel::new(ghcb, Tracked(&mut cs));
    let ghcb_hv_h = GhcbHyperPageHandle(ghcb, hyperv);
    assert(ghcb_hv_h.wf());
    let mut vmsa: VBox<VmsaPage>;
    loop
        invariant
            cs.inv_stage_ap_wait(),
            nextvmpl_id@.vmpl == RICHOS_VMPL as nat,
        ensures
            vmsa.is_vmpl0_private_page(),
            vmsa@.vmpl.spec_eq(RICHOS_VMPL),
    {
        let tracked mut vmsa_lock = cs.lockperms.tracked_remove(spec_RICHOS_VMSA_lockid());
        let (vmsa_vec_ptr, Tracked(mut vmsa_vec_perm), Tracked(mut vmsa_lock0)) =
            RICHOS_VMSA().acquire(Tracked(vmsa_lock), Tracked(&cs.snpcore.coreid));
        proof {
            vmsa_lock = vmsa_lock0;
        }
        let mut vmsa_vec = vmsa_vec_ptr.take(Tracked(&mut vmsa_vec_perm));
        let mut vmsa_opt: Option<VBox<VmsaPage>> = None;
        if vmsa_vec.len() > cpu_id {
            vmsa_opt = vmsa_vec.remove(cpu_id);
            vmsa_vec.insert(cpu_id, None);
        }
        vmsa_vec_ptr.put(Tracked(&mut vmsa_vec_perm), vmsa_vec);
        RICHOS_VMSA().release(
            Tracked(&mut vmsa_lock),
            Tracked(vmsa_vec_perm),
            Tracked(&cs.snpcore.coreid),
        );
        proof {
            cs.lockperms.tracked_insert(spec_RICHOS_VMSA_lockid(), vmsa_lock);
        }
        match vmsa_opt {
            Some(v) => {
                vmsa = v;
                break ;
            },
            _ => {},
        }
        crate::lock::fence();
    }
    new_strlit("start richos ap\n").leak_debug();
    crate::security::run_richos(
        ghcb_hv_h,
        guest_channel,
        vmsa,
        cpu.secret,
        Tracked(nextvmpl_id),
        Tracked(&mut cs),
    );
    loop {
    }
}

} // verus!
} // verus!
verus! {

pub open spec fn shared_mem_range() -> Map<int, (int, nat)> {
    map![
            spec_ALLOCATOR().lockid() => spec_ALLOCATOR().ptr_range(),
            spec_CONSOLE_lockid() => spec_CONSOLE_range(),
            spec_PT_lockid() => spec_PT_range(),
        ]
}

} // verus!
verus! {

impl SnpCoreSharedMem {
    // inv + allocator + console
    pub open spec fn inv_ac(&self) -> bool {
        &&& self.inv()
    }
}

} // verus!
pub use ap::ap_call;
verus! {

#[no_mangle]
#[verifier(external_body)]
// BSP starts with a set of snpcore for BSP at VMPL0 - VMPL4 and a set of snpcore for APs at VMPL0
pub extern "C" fn bsp_call(
    mp_ptr: crate::ptr::SnpPPtr<MonitorParams>,
    Tracked(mp_perms): Tracked<MonitorParamPerms>,
    Tracked(alloc_perm): Tracked<SnpPointsTo<VeriSMoAllocator>>,
    Tracked(memcc): Tracked<SnpMemCoreConsole>,
    Tracked(ap_ids): Tracked<Map<int, CoreIdPerm>>,
    Tracked(nextvmpl_ids): Tracked<Map<int, CoreIdPerm>>,
    Tracked(initial_locks): Tracked<Map<int, LockPermRaw>>,
    Tracked(preval_free_perms): Tracked<RawMemPerms>,
    Tracked(gdt_perm): Tracked<&SnpPointsTo<crate::snp::cpu::GDT>>,
)
    requires
        memcc.memperm.mem_perms_e820_invalid(mp_perms@.e820()),
        memcc.wf(),
        preval_free_perms.mem_perms_e820_valid(mp_perms@.e820()),
        preval_free_perms.wf(),
        mp_perms@.wf_at(mp_ptr),
        mp_ptr.is_constant(),
        is_alloc_perm(alloc_perm@),
        gdt_perm@.wf_const_default(
            memcc.cc.snpcore.regs[RegName::GdtrBaseLimit].val::<Gdtr>().base.vspec_cast_to(),
        ),
        initial_locks.contains_clean_locks(memcc.cc.snpcore.cpu(), shared_mem_range()),
        contains_PT(initial_locks),
        initial_locks[spec_PT_lockid()]@.is_unlocked(
            memcc.cc.snpcore.cpu(),
            spec_PT_lockid(),
            initial_locks[spec_PT_lockid()]@.points_to.range(),
        ),
        spec_ap_ids_wf(ap_ids, BSP as int),
        forall|i| spec_cpumap_contains_cpu(nextvmpl_ids, i, 1),
    ensures
        false,
{
    let tracked mut nextvmpl_ids = nextvmpl_ids;
    let tracked mut lockperms = initial_locks;
    assert(lockperms.contains_key(spec_CONSOLE_lockid()));
    let tracked mut alloc_lock = lockperms.tracked_remove(spec_ALLOCATOR_lockid());
    assert(alloc_lock === initial_locks[spec_ALLOCATOR_lockid()]);
    assert(spec_CONSOLE_lockid() != spec_ALLOCATOR_lockid());
    let tracked mut console_lock = lockperms.tracked_remove(spec_CONSOLE_lockid());
    let tracked mut pt_lock = lockperms.tracked_remove(spec_PT_lockid());
    assert(console_lock === initial_locks[spec_CONSOLE_lockid()]);
    let (mpbox, hvparam_page, Tracked(mut cc), Tracked(mut mp_gperms)) = init_mem(
        mp_ptr,
        Tracked(mp_perms),
        Tracked(alloc_perm),
        Tracked(&mut alloc_lock),
        Tracked(memcc),
        Tracked(preval_free_perms),
    );
    let mparam = mpbox.borrow();
    let tracked SnpCoreConsole { snpcore, console } = cc;
    proof {
        let tracked console = console.tracked_remove(0);
        console_lock.tracked_bind_new::<Console>(Console::invfn(), console.tracked_into());
        assert(Console::invfn()(console@.value()));
        assert(console_lock@.invfn.inv::<Console>(console@.value()));
        assert(pt_lock@.invfn.inv::<TrackedPTEPerms>(pt_lock@.points_to.value()));
    }
    let tracked mut lockperms = Map::tracked_empty();
    proof {
        lockperms.tracked_insert(spec_CONSOLE_lockid(), console_lock);
        lockperms.tracked_insert(spec_ALLOCATOR_lockid(), alloc_lock);
        lockperms.tracked_insert(spec_PT_lockid(), pt_lock);
        assert(lockperms.contains_key(spec_ALLOCATOR_lockid()));
    }
    let tracked mut cs = SnpCoreSharedMem { snpcore, lockperms };
    proof {
        assert(cs.snpcore.inv());
        assert(console_lock@.is_unlocked(
            cs.snpcore.cpu(),
            console_lock@.lockid(),
            console_lock@.ptr_range(),
        ));
        assert(alloc_lock@.is_unlocked(
            cs.snpcore.cpu(),
            alloc_lock@.lockid(),
            alloc_lock@.ptr_range(),
        ));
        assert(cs.wf_pt());
        assert(contains_ALLOCATOR(cs.lockperms));
        assert(contains_CONSOLE(cs.lockperms));
        assert(contains_PT(cs.lockperms));
    }
    init_idt(Tracked(&mut cs));
    let tracked MonitorParamGlobalPerms { cpuid, secret, richos, acpi } = mp_gperms;
    let cpuid_page = VBox::from_raw(mparam.cpuid_page.into(), Tracked(cpuid.tracked_into()));
    init_cpu_for_crypto(cpuid_page.borrow(), Tracked(&mut cs));
    let ghcb = GhcbHandle::alloc_ghcb_handle(Tracked(&mut cs));
    let (hyperv, ghcb) = HyperPageHandle::new_shared_page(PAGE_SIZE, ghcb, Tracked(&mut cs));
    let ghcb_hv_h = GhcbHyperPageHandle(ghcb, hyperv);
    let ghcb_hv_h = hyperv_register(ghcb_hv_h, Tracked(&mut cs));
    let secret_page = VBox::<SnpSecretsPageLayout>::from_raw(
        mparam.secret_page.into(),
        Tracked(secret.tracked_into()),
    );
    let gdtr = GdtBaseLimit.read(Tracked(cs.snpcore.regs.tracked_borrow(RegName::GdtrBaseLimit)));
    let gdt_ptr = SnpPPtr::from_usize(gdtr.base.into());
    let gdt = gdt_ptr.borrow(Tracked(gdt_perm));
    let mut ghcb_hv_h = ghcb_hv_h.start_all_ap(
        mparam.cpu_count.into(),
        secret_page.borrow(),
        gdt,
        Tracked(ap_ids),
        Tracked(&mut cs),
    );
    let GhcbHyperPageHandle(ghcb, hyperv) = ghcb_hv_h;
    let (guest_channel, ghcb) = SnpGuestChannel::new(ghcb, Tracked(&mut cs));
    assert(richos@.wf_const_default(
        (mparam.richos_start.vspec_cast_to(), mparam.richos_size.vspec_cast_to()),
    ));
    let tracked nextvmpl_id = nextvmpl_ids.tracked_remove(BSP as int);
    crate::security::start_richos(
        mparam,
        secret_page.borrow(),
        cpuid_page,
        hvparam_page,
        guest_channel,
        GhcbHyperPageHandle(ghcb, hyperv),
        Tracked(nextvmpl_id),
        Tracked(richos),
        Tracked(acpi),
        Tracked(&mut cs),
    );
    loop {
    }
}

} // verus!

================
File: ./source/verismo/src/security/monitor.rs
================

use alloc::vec::Vec;

use self::mem::osmem_adjust;
use super::*;
use crate::boot::linux::*;
use crate::boot::monitor_params::*;
use crate::debug::VPrint;
use crate::lock::*;
use crate::mshyper::*;
use crate::security::pcr::*;
use crate::snp::cpu::UpdateVMPL;
use crate::snp::cpuid::SnpCpuidTable;

pub const MAX_LOCK_REQ: usize = 4;

pub const INVALID_REQ: u64 = 1;

verus! {

pub fn fill_vec<T>(vec: &mut Vec<Option<T>>, n: usize)
    ensures
        old(vec).len() < n ==> vec.len() == n,
        vec@.subrange(0, old(vec).len() as int) =~~= old(vec)@,
        old(vec).len() >= n ==> *vec === *old(vec),
        forall|i| old(vec).len() <= i < n ==> vec[i] === None,
{
    if vec.len() >= n {
        return ;
    }
    let ghost oldvec = *vec;
    while vec.len() < n
        invariant
            vec.len() <= n,
            vec.len() >= oldvec.len(),
            n >= oldvec.len(),
            forall|i| 0 <= i < oldvec.len() ==> vec[i] === oldvec[i],
            forall|i| oldvec.len() <= i < vec.len() ==> vec[i] === None,
    {
        let ghost prev_vec = *vec;
        vec.push(None);
    }
}

fn create_lock_entries(priv_req: &LockKernReq) -> (ret: Vec<(usize, usize)>)
    requires
        priv_req.wf(),
    ensures
        forall|k| 0 <= k < ret.len() ==> ret[k].0.spec_valid_pn_with(ret[k].1 as nat),
{
    let mut entries = Vec::<(usize, usize)>::new();
    let mut i = 0;
    while i < 256
        invariant
            i <= 256,
            entries.len() == i,
            priv_req.wf(),
            forall|k|
                0 <= k < i ==> entries[k].0 === priv_req@[k].start.vspec_cast_to() && entries[k].1
                    == priv_req@[k].end@.val - priv_req@[k].start@.val,
            forall|k| 0 <= k < i ==> entries[k].0.spec_valid_pn_with(entries[k].1 as nat),
    {
        let val = priv_req.index(i);
        let mut end = val.end;
        let mut start = val.start;
        end.declassify();
        start.declassify();
        let end: usize = end.into();
        let start: usize = start.into();
        if end <= start || !end.check_valid_pn(0) {
            break ;
        }
        entries.push((start, end - start));
        i = i + 1;
    }
    entries
}

} // verus!
verismo_simple! {
#[derive(Copy, VClone)]
pub struct PageRange {
    pub start: u64,
    pub end: u64,
}
}

verus! {

pub const SNP_VMPL_REQ_MAGIC: u64 = 0xfeeddead;

#[derive(SpecGetter, SpecSetter)]
pub struct MonitorHandle<'a> {
    handle: GhcbHyperPageHandle,
    guest_channel: SnpGuestChannel,
    gvca_page: Option<VBox<OnePage>>,
    vmsa: VBox<VmsaPage>,
    secret: &'a SnpSecretsPageLayout,
    stat: u64,
}

impl<'a> MonitorHandle<'a> {
    pub closed spec fn wf(&self) -> bool {
        &&& self.handle.wf()
        &&& self.guest_channel.wf()
        &&& self.gvca_page.wf()
        &&& self.vmsa.is_vmsa_page()
        &&& self.secret.wf_mastersecret()
        &&& self.gvca_page.is_Some() ==> self.wf_registered()
    }

    pub closed spec fn wf_registered(&self) -> bool {
        let snp = self.gvca_page.get_Some_0().snp();
        &&& self.handle.wf()
        &&& self.guest_channel.wf()
        &&& self.gvca_page.is_Some()
        &&& self.gvca_page.get_Some_0().is_page()
        &&& !snp.is_confidential_to(1)
        &&& snp.is_confidential_to(2)
        &&& snp.is_confidential_to(3)
        &&& snp.is_confidential_to(4)
        &&& snp.pte().spec_encrypted()
        //&&& snp.rmp@[VMPL::from_int(RICHOS_VMPL as int)].is_super_of(OSMemPermSpec::readwrite().to_value().to_page_perm())

        &&& self.vmsa.is_vmsa_page()
        &&& self.secret.wf_mastersecret()
    }
}

pub fn run_richos(
    handle: GhcbHyperPageHandle,
    guest_channel: SnpGuestChannel,
    vmsa: VBox<VmsaPage>,
    secret: &SnpSecretsPageLayout,
    Tracked(nextvmpl_id): Tracked<CoreIdPerm>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        0 < nextvmpl_id@.vmpl < 4,
        nextvmpl_id@.vmpl === vmsa@.vmpl.vspec_cast_to(),
        nextvmpl_id@.vmpl == RICHOS_VMPL,
        handle.wf(),
        guest_channel.wf(),
        old(cs).inv_stage_verismo(),
        vmsa.is_vmpl0_private_page(),
        secret.wf_mastersecret(),
    ensures
        false,
{
    proof {
        reveal_strlit("Run richos.");
    }
    new_strlit("Run richos.").leak_debug();
    let (handle, vmsa) = handle.register_vmsa(
        RICHOS_VMPL as u8,
        vmsa,
        Tracked(nextvmpl_id),
        Tracked(cs),
    );
    let mut gvca_page = None;
    let mut mh = MonitorHandle { handle, guest_channel, vmsa, gvca_page, secret, stat: 0 };
    assert(cs.inv_stage_verismo());
    loop
        invariant
            mh.wf(),
            cs.inv_stage_verismo(),
            vmsa.is_vmsa_page(),
    {
        let GhcbHyperPageHandle(ghcb, hypercall) = mh.handle;
        let (ghcb, vmsa) = ghcb.switch_to_next_vmpl(mh.vmsa, Tracked(&mut cs.snpcore));
        mh.handle = GhcbHyperPageHandle(ghcb, hypercall);
        mh.vmsa = vmsa;
        mh = mh.handle_richos(Tracked(cs));
    }
}

} // verus!
verus! {

#[derive(VPrint, IsConstant)]
pub struct GvcaHeader {
    pub gpa: u64,
    pub op: u32,
    pub cpu: u32,
    pub other: u64,
}

} // verus!
verus! {

impl GvcaHeader {
    pub fn from(rbx: u64, rcx: u64, rdx: u64) -> (ret: GvcaHeader)
        ensures
            ret.gpa === rbx,
            ret.op as int == rcx as u32 as int,
            ret.cpu as int == (rcx >> 32u64) as u32,
            ret.other == rdx,
    {
        GvcaHeader { gpa: rbx, op: rcx as u32, cpu: (rcx >> 32u64) as u32, other: rdx }
    }

    pub fn gpn(&self) -> (ret: usize)
        requires
            self.gpa.spec_valid_addr_with(0),
        ensures
            ret == (self.gpa as int).to_page(),
    {
        self.gpa.to_page() as usize
    }

    pub closed spec fn spec_has_gvca(&self) -> bool {
        self.gpa.spec_valid_addr_with(PAGE_SIZE as nat)
    }

    pub fn has_gvca(&self) -> (ret: bool)
        ensures
            ret == self.spec_has_gvca(),
    {
        self.gpa.check_valid_addr(PAGE_SIZE as u64)
    }

    pub fn npages(&self) -> (ret: u16)
        ensures
            ret as int == (self.gpa as int) % PAGE_SIZE!(),
    {
        (self.gpa as usize % PAGE_SIZE) as u16
    }

    pub fn cpu(&self) -> (ret: u32)
        ensures
            ret == self.cpu,
    {
        self.cpu
    }
}

} // verus!
verus! {

#[is_variant]
#[derive(SpecIntEnum)]
pub enum VmplReqOp {
    WakupAp = 0xffff_fffe,
    SetPrivate = 0x1,
    SetShared = 0x2,
    Register = 0x3,
    ExtendPcr = 0x4,
    LockKernExe = 0x5,
    Attest = 0x6,
    Secret = 0x7,
    Encrypt = 0x8,
    Decrypt = 0x9,
}

} // verus!
verismo_simple! {
#[repr(C, align(1))]
pub struct EncryptReq {
    pub index: u32,
    pub data: Array<u8, 4092>,
}
}

verismo_simple! {
    #[repr(C, align(1))]
    #[derive(VClone, Copy)]
    pub struct LockKernEntry {
        pub start: u64,
        pub end: u64,
    }

    pub type LockKernReq = [LockKernEntry; {PAGE_SIZE/16}];
}

verus! {

impl<'a> MonitorHandle<'a> {
    pub fn parse_and_handle_vmpl_req(self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret:
        Self)
        requires
            self.wf(),
            old(cs).inv_stage_verismo(),
        ensures
            ret.wf(),
            cs.inv_stage_verismo(),
    {
        let mut mhandle = self;
        let GhcbHyperPageHandle(ghcb, hypercall) = mhandle.handle;
        let vmsa = mhandle.vmsa;
        assert(vmsa.is_vmsa_page());
        let (vmsapage_ptr, Tracked(vmsa_perm)) = vmsa.into_raw();
        let vmsa_ptr = vmsapage_ptr;
        assert(vmsa_perm@.wf());
        let (mut rax, Tracked(vmsa_perm)) = vmsa_ptr.rax().copy_with::<VmsaPage>(
            Tracked(vmsa_perm),
        );
        assert(vmsa_perm@.wf());
        let mut magic_check = rax.sec_eq(&SNP_VMPL_REQ_MAGIC.into());
        magic_check.declassify();
        let magic_check = magic_check.reveal_value();
        if !magic_check {
            vc_terminate(SM_TERM_UNSUPPORTED, Tracked(&mut cs.snpcore));
        }
        let (mut rbx, Tracked(vmsa_perm)) = vmsa_ptr.rbx().copy_with::<VmsaPage>(
            Tracked(vmsa_perm),
        );
        let (mut rcx, Tracked(vmsa_perm)) = vmsa_ptr.rcx().copy_with::<VmsaPage>(
            Tracked(vmsa_perm),
        );
        let (mut rdx, Tracked(vmsa_perm)) = vmsa_ptr.rdx().copy_with::<VmsaPage>(
            Tracked(vmsa_perm),
        );
        rbx.declassify();
        rcx.declassify();
        rdx.declassify();
        let req = GvcaHeader::from(rbx.into(), rcx.into(), rdx.into());
        // Restore ghcb MSR value due to a hyperv bug;
        let ghcb_addr = ghcb.get_const_addr();
        let tracked mut ghcb_msr_perm = cs.snpcore.regs.tracked_remove(GHCB_REGID());
        MSR_GHCB().write(ghcb_addr.into(), Tracked(&mut ghcb_msr_perm));
        proof {
            cs.snpcore.regs.tracked_insert(GHCB_REGID(), ghcb_msr_perm);
        }
        // Check VMPL privilege.
        let (vmpl, Tracked(vmsa_perm)) = vmsa_ptr.vmpl().copy_with::<VmsaPage>(Tracked(vmsa_perm));
        let mut check_vmpl = vmpl.sec_eq(&u8_s::constant(RICHOS_VMPL));
        check_vmpl.declassify();
        if !check_vmpl.reveal_value() {
            vc_terminate(SM_TERM_RICHOS_ERR(0), Tracked(&mut cs.snpcore))
        };
        mhandle.vmsa = VBox::from_raw(vmsa_ptr.to_usize(), Tracked(vmsa_perm));
        mhandle.handle = GhcbHyperPageHandle(ghcb, hypercall);
        let ret = mhandle.handle_vmpl_request(&req, RICHOS_VMPL, Tracked(cs));
        match ret {
            Err(code) => { vc_terminate(SM_TERM_RICHOS_ERR(code as u64), Tracked(&mut cs.snpcore))
            },
            Ok(mh) => {
                mhandle = mh;
            },
        }
        mhandle
    }

    fn handle_mk_private_shared(
        self,
        vmpl: u8,
        gpn: usize,
        npages: u16,
        is_priv: bool,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: Result<Self, u8>)
        requires
            self.wf(),
            old(cs).inv_stage_verismo(),
        ensures
            ret.is_Ok() ==> ret.get_Ok_0().wf(),
            cs.inv_stage_verismo(),
            cs.only_lock_reg_coremode_updated(
                *old(cs),
                set![],
                set![spec_OSMEM_lockid(), spec_PT_lockid()],
            ),
    {
        if npages == 0 {
            return Ok(self);
        }
        if !gpn.check_valid_pn(npages as usize) {
            new_strlit("Invalid GPN npages pair\n").leak_debug();
            return Err(SM_TERM_RICHOS_ERR(0) as u8);
        }
        let ghost cs1 = *cs;
        let ret = self._handle_mk_private_shared(vmpl, gpn, npages, is_priv, Tracked(cs));
        let ghost cs2 = *cs;
        match ret {
            Ok((mhandle, real_npages)) => {
                if real_npages < npages as usize {
                    let ret = mhandle.handle_mk_private_shared(
                        vmpl,
                        gpn + real_npages,
                        npages - real_npages as u16,
                        is_priv,
                        Tracked(cs),
                    );
                    proof {
                        cs1.lemma_update_prop(
                            cs2,
                            *cs,
                            set![],
                            set![spec_OSMEM_lockid(), spec_PT_lockid()],
                            set![],
                            set![spec_OSMEM_lockid(), spec_PT_lockid()],
                        );
                    }
                    ret
                } else {
                    Ok(mhandle)
                }
            },
            Err(e) => { Err(e) },
        }
    }

    fn _handle_mk_private_shared(
        self,
        _vmpl: u8,
        gpn: usize,
        npages: u16,
        is_priv: bool,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: Result<(Self, usize), u8>)
        requires
            self.wf(),
            old(cs).inv_stage_verismo(),
            npages > 0,
        ensures
            ret.is_Ok() ==> ret.get_Ok_0().0.wf(),
            ret.is_Ok() ==> ret.get_Ok_0().1 <= npages,
            cs.inv_stage_verismo(),
            cs.only_lock_reg_coremode_updated(
                *old(cs),
                set![],
                set![spec_OSMEM_lockid(), spec_PT_lockid()],
            ),
    {
        let ghost cs1 = *cs;
        let tracked osmem_lock = cs.lockperms.tracked_remove(spec_OSMEM_lockid());
        let (osmem_ptr, Tracked(mut osmem_perm), Tracked(mut osmem_lock)) = OSMEM().acquire(
            Tracked(osmem_lock),
            Tracked(&cs.snpcore.coreid),
        );
        let mut osmem = osmem_ptr.take(Tracked(&mut osmem_perm));
        let mut entry;
        let mut i;
        if let Some(e) = osmem_check_and_get(&mut osmem, gpn, OSMemPerm::ram()) {
            entry = e.1;
            i = e.0;
        } else {
            osmem_ptr.put(Tracked(&mut osmem_perm), osmem);
            OSMEM().release(
                Tracked(&mut osmem_lock),
                Tracked(osmem_perm),
                Tracked(&cs.snpcore.coreid),
            );
            proof {
                cs.lockperms.tracked_insert(spec_OSMEM_lockid(), osmem_lock);
            }
            (new_strlit("Invalid perm from richos\n"), gpn).leak_debug();
            return Result::Err(SM_TERM_RICHOS_ERR(0) as u8);
        }
        let entry_start: usize = entry.start_page.into();
        let entry_end: usize = entry.end();
        let expected_gpn_end = gpn + npages as usize;
        let npages = if entry_end > expected_gpn_end {
            npages as usize
        } else {
            entry_end - gpn
        };
        let (left, entry, right) = osmem_entry_split(entry, gpn, npages);
        let mut mhandle = self;
        let GhcbHyperPageHandle(mut ghcb_h, hyperpage_h) = mhandle.handle;
        let ghost cs2 = *cs;
        // Should we reset rmp permission? Not necessary.
        // The memory still belongs to rich os and thus we do not need to reset;
        // VeriSMo never invalidate a page explicitly.
        // Instead, it relies on hypervisor's rmpupdate to automatically invalidate a page.
        // If hypervisor did not run rmpupdate to change the page to shared,
        // verismo will see a double-validation err when it pvalidate it again;
        // If hypervisor changes the mapping, the validation succeeds only for an invalidated page which has resetted rmp perms (rmpupdate will reset perms).
        // If hypervisor do not changes the mapping, the next validation only succeeds if the hypervisor has marked the page as shared.
        // In both cases, the page will become vmpl0 private.
        // That is also why verismo mk_shared do not call pvalidate(false).
        let (ghcb_h, entry) = osmem_adjust(ghcb_h, entry, !is_priv, Tracked(cs));
        let entry = osmem_entry_merge(left, entry);
        let entry = osmem_entry_merge(entry, right);
        osmem.insert(i, entry);
        proof {
            assert(osmem_wf(osmem@));
        }
        osmem_ptr.put(Tracked(&mut osmem_perm), osmem);
        OSMEM().release(Tracked(&mut osmem_lock), Tracked(osmem_perm), Tracked(&cs.snpcore.coreid));
        let ghost cs3 = *cs;
        proof {
            cs.lockperms.tracked_insert(spec_OSMEM_lockid(), osmem_lock);
        }
        proof {
            LockMap::lemma_lock_update_auto();
            SnpCore::lemma_regs_update_auto();
            assert(cs.snpcore.only_reg_coremode_updated(cs1.snpcore, set![]));
            assert(cs.lockperms[spec_OSMEM_lockid()]@.points_to.only_val_updated(
                cs1.lockperms[spec_OSMEM_lockid()]@.points_to,
            ));
            assert(cs.lockperms[spec_PT_lockid()]@.points_to.only_val_updated(
                cs1.lockperms[spec_PT_lockid()]@.points_to,
            ));
            assert(cs.lockperms.updated_lock(
                &cs1.lockperms,
                set![spec_OSMEM_lockid(), spec_PT_lockid()],
            )) by {
                assert forall|id: int|
                    !(set![spec_OSMEM_lockid(), spec_PT_lockid()].contains(id))
                        && cs1.lockperms.contains_key(id) implies (#[trigger] cs.lockperms[id])
                    === cs1.lockperms[id] && cs.lockperms.contains_key(id) by {
                    assert(id != spec_OSMEM_lockid());
                    assert(id != spec_PT_lockid());
                    assert(cs1.lockperms.contains_key(id));
                    assert(cs2.lockperms.contains_key(id));
                    assert(cs3.lockperms.contains_key(id));
                    assert(cs.lockperms.contains_key(id));
                }
            }
        }
        mhandle.handle = GhcbHyperPageHandle(ghcb_h, hyperpage_h);
        //new_strlit("End of handle_mk_private_shared\n").leak_debug();
        Result::Ok((mhandle, npages))
    }

    fn handle_wakeup_ap(
        &self,
        vmsa_gpn: usize,
        cpu: usize,
        vmpl: u8,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: Result<(), u8>)
        requires
            vmsa_gpn.spec_valid_pn_with(1),
            1 <= vmpl < 4,
            old(cs).inv_stage_verismo(),
            cpu < 0x10000_0000,
            self.wf(),
        ensures
            cs.inv_stage_verismo(),
            cs.only_lock_reg_coremode_updated(
                *old(cs),
                set![],
                set![spec_OSMEM_lockid(), spec_PT_lockid(), spec_RICHOS_VMSA_lockid()],
            ),
    {
        let vmsa_gpa = vmsa_gpn.to_addr();
        if !vmsa_gpa.check_valid_addr(PAGE_SIZE) {
            return Err(SM_TERM_RICHOS_ERR(3) as u8);
        }
        let ghost used_locks =
            set![spec_OSMEM_lockid(), spec_PT_lockid(), spec_RICHOS_VMSA_lockid()];
        assert(used_locks =~~= set![spec_OSMEM_lockid(), spec_PT_lockid()].union(
            set![spec_RICHOS_VMSA_lockid()],
        ));
        assert(set![spec_OSMEM_lockid(), spec_PT_lockid()] =~~= set![spec_OSMEM_lockid()].union(
            set![spec_PT_lockid()],
        ));
        assert(cs.lockperms.contains_key(spec_OSMEM_lockid()));
        let ghost cs1 = *cs;
        let tracked osmem_lock = cs.lockperms.tracked_remove(spec_OSMEM_lockid());
        let (osmem_ptr, Tracked(mut osmem_perm), Tracked(mut osmem_lock)) = OSMEM().acquire(
            Tracked(osmem_lock),
            Tracked(&cs.snpcore.coreid),
        );
        let mut osmem = osmem_ptr.take(Tracked(&mut osmem_perm));
        let ret = if let Some((vmsa, osperm)) = osmem_check_and_get_page::<VmsaPage>(
            &mut osmem,
            vmsa_gpn,
            OSMemPerm::ram(),
            Tracked(cs),
        ) {
            Ok(vmsa)
        } else {
            Err(SM_TERM_RICHOS_ERR(4) as u8)
        };
        osmem_ptr.put(Tracked(&mut osmem_perm), osmem);
        OSMEM().release(Tracked(&mut osmem_lock), Tracked(osmem_perm), Tracked(&cs.snpcore.coreid));
        proof {
            cs.lockperms.tracked_insert(spec_OSMEM_lockid(), osmem_lock);
            assert(cs.only_lock_reg_coremode_updated(
                cs1,
                set![],
                set![spec_OSMEM_lockid(), spec_PT_lockid()],
            ));
        }
        let ghost cs2 = *cs;
        match ret {
            Err(e) => {
                return Err(e);
            },
            _ => {},
        }
        let vmsa = ret.unwrap();
        let (vmsa_ptr, Tracked(vmsa_perm)) = vmsa.into_raw();
        let tracked vmsa_perm = vmsa_perm.tracked_into_raw();
        rmp_reset_vmpl_perm(vmsa_gpn, Tracked(&mut cs.snpcore), Tracked(&mut vmsa_perm));
        proof {
            assert(vmsa_perm@.snp().is_vmpl0_private());
        }
        let mut vmsa = VBox::<VmsaPage>::from_raw(
            vmsa_ptr.to_usize(),
            Tracked(vmsa_perm.tracked_into()),
        );
        vmsa.box_update(UpdateVMPL { vmpl: RICHOS_VMPL as u8 });
        let tracked mut vmsa_lock = cs.lockperms.tracked_remove(spec_RICHOS_VMSA_lockid());
        let (vmsa_vec_ptr, Tracked(mut vmsa_vec_perm), Tracked(mut vmsa_lock)) =
            RICHOS_VMSA().acquire(Tracked(vmsa_lock), Tracked(&cs.snpcore.coreid));
        let mut vmsa_vec = vmsa_vec_ptr.take(Tracked(&mut vmsa_vec_perm));
        let ghost prev_vmsa_vec = vmsa_vec;
        fill_vec(&mut vmsa_vec, cpu + 1);
        vmsa_vec.set(cpu, Some(vmsa));
        #[verusfmt::skip]
        proof {
            assert(richos_vmsa_invfn()(vmsa_vec)) by {
                assert forall|i| 0 <= i < vmsa_vec.len() implies
                    (vmsa_vec[i].is_Some() ==> is_richos_vmsa_box(#[trigger] vmsa_vec[i].get_Some_0()))
                by {
                    if vmsa_vec[i].is_Some() {
                        assert(i == cpu || prev_vmsa_vec[i] === vmsa_vec[i]);
                    }
                }
            }
        }
        vmsa_vec_ptr.put(Tracked(&mut vmsa_vec_perm), vmsa_vec);
        RICHOS_VMSA().release(
            Tracked(&mut vmsa_lock),
            Tracked(vmsa_vec_perm),
            Tracked(&cs.snpcore.coreid),
        );
        proof {
            cs.lockperms.tracked_insert(spec_RICHOS_VMSA_lockid(), vmsa_lock);
            cs1.lemma_update_prop(
                cs2,
                *cs,
                set![],
                set![spec_OSMEM_lockid(), spec_PT_lockid()],
                set![],
                set![spec_RICHOS_VMSA_lockid()],
            );
        }
        Ok(())
    }

    pub fn handle_register(
        &mut self,
        gpn: usize,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: bool)
        requires
            old(cs).inv_stage_verismo(),
            old(self).wf(),
        ensures
            self.wf(),
            cs.inv_stage_verismo(),
            cs.only_lock_reg_coremode_updated(
                *old(cs),
                set![],
                set![spec_OSMEM_lockid(), spec_PT_lockid()],
            ),
            *self === (*old(self)).spec_set_gvca_page(self.spec_gvca_page()),
            ret ==> self.wf_registered(),
    {
        let tracked osmem_lock = cs.lockperms.tracked_remove(spec_OSMEM_lockid());
        let (osmem_ptr, Tracked(mut osmem_perm), Tracked(mut osmem_lock)) = OSMEM().acquire(
            Tracked(osmem_lock),
            Tracked(&cs.snpcore.coreid),
        );
        let mut osmem = osmem_ptr.take(Tracked(&mut osmem_perm));
        let ret = if let Some((gvca, osperm)) = osmem_check_and_get_page(
            &mut osmem,
            gpn,
            OSMemPerm::empty().set_read(1).set_write(1),
            Tracked(cs),
        ) {
            self.gvca_page = Some(gvca);
            new_strlit("Register gvca_page\n").leak_debug();
            assume(self.wf_registered());
            true
        } else {
            new_strlit("failed to register GVCA\n").leak_debug();
            false
        };
        osmem_ptr.put(Tracked(&mut osmem_perm), osmem);
        OSMEM().release(Tracked(&mut osmem_lock), Tracked(osmem_perm), Tracked(&cs.snpcore.coreid));
        proof {
            cs.lockperms.tracked_insert(spec_OSMEM_lockid(), osmem_lock);
        }
        new_strlit("Register gvca_page end\n").leak_debug();
        ret
    }

    fn handle_extend_pcr(self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret: Self)
        requires
            old(cs).inv_stage_verismo(),
            old(cs).inv_stage_pcr(),
            self.wf(),
        ensures
            ret.wf(),
            cs.inv_stage_verismo(),
            cs.inv_stage_pcr(),
            cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_PCR_lockid()]),
    {
        let mut mhandle = self;
        if let Some(gvca) = mhandle.gvca_page {
            let req: VBox<ExtendPCRReq> = gvca.to();
            let val = req.copy_val();
            extend_pcr(0, &val, Tracked(cs));
            proof {
                assert(req.snp() === gvca.snp());
                assert(req.id() === gvca.id());
            }
            mhandle.gvca_page = Some(req.to());
        } else {
            new_strlit("Please register gvca page!!\n").leak_debug();
        }
        mhandle
    }

    fn handle_attest(self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret: Self)
        requires
            self.gvca_page.is_Some(),
            old(cs).inv_stage_verismo(),
            old(cs).inv_stage_pcr(),
            self.wf(),
        ensures
            ret.wf(),
            cs.inv_stage_verismo(),
            cs.inv_stage_pcr(),
            ret.wf(),
    {
        let mut mhandle = self;
        new_strlit("[Attest] start\n").leak_debug();
        if let Some(gvca) = mhandle.gvca_page {
            let GhcbHyperPageHandle(ghcb_h, hyperpage_h) = mhandle.handle;
            new_strlit("[Attest]: new report\n").leak_debug();
            let report = VBox::new_uninit(Tracked(cs));
            let (report, guest_channel, ghcb_h) = attest_pcr(
                mhandle.guest_channel,
                ghcb_h,
                mhandle.secret,
                0,
                report,
                Tracked(cs),
            );
            mhandle.handle = GhcbHyperPageHandle(ghcb_h, hyperpage_h);
            mhandle.guest_channel = guest_channel;
            assert(gvca.wf());
            // TODO: information flow declassification.
            assume(report@.is_constant_to(RICHOS_VMPL as nat));
            let mut gvca_report = gvca.to();
            mhandle.gvca_page = Some(gvca_report.set(report).to());
        } else {
            new_strlit("Please register gvca page!!\n").leak_debug();
        }
        mhandle
    }

    fn handle_lock_kernel(self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret: Self)
        requires
            old(cs).inv_stage_verismo(),
            self.wf(),
            self.gvca_page.is_Some(),
        ensures
            ret.wf(),
            ret.gvca_page.is_Some(),
            cs.inv_stage_verismo(),
            cs.only_lock_reg_coremode_updated(
                *old(cs),
                set![],
                set![spec_OSMEM_lockid(), spec_PT_lockid()],
            ),
    {
        new_strlit("handle_lock_kernel\n").leak_debug();
        let mut mhandle = self;
        assert(spec_size::<LockKernReq>() == PAGE_SIZE) by {
            assert(spec_size::<LockKernEntry>() == 16);
            assert(spec_size::<LockKernEntry>() * (PAGE_SIZE / 16) == PAGE_SIZE);
        }
        new_strlit("gvca_page\n").leak_debug();
        let req: VBox<LockKernReq> = mhandle.gvca_page.unwrap().to();
        let (gvca_ptr, Tracked(mut perm)) = req.into_raw();
        new_strlit("priv_req\n").leak_debug();
        let priv_req = gvca_ptr.take(Tracked(&mut perm));
        new_strlit("create_lock_entries\n").leak_debug();
        let mut entries = create_lock_entries(&priv_req);
        new_strlit("Lock Kernel enter\n").leak_debug();
        let tracked osmem_lock = cs.lockperms.tracked_remove(spec_OSMEM_lockid());
        let (osmem_ptr, Tracked(mut osmem_perm), Tracked(mut osmem_lock)) = OSMEM().acquire(
            Tracked(osmem_lock),
            Tracked(&cs.snpcore.coreid),
        );
        let mut osmem = osmem_ptr.take(Tracked(&mut osmem_perm));
        lock_kernel(&mut osmem, &entries, Tracked(cs));
        osmem_ptr.put(Tracked(&mut osmem_perm), osmem);
        OSMEM().release(Tracked(&mut osmem_lock), Tracked(osmem_perm), Tracked(&cs.snpcore.coreid));
        proof {
            cs.lockperms.tracked_insert(spec_OSMEM_lockid(), osmem_lock);
        }
        gvca_ptr.put(Tracked(&mut perm), priv_req);
        mhandle.gvca_page = Some(
            VBox::from_raw(gvca_ptr.to_usize(), Tracked(perm.tracked_into_raw().tracked_into())),
        );
        new_strlit("Lock Kernel end\n").leak_debug();
        mhandle
    }

    fn handle_vmpl_request(
        self,
        req: &GvcaHeader,
        vmpl: u8,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: Result<Self, u8>)
        requires
            old(cs).inv_stage_verismo(),
            1 <= vmpl < 4,
            self.wf(),
        ensures
            ret.is_Ok() ==> ret.get_Ok_0().wf(),
            cs.inv_stage_verismo(),
    //cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_OSMEM_lockid(), spec_PT_lockid()]),

    {
        let op = VmplReqOp::from_u64(req.op as u64);
        match op {
            Option::Some(op) => match op {
                VmplReqOp::WakupAp if req.has_gvca() => {
                    let vmsa_gpn = req.gpn();
                    let cpu = req.cpu();
                    let ret = self.handle_wakeup_ap(vmsa_gpn, cpu as usize, vmpl, Tracked(cs));
                    match ret {
                        Ok(_) => { Ok(self) },
                        Err(e) => { Err(e) },
                    }
                },
                VmplReqOp::SetPrivate if req.has_gvca() => {
                    self.handle_mk_private_shared(vmpl, req.gpn(), req.npages(), true, Tracked(cs))
                },
                VmplReqOp::SetShared if req.has_gvca() => {
                    self.handle_mk_private_shared(vmpl, req.gpn(), req.npages(), false, Tracked(cs))
                },
                VmplReqOp::Register if req.has_gvca() => {
                    new_strlit("Register").leak_debug();
                    let mut h = self;
                    if h.handle_register(req.gpn(), Tracked(cs)) {
                        Ok(h)
                    } else {
                        Err(SM_TERM_RICHOS_ERR(4) as u8)
                    }
                },
                VmplReqOp::Attest if req.has_gvca() && self.gvca_page.is_some() => {
                    new_strlit("Attest").leak_debug();
                    Ok(self.handle_attest(Tracked(cs)))
                },
                VmplReqOp::ExtendPcr if req.has_gvca() => {
                    new_strlit("ExtendPcr").leak_debug();
                    Ok(self.handle_extend_pcr(Tracked(cs)))
                },
                VmplReqOp::LockKernExe if req.has_gvca() && self.gvca_page.is_some() => {
                    new_strlit("Lock Kernel").leak_debug();
                    Ok(self.handle_lock_kernel(Tracked(cs)))
                },
                VmplReqOp::Secret if req.has_gvca() => {
                    new_strlit("Secret").leak_debug();
                    Ok(self)
                    //handle_secret_gen(&gvca, state)

                },
                VmplReqOp::Encrypt if req.has_gvca() => {
                    new_strlit("Encrypt").leak_debug();
                    Ok(self)
                    //handle_encrypt(&gvca, true, state)

                },
                VmplReqOp::Decrypt if req.has_gvca() => {
                    new_strlit("Decrypt").leak_debug();
                    Ok(self)
                    //handle_encrypt(&gvca, false, state)

                },
                _ => { Ok(self) },
            },
            Option::None => { Err(SM_TERM_UNSUPPORTED as u8) },
        }
    }

    pub fn handle_richos(self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret: Self)
        requires
            self.wf(),
            old(cs).inv_stage_verismo(),
        ensures
            cs.inv_stage_verismo(),
            //cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_OSMEM_lockid()]),
            ret.wf(),
    {
        let mut mhandle = self;
        let (vmsa_ptr, Tracked(vmsa_perm)) = mhandle.vmsa.into_raw();
        let (mut exit_code, Tracked(vmsa_perm)) = vmsa_ptr.guest_error_code().copy_with::<VmsaPage>(
            Tracked(vmsa_perm),
        );
        mhandle.vmsa = VBox::from_raw(vmsa_ptr.to_usize(), Tracked(vmsa_perm));
        proof {
            assert(exit_code.wf_value());
        }
        exit_code.declassify();
        match SVMExitCode::from_u64(exit_code.into()) {
            Option::Some(code) => match code {
                SVMExitCode::VmgExit => {
                    mhandle = mhandle.parse_and_handle_vmpl_req(Tracked(cs));
                },
                _ => {
                    vc_terminate(SM_TERM_UNSUPPORTED, Tracked(&mut cs.snpcore));
                },
            },
            Option::None => {
                vc_terminate(SM_TERM_UNSUPPORTED, Tracked(&mut cs.snpcore));
            },
        }
        // Clear error code

        mhandle.vmsa.set_guest_error_code(u64_s::new(0));
        mhandle
    }
}

} // verus!
verus! {

pub fn start_richos(
    mparam: &MonitorParams,
    secret: &SnpSecretsPageLayout,
    cpuid: VBox<SnpCpuidTable>,
    hvparam: VBox<OnePage>,
    guest_channel: SnpGuestChannel,
    handle: GhcbHyperPageHandle,
    Tracked(nextvmpl_id): Tracked<CoreIdPerm>,
    Tracked(richos_perm): Tracked<SnpPointsToRaw>,
    Tracked(acpi_perms): Tracked<Map<int, SnpPointsToRaw>>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        old(cs).inv_stage_verismo(),
        mparam.mp_wf(),
        secret.wf_mastersecret(),
        cpuid.is_default_page(),
        hvparam.is_default_page(),
        hvparam@.is_constant(),
        cpuid@.is_constant(),
        handle.wf(),
        guest_channel.wf(),
        nextvmpl_id@.vmpl == RICHOS_VMPL,
        spec_is_default_pages_const_to_vmpl(
            acpi_perms,
            (mparam.acpi@.val as int).to_page(),
            (mparam.acpi_size@.val as int).to_page() as nat,
            RICHOS_VMPL as nat,
        ),
        richos_perm@.wf_const_default(
            (mparam.richos_start.vspec_cast_to(), mparam.richos_size.vspec_cast_to()),
        ),
    ensures
        false,
{
    proof {
        reveal_strlit("Start richos ");
    }
    (new_strlit("Start richos "), mparam.richos_size).leak_debug();
    if mparam.richos_size.reveal_value() == 0 {
        vc_terminate(SM_TERM_RICHOS_ERR(1), Tracked(&mut cs.snpcore));
    }
    let ghost cs1 = *cs;
    let tracked osmem_lock = cs.lockperms.tracked_remove(spec_OSMEM_lockid());
    let (osmem_ptr, Tracked(mut osmem_perm), Tracked(mut osmem_lock)) = OSMEM().acquire(
        Tracked(osmem_lock),
        Tracked(&cs.snpcore.coreid),
    );
    let mut osmem = osmem_ptr.take(Tracked(&mut osmem_perm));
    let ghost cs2 = *cs;
    // Assign ACPI table to VMPL2
    let acpi_start_page = mparam.acpi.reveal_value().to_page() as usize;
    let acpi_pages = mparam.acpi_size.reveal_value().to_page() as usize;
    osmem_add(
        &mut osmem,
        acpi_start_page,
        acpi_pages,
        OSMemPerm::empty().set_write(1).set_read(1).set_kern_exe(1),
        false,
        Tracked(acpi_perms),
        Tracked(&mut cs.snpcore),
    );
    let ghost cs3 = *cs;
    proof {
        assert(cs.inv_stage_common());
        assert(RICHOS_VMPL == 1);
    }
    new_strlit("GhcbHyperPageHandle\n").leak_debug();
    let GhcbHyperPageHandle(ghcb_h, hyperpage_h) = handle;
    new_strlit("richos_early_ghcb\n").leak_debug();
    let (richos_early_ghcb, ghcb_h) = VBox::<OnePage>::new_shared_page(
        0x2000000,
        ghcb_h,
        Tracked(cs),
    );
    let handle = GhcbHyperPageHandle(ghcb_h, hyperpage_h);
    let ghost cs4 = *cs;
    let (vmsa, cpuid) = load_bzimage_to_vmsa(
        &mut osmem,
        mparam,
        RICHOS_VMPL,
        secret,
        cpuid,
        hvparam,
        richos_early_ghcb,
        Tracked(richos_perm),
        Tracked(cs),
    );
    let ghost cs5 = *cs;
    proof {
        cs2.lemma_update_prop(
            cs3,
            cs4,
            set![],
            set![],
            set![],
            set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
        );
        cs2.lemma_update_prop(
            cs4,
            cs5,
            set![],
            set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
            set![],
            set![spec_ALLOCATOR_lockid()],
        );
    }
    osmem_ptr.put(Tracked(&mut osmem_perm), osmem);
    OSMEM().release(Tracked(&mut osmem_lock), Tracked(osmem_perm), Tracked(&cs.snpcore.coreid));
    proof {
        cs.lockperms.tracked_insert(spec_OSMEM_lockid(), osmem_lock);
        assert(cs.only_lock_reg_coremode_updated(
            cs1,
            set![],
            set![spec_OSMEM_lockid(), spec_PT_lockid(), spec_ALLOCATOR_lockid()],
        ));
    }
    let ghost cs6 = *cs;
    // enable VTL protection
    //
    let handle = handle.config_partition(Tracked(cs));
    run_richos(handle, guest_channel, vmsa, secret, Tracked(nextvmpl_id), Tracked(cs));
}

} // verus!

================
File: ./source/verismo/src/security/mod.rs
================

use alloc::vec::Vec;

use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::arch::entities::VMPL;
use crate::debug::VPrintAtLevel;
use crate::global::*;
use crate::pgtable_e::va_to_pa;
use crate::ptr::*;
use crate::registers::*;
use crate::snp::cpu::VmsaPage;
use crate::snp::ghcb::*;
use crate::snp::SnpCoreSharedMem;
use crate::trusted_hacl::*;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::vbox::*;
use crate::*;

mod mem;
mod monitor;
pub mod pcr;
pub mod secret;
pub use mem::*;
pub use monitor::*;
pub use secret::*;

verus! {

pub const USER_DATA_LEN: usize = 64;

pub const MAX_AUTHTAG_LEN: usize = 32;

pub const SNP_AEAD_AES_256_GCM: u8 = 1;

pub const MSG_HDR_VER: u8 = 1;

pub const SNP_GUEST_MSG_TYPE_REQ: u8 = 5;

pub const SNP_GUEST_MSG_TYPE_RESP: u8 = 6;

pub const SNP_ERR_REQ_RESP_MSG: u8 = 0xff;

pub const VERISMO_VMPCK_ID: u8 = 0;

} // verus!
verus! {

pub open spec fn is_richos_vmsa_box(vmsa: VBox<VmsaPage>) -> bool {
    &&& vmsa.snp().is_vmpl0_private()
    &&& vmsa.is_page()
    &&& vmsa@.vmpl.spec_eq(RICHOS_VMPL)
}

pub open spec fn richos_vmsa_invfn() -> spec_fn(Vec<Option<VBox<VmsaPage>>>) -> bool {
    |vec: Vec<Option<VBox<VmsaPage>>>|
        forall|i|
            0 <= i < vec@.len() ==> (vec[i].is_Some() ==> is_richos_vmsa_box(
                #[trigger] vec[i].get_Some_0(),
            ))
}

} // verus!
verismo_simple! {
    #[repr(C, align(1))]
    #[derive(Copy, VClone)]
    pub struct SecretsOSArea {
        msg_seqno_0: u32,
        msg_seqno_1: u32,
        msg_seqno_2: u32,
        msg_seqno_3: u32,
        ap_jump_table_pa: u64,
        reserved_1: Array<u8, 40>,
        guest_usage: Array<u8, 32>,
    }
}

verus! {

pub const MAX_SNP_MSG_SZ: u16 = 4000;

pub const MAX_SNP_MSG_SZ_USIZE: usize = MAX_SNP_MSG_SZ as usize;

} // verus!
verismo_simple! {
    type SnpGuestMsgPayload = Array<u8, MAX_SNP_MSG_SZ_USIZE>;
    #[repr(C, align(1))]
    #[derive(SpecGetter, SpecSetter)]
    pub struct SnpGuestMsg {
        #[def_offset]
        pub snphdr: SnpGuestMsgHdr,
        #[def_offset]
        pub payload: Array<u8, MAX_SNP_MSG_SZ_USIZE>,
    }
}

verismo_simple! {
    #[repr(C, align(1))]
    pub struct SnpReportReq {
        /* user data that should be included in the report */
        pub user_data: Array<u8_s, 64>,

        /* The vmpl level to be included in the report */
        pub vmpl: u32_t,

        /* Must be zero filled */
        pub reserved: Array<u8_t, 28>,
    }
}

verismo_simple! {
    type SnpReportResp = OnePage;

    pub struct SnpGuestChannel {
        pub req: VBox<SnpGuestMsg>,
        pub resp: VBox<SnpGuestMsg>,
    }

    impl SnpGuestChannel {
        pub open spec fn only_val_updated(&self, prev: Self) -> bool {
            &&& self.req.only_val_updated(prev.req)
            &&& self.resp.only_val_updated(prev.resp)
        }
    }
}

verismo_simple! {
    pub struct TrackedSecretsOSAreaPerms {
        pub perms: Tracked<SnpPointsTo<SecretsOSArea>>
    }
}

verus! {

proof fn proof_msg_hdr_size()
    ensures
        spec_size::<SnpGuestMsgHdr>() == AsNat!(0x60),
{
}

impl SnpGuestChannel {
    pub open spec fn wf(&self) -> bool {
        &&& self.req.is_shared_page()
        &&& self.resp.is_shared_page()
    }
}

} // verus!

================
File: ./source/verismo/src/security/pcr.rs
================

use alloc::vec::Vec;

use super::secret::cal2_sha512;
use super::*;

pub type U256 = Array<u8_s, 32>;

verismo_simple! {
pub struct SHA256WithChain {
    pub val: [u8; 32],
    pub chain: Tracked<Seq<Seq<u8>>>,
}
}

verismo_simple! {
#[derive(SpecGetter, SpecSetter)]
pub struct ExtendPCRReq {
    #[def_offset]
    pub val: Array<u8, 64>,
    pub reserved: [u8; {0x1000 - 64}],
}
}

verus! {

pub open spec fn pcr_invfn() -> spec_fn(Vec<SHA512Type>) -> bool {
    |vec: Vec<SHA512Type>| vec.len() >= 1 && forall|i| 0 < i < vec.len() ==> vec[i].wf()
}

} // verus!
verus! {

//#[verifier(external_body)]
pub fn extend_pcr(
    index: usize,
    data: &Array<u8_s, USER_DATA_LEN>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        data.wf(),
        old(cs).inv_stage_pcr(),
    ensures
        cs.inv_stage_pcr(),
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_PCR_lockid()]),
{
    (new_strlit("extend_pcr"), index).leak_debug();
    let tracked pcr_lock = cs.lockperms.tracked_remove(spec_PCR_lockid());
    let (pcr_ptr, Tracked(mut pcr_perm), Tracked(mut pcr_lock)) = PCR().acquire(
        Tracked(pcr_lock),
        Tracked(&cs.snpcore.coreid),
    );
    assert(pcr_invfn()(pcr_perm@.get_value()));
    let mut pcr = pcr_ptr.take(Tracked(&mut pcr_perm));
    proof {
        assert(pcr@.wf());
        assert(pcr_invfn()(pcr));
    }
    if pcr.len() < index {
        new_strlit("pcr.len() < index").leak_debug();
    } else if pcr.len() == index {
        pcr.push(cal_sha512(data));
    } else {
        let pcr_data = pcr[index].clone();
        pcr.set(index, cal2_sha512(&pcr_data, data));
    }
    assert forall|i| 0 < i < pcr.len() implies pcr[i].wf() by {}
    assert(pcr_invfn()(pcr));
    pcr_ptr.put(Tracked(&mut pcr_perm), pcr);
    PCR().release(Tracked(&mut pcr_lock), Tracked(pcr_perm), Tracked(&cs.snpcore.coreid));
    proof {
        cs.lockperms.tracked_insert(spec_PCR_lockid(), pcr_lock);
    }
}

#[inline]
pub fn attest_pcr(
    guest_channel: SnpGuestChannel,
    ghcb: GhcbHandle,
    secret: &SnpSecretsPageLayout,
    index: usize,
    report: VBox<OnePage>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: (VBox<OnePage>, SnpGuestChannel, GhcbHandle))
    requires
        guest_channel.wf(),
        ghcb.ghcb_wf(),
        secret.wf_mastersecret(),
        report.wf(),
        report.snp() === SwSnpMemAttr::spec_default(),
        old(cs).inv_stage_pcr(),
    ensures
        cs.inv_stage_pcr(),
        cs.only_lock_reg_coremode_updated(
            *old(cs),
            set![],
            set![spec_PCR_lockid(), spec_ALLOCATOR_lockid()],
        ),
        ret.0.wf(),
        ret.0.only_val_updated(report),
        ret.1.wf(),
        ret.1.only_val_updated(guest_channel),
        ret.2.ghcb_wf(),
        ret.2.only_val_updated(ghcb),
{
    let ghost cs1 = *cs;
    let tracked pcr_lock = cs.lockperms.tracked_remove(spec_PCR_lockid());
    let (pcr_ptr, Tracked(mut pcr_perm), Tracked(mut pcr_lock)) = PCR().acquire(
        Tracked(pcr_lock),
        Tracked(&cs.snpcore.coreid),
    );
    let pcr = pcr_ptr.borrow(Tracked(&pcr_perm));
    assert(pcr_invfn()(pcr_perm@.get_value()));
    if index >= pcr.len() {
        (new_strlit("PCR index not found:"), index).leak_debug();
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(&mut cs.snpcore));
    }
    let pcr_data = pcr[index].clone();
    PCR().release(Tracked(&mut pcr_lock), Tracked(pcr_perm), Tracked(&cs.snpcore.coreid));
    proof {
        cs.lockperms.tracked_insert(spec_PCR_lockid(), pcr_lock);
    }
    let ghost cs2 = *cs;
    let ret = guest_channel.attest(ghcb, secret, pcr_data, report, Tracked(cs));
    proof {
        cs1.lemma_update_prop(
            cs2,
            *cs,
            set![],
            set![spec_PCR_lockid()],
            set![],
            set![spec_ALLOCATOR_lockid()],
        );
        assert(set![spec_PCR_lockid()].union(set![spec_ALLOCATOR_lockid()])
            =~~= set![spec_PCR_lockid(), spec_ALLOCATOR_lockid()]);
    }
    ret
}

} // verus!

================
File: ./source/verismo/src/security/secret.rs
================

use super::*;
use crate::mshyper::GhcbHyperPageHandle;
use crate::trusted_hacl::SHA512Type;

verismo_simple! {
#[repr(C, align(1))]
#[derive(SpecGetter, SpecSetter, Copy, VClone)]
pub struct SnpSecretsPageLayout {
    version: u32,
    imien: u32,
    fms: u32,
    reserved_2: u32,
    gosvw: Array<u8, 16>,
    vmpck0: AESKey256,
    vmpck1: AESKey256,
    vmpck2: AESKey256,
    vmpck3: AESKey256,
    os_area: SecretsOSArea,
    reserved_3: Array<u8, 3840>,
}
}

verismo_simple! {
    #[repr(C, align(1))]
    #[derive(SpecGetter, SpecSetter, VClone, Copy)]
    pub struct SnpGuestMsgHdr {
        #[def_offset]
        authtag: AESAuthTag,
        msg_seqno: u64,
        reserved1: u64,
        #[def_offset]
        algo: u8,
        hdr_version: u8,
        hdr_sz: u16,
        msg_type: u8,
        msg_version: u8,
        msg_sz: u16,
        reserved2: u32,
        msg_vmpck: u8,
        reserved3: Array<u8, 35>,
    }
}

verus! {

impl SnpSecretsPageLayout {
    pub closed spec fn closed_wf_mastersecret(&self) -> bool {
        &&& self.vmpck0@.is_fullsecret()
        &&& self.vmpck1@.is_constant_to(1)
        &&& self.vmpck1@.is_fullsecret_to(2)
        &&& self.vmpck1@.is_fullsecret_to(3)
        &&& self.vmpck1@.is_fullsecret_to(4)
        &&& self.vmpck2@.is_constant_to(2)
        &&& self.vmpck2@.is_fullsecret_to(1)
        &&& self.vmpck2@.is_fullsecret_to(3)
        &&& self.vmpck2@.is_fullsecret_to(4)
        &&& self.vmpck3@.is_constant_to(3)
        &&& self.vmpck3@.is_fullsecret_to(1)
        &&& self.vmpck3@.is_fullsecret_to(2)
        &&& self.vmpck3@.is_fullsecret_to(4)
        &&& self.os_area.is_constant()
    }

    pub open spec fn wf_mastersecret(&self) -> bool {
        &&& self.wf()
        &&& self.closed_wf_mastersecret()
    }
}

} // verus!
verismo_simple! {
impl SnpGuestChannel {
    pub fn new(handle: GhcbHandle, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret: (SnpGuestChannel, GhcbHandle))
    requires
        handle.ghcb_wf(),
        old(cs).inv(),
    ensures
        cs.inv(),
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_ALLOCATOR_lockid(), spec_PT_lockid()]),
        ret.0.wf(),
        ret.1.ghcb_wf(),
    {
        let ghost oldcs = *cs;
        let (req, handle) = handle.new_shared_vpage(Tracked(cs));
        let ghost prevcs = *cs;
        let (resp, handle) = handle.new_shared_vpage(Tracked(cs));
        /*
        let (report_req, handle) = VBox::new_aligned(
            8, SnpReportReq{user_data: Array::new(0), vmpl: 0, reserved: Array::new(0)}
            Tracked(cs));*/
        proof{
            oldcs.lemma_update_prop(prevcs, *cs, set![], set![spec_ALLOCATOR_lockid(), spec_PT_lockid()], set![], set![spec_ALLOCATOR_lockid(), spec_PT_lockid()]);
        }
        (SnpGuestChannel {req, resp}, handle)
    }
}

}

verus! {

pub struct FillSecretForVMPL<'a> {
    pub master_secret: &'a SnpSecretsPageLayout,
    pub vmpl: u8,
}

pub struct FillSecretForVMPLOut;

impl<'a, 'b> MutFnTrait<'a, FillSecretForVMPL<'b>, FillSecretForVMPLOut> for SnpSecretsPageLayout {
    open spec fn spec_update_requires(&self, params: FillSecretForVMPL<'b>) -> bool {
        let FillSecretForVMPL { master_secret, vmpl } = params;
        &&& master_secret.wf_mastersecret()
        &&& self.is_constant()
        &&& 0 < vmpl < 4
    }

    open spec fn spec_update(
        &self,
        prev: &Self,
        params: FillSecretForVMPL<'b>,
        ret: FillSecretForVMPLOut,
    ) -> bool {
        let FillSecretForVMPL { master_secret, vmpl } = params;
        &&& if vmpl == 1 {
            self.spec_vmpck1() === master_secret.spec_vmpck1()
        } else if vmpl == 2 {
            self.spec_vmpck2() === master_secret.spec_vmpck2()
        } else if vmpl == 3 {
            self.spec_vmpck3() === master_secret.spec_vmpck3()
        } else {
            true
        }
        &&& self.is_constant_to(vmpl as nat)
    }

    fn box_update(&'a mut self, params: FillSecretForVMPL<'b>) -> (ret: FillSecretForVMPLOut) {
        let FillSecretForVMPL { master_secret, vmpl } = params;
        assert(self.is_constant());
        assert(self.is_constant_to(vmpl as nat));  // proof required
        if vmpl == 1 {
            self.vmpck1 = master_secret.vmpck1.clone();
        } else if vmpl == 2 {
            self.vmpck2 = master_secret.vmpck2.clone();
        } else if vmpl == 3 {
            self.vmpck3 = master_secret.vmpck3.clone();
        }
        FillSecretForVMPLOut
    }
}

} // verus!
verus! {

proof fn proof_msg_hdr_size()
    ensures
        spec_size::<SnpGuestMsgHdr>() == 0x60,
{
}

} // verus!
verismo_simple! {
pub fn enc_dec(
    enc: bool,
    key: &AESKey256,
    iv: &AESInv,
    ad: &AESAddInfo,
    tag: &AESAuthTag,
    len: u32,
    plain_addr: usize_t,
    cipher_addr: usize_t,
    Tracked(plain_perm): Tracked<SnpPointsToRaw>,
    Tracked(cipher_perm): Tracked<SnpPointsToRaw>,
) -> (perms: (Tracked<SnpPointsToRaw>, Tracked<SnpPointsToRaw>))
requires
    plain_perm@.wf_default((plain_addr as int, len as nat)),
    cipher_perm@.wf_default((cipher_addr as int, len as nat)),
    key@.is_fullsecret(),
ensures
    enc ==> perms.0@ === plain_perm,
    enc ==> perms.1@@.wf_const_default((cipher_addr as int, len as nat)),
    enc ==> perms.1@@.only_val_updated(cipher_perm@),
    !enc ==> perms.1@ === cipher_perm,
    !enc ==> perms.0@@.wf_default((plain_addr as int, len as nat)),
    !enc ==> perms.0@@.only_val_updated(plain_perm@),
{
    let tracked mut cipher_perm = cipher_perm;
    let tracked mut plain_perm = plain_perm;
    if enc {
        raw_encrypt(key, iv, ad, tag, len, plain_addr, cipher_addr,
            Tracked(&plain_perm), Tracked(&mut cipher_perm));
        assert(cipher_perm@.bytes().is_constant());
        assert(cipher_perm@.wf_const_default((cipher_addr as int, len as nat)));
    } else {
        raw_decrypt(key, iv, ad, tag, len, plain_addr, cipher_addr,
            Tracked(&mut plain_perm), Tracked(&cipher_perm));
        assert(plain_perm@.bytes().wf());
        assert(plain_perm@.wf_default((plain_addr as int, len as nat)));
    }
    (Tracked(plain_perm), Tracked(cipher_perm))
}
}

verismo_simple! {
pub fn enc_dec_payload(
    enc: bool,
    key: &AESKey256,
    msg: VBox<SnpGuestMsg>,
    msg_no: u64_s,
    plain_addr: usize_t,
    Tracked(plain_perm): Tracked<SnpPointsToRaw>,
) -> (ret: (VBox<SnpGuestMsg>, Tracked<SnpPointsToRaw>))
requires
    msg_no.wf(),
    msg.wf(),
    msg@.is_constant_to(RICHOS_VMPL as nat),
    msg@.snphdr.is_constant(),
    msg.snp().is_default(),
    key@.is_fullsecret(),
    msg@.snphdr.spec_msg_sz() <= MAX_SNP_MSG_SZ,
    plain_perm@.wf_default((plain_addr as int, msg@.snphdr.spec_msg_sz() as nat)),
ensures
    ret.0.wf(),
    ret.0.only_val_updated(msg),
    !enc ==> ret.0@ === msg@,
    ret.1@@.wf_default((plain_addr as int, msg@.snphdr.spec_msg_sz() as nat)),
    !enc ==> ret.1@@.only_val_updated(plain_perm@),
    enc ==> ret.1@ === plain_perm,
    ret.0@.snphdr.is_constant(),
    enc ==> ret.0@.is_constant_to(RICHOS_VMPL as nat),
{
    let mut iv: AESInv = Array::new(0u8_s);
    iv.set(0usize_t, (msg_no as u8));
    iv.set(1usize_t, ((msg_no >> 8) as u8));
    iv.set(2usize_t, ((msg_no >> 16) as u8));
    iv.set(3usize_t, ((msg_no >> 24) as u8));
    let mut msg_len = msg.borrow().snphdr.msg_sz as u32;

    proof{
        proof_into_is_constant::<_, SecSeqByte>(msg@);
    }
    let (msg_ptr, Tracked(mut msg_perm)) = msg.into_raw();

    let tracked mut msg_perm = msg_perm.tracked_into_raw();
    let ghost old_msg_perm = msg_perm;
    let tracked (mut msg_hdr_perm, mut payload_perm) = msg_perm.tracked_split(spec_size::<SnpGuestMsgHdr>());
    let tracked (mut left, mut ad_perm_right) = msg_hdr_perm.tracked_split(SnpGuestMsgHdr::spec_algo_offset());
    let tracked (ad_perm, right) = ad_perm_right.tracked_split(spec_size::<AESAddInfo>());

    let tracked (tag_perm, mid) = left.tracked_split(spec_size::<AESAuthTag>());

    let tracked (cipher_perm, payload_right) = payload_perm.tracked_split(msg_len as nat);
    proof{
        assert(old_msg_perm@.bytes() =~~= tag_perm@.bytes() + mid@.bytes() + ad_perm@.bytes() + right@.bytes() + cipher_perm@.bytes() + payload_right@.bytes());
        assert(old_msg_perm@.bytes().is_constant_to(RICHOS_VMPL as nat));
    }
    let tag: VBox<AESAuthTag> = VBox::from_raw(
        msg_ptr.snphdr().authtag().to_usize(), Tracked(tag_perm.tracked_into()));
    let ad: VBox<AESAddInfo> = VBox::from_raw(
        msg_ptr.snphdr().algo().to_usize(), Tracked(ad_perm.tracked_into()));
    let cipher_addr: usize_t = msg_ptr.payload().to_usize();

    let (Tracked(plain_perm), Tracked(cipher_perm)) = enc_dec(
        enc, key, &iv, ad.borrow(), tag.borrow(), msg_len,
        plain_addr, cipher_addr,
        Tracked(plain_perm),
        Tracked(cipher_perm),
    );
    proof{cipher_perm@.bytes().is_constant_to(RICHOS_VMPL as nat);}
    let (_, Tracked(tag_perm)) = tag.into_raw();
    let (_, Tracked(ad_perm)) = ad.into_raw();
    proof{
        let tracked tag_perm = tag_perm.tracked_into_raw();
        let tracked ad_perm = ad_perm.tracked_into_raw();
        msg_perm =  tag_perm.tracked_join(mid)
                            .tracked_join(ad_perm)
                            .tracked_join(right)
                            .tracked_join(cipher_perm)
                            .tracked_join(payload_right);
        assert(msg_perm@.bytes() =~~= tag_perm@.bytes() + mid@.bytes() + ad_perm@.bytes() + right@.bytes() + cipher_perm@.bytes() + payload_right@.bytes());
        assert(msg_perm@.bytes().is_constant());
        proof_into_is_constant::<_, SnpGuestMsg>(msg_perm@.bytes());
    }
    (VBox::from_raw(msg_ptr.to_usize(), Tracked(msg_perm.tracked_into())), Tracked(plain_perm))
}
}

verus! {

pub fn enc_payload(
    secret: &SnpSecretsPageLayout,
    version: u8,
    msg_no: u64,
    msg_type: u8,
    payload_addr: usize,
    len: u16,
    Tracked(payload_perm): Tracked<SnpPointsToRaw>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: (Result<VBox<SnpGuestMsg>, u8>, Tracked<SnpPointsToRaw>))
    requires
        old(cs).inv(),
        len <= MAX_SNP_MSG_SZ,
        secret.wf_mastersecret(),
        payload_perm@.wf_default((payload_addr as int, len as nat)),
    ensures
        cs.inv(),
        ret.0.is_Ok() ==> ret.0.get_Ok_0().wf(),
        ret.0.is_Ok() ==> ret.0.get_Ok_0()@.is_constant(),
        ret.0.is_Ok() ==> ret.0.get_Ok_0().snp() === SwSnpMemAttr::spec_default(),
        payload_perm == ret.1@,
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_ALLOCATOR_lockid()]),
{
    let mut msg: VBox<SnpGuestMsg> = VBox::new_uninit(Tracked(cs));
    let mut snphdr: SnpGuestMsgHdr = msg.copy_snphdr();
    snphdr.algo = SNP_AEAD_AES_256_GCM.into();
    snphdr.hdr_version = MSG_HDR_VER.into();
    snphdr.hdr_sz = sizeof::<SnpGuestMsgHdr>().into();
    snphdr.msg_type = msg_type.into();
    snphdr.msg_version = version.into();
    snphdr.msg_seqno = msg_no.into();
    snphdr.msg_vmpck = VERISMO_VMPCK_ID.into();
    snphdr.msg_sz = len.into();
    assert(snphdr.is_constant());
    msg.set_snphdr(snphdr);
    let (msg, Tracked(payload_perm)) = enc_dec_payload(
        true,
        &secret.vmpck0,
        msg,
        msg_no.into(),
        payload_addr,
        Tracked(payload_perm),
    );
    (Ok(msg), Tracked(payload_perm))
}

} // verus!
verus! {

pub const SHA512_LEN2: usize = SHA512_LEN * 2;

pub fn cal2_sha512(input1: &SHA512Type, input2: &SHA512Type) -> (ret: SHA512Type)
    requires
        input1.wf(),
        input2.wf(),
    ensures
        ret.wf(),
        ret === spec_cal_sha512((input1@ + input2@).vspec_cast_to()),
{
    let mut tmp_buf: Array<u8_s, SHA512_LEN2> = Default::default();
    let mut i = 0;
    while i < SHA512_LEN
        invariant
            0 <= i <= SHA512_LEN,
            forall|k: int| 0 <= k < i ==> tmp_buf[k] == input1[k],
            forall|k: int| 0 <= k < i ==> tmp_buf[k + SHA512_LEN] == input2[k],
    {
        tmp_buf.set2(i, *(input1.index2(i)));
        tmp_buf.set2(i + SHA512_LEN, *(input2.index2(i)));
        i = i + 1;
    }
    assert(tmp_buf@ =~~= input1@ + input2@);
    cal_sha512(&tmp_buf)
}

pub fn cal_sha512<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(buf: &T) -> (ret:
    SHA512Type)
    requires
        buf.wf(),
    ensures
        ret.wf(),
        ret === spec_cal_sha512(buf.vspec_cast_to()),
{
    let mut ret: SHA512Type = Default::default();
    trusted_cal_sha512(buf, &mut ret);
    ret
}

} // verus!
verus! {

impl SnpGuestChannel {
    pub fn attest(
        self,
        ghcb: GhcbHandle,
        secret: &SnpSecretsPageLayout,
        user_data: Array<u8_s, USER_DATA_LEN>,
        result: VBox<OnePage>,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: (VBox<OnePage>, Self, GhcbHandle))
        requires
            secret.wf_mastersecret(),
            result.wf(),
            result.snp() === SwSnpMemAttr::spec_default(),
            user_data.wf(),
            old(cs).inv(),
            ghcb.ghcb_wf(),
            self.wf(),
        ensures
            cs.inv(),
            cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_ALLOCATOR_lockid()]),
            ret.0.only_val_updated(result),
            ret.0.wf(),
            ret.1.wf(),
            ret.1.only_val_updated(self),
            ret.2.ghcb_wf(),
            ret.2.only_val_updated(ghcb),
    {
        proof {
            assert(set![spec_ALLOCATOR_lockid()].union(set![spec_ALLOCATOR_lockid()])
                =~~= set![spec_ALLOCATOR_lockid()]);
        }
        let mut req = self.req;
        let prev_msg_no: u64 = secret.os_area.msg_seqno_0.into();
        if prev_msg_no == MAXU64 {
            new_strlit(
                "msg_no is too large for vmpl communication. Please reset secret\n.",
            ).leak_debug();
            vc_terminate(SM_TERM_UNSUPPORTED, Tracked(&mut cs.snpcore));
        }
        let msg_no = (prev_msg_no + 1) as u64;
        let version = 1;
        let report_req = SnpReportReq { user_data, vmpl: 0, reserved: Array::new(0) };
        let ghost cs1 = *cs;
        let payload = VBox::<SnpReportReq>::new(report_req, Tracked(cs));
        let (payload_ptr, Tracked(payload_perm)) = payload.into_raw();
        let payload_addr = payload_ptr.to_usize();
        let ghost cs2 = *cs;
        let (encrypted, Tracked(payload_perm)) = enc_payload(
            secret,
            version,
            msg_no,
            SNP_GUEST_MSG_TYPE_REQ,
            payload_addr,
            size_of::<SnpReportReq>() as u16,
            Tracked(payload_perm.tracked_into_raw()),
            Tracked(cs),
        );
        let payload = VBox::<SnpReportReq>::from_raw(
            payload_addr,
            Tracked(payload_perm.tracked_into()),
        );
        if let Ok(encrypted) = encrypted {
            req = req.set(encrypted);
        } else {
            vc_terminate(SM_EVERCRYPT_EXIT, Tracked(&mut cs.snpcore));
        }
        let req_gpa = req.get_const_addr() as u64;
        let resp_gpa = self.resp.get_const_addr() as u64;
        let ghost cs3 = *cs;
        let ghcb = ghcb.ghcb_guest_request(req_gpa, resp_gpa, Tracked(cs));
        // Create a copy of private reponse message for verification.
        // This is necessary to avoid attacker skip the checking.
        let ghost cs4 = *cs;
        let mut private_resp = VBox::<SnpGuestMsg>::new_uninit(Tracked(cs));
        let (presp_ptr, Tracked(mut presp_perm)) = private_resp.into_raw();
        let (resp_ptr, Tracked(resp_perm)) = self.resp.into_raw();
        let tracked resp_perm = resp_perm.tracked_into_raw();
        let tracked mut presp_perm = presp_perm.tracked_into_raw();
        mem_copy(
            resp_ptr.to_usize(),
            presp_ptr.to_usize(),
            size_of::<SnpGuestMsg>(),
            Tracked(&resp_perm),
            Tracked(&mut presp_perm),
        );
        private_resp = VBox::from_raw(presp_ptr.to_usize(), Tracked(presp_perm.tracked_into()));
        let resp = VBox::from_raw(resp_ptr.to_usize(), Tracked(resp_perm.tracked_into()));
        let (rc, result, _) = verify_and_dec_payload(
            &secret.vmpck0,
            private_resp,
            msg_no,
            version,
            SNP_GUEST_MSG_TYPE_RESP,
            result,
        );
        if rc > 0 {
            vc_terminate(SM_EVERCRYPT_EXIT, Tracked(&mut cs.snpcore));
        }
        proof {
            cs1.lemma_update_prop(
                cs2,
                cs3,
                set![],
                set![spec_ALLOCATOR_lockid()],
                set![],
                set![spec_ALLOCATOR_lockid()],
            );
            cs1.lemma_update_prop(
                cs3,
                cs4,
                set![],
                set![spec_ALLOCATOR_lockid()],
                set![],
                set![spec_ALLOCATOR_lockid()],
            );
            cs1.lemma_update_prop(
                cs4,
                *cs,
                set![],
                set![spec_ALLOCATOR_lockid()],
                set![],
                set![spec_ALLOCATOR_lockid()],
            );
        }
        (result, SnpGuestChannel { req, resp }, ghcb)
    }
}

} // verus!
verus! {

fn verify_and_dec_payload<T: IsConstant + SpecSize + WellFormed + VTypeCast<SecSeqByte>>(
    key: &AESKey256,
    msg: VBox<SnpGuestMsg>,
    old_msg_no: u64,
    version: u8,
    msg_type: u8,
    result: VBox<T>,
) -> (ret: (u8, VBox<T>, VBox<SnpGuestMsg>))
    requires
        old_msg_no < 0xffff_ffff_ffff_ffff,
        msg.snp() == SwSnpMemAttr::spec_default(),
        msg.wf(),
        msg@.is_constant(),
        key@.is_fullsecret(),
        result.wf(),
        result.snp() == SwSnpMemAttr::spec_default(),
    ensures
        ret.1.wf(),
        ret.2.wf(),
        ret.1.only_val_updated(result),
        ret.2.only_val_updated(msg),
{
    let expected_msg_no = old_msg_no + 1;
    let hdr = msg.borrow().snphdr;
    /* Verify that the sequence counter is incremented by 1 */
    let msg_no = hdr.msg_seqno.reveal_value();
    if msg_no != expected_msg_no {
        ((new_strlit("wrong seq number"), msg_no), expected_msg_no).leak_debug();
        return (SNP_ERR_REQ_RESP_MSG, result, msg);
    }  /* Verify response message type and version number. */

    if (msg_type != hdr.msg_type.reveal_value()) || (hdr.msg_version.reveal_value() != version) {
        new_strlit("wrong msg type").leak_debug();
        return (SNP_ERR_REQ_RESP_MSG, result, msg);
    }
    let msg_sz: usize = hdr.msg_sz.into();
    if msg_sz == 0 || msg_sz > (MAX_SNP_MSG_SZ as usize) || msg_sz >= size_of::<T>() {
        new_strlit("wrong msg size").leak_debug();
        return (SNP_ERR_REQ_RESP_MSG, result, msg);
    }
    let decrypted_addr = result.get_const_addr();
    let (decrypted_ptr, Tracked(decrypted_perm)) = result.into_raw();
    let tracked mut decrypted_perm = decrypted_perm.tracked_into_raw();
    let tracked (payload_perm, right_perm) = decrypted_perm.trusted_split(
        msg@.snphdr.spec_msg_sz().vspec_cast_to(),
    );
    /* Decrypt the payload */
    let (msg, Tracked(payload_perm)) = enc_dec_payload(
        false,
        key,
        msg,
        msg_no.into(),
        decrypted_addr,
        Tracked(payload_perm),
    );
    proof {
        decrypted_perm = payload_perm.trusted_join(right_perm);
    }
    (0, VBox::from_raw(decrypted_addr, Tracked(decrypted_perm.tracked_into())), msg)
}

} // verus!

================
File: ./source/verismo/src/security/mem.rs
================

use alloc::vec::Vec;

use super::*;
use crate::allocator::VeriSMoAllocator;
use crate::arch::rmp::*;
use crate::boot::params::*;
use crate::debug::{VPrint, VPrintAtLevel};
use crate::pgtable_e::check_is_encrypted;
use crate::snp::mem::{spec_is_shared_page_perms, spec_is_vmprivate_const_perms};

verus! {

pub open spec fn spec_is_default_pages_const_to_vmpl(
    page_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    npages: nat,
    vmpl: nat,
) -> bool {
    forall|i|
        start_page <= i < (start_page + npages) ==> (#[trigger] page_perms.contains_key(i)
            && page_perms[i]@.wf_default((i.to_addr(), PAGE_SIZE as nat))
            && page_perms[i]@.bytes().is_constant_to(vmpl))
}

pub const RICHOS_VMPL: u8 = 1;

pub const VERISMO_RSVD_MEM: usize = 0x100000;

} // verus!
#[vbit_struct(OSMemPerm, u8)]
pub struct OSMemPermSpec {
    #[vbits(0, 0)]
    pub read: u8,
    #[vbits(1, 1)]
    pub write: u8,
    #[vbits(2, 2)]
    pub user_exe: u8,
    #[vbits(3, 3)]
    pub kern_exe: u8,
    // #[vbits(4, 4)]
    // pub sss: u8,
}

verus! {

impl OSMemPermSpec {
    pub open spec fn is_super_of(&self, other: Self) -> bool {
        &&& self.spec_read() <= other.spec_read()
        &&& self.spec_write() <= other.spec_write()
        &&& self.spec_user_exe() <= other.spec_user_exe()
        &&& self.spec_kern_exe() <= other.spec_kern_exe()
    }
}

} // verus!
verus! {

impl OSMemPermSpec {
    pub closed spec fn ram() -> Self {
        OSMemPermSpec::empty().spec_set_user_exe(1).spec_set_read(1).spec_set_write(
            1,
        ).spec_set_kern_exe(1)
    }

    pub closed spec fn readwrite() -> Self {
        Self::empty().spec_set_read(1).spec_set_write(1)
    }

    pub closed spec fn readonly() -> Self {
        Self::empty().spec_set_read(1)
    }

    pub closed spec fn locked_kern() -> Self {
        Self::empty().spec_set_kern_exe(1).spec_set_read(1).spec_set_write(0).spec_set_user_exe(0)
    }

    pub closed spec fn user_exeonly() -> Self {
        Self::empty().spec_set_user_exe(1)
    }
}

} // verus!
verus! {

impl OSMemPerm {
    pub open spec fn to_page_perm(&self) -> PagePerm {
        PagePerm::from_int(self.value as int)
    }

    #[verifier(external_body)]
    pub fn is_super_of(&self, other: &Self) -> (ret: bool)
        ensures
            ret == self@.is_super_of(other@),
    {
        (self.value & other.value) == other.value
    }

    pub const fn ram() -> (ret: Self)
        ensures
            ret@ === OSMemPermSpec::ram(),
    {
        Self::empty().set_read(1).set_write(1).set_user_exe(1).set_kern_exe(1)
    }

    pub const fn readonly() -> (ret: Self)
        ensures
            ret@ === OSMemPermSpec::readonly(),
    {
        Self::empty().set_read(1)
    }

    pub const fn locked_kern() -> (ret: Self)
        ensures
            ret@ === OSMemPermSpec::locked_kern(),
    {
        Self::empty().set_kern_exe(1).set_read(1).set_write(0).set_user_exe(0)
    }

    pub const fn user_exeonly() -> (ret: Self)
        ensures
            ret@ === OSMemPermSpec::user_exeonly(),
    {
        Self::empty().set_user_exe(1)
    }
}

} // verus!
verismo_simple! {
#[derive(VPrint)]
pub struct OSMemEntry {
    pub start_page: usize,
    pub npages: usize,
    pub osperm: u8,
    pub page_perms: Tracked<Map<int, SnpPointsToRaw>>,
}
}

pub type OSMem = Vec<OSMemEntry>;

verus! {

pub open spec fn os_mem_valid_snp(osperm: OSMemPerm, snp: SwSnpMemAttr) -> bool {
    let pte = snp.pte();
    let rmp = snp.rmp@;
    let rmp_perm = rmp.perms[VMPL::from_int(RICHOS_VMPL as int)];
    if rmp.spec_validated() {
        &&& rmp_perm =~~= osperm.to_page_perm()
        &&& snp.wf()
        &&& pte.spec_encrypted()
    } else {
        &&& snp === SwSnpMemAttr::shared()
        &&& osperm@ === OSMemPermSpec::ram()
        &&& !pte.spec_encrypted()
    }
}

pub open spec fn spec_contains_page_perm(
    page_perms: Map<int, SnpPointsToRaw>,
    i: int,
    osperm: OSMemPerm,
) -> bool {
    let page_perm = page_perms[i]@;
    &&& page_perms.contains_key(i)
    &&& page_perm.wf_not_null((i.to_addr(), PAGE_SIZE as nat))
    &&& os_mem_valid_snp(osperm, page_perm.snp())
    &&& page_perm.bytes().is_constant_to(RICHOS_VMPL as nat)
}

pub open spec fn spec_contains_page_perms(
    page_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    npages: nat,
    osperm: OSMemPerm,
) -> bool {
    &&& forall|i| #[trigger]
        page_perms.contains_key(i) ==> spec_contains_page_perm(page_perms, i, osperm)
    &&& page_perms.dom() =~~= Set::new(|i: int| start_page <= i < start_page + npages)
}

} // verus!
verismo_simple! {
impl OSMemEntry {
    pub open spec fn osmem_invfn() -> spec_fn(Vec<OSMemEntry>) -> bool {
        |v: Vec<OSMemEntry>| osmem_wf(v@)
    }

    pub open spec fn spec_osperm(&self) -> OSMemPerm {
        OSMemPerm::spec_new(self.osperm as u8)
    }

    pub fn osperm(&self) -> (ret: OSMemPerm)
    requires
        self.is_constant()
    ensures
        ret === self.spec_osperm()
    {
        OSMemPerm::new(self.osperm.into())
    }

    pub fn end(&self) -> (ret: usize_t)
    requires
        self.wf()
    ensures
        ret.spec_valid_pn_with(0),
        ret == self.spec_end(),
    {
        self.start_page.add(self.npages).into()
    }

    pub fn npages(&self) -> (ret: usize_t)
    requires
        self.wf()
    ensures
        ret == self.npages@.val,
    {
        self.npages.into()
    }

    pub open spec fn open_wf(&self) -> bool {
        &&& self.is_constant()
        &&& self.start_page.spec_valid_pn_with(self.npages as nat)
        &&& spec_contains_page_perms(
            self.page_perms@,
            self.start_page as int,
            self.npages as nat,
            self.spec_osperm(),
        )
    }

    pub closed spec fn closed_wf(&self) -> bool {
        self.open_wf()
    }

    pub open spec fn wf(&self) -> bool {
        &&& self.closed_wf()
        &&& self.is_constant()
    }

    pub open spec fn wf_kern_cleared(&self) -> bool {
        &&& self.closed_wf()
        &&& self.is_constant()
        &&& self.spec_osperm().spec_kern_exe() == 0
    }

    pub proof fn proof_open_wf(&self)
    ensures
        self.wf() == self.open_wf(),
    {}

    pub proof fn proof_contains(&self, gpn: int)
    requires
        self.wf(),
        self.spec_start() <= gpn < self.spec_end(),
    ensures
        self.start_page.spec_valid_pn_with(self.npages as nat),
        spec_contains_page_perm(self.page_perms@, gpn, self.spec_osperm())
    {
        assert(self.page_perms@.contains_key(gpn));
    }

    pub open spec fn spec_start(&self) -> int {
        self.start_page as int
    }

    pub open spec fn spec_end(&self) -> int {
        (self.start_page + self.npages) as int
    }
}
}
verus! {

pub open spec fn osmem_wf(osmem: Seq<OSMemEntry>) -> bool {
    &&& osmem.is_constant()
    &&& forall|i| 0 <= i < osmem.len() ==> osmem[i].wf()
}

pub open spec fn osmem_wf_kern_cleared(osmem: Seq<OSMemEntry>) -> bool {
    &&& osmem.is_constant()
    &&& forall|i| 0 <= i < osmem.len() ==> osmem[i].wf_kern_cleared()
}

pub fn osmem_adjust(
    ghcb_h: GhcbHandle,
    entry: OSMemEntry,
    shared: bool,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: (GhcbHandle, OSMemEntry))
    requires
        ghcb_h.ghcb_wf(),
        entry.wf(),
        old(cs).inv(),
    ensures
        cs.inv(),
        ret.0.ghcb_wf(),
        ret.1.wf(),
        ret.1.osperm == entry.osperm,
        ret.1.start_page == entry.start_page,
        ret.1.npages == entry.npages,
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_PT_lockid()]),
{
    let mut entry = entry;
    let start_page: usize = entry.start_page.into();
    let npages: usize = entry.npages.into();
    if npages == 0 {
        return (ghcb_h, entry);
    }
    let ghost old_page_perms = entry.page_perms@;
    proof {
        assert(old_page_perms.dom() =~~= Set::new(
            |i| start_page as int <= i < start_page + npages,
        ));
    }
    let Tracked(mut page_perms) = entry.page_perms;
    let mut ghcb_h = ghcb_h;
    if !shared {
        assert forall|i|
            start_page <= i < (start_page + npages) implies #[trigger] page_perms.contains_key(i)
            && page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)) by {
            assert(page_perms.contains_key(i));
            assert(spec_contains_page_perm(page_perms, i, entry.spec_osperm()));
        }
        ghcb_h =
        ghcb_h.mk_private(start_page as u64, npages as u64, Tracked(cs), Tracked(&mut page_perms));
        // TODO: zero all pages;
        let start = start_page.to_addr();
        let size = npages.to_addr();
        proof {
            assert forall|i|
                (start as int).to_page() <= i < ((start
                    + size) as int).to_page() implies #[trigger] page_perms.contains_key(i)
                && page_perms[i]@.snp_wf_range((i.to_addr(), PAGE_SIZE as nat)) by {
                assert(old_page_perms.contains_key(i));
                assert(page_perms.contains_key(i));
            }
            assert(forall|i|
                ((start as int).to_page() <= i < ((start + size) as int).to_page()) ==> (
                #[trigger] page_perms.contains_key(i) && page_perms[i]@.snp_wf_range(
                    (i.to_addr(), PAGE_SIZE as nat),
                )));
        }
        mem_set_zeros2(start, size, Tracked(&mut page_perms));
        let attr = RmpAttr::empty().set_vmpl(RICHOS_VMPL as u64).set_perms(
            entry.osperm.into(),
        ).set_vmsa(0);
        proof {
            assert(start_page.spec_valid_pn_with(npages as nat));
            assert forall|i|
                start_page <= i < (start_page + npages) implies #[trigger] page_perms.contains_key(
                i,
            ) && spec_rmpadjmem_requires_at(page_perms[i], i, attr@) by {
                assert(old_page_perms.contains_key(i));
                assert(page_perms.contains_key(i));
                let perm = page_perms[i];
                assert(perm@.bytes().is_constant_to(attr.spec_vmpl() as nat));
                assert(perm@.snp().rmp@.spec_validated());
            }
            assert(spec_rmpadjmem_requires(page_perms, start_page as int, npages as nat, attr@));
        }
        rmpadjmem(start_page, npages, attr, Tracked(&mut cs.snpcore), Tracked(&mut page_perms));
    } else {
        ghcb_h =
        ghcb_h.mk_shared(start_page as u64, npages as u64, Tracked(cs), Tracked(&mut page_perms));
    }
    entry.page_perms = Tracked(page_perms);
    proof {
        assert(entry.start_page.spec_valid_pn_with(npages as nat));
        assert(entry.page_perms@.dom() =~~= old_page_perms.dom());
        assert(entry.page_perms@.dom() =~~= Set::new(
            |i| start_page as int <= i < start_page + npages,
        ));
        assert forall|i| #[trigger] page_perms.contains_key(i) implies spec_contains_page_perm(
            page_perms,
            i,
            entry.spec_osperm(),
        ) by {
            assert(page_perms.contains_key(i));
            let page_perm = page_perms[i];
            assert(os_mem_valid_snp(entry.spec_osperm(), page_perm@.snp()));
            assert(page_perm@.bytes().is_constant_to(RICHOS_VMPL as nat));
        }
        assert(spec_contains_page_perms(
            entry.page_perms@,
            start_page as int,
            npages as nat,
            entry.spec_osperm(),
        ));
        assert(entry.wf());
    }
    (ghcb_h, entry)
}

pub fn osmem_add(
    osmem: &mut Vec<OSMemEntry>,
    start_page: usize,
    npages: usize,
    osperm: OSMemPerm,
    shared: bool,
    Tracked(page_perms): Tracked<Map<int, SnpPointsToRaw>>,
    Tracked(snpcore): Tracked<&mut SnpCore>,
)
    requires
        osmem_wf(old(osmem)@),
        start_page.spec_valid_pn_with(npages as nat),
        !shared ==> spec_is_default_pages_const_to_vmpl(
            page_perms,
            start_page as int,
            npages as nat,
            RICHOS_VMPL as nat,
        ),
        old(snpcore).inv(),
        shared ==> osperm@ === OSMemPermSpec::ram(),
        shared ==> spec_is_shared_page_perms(page_perms, start_page as int, npages as nat),
    ensures
        osmem_wf(osmem@),
        osmem@ === old(osmem)@.push(osmem@.last()),
        osmem@.last().start_page.spec_eq(start_page),
        osmem@.last().npages.spec_eq(npages),
        osmem@.last().spec_osperm() === osperm,
        spec_contains_page_perms(
            osmem@.last().page_perms@,
            start_page as int,
            npages as nat,
            osperm,
        ),
        *snpcore === *old(snpcore),
{
    let tracked mut page_perms = page_perms;
    let tracked mut page_perms = page_perms.tracked_remove_keys(
        Set::new(|k| start_page <= k < (start_page + npages)),
    );
    let ghost old_page_perms = page_perms;
    if !shared {
        let attr = RmpAttr::empty().set_vmpl(RICHOS_VMPL as u64).set_perms(
            osperm.value as u64,
        ).set_vmsa(0);
        proof {
            assert(start_page.spec_valid_pn_with(npages as nat));
            assert forall|i| old_page_perms.contains_key(i) implies spec_rmpadjmem_requires_at(
                old_page_perms[i],
                i,
                attr@,
            ) by {
                assert(start_page <= i < start_page + npages);
                assert(spec_is_default_pages_const_to_vmpl(
                    old_page_perms,
                    start_page as int,
                    npages as nat,
                    RICHOS_VMPL as nat,
                ));
                assert(old_page_perms[i]@.wf_const_default((i.to_addr(), PAGE_SIZE as nat)));
            }
            assert(spec_rmpadjmem_requires((page_perms), start_page as int, npages as nat, attr@));
        }
        rmpadjmem(start_page, npages, attr, Tracked(snpcore), Tracked(&mut page_perms));
    }
    let entry = OSMemEntry {
        start_page: start_page.into(),
        npages: npages.into(),
        osperm: osperm.value.into(),
        page_perms: Tracked(page_perms),
    };
    osmem.push(entry);
    proof {
        assert forall|i| 0 <= i < osmem.len() implies osmem[i].wf() by {
            if i == osmem.len() - 1 {
                assert forall|k| page_perms.contains_key(k) implies spec_contains_page_perm(
                    page_perms,
                    k,
                    osperm,
                ) by {
                    assert(start_page <= k < (start_page + npages));
                    assert(old_page_perms.contains_key(k));
                    assert(page_perms.contains_key(k));
                    assert(page_perms[k]@.wf_not_null((k.to_addr(), PAGE_SIZE as nat)));
                    assert(os_mem_valid_snp(osperm, page_perms[k]@.snp()));
                    assert(page_perms[k]@.bytes() === old_page_perms[k]@.bytes());
                }
                assert(page_perms.dom() =~~= Set::new(|k| start_page <= k < (start_page + npages)));
                assert(spec_contains_page_perms(
                    page_perms,
                    start_page as int,
                    npages as nat,
                    osperm,
                ));
            }
        }
    }
}

pub fn osmem_entry_merge(left: OSMemEntry, right: OSMemEntry) -> (ret: OSMemEntry)
    requires
        left.wf(),
        right.wf(),
        right.spec_start() == left.spec_end(),
        left.osperm == right.osperm,
    ensures
        ret.wf(),
        ret.osperm == left.osperm,
        ret.spec_start() == left.spec_start(),
        ret.spec_end() == right.spec_end(),
{
    let mut ret = left;
    ret.npages = ret.npages.add(right.npages);
    let Tracked(mut page_perms) = ret.page_perms;
    let Tracked(right_page_perms) = right.page_perms;
    proof {
        page_perms.tracked_union_prefer_right(right_page_perms);
    }
    ret.page_perms = Tracked(page_perms);
    ret
}

pub fn osmem_entry_split(entry: OSMemEntry, start: usize, npages: usize) -> (ret: (
    OSMemEntry,
    OSMemEntry,
    OSMemEntry,
))
    requires
        entry.wf(),
        entry.spec_start() <= start,
        entry.spec_end() >= start + npages,
    ensures
        ret.0.wf(),
        ret.1.wf(),
        ret.2.wf(),
        ret.1.spec_start() == start,
        ret.1.spec_end() == start + npages,
        ret.0.spec_start() == entry.spec_start(),
        ret.0.spec_end() == ret.1.spec_start(),
        ret.2.spec_start() == ret.1.spec_end(),
        ret.2.spec_end() == entry.spec_end(),
        ret.0.osperm == entry.osperm,
        ret.1.osperm == entry.osperm,
        ret.2.osperm == entry.osperm,
{
    let Tracked(mut page_perms) = entry.page_perms;
    let tracked perms0 = page_perms.tracked_remove_keys(
        Set::new(|i| entry.spec_start() <= i < start),
    );
    let tracked perms1 = page_perms.tracked_remove_keys(Set::new(|i| start <= i < start + npages));
    let tracked perms2 = page_perms.tracked_remove_keys(
        Set::new(|i| start + npages <= i < entry.spec_end()),
    );
    let entry0 = OSMemEntry {
        start_page: entry.start_page,
        npages: (start - entry.start_page.reveal_value()).into(),
        osperm: entry.osperm,
        page_perms: Tracked(perms0),
    };
    let entry1 = OSMemEntry {
        start_page: start.into(),
        npages: npages.into(),
        osperm: entry.osperm,
        page_perms: Tracked(perms1),
    };
    let entry2 = OSMemEntry {
        start_page: (start + npages).into(),
        npages: (entry.npages.reveal_value() - npages + entry.start_page.reveal_value()
            - start).into(),
        osperm: entry.osperm,
        page_perms: Tracked(perms2),
    };
    (entry0, entry1, entry2)
}

pub fn osmem_find(osmem: &Vec<OSMemEntry>, vpage: usize) -> (ret: Option<usize>)
    requires
        osmem_wf(osmem@),
    ensures
        ret.is_Some() ==> (0 <= ret.unwrap() < osmem.len()
            && osmem[ret.unwrap() as int].spec_start() <= vpage
            < osmem[ret.unwrap() as int].spec_end()),
{
    let mut i = 0;
    while i < osmem.len()
        invariant
            0 <= i <= osmem.len(),
            osmem_wf(osmem@),
        ensures
            0 <= i <= osmem.len(),
            i < osmem.len() ==> osmem[i as int].spec_start() <= vpage < osmem[i as int].spec_end(),
    {
        let (start_page, npages): (usize, usize) = (
            osmem[i].start_page.into(),
            osmem[i].npages.into(),
        );
        proof {
            assert(osmem[i as int].wf());
        }
        let end_page = start_page + npages;
        if vpage >= start_page && vpage < end_page {
            break ;
        }
        i = i + 1;
    }
    if i < osmem.len() {
        Some(i)
    } else {
        None
    }
}

} // verus!
verus! {

pub open spec fn spec_ensure_check_osperm(
    ppage: int,
    osperm: OSMemPerm,
    entry: OSMemEntry,
) -> bool {
    let perm = osperm@;
    let is_full = perm.spec_read() > 0 && perm.spec_write() > 0 && perm.spec_kern_exe() > 0
        && perm.spec_user_exe() > 0;
    &&& entry.spec_start() <= ppage < entry.spec_end()
    &&& entry.spec_osperm()@.is_super_of(osperm@)
}

pub fn osmem_check(osmem: &Vec<OSMemEntry>, ppage: usize, osperm: OSMemPerm) -> (ret: bool)
    requires
        osmem_wf(osmem@),
    ensures
        ret ==> (osperm.value == 0 || exists|i|
            0 <= i < osmem.len() && spec_ensure_check_osperm(ppage as int, osperm, osmem[i])),
{
    match osmem_find(osmem, ppage) {
        Some(i) => { OSMemPerm::new(osmem[i].osperm.into()).is_super_of(&osperm) },
        _ => { false },
    }
}

pub fn osmem_check_and_get(osmem: &mut Vec<OSMemEntry>, ppage: usize, osperm: OSMemPerm) -> (ret:
    Option<(usize, OSMemEntry)>)
    requires
        osmem_wf(old(osmem)@),
    ensures
        ret.is_None() ==> old(osmem) === osmem,
        ret.is_Some() ==> osmem@ === old(osmem)@.remove(ret.get_Some_0().0 as int)
            && ret.get_Some_0().1 === old(osmem)@[ret.get_Some_0().0 as int] && 0
            <= ret.get_Some_0().0 < old(osmem)@.len(),
        ret.is_Some() ==> spec_ensure_check_osperm(ppage as int, osperm, ret.get_Some_0().1),
{
    match osmem_find(osmem, ppage) {
        Some(i) if OSMemPerm::new(osmem[i].osperm.into()).is_super_of(&osperm) => {
            Option::Some((i, osmem.remove(i)))
        },
        _ => { Option::None },
    }
}

pub fn osmem_check_and_get_page<T: IsConstant + SpecSize + WellFormed + VTypeCast<SecSeqByte>>(
    osmem: &mut Vec<OSMemEntry>,
    ppage: usize,
    osperm: OSMemPerm,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: Option<(VBox<T>, OSMemPerm)>)
    requires
        spec_size::<T>() == PAGE_SIZE,
        osmem_wf(old(osmem)@),
        old(cs).inv(),
    ensures
        osmem_wf(osmem@),
        ret.is_Some() ==> ret.get_Some_0().0.wf(),
        ret.is_Some() ==> ret.get_Some_0().0.is_page(),
        ret.is_Some() ==> ret.get_Some_0().0.snp().encrypted(),
        ret.is_Some() ==> ret.get_Some_0().0.snp().rmp@.spec_validated(),
        ret.is_Some() ==> ret.get_Some_0().0.id() == (ppage as int).to_addr(),
        ret.is_Some() ==> os_mem_valid_snp(ret.get_Some_0().1, ret.get_Some_0().0.snp()),
        cs.inv(),
        (*cs).only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
{
    let ghost old_osmem = *osmem;
    if let Some(e) = osmem_check_and_get(osmem, ppage, osperm) {
        let (i, entry) = e;
        assert(old_osmem[i as int].wf());
        assert(entry.wf());
        // TODO: insert two entry skipping vmsa;
        let npages: usize = entry.npages.into();
        if npages > 1 {
            let (left, mid, right) = osmem_entry_split(entry, ppage, 1);
            let osperm = mid.osperm();
            let Tracked(mut page_perms) = mid.page_perms;
            assert(page_perms.contains_key(ppage as int));
            let tracked page_perm = page_perms.tracked_remove(ppage as int);
            let check = check_is_encrypted(ppage.to_addr(), Tracked(&page_perm), Tracked(cs));
            match check {
                Some(encrypted) if encrypted => {},
                _ => {
                    new_strlit("Do not use richos-shared/unmapped memory!").leak_debug();
                    return None;
                },
            }
            assert(page_perm@.wf_not_null(((ppage as int).to_addr(), PAGE_SIZE!() as nat)));
            if right.npages.reveal_value() > 0 {
                osmem.insert(i, right);
            }
            if left.npages.reveal_value() > 0 {
                osmem.insert(i, left);
            }
            Some((VBox::from_raw(ppage.to_addr(), Tracked(page_perm.tracked_into())), osperm))
        } else {
            None
        }
    } else {
        None
    }
}

} // verus!
use global::*;
verus! {

pub fn _osmem_add_ram_from_allocator(
    allocator: &mut VeriSMoAllocator,
    tmposmem: Vec::<OSMemEntry>,
    Tracked(snpcore): Tracked<&mut SnpCore>,
) -> (ret: Vec::<OSMemEntry>)
    requires
        old(snpcore).inv(),
        old(allocator)@.inv(),
        old(allocator).wf(),
        tmposmem.len() == 0,
    ensures
        osmem_wf(ret@),
        snpcore.inv(),
        *snpcore == *old(snpcore),
        allocator@.inv(),
        allocator.wf(),
{
    let ghost prevcore = *snpcore;
    let mut tmposmem = tmposmem;
    new_strlit("remove ranges\n").leak_debug();
    let mut range_mem = allocator.remove_one_range();
    while range_mem.is_some()
        invariant
            osmem_wf(tmposmem@),
            snpcore.inv(),
            *snpcore === prevcore,
            allocator@.inv(),
            allocator.wf(),
            range_mem.is_None() ==> (allocator@.len() == 0),
            range_mem.is_Some() ==> range_mem.get_Some_0().1@@.wf_const_default(
                (range_mem.get_Some_0().0.0 as int, range_mem.get_Some_0().0.1 as nat),
            ),
        ensures
            osmem_wf(tmposmem@),
            range_mem.is_None(),
            allocator@.inv(),
            allocator.wf(),
            allocator@.len() == 0,
            snpcore.inv(),
            *snpcore === prevcore,
    {
        let (range, perm) = range_mem.unwrap();
        let start_addr: usize = page_align_up(range.0);
        let start_page = start_addr.to_page();
        let end_page = (range.1 + range.0).to_page();
        let end_addr = end_page.to_addr();
        let Tracked(mut perm) = perm;
        assert(perm@.range().0 == range.0);
        assert(perm@.range().1 == range.1);
        if start_page < end_page {
            assert(perm@.range().end() >= end_addr);
            assert(perm@.size() >= PAGE_SIZE);
            assert(0 <= start_addr - range.0 < PAGE_SIZE);
            let npages = end_page - start_page;
            let ghost aligned_size = end_addr - start_addr;
            assert(aligned_size == npages * PAGE_SIZE!());
            assert(perm@.wf_const_default(perm@.range()));
            let tracked (left, mut aligned_perm) = perm.trusted_split(
                (start_addr - range.0) as nat,
            );
            let tracked (aligned_perm, right) = aligned_perm.trusted_split(aligned_size as nat);
            let tracked page_perms = aligned_perm.tracked_to_pages();
            assert(aligned_perm@.wf_const_default(aligned_perm@.range()));
            proof {
                assert forall|i|
                    start_page <= i < (start_page
                        + npages) implies #[trigger] page_perms.contains_key(i)
                    && page_perms[i]@.wf_default((i.to_addr(), PAGE_SIZE as nat))
                    && page_perms[i]@.bytes().is_constant_to(RICHOS_VMPL as nat) by {
                    assert(page_perms.contains_key(i));
                    let offset = i.to_addr() - aligned_perm@.range().0;
                    assert(page_perms[i]@.bytes() =~~= aligned_perm@.bytes().subrange(
                        offset,
                        offset + PAGE_SIZE as int,
                    ));
                    assert(aligned_perm@.bytes().is_constant());
                    assert(page_perms[i]@.bytes().is_constant()) by {
                        let b = page_perms[i]@.bytes();
                        assert forall|k| 0 <= k < b.len() implies b[k].is_constant() by {
                            let sub = aligned_perm@.bytes().subrange(
                                offset,
                                offset + PAGE_SIZE as int,
                            );
                            assert(b[k] === sub[k]);
                            assert(sub[k] === aligned_perm@.bytes()[offset + k]);
                        }
                    }
                }
            }
            osmem_add(
                &mut tmposmem,
                start_page,
                npages,
                OSMemPerm::ram(),
                false,
                Tracked(page_perms),
                Tracked(snpcore),
            );
        }
        new_strlit("remove one\n").leak_debug();
        range_mem = allocator.remove_one_range();
    }
    tmposmem
}

pub fn add_osmem_to_e820(e820: VBox<E820Table>, e820_count: usize, osmem: &Vec<OSMemEntry>) -> (ret:
    VBox<E820Table>)
    requires
        e820@.is_constant(),
        e820.snp().is_vmpl0_private(),
        e820_count + osmem@.len() <= E820Table::spec_len(),
        osmem_wf(osmem@),
    ensures
        ret.only_val_updated(e820),
        ret@.is_constant(),
{
    let mut i = 0;
    let ghost old_e820 = e820;
    let mut e820 = e820;
    while i < osmem.len()
        invariant
            e820_count + osmem@.len() <= E820Table::spec_len(),
            e820.only_val_updated(old_e820),
            e820@.is_constant(),
            e820.snp().is_vmpl0_private(),
            osmem_wf(osmem@),
    {
        let entry = &osmem[i];
        assert(osmem[i as int].wf());
        (new_strlit("update: "), e820_count + i).leak_debug();
        e820.box_update(
            ArrayUpdate {
                index: (e820_count + i),
                val: E820Entry {
                    addr: entry.start_page.to_addr().into(),
                    size: entry.npages.to_addr().into(),
                    memty: E820_TYPE_RAM.into(),
                },
            },
        );
        i = i + 1;
    }
    e820
}

pub fn add_osmem_to_mut_e820(e820: &mut E820Table, e820_count: usize, osmem: &Vec<OSMemEntry>)
    requires
        old(e820)@.is_constant(),
        e820_count + osmem@.len() <= E820Table::spec_len(),
        osmem_wf(osmem@),
    ensures
        e820@.is_constant(),
{
    let mut i = 0;
    let ghost old_e820 = *e820;
    while i < osmem.len()
        invariant
            e820_count + osmem@.len() <= E820Table::spec_len(),
            e820@.is_constant(),
            osmem_wf(osmem@),
    {
        let entry = &osmem[i];
        assert(osmem[i as int].wf());
        (new_strlit("update: "), e820_count + i).leak_debug();
        e820.set(
            e820_count + i,
            E820Entry {
                addr: entry.start_page.to_addr().into(),
                size: entry.npages.to_addr().into(),
                memty: E820_TYPE_RAM.into(),
            },
        );
        i = i + 1;
    }
}

pub fn osmem_add_ram_from_allocator(
    osmem: &mut OSMem,
    vmpl: u8,
    e820: VBox<E820Table>,
    e820_size: &mut usize,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: Result<VBox<E820Table>, u8>)
    requires
        e820@.is_constant(),
        e820.snp().is_vmpl0_private(),
        0 <= *old(e820_size) <= e820@@.len(),
        osmem_wf(old(osmem)@),
        old(cs).inv(),
    ensures
        osmem_wf((osmem)@),
        0 <= *(e820_size) <= e820@@.len(),
        ret.is_Ok() ==> ret.get_Ok_0().only_val_updated(e820),
        ret.is_Ok() ==> ret.get_Ok_0()@.is_constant(),
        cs.inv(),
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_ALLOCATOR_lockid()]),
{
    let tmposmem = create_osmem_from_allocator(vmpl, Tracked(cs));
    // Terminate if there are too many regions.
    let e820_count = *e820_size;
    if (E820Table::const_len() - e820_count) < tmposmem.len() {
        new_strlit("too many entries\n").leak_debug();
        return Err(SM_TERM_MEM as u8);
    }
    new_strlit("Install e820\n").leak_debug();
    let e820 = add_osmem_to_e820(e820, e820_count, &tmposmem);
    *e820_size = e820_count + tmposmem.len();
    new_strlit("append osmem\n").leak_debug();
    let reserved = 0;
    let mut tmposmem = tmposmem;
    osmem.append(&mut tmposmem);
    new_strlit("end of adjust\n").leak_debug();
    Ok(e820)
}

pub fn add_ram_from_allocator(
    vmpl: u8,
    e820: &mut E820Table,
    e820_size: &mut usize,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: Result<OSMem, u8>)
    requires
        old(e820)@.is_constant(),
        0 <= *old(e820_size) <= old(e820)@.len(),
        old(cs).inv(),
    ensures
        ret.is_Ok() ==> osmem_wf(ret.get_Ok_0()@),
        0 <= *(e820_size) <= e820@.len(),
        ret.is_Ok() ==> e820@.is_constant(),
        cs.inv(),
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_ALLOCATOR_lockid()]),
{
    let tmposmem = create_osmem_from_allocator(vmpl, Tracked(cs));
    // Terminate if there are too many regions.
    let e820_count = *e820_size;
    if (E820Table::const_len() - e820_count) < tmposmem.len() {
        new_strlit("too many entries\n").leak_debug();
        return Err(SM_TERM_MEM as u8);
    }
    new_strlit("Install e820\n").leak_debug();
    add_osmem_to_mut_e820(e820, e820_count, &tmposmem);
    *e820_size = e820_count + tmposmem.len();
    Ok(tmposmem)
}

pub fn create_osmem_from_allocator(vmpl: u8, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (osmem:
    Vec<OSMemEntry>)
    requires
        old(cs).inv(),
    ensures
        osmem_wf((osmem)@),
        cs.inv(),
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![spec_ALLOCATOR_lockid()]),
{
    let ghost oldcs = *cs;
    // reserved some memory for security module.
    // reserved will be automatically release in the end of the function.
    new_strlit("reserve memory\n").leak_debug();
    let reserved = VBox::<Array<u8, VERISMO_RSVD_MEM>>::new_aligned_uninit(PAGE_SIZE, Tracked(cs));
    let mut tmposmem = Vec::with_capacity(16);
    let ghost prevcs = *cs;
    // Acquire allocator to prevent usage of allocator, so that we can
    // safely assign remaining memory to vmpl2.
    let tracked mut alloc_lockperm = cs.lockperms.tracked_remove(spec_ALLOCATOR_lockid());
    let (alloc_ptr, Tracked(mut allocperm), Tracked(alloc_lockperm)) = ALLOCATOR().acquire(
        Tracked(alloc_lockperm),
        Tracked(&cs.snpcore.coreid),
    );
    let mut allocator = alloc_ptr.take(Tracked(&mut allocperm));
    proof {
        assert(alloc_lockperm@.invfn.inv(allocator));
        assert(VeriSMoAllocator::invfn()(allocator));
    }
    // Find all remaining memory pages
    let tmposmem = _osmem_add_ram_from_allocator(
        &mut allocator,
        tmposmem,
        Tracked(&mut cs.snpcore),
    );
    // Release allocator to allow use of allocato in the future.
    // Only a small amount of reserved memory is left in the allocator.
    proof {
        assert(VeriSMoAllocator::invfn()(allocator));
        assert(alloc_lockperm@.invfn.inv(allocator));
        assert(allocator@.inv());
    }
    new_strlit("ready to release memory\n").leak_debug();
    alloc_ptr.put(Tracked(&mut allocperm), allocator);
    ALLOCATOR().release(
        Tracked(&mut alloc_lockperm),
        Tracked(allocperm),
        Tracked(&cs.snpcore.coreid),
    );
    proof {
        cs.lockperms.tracked_insert(spec_ALLOCATOR_lockid(), alloc_lockperm);
        oldcs.lemma_update_prop(
            prevcs,
            *cs,
            set![],
            set![spec_ALLOCATOR_lockid()],
            set![],
            set![spec_ALLOCATOR_lockid()],
        );
    }
    tmposmem
}

fn _lock_kernel(
    osmem: &mut OSMem,
    range: &(usize, usize),
    Tracked(snpcore): Tracked<&mut SnpCore>,
) -> (ret: usize)
    requires
        osmem_wf(old(osmem)@),
        (range.0 as int).spec_valid_pn_with(range.1 as nat),
        old(snpcore).inv(),
    ensures
        osmem_wf(osmem@),
        snpcore.inv(),
        *snpcore === *old(snpcore),
{
    let mut start = range.0;
    let end = range.0 + range.1;
    while start < end
        invariant
            osmem_wf(osmem@),
            start <= end,
            (start as int).spec_valid_pn_with((end - start) as nat),
            snpcore.inv(),
            *snpcore === *old(snpcore),
        ensures
            osmem_wf(osmem@),
            (start as int).spec_valid_pn_with((end - start) as nat),
            snpcore.inv(),
            *snpcore === *old(snpcore),
    {
        if let Some(e) = osmem_check_and_get(osmem, start, OSMemPerm::ram()) {
            let (i, mut entry) = e;
            assert(entry.wf());
            // no need to check encrytion bit since rmpadjust will
            // throw error when it is a shared page.
            /*if entry.encrypted.reveal_value() != 1 {
                    new_strlit("Cannot set shared memory to kernel page\n").leak_debug();
                    break;
                }*/
            let next_start: usize = (entry.start_page.add(entry.npages)).into();
            let tmp_end = if end > next_start {
                next_start
            } else {
                end
            };
            let tmp_start = start;
            assert(tmp_end >= start);
            let tmp_npages = tmp_end - start;
            proof {
                assert(entry.wf());
            }
            let (left, mid, right) = osmem_entry_split(entry, tmp_start, tmp_npages);
            let Tracked(mut page_perms) = mid.page_perms;
            let attr = RmpAttr::empty().set_vmpl(RICHOS_VMPL as u64).set_perms(
                OSMemPerm::new(mid.osperm.into()).set_write(0).value as u64,
            ).set_vmsa(0);
            proof {
                assert forall|i|
                    tmp_start <= i < tmp_start
                        + tmp_npages implies #[trigger] page_perms.contains_key(i)
                    && spec_rmpadjmem_requires_at(page_perms[i], i, attr@) by {
                    assert(page_perms.contains_key(i));
                    assert(spec_contains_page_perm(page_perms, i, mid.spec_osperm()));
                    assert(page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)));
                }
                assert(page_perms.dom() =~~= Set::new(
                    |i| tmp_start <= i < (tmp_start + tmp_npages),
                ));
            }
            rmpadjmem(tmp_start, tmp_npages, attr, Tracked(snpcore), Tracked(&mut page_perms));
            (new_strlit("\nLock Kernel "), (tmp_start, tmp_end)).leak_debug();
            if left.npages.reveal_value() > 0 {
                osmem.push(left);
            }
            if right.npages.reveal_value() > 0 {
                osmem.push(right);
            }
            start = tmp_end;
        } else {
            break ;
        }
    }
    return start;
}

fn clear_kern_if_private(entry: OSMemEntry, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret:
    OSMemEntry)
    requires
        entry.wf(),
        old(cs).inv(),
    ensures
        ret.wf_kern_cleared(),
        cs.inv(),
        cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
{
    let ghost old_osperm = entry.spec_osperm();
    let OSMemEntry { page_perms: Tracked(mut page_perms), start_page, npages, osperm } = entry;
    let osperm = OSMemPerm::new(osperm.into()).set_kern_exe(0);
    let mut tmp_start: usize = start_page.into();
    let npages: usize = npages.into();
    let ghost start = tmp_start;
    proof {
        assert(page_perms.dom() =~~= Set::new(|i| tmp_start <= i < (tmp_start + npages)));
    }
    (new_strlit("\nClear Kern_exe "), (tmp_start, npages)).leak_debug();
    let end = tmp_start + npages;
    let tracked mut new_page_perms = Map::tracked_empty();
    while tmp_start < end
        invariant
            start <= tmp_start <= end,
            (start as int).spec_valid_pn_with((end - start) as nat),
            osperm@ === old_osperm@.spec_set_kern_exe(0),
            spec_contains_page_perms(
                page_perms,
                tmp_start as int,
                (end - tmp_start) as nat,
                old_osperm,
            ),
            cs.inv(),
            cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
            spec_contains_page_perms(
                new_page_perms,
                start as int,
                (tmp_start - start) as nat,
                osperm,
            ),
    {
        let attr = RmpAttr::empty().set_vmpl(RICHOS_VMPL as u64).set_perms(
            osperm.value as u64,
        ).set_vmsa(0);
        assert(page_perms.contains_key(tmp_start as int));
        proof {
            assert(spec_contains_page_perm(page_perms, tmp_start as int, old_osperm));
            assert(page_perms[tmp_start as int]@.wf_range(
                ((tmp_start as int).to_addr(), PAGE_SIZE as nat),
            ));
        }
        let tracked page_perm = page_perms.tracked_remove(tmp_start as int);
        proof {
            assert(os_mem_valid_snp(old_osperm, page_perm@.snp()));
        }
        let check = if tmp_start < 0x1000 {
            Some(true)
        } else {
            check_is_encrypted(tmp_start.to_addr(), Tracked(&page_perm), Tracked(cs))
        };
        match check {
            Some(encrypted) => {
                if encrypted {
                    assert(spec_rmpadjmem_requires_at(page_perm, (tmp_start as int), attr@));
                    rmpadjust_check(
                        tmp_start.to_addr() as u64,
                        attr,
                        Tracked(&mut cs.snpcore),
                        Tracked(&mut page_perm),
                    );
                    assert(os_mem_valid_snp(osperm, page_perm@.snp()));
                } else {
                    assert(os_mem_valid_snp(osperm, page_perm@.snp()));
                }
            },
            None => {
                new_strlit("Do not support unmapped memory!").leak_debug();
                vc_terminate(SM_TERM_RICHOS_ERR(0), Tracked(&mut cs.snpcore));
            },
        }
        proof {
            new_page_perms.tracked_insert(tmp_start as int, page_perm);
            assert forall|i| #[trigger]
                new_page_perms.contains_key(i) implies spec_contains_page_perm(
                new_page_perms,
                i,
                osperm,
            ) by {
                if tmp_start == i {
                    assert(new_page_perms[i] === page_perm);
                    assert(page_perm@.wf_not_null((i.to_addr(), PAGE_SIZE as nat)));
                    assert(os_mem_valid_snp(osperm, page_perm@.snp()));
                    assert(page_perm@.bytes().is_constant_to(RICHOS_VMPL as nat));
                }
            }
            assert(new_page_perms.dom() =~~= Set::new(|i: int| start <= i < tmp_start + 1));
        }
        tmp_start = tmp_start + 1;
    }
    OSMemEntry {
        osperm: osperm.value.into(),
        page_perms: Tracked(new_page_perms),
        start_page,
        npages: npages.into(),
    }
}

#[inline]
fn _clear_kern_exe(osmem: &mut OSMem, Tracked(cs): Tracked<&mut SnpCoreSharedMem>)
    requires
        osmem_wf(old(osmem)@),
        old(cs).inv(),
    ensures
        osmem_wf(osmem@),
        osmem_wf_kern_cleared(osmem@),
        cs.inv(),
        cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
{
    let mut i = 0;
    while i < osmem.len()
        invariant
            0 <= i <= osmem.len(),
            osmem_wf(osmem@),
            osmem_wf_kern_cleared(osmem@.take(i as int)),
            cs.inv(),
            cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
    {
        let mut entry = osmem.remove(i);
        proof {
            assert(entry.wf());
        }
        let entry = clear_kern_if_private(entry, Tracked(cs));
        osmem.insert(i, entry);
        i = i + 1;
    }
    assert(osmem@ =~~= osmem@.take(osmem@.len() as int));
}

pub fn lock_kernel(
    osmem: &mut OSMem,
    ranges: &Vec<(usize, usize)>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        osmem_wf(old(osmem)@),
        forall|i|
            0 <= i < ranges@.len() ==> (ranges@[i].0 as int).spec_valid_pn_with(
                ranges@[i].1 as nat,
            ),
        old(cs).inv(),
    ensures
        osmem_wf(osmem@),
        osmem_wf_kern_cleared(osmem@),
        cs.inv(),
        cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
{
    let mut i = 0;
    while i < ranges.len()
        invariant
            osmem_wf(osmem@),
            forall|i|
                0 <= i < ranges@.len() ==> (ranges@[i].0 as int).spec_valid_pn_with(
                    ranges@[i].1 as nat,
                ),
            cs.inv(),
            cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
    {
        let range = &ranges[i];
        let end = _lock_kernel(osmem, range, Tracked(&mut cs.snpcore));
        if end < range.1 {
            vc_terminate(SM_TERM_RICHOS_ERR(10), Tracked(&mut cs.snpcore));
        }
        i = i + 1;
    }
    _clear_kern_exe(osmem, Tracked(cs));
}

} // verus!

================
File: ./source/verismo/src/linkedlist/mod.rs
================

use core::ptr;

use verismo_macro::*;

use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::ptr::*;
use crate::tspec_e::*;
use crate::*;

verismo_simple! {
    #[repr(C, align(1))]
    #[derive(VDefault)]
    pub struct Node<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> {
        pub next: usize_t,
        pub val: T,
    }

    pub tracked struct VSnpPointsToNode<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> {
        pub next: SnpPointsTo<usize_t>,
        pub val: SnpPointsTo<T>,
    }

    pub ghost struct SpecListItem<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> {
        pub ptr: SnpPPtr<Node<T>>,
        pub snp: SwSnpMemAttr,
        pub val: T,
    }

    // Cannot directly prove that the linked list is not a circle.
    // Cannot directly prove self.spec_index_of(nodeptr) is unique.
    // However, those properties are implicitly indicated
    // by the fact that each memory token has unique id.
    //
    // Such limitation requires the users to provide a ghost index for node access/
    pub struct LinkedList<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> {
        // a reversed list of ptrs
        pub ptrs: Ghost<Seq<SpecListItem<T>>>,
        pub perms: Tracked<Map<nat, SnpPointsTo<Node<T>>>>,
        pub head: usize_t,
    }
}

verus! {

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> SnpPPtr<Node<T>> {
    pub fn set_next(&self, next: usize_t, Tracked(perm): Tracked<SnpPointsTo<Node<T>>>) -> (newperm:
        Tracked<SnpPointsTo<Node<T>>>)
        requires
            (perm)@.ptr_not_null_wf(*self),
            perm@.value().is_Some(),
            next.wf(),
            next.is_constant(),
        ensures
            newperm@@.ptr_not_null_wf(*self),
            newperm@@.value().get_Some_0().next === next,
            newperm@@.value().get_Some_0().val === (perm)@.value().get_Some_0().val,
            newperm@@.snp() === perm@.snp(),
            newperm@@.value().is_Some(),
    {
        let tracked mut mp = perm;
        let tracked mut mp1;
        let tracked mut mp2;
        let ghost old_perm = perm;
        proof {
            let value = mp@.value();
            let tracked tmp = mp.trusted_into_raw();
            assert(tmp@.bytes() === value.get_Some_0().vspec_cast_to());
            let tracked (rmp1, rmp2) = tmp.trusted_split(spec_size::<usize>());
            assert(rmp1@.snp() === rmp2@.snp());
            mp1 = rmp1.trusted_into();
            mp2 = rmp2;
            let bytes1 = old_perm@.value().get_Some_0().next.sec_bytes();
            let bytes2 = old_perm@.value().get_Some_0().val.sec_bytes();
            let bytes3 = old_perm@.value().get_Some_0().sec_bytes();
            assert(bytes3.subrange(bytes1.len() as int, bytes3.len() as int) =~~= bytes2);
            assert(mp2@.bytes() === bytes2);
            assert(mp1@.snp() === mp2@.snp());
            assert(mp1@.value().is_Some());
            assert(mp1@.value().get_Some_0() === value.get_Some_0().next) by {
                let v1 = mp1@.value().get_Some_0();
                let v2 = value.get_Some_0();
                assert(v1 === v1.sec_bytes().vspec_cast_to());
                assert(v2.next === v2.next.sec_bytes().vspec_cast_to());
                assert(v1.sec_bytes() =~~= v2.next.sec_bytes());
            }
        }
        self.to::<usize_t>().replace(Tracked(&mut mp1), next);
        proof {
            let value = mp1@.value();
            mp = mp1.trusted_into_raw().trusted_join(mp2).trusted_into();
            let v2 = mp@.value().get_Some_0();
            let v1 = value.get_Some_0();
            assert(v1.sec_bytes() =~~= (v2.sec_bytes().subrange(0, spec_size::<usize>() as int)));
            //Node::<T>::trusted_to_stream(v2);
            assert(v2.sec_bytes().take(spec_size::<usize>() as int) =~~= (v2.next.sec_bytes()));
            proof_cast_from_seq_unique(v2);
            proof_cast_from_seq_unique(v1);
            assert(mp2@.bytes() =~~= old_perm@.value().get_Some_0().val.sec_bytes());
            assert(v2.sec_bytes().skip(spec_size::<usize>() as int) =~~= v2.val.sec_bytes());
            assert(mp2@.bytes() =~~= v2.sec_bytes().skip(spec_size::<usize>() as int));
            proof_cast_from_seq_unique(mp@.value().get_Some_0().val);
            proof_cast_from_seq_unique(old_perm@.value().get_Some_0().val);
            assert(mp@.value().get_Some_0().next === value.get_Some_0());
        }
        Tracked(mp)
    }
}

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> Node<T> {
    pub fn next_ptr(&self) -> (ret: SnpPPtr<Node<T>>)
        requires
            self.wf(),
        ensures
            ret.uptr === self.next,
            ret.wf(),
    {
        SnpPPtr::from_usize(self.next)
    }
}

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> LinkedList<T> {
    spec fn wf_perm_val(&self, i: nat) -> bool
        recommends
            i < self.ptrs@.len(),
    {
        let val = self.perms@[i]@.value().get_Some_0();
        //&&& self.head.is_constant() == val.is_constant()
        &&& self.head.is_constant() == val.next.is_constant()
        &&& self.perms@[i].view().value().is_Some()
        &&& (i > 0 ==> self.perms@[i].view().value().get_Some_0().next as int == self.ptrs@[i
            - 1 as int].ptr.id())
        &&& (i == 0) == (!self.perms@[i].view().value().get_Some_0().next.spec_valid_addr_with(
            spec_size::<Node<T>>(),
        ))
    }

    spec fn wf_perm(&self, i: nat) -> bool
        recommends
            i < self.ptrs@.len(),
    {
        let spec_item = self.ptrs@[i as int];
        let point_to = self.perms@[i]@;
        &&& self.perms@.dom().contains(i)
        &&& point_to.ptr_not_null_wf(spec_item.ptr)
        &&& point_to.snp() === spec_item.snp
        &&& point_to.value().get_Some_0().val === spec_item.val
        &&& self.perms@[i]@.is_valid_private()
        &&& self.wf_perm_val(i)
    }

    spec fn wf_perms(&self) -> bool {
        forall|i: nat| i < self.ptrs@.len() ==> self.wf_perm(i)
    }

    spec fn wf_head(&self) -> bool {
        &&& if self.ptrs@.len() == 0 {
            !self.head.spec_valid_addr_with(spec_size::<Node<T>>())
        } else {
            &&& self.head == self.ptrs@.last().ptr.id()
            &&& self.head.spec_valid_addr_with(spec_size::<Node<T>>())
        }
        &&& self.head.is_constant()
    }

    pub open spec fn inv(&self) -> bool {
        self.inv_hidden() && self.is_constant()
    }

    pub closed spec fn inv_hidden(&self) -> bool {
        self.wf_perms() && self.wf_head() && self.wf()
    }

    pub closed spec fn view(&self) -> Seq<SpecListItem<T>> {
        self.ptrs@
    }

    pub open spec fn contains_ptr_at(&self, ptr: SnpPPtr<Node<T>>, idx: int) -> bool {
        &&& ptr.not_null() ==> (0 <= idx < self@.len() && self@[idx].ptr.id() == ptr.id())
        &&& ptr.is_null() <==> idx == self@.len()
        &&& ptr.is_constant()
    }

    /// Create a new LinkedList
    pub const fn new() -> (ret: LinkedList<T>)
        ensures
            ret.inv(),
            ret@ =~~= Seq::empty(),
            ret.is_constant(),
    {
        LinkedList {
            ptrs: Ghost(Seq::empty()),
            perms: Tracked(Map::tracked_empty()),
            head: INVALID_ADDR,
        }
    }
}

} // verus!
verus! {

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> LinkedList<T> {
    /// Return `true` if the list is empty
    pub fn is_empty(&self) -> (ret: bool)
        requires
            self.inv(),
        ensures
            ret == (self@.len() == 0),
    {
        !self.head.check_valid_addr(size_of::<Node<T>>())
    }
}

} // verus!
verus! {

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> SpecDefault for LinkedList<T> {
    spec fn spec_default() -> Self;
}

} // verus!
verus! {

impl<T> LinkedList<T> where T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte> {
    #[verifier(external_body)]
    pub broadcast proof fn axiom_default()
        ensures
            (#[trigger] Self::spec_default()).is_constant(),
            Self::spec_default()@.len() == 0,
    {
    }

    /// Push `ptr` to the front of the list
    pub fn push(&mut self, ptr: SnpPPtr<Node<T>>, Tracked(perm): Tracked<SnpPointsTo<Node<T>>>)
        requires
            old(self).spec_valid_item(ptr, perm),
            (*old(self)).inv(),
            ptr.is_constant(),
            perm@.value().get_Some_0().next.is_constant(),
            perm@.value().is_Some(),
        ensures
            (*self).inv(),
            self@ === old(self)@.push(
                SpecListItem { ptr, snp: perm@.snp(), val: perm@.value().get_Some_0().val },
            ),
    {
        self.insert(SnpPPtr::nullptr(), ptr, Ghost(self@.len() as int), Tracked(perm));
    }

    pub open spec fn spec_valid_item(
        &self,
        ptr: SnpPPtr<Node<T>>,
        perm: SnpPointsTo<Node<T>>,
    ) -> bool {
        let next = perm@.value().get_Some_0().next;
        &&& perm@.ptr_not_null_private_wf(
            ptr,
        )
        //&&& next.spec_valid_addr_with(spec_size::<Node<T>>())

    }

    pub open spec fn spec_pop(
        new: Self,
        olds: Self,
        ret: Option<(SnpPPtr<Node<T>>, Tracked<SnpPointsTo<Node<T>>>)>,
    ) -> bool {
        let len = olds@.len();
        &&& if let Some(ptr) = ret {
            &&& len > 0
            &&& new@ === olds@.drop_last()
            &&& olds@.last().ptr.id() === ptr.0.id()
            &&& olds@.last().snp === ptr.1@@.snp()
            &&& olds@.last().val === ptr.1@@.value().get_Some_0().val
            &&& new.spec_valid_item(ptr.0, ptr.1@)
        } else {
            &&& len == 0
            &&& olds === new
        }
    }

    pub open spec fn spec_pop2(
        new: Self,
        olds: Self,
        ret: Option<(SnpPPtr<Node<T>>, Tracked<SnpPointsTo<Node<T>>>)>,
    ) -> bool {
        let len = olds@.len();
        &&& if let Some(ptr) = ret {
            &&& len > 0
            &&& new@ === olds@.drop_last()
            &&& olds@.last().ptr.id() === ptr.0.id()
            &&& olds@.last().snp === ptr.1@@.snp()
            &&& olds@.last().val
                === ptr.1@@.value().get_Some_0().val
            //&&& new.spec_valid_item(ptr.0, ptr.1@)

        } else {
            &&& len == 0
            &&& olds === new
        }
    }

    pub closed spec fn spec_node(&self, i: nat) -> Node<T> {
        self.perms@[i]@.value().get_Some_0()
    }

    #[verifier(inline)]
    pub open spec fn spec_index_of(&self, nodeptr: SnpPPtr::<Node<T>>) -> int {
        self._spec_index_of(nodeptr.id())
    }

    pub open spec fn _spec_index_of(&self, id: int) -> int {
        if self@.len() > 0 && self.head == id {
            (self@.len() - 1)
        } else {
            choose|i: int| id == self@[i].ptr.id() && 0 <= i < self@.len()
        }
    }

    pub open spec fn _spec_has_index_of(&self, nodeptr: int) -> bool {
        &&& self@.len() > 0
        &&& exists|i| nodeptr == self@[i].ptr.id() && 0 <= i < self@.len()
    }

    pub open spec fn spec_has_index_of(&self, nodeptr: SnpPPtr::<Node<T>>) -> bool {
        self._spec_has_index_of(nodeptr.id())
    }

    pub fn head_ptr(&self) -> (ret: SnpPPtr<Node<T>>)
        requires
            self.inv(),
        ensures
            ret.uptr === self.head,
            ret.is_constant(),
            self@.len() > 0 ==> self@.last().ptr.id() == ret.id(),
            self@.len() > 0 ==> ret.not_null(),
            (self@.len() == 0) == (ret.is_null()),
    {
        SnpPPtr::from_usize(self.head)
    }

    pub open spec fn reverse_index(&self, i: int) -> int {
        self@.len() - i - 1
    }

    pub fn pop(&mut self) -> (ret: Option<(SnpPPtr<Node<T>>, Tracked<SnpPointsTo<Node<T>>>)>)
        requires
            old(self).inv(),
        ensures
            self.inv(),
            Self::spec_pop2(*self, *old(self), ret),
            Self::spec_pop(*self, *old(self), ret),
    {
        if self.is_empty() {
            return None
        } else {
            let ghost i = (self.ptrs@.len() - 1) as nat;
            proof {
                assert(self.ptrs@.len() != 0);
                assert(self.wf_perm(i));
                assert(self.wf_head());
                if i > 0 {
                    assert(self.wf_perm((i - 1) as nat));
                }
                assert(self.ptrs@.last().ptr.id() == self.perms@[i]@.pptr());
                assert(self.head == old(self).ptrs@.last().ptr.id());
            }
            // Advance head pointer
            let tracked first_perm = self.perms.borrow_mut().tracked_remove(i);
            let headptr = SnpPPtr::<Node<T>>::from_usize(self.head);
            proof {
                assert(self.head == first_perm@.pptr());
                assert(headptr.id() == first_perm@.pptr());
            }
            let ret = self.head;
            self.head = headptr.borrow(Tracked(&first_perm)).next;
            proof {
                assert(ret == old(self).ptrs@.last().ptr.id());
                self.ptrs@ = self.ptrs@.drop_last();
                assert(self.wf_perms()) by {
                    assert forall|i: nat| 0 <= i < self.ptrs@.len() implies self.wf_perm(i) by {
                        assert(i < old(self).ptrs@.len());
                        assert(old(self).wf_perm(i));
                    }
                }
                assert(self.wf_head()) by {
                    if self.ptrs@.len() == 0 {
                    } else {
                        assert(self.head == self.ptrs@.last().ptr.id());
                    }
                }
            }
            let ret = Some((SnpPPtr::from_usize(ret), Tracked(first_perm)));
            proof {
                assert(self.ptrs@ === old(self).ptrs@.drop_last());
                assert(ret.get_Some_0().0.id() == ret.get_Some_0().1@@.pptr());
                assert(ret.get_Some_0().1@ === old(self).perms@[(old(self).ptrs@.len()
                    - 1) as nat]);
            }
            // Strange: Need to call wf and is_constant explicitly to ensure tracked is true.
            assert(ret.get_Some_0().1.wf());
            assert(ret.get_Some_0().1.is_constant());
            ret
        }
    }

    pub fn remove_iter<F: FnOnce(SnpPPtr<Node<T>>) -> bool + core::marker::Copy>(
        &mut self,
        cond_fn: F,
        max_len: usize,
    ) -> (ret: (Self, Ghost<Seq<int>>, Ghost<Seq<int>>))
        requires
            old(self).inv(),
            forall|var: SnpPPtr<Node<T>>| var.is_constant() ==> cond_fn.requires((var,)),
        ensures
            self.inv(),
            ret.0@.len() <= max_len as nat,
            ret.0@.len() < max_len as nat ==> forall|k: int|
                0 <= k < self@.len() ==> cond_fn.ensures((self@[k].ptr,), false),
            forall|k: int| 0 <= k < ret.0@.len() ==> cond_fn.ensures((ret.0@[k].ptr,), true),
            is_subseq_via_index(ret.0@, old(self)@, ret.1@),
            is_subseq_via_index(self@, old(self)@, ret.2@),
            (ret.0@.len() == 0) ==> old(self) === self,
            ret.0@.len() == 1 ==> self@ =~~= old(self)@.remove(ret.1@[0]),
            ret.0.inv(),
    {
        let mut prev_ptr: SnpPPtr<Node<T>> = SnpPPtr::nullptr();
        let mut cur_ptr: SnpPPtr<Node<T>> = SnpPPtr::from_usize(self.head);
        let ghost mut d: int = 0;
        let mut ret = Self::new();
        let ghost mut keep_idx = Seq::new(self@.len(), |i: int| i);
        let ghost mut removed_idx = Seq::empty();
        let mut removed: usize = 0;
        if self.is_empty() {
            return (ret, Ghost(removed_idx), Ghost(keep_idx));
        }
        proof {
            proof_empty_is_subs(old(self)@);
            assert(removed.is_constant());
            assert(max_len.is_constant());
        }
        //&& removed < max_len
        while cur_ptr.check_valid() && removed < max_len
            invariant
                self.inv(),
                ret.inv(),
                forall|var: SnpPPtr<Node<T>>| var.is_constant() ==> cond_fn.requires((var,)),
                old(self).is_constant(),
                self.is_constant(),
                removed.is_constant(),
                max_len.is_constant(),
                cur_ptr.is_constant(),
                prev_ptr.is_constant(),
                ret.is_constant(),
                if d < self@.len() {
                    cur_ptr.id() == self@[self@.len() - d - 1].ptr.id()
                } else {
                    cur_ptr.is_null()
                },
                d > 0 ==> prev_ptr.id() == self@[self@.len() - d].ptr.id(),
                d == 0 ==> prev_ptr.is_null(),
                removed <= max_len,
                removed == ret@.len(),
                old(self)@.len() != 0,
                removed as nat + self@.len() == old(self)@.len(),
                0 <= d <= self@.len(),
                (d == self@.len()) == (cur_ptr.is_null()),
                keep_idx.len() + removed_idx.len() == old(self)@.len(),
                removed_idx.len() == removed as nat,
                forall|k: int|
                    self@.len() - d <= k < self@.len() ==> cond_fn.ensures((self@[k].ptr,), false),
                forall|k: int| 0 <= k < ret@.len() ==> cond_fn.ensures((ret@[k].ptr,), true),
                is_subseq_via_index(self@, old(self)@, keep_idx),
                is_subseq_via_index(ret@, old(self)@, removed_idx),
                ret@.len() == 0 ==> (old(self) === self && keep_idx === Seq::new(
                    self@.len(),
                    |i: int| i,
                ) && removed_idx === Seq::empty()),
                ret@.len() == 1 ==> self@ =~~= old(self)@.remove(removed_idx[0]),
        {
            let ghost prev_self = *self;
            let ghost prev_ret = ret;
            let ghost i = self@.len() - d - 1;
            proof {
                self.wf_perm(0);
                assert(self.wf_head());
                assert(0 <= i < self@.len());
                assert(self.wf_perm(i as nat));
                if i + 1 < self@.len() {
                    assert(self.wf_perm((i + 1) as nat));
                }
                if i > 0 {
                    assert(self.wf_perm((i - 1) as nat));
                }
                assert(self.perms@ =~~= self.perms@.remove(i as nat).insert(
                    i as nat,
                    self.perms@[i as nat],
                ));
            }
            assert(self.wf_perm(i as nat));
            assert(self@[i].ptr.id() === cur_ptr.id());
            let tracked cur_perm = self.perms.borrow_mut().tracked_remove(i as nat);
            let next = cur_ptr.borrow(Tracked(&cur_perm)).next;
            proof {
                self.perms.borrow_mut().tracked_insert(i as nat, cur_perm);
                assert(self@[i].ptr.id() === cur_ptr.id());
                assert(self@ =~~= prev_self@);
            }
            let cptr = cur_ptr.clone();
            assert(cptr === cur_ptr);
            assert(cond_fn.requires((cptr,)));
            if cond_fn(cptr) {
                assert(self.head != INVALID_ADDR);
                let prevptr = prev_ptr.clone();
                assert(prevptr === prev_ptr);
                if let Option::Some((cur_ptr, cur_perm)) = self._remove(
                    prevptr,
                    cur_ptr,
                    Ghost(i),
                ) {
                    let ghost (mut keep, mut removeditems) = (prev_self@, ret@);
                    let ghost cur_ptr_perm = SpecListItem {
                        ptr: cur_ptr,
                        snp: cur_perm@@.snp(),
                        val: cur_perm@@.value().get_Some_0().val,
                    };
                    proof {
                        assert(self@ =~~= prev_self@.remove(i));
                        if removed == 0 {
                            assert(keep_idx[i] == i);
                        }
                        proof_remove_keep(
                            old(self)@,
                            keep,
                            removeditems,
                            &mut keep_idx,
                            &mut removed_idx,
                            i,
                        );
                        if removed_idx.len() == 1 {
                            assert(removed_idx[removed_idx.len() - 1] == i);
                        }
                    }
                    assert(cur_ptr.id() === old(self)@[removed_idx.last()].ptr.id());
                    assert(cur_ptr === old(self)@[removed_idx.last()].ptr) by {
                        cur_ptr.axiom_id_equal(old(self)@[removed_idx.last()].ptr);
                    }
                    ret.push(cur_ptr, cur_perm);
                    removed = removed + 1;
                    proof {
                        d = d - 1;
                    }
                } else {
                    assert(false);
                }
            } else {
                prev_ptr = SnpPPtr::from_usize(cur_ptr.to_usize());
                proof {
                    assert(self@[i].ptr.id() === cur_ptr.id());
                    self@[i].ptr.axiom_id_equal(cur_ptr);
                    assert(cur_ptr === self@[i].ptr);
                    assert(cond_fn.ensures((cur_ptr,), false));
                }
            }
            cur_ptr = SnpPPtr::from_usize(next);
            proof {
                d = d + 1;
                let ghost i = self@.len() - d - 1;
                assert forall|k: int| i < k < self@.len() implies cond_fn.ensures(
                    (self@[k].ptr,),
                    false,
                ) by {
                    if self@.len() != prev_self@.len() {
                        assert(prev_self@.len() == self@.len() + 1);
                        assert(i + 1 < k + 1 < prev_self@.len());
                        assert(prev_self@[k + 1] === self@[k]);
                        assert(cond_fn.ensures((prev_self@[k + 1].ptr,), false));
                    }
                }
                assert forall|k: int| 0 <= k < ret@.len() implies cond_fn.ensures(
                    (ret@[k].ptr,),
                    true,
                ) by {
                    if ret@.len() != prev_ret@.len() {
                        assert(prev_ret@.len() == ret@.len() - 1);
                        if (0 <= k < prev_ret@.len()) {
                            assert(prev_ret@[k] === ret@[k]);
                            assert(cond_fn.ensures((prev_ret@[k].ptr,), true));
                        }
                    }
                }
            }
        }
        (ret, Ghost(removed_idx), Ghost(keep_idx))
    }

    fn _remove(
        &mut self,
        prev: SnpPPtr<Node<T>>,
        cur: SnpPPtr<Node<T>>,
        Ghost(i): Ghost<int>,
    ) -> (ret: Option<(SnpPPtr<Node<T>>, Tracked<SnpPointsTo<Node<T>>>)>)
        requires
            1 <= i + 1 <= old(self)@.len(),
            old(self).perms@[i as nat]@.ptr_not_null_private_wf(cur),
            i + 1 < old(self)@.len() ==> old(self).perms@[(i + 1) as nat]@.ptr_not_null_private_wf(
                prev,
            ),
            i + 1 == old(self)@.len() ==> prev.is_null(),
            old(self).inv(),
        ensures
            self.inv(),
            ret.is_Some() ==> self@ =~~= old(self)@.remove(i),
            (old(self)@.len() > 0) ==> self@.len() + 1 == old(self)@.len(),
            (old(self)@.len() == 0) ==> ret.is_None(),
            (old(self)@.len() > 0) ==> ret === Some((cur, ret.get_Some_0().1))
                && ret.get_Some_0().1@ === old(self).perms@[i as nat] && old(self).spec_valid_item(
                cur,
                ret.get_Some_0().1@,
            ),
    {
        if self.is_empty() {
            return None;
        }
        assert(self@.len() >= 1);
        let ghost old_self = *self;
        proof {
            assert(self.inv());
            assert(self.wf_perm(i as nat));
            if prev.not_null() {
                assert(self.wf_perm((i + 1) as nat));
            }
            if (i != 0) {
                assert(self.wf_perm((i - 1) as nat));
            }
        }
        let tracked cur_perm = self.perms.borrow_mut().tracked_remove(i as nat);
        let cur_node = cur.borrow(Tracked(&cur_perm));
        if prev.check_valid() {
            let tracked mut prev_perm = self.perms.borrow_mut().tracked_remove((i + 1) as nat);
            proof {
                assert(prev_perm@.snp() === old_self.perms@[(i + 1) as nat]@.snp());
            }
            let mut prev_node = prev.take(Tracked(&mut prev_perm));
            prev_node.next = cur_node.next;
            prev.put(Tracked(&mut prev_perm), prev_node);
            assert(prev_perm@.snp() === old_self.perms@[(i + 1) as nat]@.snp());
            proof {
                if !cur_node.next.spec_valid_addr_with(spec_size::<Node<T>>()) {
                    assert(i == 0);
                }
                self.perms.borrow_mut().tracked_insert((i + 1) as nat, prev_perm);
            }
        } else {
            self.head = cur_node.next;
        }
        proof {
            self.ptrs@ = self.ptrs@.remove(i);
            assert(self.ptrs@.len() + 1 == old_self.ptrs@.len());
            let convert = |j: nat|
                if j < i {
                    j
                } else {
                    (j + 1) as nat
                };
            let key_map = Map::<nat, nat>::new(
                |j: nat| 0 <= j < self.ptrs@.len(),
                |j: nat| convert(j),
            );
            assert forall|j: nat| key_map.dom().contains(j) implies self.perms@.dom().contains(
                convert(j),
            ) by {
                assert(old(self).wf_perm(convert(j)));
            }
            self.perms.borrow_mut().tracked_map_keys_in_place(key_map);
            assert forall|j: nat| j < self.ptrs@.len() implies self.wf_perm(j) by {
                if j < i {
                    assert(old_self.wf_perm(j));
                } else {
                    assert(old_self.wf_perm(j + 1));
                    assert(self.ptrs@[j as int] === old_self.ptrs@[(j + 1) as int]);
                    assert(self.perms@[i as nat]@.value().get_Some_0().next
                        == old_self.perms@[i as nat]@.value().get_Some_0().next);
                    assert(self.perms@[i as nat]@.pptr() === old_self.perms@[i as nat + 1]@.pptr());
                    assert(self.perms@[i as nat]@.snp() === old_self.perms@[i as nat + 1]@.snp());
                    if j != i {
                        assert(self.perms@[j] === old_self.perms@[j + 1]);
                    }
                }
            }
            assert(self.wf_perms());
            assert(self.wf_head());
            assert(self.wf());
        }
        Some((cur, Tracked(cur_perm)))
    }
}

} // verus!
verismo_simple! {
impl<T> LinkedList<T>
where
    T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>,
{
        // prev: prev node ptr
        // idx: current node index (n - index)
        pub fn remove(&mut self, prev: SnpPPtr<Node<T>>, Ghost(idx): Ghost<int>) -> (ret: Tracked<SnpPointsTo<Node<T>>>)
        requires
            old(self).inv(),
            old(self)@.len() > 0,
            0 <= idx < old(self)@.len(),
            prev.not_null() ==> old(self)@[idx].ptr.not_null(),
            old(self).contains_ptr_at(prev, idx + 1)
        ensures
            self.inv(),
            self@ =~~= old(self)@.remove(
                idx
            ),
            self@.len() == old(self)@.len() - 1,
            ret@@.ptr_not_null_wf(old(self)@[idx].ptr),
            ret@@.value().is_Some(),
            ret@@.snp() === old(self)@[idx].snp,
        {
            let ghost i = idx;
            let ghost old_self = *self;
            proof {
                if prev.not_null() {
                    assert(self.wf_perm((i + 1) as nat));
                }
                assert(self.wf_perm(i as nat));
                if (i != 0) {
                    assert(self.wf_perm((i - 1) as nat));
                }
            }
            let prev_is_null = !prev.check_valid();
            let cur = if prev_is_null {
                self.head_ptr()
            } else {
                self.node_at(prev.clone(), Ghost(i + 1)).next_ptr()
            };
            let next = self.node_at(cur.clone(), Ghost(i)).next;
            if prev_is_null {
                self.head = next;
            } else {
                let tracked mut prev_perm = self.perms.borrow_mut().tracked_remove((i + 1) as nat);
                proof {
                    assert(prev_perm@.snp() === old_self.perms@[(i + 1) as nat]@.snp());
                }
                let Tracked(prev_perm) = prev.set_next(next, Tracked(prev_perm));
                proof{
                    self.perms.borrow_mut().tracked_insert((i + 1) as nat, prev_perm);
                 }
            }
            let tracked cur_perm = self.perms.borrow_mut().tracked_remove(i as nat);
            proof {
                self.ptrs@ = self.ptrs@.remove(i);
                assert(self.ptrs@.len() + 1 == old_self.ptrs@.len());

                let convert = |j: nat| if j < i {j} else {(j+1) as nat};
                let key_map = Map::<nat, nat>::new(
                    |j: nat| 0 <= j < self.ptrs@.len(),
                    |j: nat| convert(j)
                );
                assert forall |j: nat|
                    key_map.dom().contains(j)
                implies
                    self.perms@.dom().contains(convert(j))
                by {
                    assert(old(self).wf_perm(convert(j)));
                }
                let prev_perms = self.perms;
                self.perms.borrow_mut().tracked_map_keys_in_place(
                    key_map
                );

                if prev.not_null() {
                    assert(self.perms@[i as nat] === prev_perms@[(i + 1) as nat]);
                    assert(self.perms@[i as nat]@.value().get_Some_0().next == old_self.perms@[i as nat]@.value().get_Some_0().next);
                    assert(self.perms@[i as nat]@.pptr() === old_self.perms@[i as nat + 1]@.pptr());
                    assert(self.perms@[i as nat]@.snp() === old_self.perms@[i as nat+ 1]@.snp());
                }

                assert forall |j: nat| j < self.ptrs@.len()
                implies self.wf_perm(j)
                by {
                    if j < i {
                        assert(old_self.wf_perm(j));
                    } else {
                        assert(old_self.wf_perm(j + 1));
                        assert(self.ptrs@[j as int] === old_self.ptrs@[(j + 1) as int]);
                        if j != i {
                            assert(self.perms@[j] === old_self.perms@[j + 1]);
                        }
                    }
                }
                assert(self.wf_perms());
                assert(self.wf_head());
                assert(self.wf());
            }
            Tracked(cur_perm)
        }

        pub fn insert(&mut self, prev_ptr: SnpPPtr<Node<T>>,
            ptr: SnpPPtr::<Node<T>>, Ghost(idx): Ghost<int>,
            Tracked(perm): Tracked<SnpPointsTo<Node<T>>>)
        requires
            old(self).spec_valid_item(ptr, perm),
            (*old(self)).inv(),
            ptr.is_constant(),
            perm@.value().get_Some_0().next.is_constant() == old(self).head.is_constant(),
            ptr.uptr.is_constant() == old(self).head.is_constant(),
            perm@.value().is_Some(),
            old(self).contains_ptr_at(prev_ptr, idx),
        ensures
            (prev_ptr.not_null()) ==> (self@ =~~= old(self)@.insert(
            idx,
            SpecListItem{ptr: ptr, snp: perm@.snp(), val: perm@.value().get_Some_0().val})),
            prev_ptr.is_null() ==> self@ =~~= old(self)@.push(
                SpecListItem{ptr: ptr, snp: perm@.snp(), val: perm@.value().get_Some_0().val}),
            self@.len() == old(self)@.len() + 1,
            self.inv(),
        {
            let ghost i = idx as nat;
            let prev_is_null = !prev_ptr.check_valid();
            proof {
                if prev_ptr.not_null() {
                    assert(self.wf_perm(i));
                }
                if i > 0 {
                    assert(self.wf_perm((i - 1) as nat));
                }
            }
            let ghost old_perm = perm;
            let ghost old_self = *self;
            let ghost oldlen = self.ptrs@.len();
            let next = if prev_is_null {
                self.head
            } else {
                self.node_at(prev_ptr.clone(), Ghost(i as int)).next
            };
            let Tracked(perm) = ptr.set_next(next, Tracked(perm));
            if prev_is_null {
                self.head = ptr.to_usize();
            } else {
                let tracked prev_perm = self.perms.borrow_mut().tracked_remove(i);
                let Tracked(prev_perm) = prev_ptr.set_next(ptr.to_usize(), Tracked(prev_perm));
                proof {
                    self.perms.borrow_mut().tracked_insert(i as nat, prev_perm);
                }
            };
            proof {
                self.perms.borrow_mut().tracked_insert(self.ptrs@.len(), perm);
                assert(self@ =~~= old_self@);
                let ghost tmp_perms = self.perms;
                self.ptrs@ = self.ptrs@.insert(i as int,
                    SpecListItem{ptr: ptr, snp: old_perm@.snp(), val: old_perm@.value().get_Some_0().val});
                let convert = |j: nat|
                    if j < i {j} else if j == i {(self.ptrs@.len() - 1) as nat} else {(j-1) as nat};

                let key_map = Map::<nat, nat>::new(
                    |j: nat| 0 <= j < self.ptrs@.len(),
                    |j: nat| convert(j)
                );
                assert forall |j: nat|
                    key_map.dom().contains(j)
                implies
                    self.perms@.dom().contains(convert(j))
                by {
                    if j != i {
                        assert(old(self).wf_perm(convert(j)));
                    }
                }
                self.perms.borrow_mut().tracked_map_keys_in_place(
                    key_map
                );

                assert(self.perms@[i] === perm);
                if i > 0 {
                    assert(self.perms@[i].view().value().get_Some_0().next as int == self.ptrs@[i-1 as int].ptr.id());
                }
                assert(self.wf_perm(i));

                assert forall |j: nat| j < self.ptrs@.len()
                implies self.wf_perm(j)
                by {
                    if j < i {
                        assert(old_self.wf_perm(j));
                    } else if j == i {
                        assert(self.wf_perm(i));
                    } else {
                        assert(old_self.wf_perm((j - 1) as nat));
                        assert(self.ptrs@[j as int] === old_self.ptrs@[(j - 1) as int]);
                        assert(self.perms@[j as nat]@.snp() === old_self.perms@[(j - 1) as nat]@.snp());
                        assert(self.perms@[j as nat]@.pptr() === old_self.perms@[(j - 1) as nat]@.pptr());
                        assert(prev_ptr.not_null());
                        if j != i + 1 {
                            assert(self.perms@[j] === old_self.perms@[(j - 1) as nat]);
                            assert(self.wf_perm(j));
                        } else {
                            assert(self.perms@[j] === tmp_perms@[i]);
                            assert(self.perms@[i] === tmp_perms@[(self@.len() - 1) as nat]);
                            assert(self.wf_perm(j));
                        }
                    }
                }
                assert(self.wf_perms());
                assert(self.wf_head());
                assert(self.wf());
            }
        }
    }
}

verismo_simple! {
    impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> Default for LinkedList<T> {
        fn default() -> (ret: Self)
        ensures
            ret.inv(),
            ret@.len() == 0,
            ret.is_constant(),
        {
            LinkedList::new()
        }
    }
}

verismo_simple! {
impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> LinkedList<T> {
    pub fn head_node(&self) -> (ret: &Node<T>)
    requires
        self.inv(),
        self@.len() > 0 ,
    ensures
        ret.val === self@[self@.len() - 1].val,
        ret.next.spec_valid_addr_with(spec_size::<Node<T>>()) ==>
            ret.next == self@[self@.len() - 2].ptr.id(),
        ret.wf(),
    {
        let nodeptr = SnpPPtr::<Node<T>>::from_usize(self.head);
        let ghost i = self@.len() - 1;
        proof {
            assert(nodeptr.id() == self@[i].ptr.id());
            assert(self.wf_perm(i as nat))
        }
        self.node_at(nodeptr, Ghost(i))
    }

    pub fn node_at(&self, nodeptr: SnpPPtr::<Node<T>>, Ghost(i): Ghost<int>) -> (ret: &Node<T>)
    requires
        self.inv(),
        nodeptr.not_null(),
        nodeptr.is_constant(),
        0 <= i < self@.len(),
        self@[i].ptr.id() == nodeptr.id(),
    ensures
        ret.next.is_constant(),
        ret === &self.spec_node(i as nat),
        i > 0 ==> (
            ret.next == self@[i - 1].ptr.id()),
        i == 0 ==> !ret.next.spec_valid_addr_with(spec_size::<Node<T>>()),
        (i > 0) == (ret.next.spec_valid_addr_with(spec_size::<Node<T>>())),
        ret.val === self@[i].val,
        ret.wf(),
    {
        //let ghost i = self.spec_index_of(*nodeptr);
        proof {
            //assert(0 <= i < self@.len());
            //assert(self@[i].ptr.id() == nodeptr.id());
            assert(self.wf_perm(i as nat));
            //assert(self.perms@[i as nat]@.ptr_not_null_wf(self@[i].ptr));
            if i > 0 {
                assert(self.wf_perm((i - 1) as nat));
            }
        }
        let tracked node_perm = self.perms.borrow().tracked_borrow(i as nat);
        nodeptr.borrow(Tracked(node_perm))
    }

    pub fn update_node_at(&mut self, Ghost(i): Ghost<int>, nodeptr: SnpPPtr::<Node<T>>, v: T)
    requires
        old(self).inv(),
        0 <= i < old(self)@.len() && old(self)@[i].ptr.id() == nodeptr.id(),
        nodeptr.not_null(),
        nodeptr.is_constant(),
        v.wf(),
    ensures
        self@ =~~= old(self)@.update(i, SpecListItem{ptr: old(self)@[i].ptr,  snp: old(self)@[i].snp, val: v}),
        self.inv(),
    {
        let ghost prev = *self;
        let ghost i = i as nat;
        //let ghost i = self.spec_index_of(*nodeptr) as nat;
        proof {
            assert(self.wf_perm(i));
        }
        let node = self.node_at(nodeptr.clone(), Ghost(i as int));
        let next = node.next;
        let tracked mut perm = self.perms.borrow_mut().tracked_remove(i);
        nodeptr.replace(Tracked(&mut perm), Node{val: v, next});
        proof {
            self.ptrs@ = self@.update(i as int, SpecListItem{ptr: self@[i as int].ptr,  snp: self@[i as int].snp, val: v});
            self.perms.borrow_mut().tracked_insert(i, perm);
            assert forall |k: nat|
                0 <= k < self@.len() && k != i
            implies
                self@[k as int] === prev@[k as int]
            by{
                assert(prev.wf_perm(k));
                assert(self.perms@[k] === prev.perms@[k]);
            }
            assert forall |k: nat|
                0 <= k < self@.len()
            implies
                self.wf_perm(k)
            by {
                if k < self@.len() - 1 {
                    assert(prev.wf_perm((k + 1) as nat));
                }
                assert(prev.wf_perm(k));
                if k != i {
                    assert(self.wf_perm(k));
                } else {
                    if i > 0 {
                        assert(self@[i - 1] === prev@[i - 1]);
                        assert(self@[i - 1].ptr.id() === prev@[i - 1].ptr.id());
                        assert(self@[i - 1].snp === prev@[i - 1].snp);
                    }
                    assert(self.perms@[i] === perm);
                    assert(self.perms@[i]@.value().get_Some_0().val === v);
                    assert(self.perms@[i]@.value().get_Some_0().next === prev.perms@[i]@.value().get_Some_0().next);
                    assert(self.wf_perm(i));
                }
            }
        }
    }
}
}

================
File: ./source/verismo/src/global.rs
================

use super::*;
use crate::allocator::VeriSMoAllocator;
use crate::debug::Console;
use crate::lock::VSpinLock;
use crate::ptr::*;
use crate::security::richos_vmsa_invfn;
use crate::tspec::*;
use crate::tspec_e::*;

#[verifier::external]
mod trusted {
    use super::*;

    #[verifier::external]
    #[global_allocator]
    pub static _ALLOCATOR: VSpinLock<VeriSMoAllocator> = VSpinLock::new(VeriSMoAllocator::new());
}

#[verifier::external]
static _CONSOLE: VSpinLock<Console> = VSpinLock::new(Console);

use crate::pgtable_e::PtePerms;
#[verifier::external]
static _PT: VSpinLock<TrackedPTEPerms> =
    VSpinLock::new(TrackedPTEPerms { perms: Tracked::assume_new() });

use crate::security::TrackedSecretsOSAreaPerms;
#[verifier::external]
static _SECRET_PERM: VSpinLock<TrackedPTEPerms> =
    VSpinLock::new(TrackedPTEPerms { perms: Tracked::assume_new() });

use alloc::vec::Vec;

use crate::security::*;
#[verifier::external]
static _OSMEM: VSpinLock<Vec<OSMemEntry>> = VSpinLock::new(Vec::new());

use crate::snp::cpu::VmsaPage;
use crate::vbox::VBox;
#[verifier::external]
static _RICHOS_VMSA: VSpinLock<Vec<Option<VBox<VmsaPage>>>> = VSpinLock::new(Vec::new());

use trusted_hacl::SHA512Type;
#[verifier::external]
pub static _PCR: VSpinLock<Vec<SHA512Type>> = VSpinLock::new(Vec::new());

verus! {

pub closed spec fn g_range(id: Globals) -> (int, nat);

} // verus!
use self::trusted::_ALLOCATOR;
use crate::lock::MapRawLockTrait;
use crate::pgtable_e::TrackedPTEPerms;

verus! {

#[gen_shared_globals]
pub enum Globals {
    #[tname(_ALLOCATOR, VeriSMoAllocator, VeriSMoAllocator::invfn())]
    ALLOCATOR,
    #[tname(_CONSOLE, Console, Console::invfn())]
    CONSOLE,
    #[tname(_PT, TrackedPTEPerms, TrackedPTEPerms::invfn())]
    PT,
    #[tname(_SECRET_PERM, TrackedPTEPerms, TrackedPTEPerms::invfn())]
    SEC_PERM,
    #[tname(_OSMEM, Vec<OSMemEntry>, OSMemEntry::osmem_invfn())]
    OSMEM,
    #[tname(_RICHOS_VMSA, Vec<Option<VBox<VmsaPage>>>, richos_vmsa_invfn())]
    RICHOS_VMSA,
    #[tname(_PCR, Vec<SHA512Type>, crate::security::pcr::pcr_invfn())]
    PCR,
}

} // verus!
verus! {

pub open spec fn is_alloc_perm(alloc_perm: SnpPointsToData<VeriSMoAllocator>) -> bool {
    &&& alloc_perm.wf_const_default(spec_ALLOCATOR().ptrid())
    &&& alloc_perm.get_value()@.inv()
    &&& alloc_perm.get_value().is_constant()
}

} // verus!
verus! {

pub trait IsConsole {
    spec fn is_console(&self) -> bool;
}

impl IsConsole for SnpPointsToRaw {
    open spec fn is_console(&self) -> bool {
        &&& self@.wf_range(spec_CONSOLE().ptr_range())
        &&& self@.snp() === SwSnpMemAttr::spec_default()
    }
}

} // verus!

================
File: ./source/verismo/src/lib.rs
================

#![no_std] // don't link the Rust standard library
#![allow(unused_variables)]
#![allow(unused_imports)]
#![allow(dead_code)]
#![allow(unused)]
#![feature(abi_x86_interrupt)]
#![feature(specialization)]
#![allow(incomplete_features)]
#![allow(non_camel_case_types)]
#![allow(non_snake_case)]
#![allow(non_upper_case_globals)]
#![feature(never_type)]
#![feature(new_uninit)]
#![feature(core_intrinsics)]

extern crate alloc;

#[macro_use]
mod tspec;
#[macro_use]
mod arch;
mod primitives_e;
//mod trusted_hacl;
mod addr_e;
mod allocator;
mod boot;
mod bsp;
pub mod debug;
mod linkedlist;
mod lock;
mod mem;
mod pgtable_e;
mod ptr; // register -> ptr
#[macro_use]
mod registers;
mod global;
mod mshyper;
mod security;
pub mod snp;
mod trusted_hacl;
mod tspec_e;
mod vbox;
mod vcell;

================
File: ./source/verismo/src/trusted_hacl/hash_s.rs
================

use super::*;

verismo! {
    pub type SHA512Type = [u8; SHA512_LEN];
}

verus! {

pub const SHA512_LEN: usize = { 512 / 8 };

pub open spec fn spec_cal_sha512(input: SecSeqByte) -> SHA512Type;

} // verus!

================
File: ./source/verismo/src/trusted_hacl/stub.rs
================

use crate::addr_e::VAddr;
use crate::ptr::*;
use crate::registers::SnpCore;
use crate::snp::ghcb::*;
use crate::tspec::*;

verus! {

#[no_mangle]
pub extern "C" fn __stack_chk_fail(Tracked(core): Tracked<&mut SnpCore>)
    requires
        old(core).inv(),
    ensures
        false,
{
    vc_terminate(SM_EVERCRYPT_ERR(1), Tracked(core))
}

#[no_mangle]
pub extern "C" fn printf() {
}

#[no_mangle]
pub extern "C" fn __fprintf_chk() {
}

#[no_mangle]
pub extern "C" fn stderr() {
}

#[no_mangle]
pub extern "C" fn exit(Tracked(core): Tracked<&mut SnpCore>)
    requires
        old(core).inv(),
    ensures
        false,
{
    vc_terminate(SM_EVERCRYPT_ERR(2), Tracked(core))
}

#[no_mangle]
pub extern "C" fn __memcpy_chk(
    dst_addr: usize,
    src_addr: usize,
    len: usize,
    dst_len: usize,
    Tracked(dst_perm0): Tracked<&mut Map<int, SnpPointsToRaw>>,
    Tracked(src_perm): Tracked<&SnpPointsToRaw>,
) -> (ret: usize)
    requires
        src_perm@.ptr_not_null_wf(src_addr as int, len as nat),
        old(dst_perm0).contains_key(0),
        old(dst_perm0)[0]@.ptr_not_null_wf(dst_addr as int, dst_len as nat),
        len > 0,
        dst_len > 0,
        src_addr != 0,
        dst_addr != 0,
    ensures
        dst_perm0.contains_key(0),
        dst_perm0[0]@.range() === old(dst_perm0)[0]@.range(),
        dst_perm0[0]@.snp() === old(dst_perm0)[0]@.snp(),
        dst_perm0[0]@.wf(),
        ret == dst_addr ==> dst_perm0[0]@.bytes() =~~= (src_perm@.bytes() + old(
            dst_perm0,
        )[0]@.bytes().skip(len as int)),
        (ret == 0) ==> (*dst_perm0) =~~= (*old(dst_perm0)),
{
    if dst_len < len {
        return 0;
    } else {
        let tracked (mut dst_perm, right_perm) = dst_perm0.tracked_remove(0).trusted_split(
            len as nat,
        );
        mem_copy(src_addr, dst_addr, len, Tracked(src_perm), Tracked(&mut dst_perm));
        proof {
            dst_perm0.tracked_insert(0, dst_perm.trusted_join(right_perm));
        }
        return dst_addr;
    }
}

#[no_mangle]
pub extern "C" fn __printf_chk() {
}

#[no_mangle]
pub extern "C" fn malloc(size: usize, Tracked(core): Tracked<&mut SnpCore>)
    requires
        old(core).inv(),
    ensures
        false,
{
    vc_terminate(SM_EVERCRYPT_ERR(3), Tracked(core));
}

#[no_mangle]
pub extern "C" fn calloc(size: usize, Tracked(core): Tracked<&mut SnpCore>)
    requires
        old(core).inv(),
    ensures
        false,
{
    vc_terminate(SM_EVERCRYPT_ERR(4), Tracked(core))
}

#[no_mangle]
pub extern "C" fn free(addr: usize, Tracked(core): Tracked<&mut SnpCore>)
    requires
        old(core).inv(),
    ensures
        false,
{
    vc_terminate(SM_EVERCRYPT_ERR(5), Tracked(core))
}

} // verus!

================
File: ./source/verismo/src/trusted_hacl/mod.rs
================

mod enc_dec_t;
mod hash_s;
mod hash_t;
mod stub;

pub use enc_dec_t::*;
pub use hash_s::*;
pub use hash_t::*;

use crate::ptr::*;
use crate::tspec_e::*;
use crate::*;

================
File: ./source/verismo/src/trusted_hacl/enc_dec_t.rs
================

use hacl_sys::*;

use super::*;

const AAD_LEN: usize = 48;
const IV_LEN: usize = 12;
const MAX_AUTHTAG_LEN: usize = 32;
const AES_256_KEY_LEN: usize = 32;

verismo_simple! {
pub type AESKey256 = [u8; AES_256_KEY_LEN];
pub type AESAuthTag = [u8; MAX_AUTHTAG_LEN];
pub type AESAddInfo = [u8; AAD_LEN];
pub type AESInv = [u8; IV_LEN];

/*
pub open spec fn ensures_encrypted_to<C: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(key: AESKey256, cipher: C, vmpl: nat) -> bool {
    key.sec_bytes().is_fullsecret_to(vmpl) ==> (cipher).sec_bytes().is_constant_to(vmpl)
}

pub open spec fn ensures_decrypted_to<P: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(key: AESKey256, plain: P, vmpl: nat) -> bool {
    key.sec_bytes().is_fullsecret_to(vmpl) ==> (plain).sec_bytes().is_fullsecret_to(vmpl)
}
*/
pub open spec fn ensures_encrypted_to(key: AESKey256, cipher: SecSeqByte, vmpl: nat) -> bool {
    &&& key@.is_fullsecret_to(vmpl) ==> (cipher).is_constant_to(vmpl)
    &&& cipher.wf()
}

// The decrypted messge might be secret or non-secret.
pub open spec fn ensures_decrypted_to(key: AESKey256, plain: SecSeqByte, vmpl: nat) -> bool {
    &&& key@.is_fullsecret_to(vmpl) ==> (plain).wf()
    &&& plain.wf()
}

/*
#[verifier(external_body)]
pub fn sev_encrypt<P: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>, C: IsConstant + WellFormed + SpecSize+ VTypeCast<SecSeqByte>>(
    key: &AESKey256,
    plain: &P,
    iv: &AESInv,
    ad: &AESAddInfo,
    tag: &AESAuthTag,
    cipher: &mut C,
) -> (ret: u8)
requires
    spec_size::<P>() == spec_size::<C>(),
ensures
    ensures_encrypted_to(*key, *cipher, 1),
    ensures_encrypted_to(*key, *cipher, 2),
    ensures_encrypted_to(*key, *cipher, 3),
    ensures_encrypted_to(*key, *cipher, 4),
{
    unsafe {
        // encrypt the plaintext
        EverCrypt_AEAD_encrypt_expand_aes256_gcm_no_check(
            key as *const _ as *mut u8,
            iv as *const _ as *mut u8,
            IV_LEN as u32,
            ad as *const _ as *mut u8,
            AAD_LEN as u32,
            plain as *const _ as *mut u8,
            size_of::<P>() as u32,
            cipher as *mut _ as *mut u8,
            tag as *const _ as *mut u8,
        )
    }
}

#[verifier(external_body)]
/// key: must be a secret
/// plain: could become not secret after decrypt if need_declass
pub fn sev_decrypt<P: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>, C: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(
    key: &AESKey256,
    cipher: &C,
    iv: &AESInv,
    ad: &AESAddInfo,
    tag: &AESAuthTag,
    plain: &mut P,
) -> (ret: u8)
requires
    spec_size::<C>() == spec_size::<P>(),
ensures
    ensures_decrypted_to(*key, *plain, 1),
    ensures_decrypted_to(*key, *plain, 2),
    ensures_decrypted_to(*key, *plain, 3),
    ensures_decrypted_to(*key, *plain, 4),
{
    unsafe {
        // decrypt the plaintext
        EverCrypt_AEAD_decrypt_expand_aes256_gcm_no_check(
            key as *const _  as *mut u8,
            iv as *const _ as *mut u8,
            IV_LEN as u32,
            ad as *const _ as *mut u8,
            AAD_LEN as u32,
            cipher as *const _ as *mut u8,
            size_of::<C>() as u32,
            tag as *const _ as *mut u8,
            plain as *mut _ as *mut u8,
        )
    }
}
*/

#[verifier(external_body)]
pub fn raw_encrypt(
    key: &AESKey256,
    iv: &AESInv,
    ad: &AESAddInfo,
    tag: &AESAuthTag,
    len: u32_s,
    plain_addr: usize,
    cipher_addr: usize,
    Tracked(plain_perm): Tracked<&SnpPointsToRaw>,
    Tracked(cipher_perm): Tracked<&mut SnpPointsToRaw>,
) -> (ret: u8)
requires
    len == plain_perm@.size(),
    len == old(cipher_perm)@.size(),
ensures
    ensures_encrypted_to(*key, cipher_perm@.bytes(), 1),
    ensures_encrypted_to(*key, cipher_perm@.bytes(), 2),
    ensures_encrypted_to(*key, cipher_perm@.bytes(), 3),
    ensures_encrypted_to(*key, cipher_perm@.bytes(), 4),
    cipher_perm@.only_val_updated(old(cipher_perm)@),
{
    unsafe {
        // encrypt the plaintext
        EverCrypt_AEAD_encrypt_expand_aes256_gcm_no_check(
            key as *const _ as *mut u8,
            iv as *const _ as *mut u8,
            IV_LEN as u32,
            ad as *const _ as *mut u8,
            AAD_LEN as u32,
            plain_addr as *mut u8,
            len.into(),
            cipher_addr as *mut u8,
            tag as *const _ as *mut u8,
        )
    }
}

#[verifier(external_body)]
/// key: must be a secret
/// plain: could become not secret after decrypt if need_declass
pub fn raw_decrypt(
    key: &AESKey256,
    iv: &AESInv,
    ad: &AESAddInfo,
    tag: &AESAuthTag,
    len: u32_s,
    plain_addr: usize,
    cipher_addr: usize,
    Tracked(plain_perm): Tracked<&mut SnpPointsToRaw>,
    Tracked(cipher_perm): Tracked<&SnpPointsToRaw>,
) -> (ret: u8)
requires
    len == old(plain_perm)@.size(),
    len == cipher_perm@.size(),
ensures
    ensures_decrypted_to(*key, plain_perm@.bytes(), 1),
    ensures_decrypted_to(*key, plain_perm@.bytes(), 2),
    ensures_decrypted_to(*key, plain_perm@.bytes(), 3),
    ensures_decrypted_to(*key, plain_perm@.bytes(), 4),
    plain_perm@.only_val_updated(old(plain_perm)@),
{
    unsafe {
        // decrypt the plaintext
        EverCrypt_AEAD_decrypt_expand_aes256_gcm_no_check(
            key as *const _  as *mut u8,
            iv as *const _ as *mut u8,
            IV_LEN as u32,
            ad as *const _ as *mut u8,
            AAD_LEN as u32,
            cipher_addr as *mut u8,
            len.into(),
            tag as *const _ as *mut u8,
            plain_addr as *mut u8,
        )
    }
}

}

================
File: ./source/verismo/src/trusted_hacl/hash_t.rs
================

use hacl_sys::*;

use super::*;

verismo! {
#[verifier(external_body)]
pub fn trusted_cal_sha512<V: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(input: &V, dst: &mut SHA512Type)
requires
    input.wf(),
ensures
    *dst === spec_cal_sha512(input.vspec_cast_to()),
    (*dst).wf(),
    forall |vmpl:nat| 1 <= vmpl <= 4 ==> input.is_constant_to(vmpl) == dst.is_constant_to(vmpl)
{
    unsafe {
        Hacl_SHA3_sha3_512(
            size_of::<V>() as u32,
            input as *const _ as _,
            dst as *mut _ as _,
        )
    }
}
}

================
File: ./source/verismo/src/lock/spincell_e.rs
================

use core::marker;
use core::sync::atomic::{AtomicU64, Ordering};

use super::*;
use crate::addr_e::*;
use crate::ptr::*;
use crate::vcell::*;

verus! {

impl SpinLock {
    // requires: check no deadlock
    pub fn lock(
        &self,
        Tracked(lockperm): Tracked<LockPermRaw>,
        Tracked(core): Tracked<&CoreIdPerm>,
    ) -> (perm: (Tracked<LockPermRaw>, Tracked<SnpPointsToRaw>))
        requires
            lockperm@.is_unlocked(core@.cpu, self.id(), lockperm@.points_to.range()),
        ensures
            self.ensures_lock(lockperm@, perm.0@@, perm.1@@),
    {
        let got = false;
        let mut perm: Tracked<SnpPointsToRaw>;
        let tracked mut tmplockperm = lockperm;
        loop
            invariant_except_break
                tmplockperm@.is_unlocked(core@.cpu, self.id(), lockperm@.points_to.range()),
                tmplockperm === lockperm,
            ensures
                self.ensures_lock((lockperm)@, tmplockperm@, perm@@),
        {
            if let Some(tmpperm) = self.trylock(Tracked(&mut tmplockperm), Tracked(core)) {
                perm = tmpperm;
                break ;
            }
        }
        (Tracked(tmplockperm), perm)
    }
}

} // verus!
verismo_simple! {
#[derive(SpecGetter, SpecSetter)]
pub struct VSpinLock<T> {
    lock: SpinLock,
    data: crate::vcell::VCell<T>,
}
}

verus! {

impl<T> VSpinLock<T> {
    #[verifier(inline)]
    pub open spec fn lockid(&self) -> int {
        self.spec_lock().id()
    }

    #[verifier(inline)]
    pub open spec fn ptrid(&self) -> int {
        lockid_to_ptr(self.lockid())
    }

    #[verifier(inline)]
    pub open spec fn ptr_range(self) -> (int, nat) {
        (self.ptrid(), spec_size::<T>())
    }
}

} // verus!
verus! {

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> VSpinLock<T> {
    #[verifier(external_body)]
    pub fn ptr(&self) -> (ret: SnpPPtr<T>)
        requires
            self.is_constant(),
        ensures
            ret.id() == self.ptrid(),
            ret.is_constant(),
    {
        let addr = unsafe { &self.data as *const _ as usize };
        SnpPPtr::from_usize(addr)
    }

    pub open spec fn lock_requires(&self, cpu: nat, lockperm: LockPermToRaw) -> bool {
        &&& (lockperm).is_unlocked(cpu, self.lockid(), self.ptr_range())
        &&& self.is_constant()
    }

    pub open spec fn lock_default_mem_requires(&self, cpu: nat, lockperm: LockPermToRaw) -> bool {
        &&& self.lock_requires(cpu, lockperm)
        &&& lockperm.points_to.snp() === SwSnpMemAttr::spec_default()
    }

    pub open spec fn unlock_requires(
        &self,
        cpu: nat,
        lockperm: LockPermToRaw,
        memperm: SnpPointsToData<T>,
    ) -> bool {
        &&& (lockperm).is_locked(cpu, self.lockid(), self.ptr_range())
        &&& (lockperm).invfn.inv(memperm.get_value())
        &&& memperm.value.is_Some()
        &&& memperm.wf_at(self.lockid())
        &&& self.is_constant()
    }

    pub open spec fn unlock_default_mem_requires(
        &self,
        cpu: nat,
        lockperm: LockPermToRaw,
        memperm: SnpPointsToData<T>,
    ) -> bool {
        &&& self.unlock_requires(cpu, lockperm, memperm)
        &&& lockperm.points_to.snp() === SwSnpMemAttr::spec_default()
    }

    pub open spec fn ensures_lock(
        &self,
        oldp: LockPermToRaw,
        newp: LockPermToRaw,
        ret_perm: SnpPointsToData<T>,
    ) -> bool {
        &&& self.spec_lock().ensures_lock_value(oldp, newp, ret_perm)
    }

    pub open spec fn ensures_unlock(
        &self,
        oldp: LockPermToRaw,
        newp: LockPermToRaw,
        in_perm: SnpPointsToData<T>,
    ) -> bool {
        &&& newp === oldp.spec_set_locked(false).spec_set_points_to(in_perm.vspec_cast_to())
        &&& newp.wf()
    }

    pub fn acquire(
        &self,
        Tracked(lockperm): Tracked<LockPermRaw>,
        Tracked(core): Tracked<&CoreIdPerm>,
    ) -> (ret: (SnpPPtr<T>, Tracked<SnpPointsTo<T>>, Tracked<LockPermRaw>))
        requires
            self.lock_requires(core@.cpu, lockperm@),
        ensures
            self.ensures_lock((lockperm)@, ret.2@@, ret.1@@),
            ret.0.id() === self.ptrid(),
            ret.0.is_constant(),
            ret.1@@.wf_at(ret.0.id()),
    {
        proof {
            lockperm@.invfn.lemma_inv::<T>();
        }
        let (lockperm, rawperm) = self.lock.lock(Tracked(lockperm), Tracked(core));
        let Tracked(rawperm) = rawperm;
        let tracked ptrperm = rawperm.trusted_into();
        (self.ptr().clone(), Tracked(ptrperm), lockperm)
    }

    pub fn acquire_(
        &self,
        Tracked(lockperms): Tracked<&mut Map<int, LockPermRaw>>,
        Tracked(core): Tracked<&CoreIdPerm>,
    ) -> (ret: (SnpPPtr<T>, Tracked<SnpPointsTo<T>>))
        requires
            old(lockperms).contains_key(self.lockid()),
            self.lock_requires(core@.cpu, old(lockperms)[self.lockid()]@),
        ensures
            *lockperms =~~= old(lockperms).insert(
                self.spec_lock().id(),
                (lockperms)[self.spec_lock().id()],
            ),
            self.ensures_lock(old(lockperms)[self.lockid()]@, (lockperms)[self.lockid()]@, ret.1@@),
            self.unlock_requires(core@.cpu, lockperms[self.lockid()]@, ret.1@@),
            ret.0.id() === self.ptrid(),
            ret.0.is_constant(),
    {
        let tracked mut lockperm = lockperms.tracked_remove(self.spec_lock().id());
        let (ptr, perm, lockperm) = self.acquire(Tracked(lockperm), Tracked(core));
        let Tracked(lockperm) = lockperm;
        proof {
            lockperms.tracked_insert(self.spec_lock().id(), lockperm);
        }
        (ptr, perm)
    }

    pub fn release(
        &self,
        Tracked(lockperm): Tracked<&mut LockPermRaw>,
        Tracked(perm): Tracked<SnpPointsTo<T>>,
        Tracked(core): Tracked<&CoreIdPerm>,
    )
        requires
            self.unlock_requires(core@.cpu, old(lockperm)@, perm@),
        ensures
            self.ensures_unlock(old(lockperm)@, lockperm@, perm@),
            lockperm@.wf(),
    {
        proof {
            lockperm@.invfn.lemma_inv::<T>();
        }
        let tracked rawperm = perm.trusted_into_raw();
        self.lock.unlock(Tracked(lockperm), Tracked(rawperm), Tracked(core));
    }

    pub const fn new(value: T) -> Self {
        VSpinLock { lock: SpinLock::new(), data: VCell::new(value) }
    }

    pub fn release_(
        &self,
        Tracked(lockperms): Tracked<&mut Map<int, LockPermRaw>>,
        Tracked(perm): Tracked<SnpPointsTo<T>>,
        Tracked(core): Tracked<&CoreIdPerm>,
    )
        requires
            old(lockperms).contains_key(self.lockid()),
            self.unlock_requires(core@.cpu, old(lockperms)[self.lockid()]@, perm@),
        ensures
            self.ensures_unlock(
                old(lockperms)[self.spec_lock().id()]@,
                lockperms[self.spec_lock().id()]@,
                perm@,
            ),
            *lockperms =~~= old(lockperms).insert(
                self.spec_lock().id(),
                lockperms[self.spec_lock().id()],
            ),
    {
        let tracked mut lockperm = lockperms.tracked_remove(self.spec_lock().id());
        self.release(Tracked(&mut lockperm), Tracked(perm), Tracked(core));
        proof {
            lockperms.tracked_insert(self.spec_lock().id(), lockperm);
        }
    }
}

} // verus!

================
File: ./source/verismo/src/lock/spin_perm_s.rs
================

use core::marker;
use core::sync::atomic::{AtomicU64, Ordering};

use super::*;
use crate::addr_e::*;
use crate::ptr::*;

verus! {

#[verifier(external_body)]
pub tracked struct LockPermRaw {
    no_copy: NoCopy,
}

pub ghost struct InvRawFn {
    pub invfn: spec_fn(SecSeqByte) -> bool,
}

#[verifier(inline)]
pub open spec fn lockid_to_ptr(lockid: int) -> int {
    lockid
}

#[verifier(inline)]
pub open spec fn ptrid_to_lockid(ptrid: int) -> int {
    ptrid
}

impl InvRawFn {
    pub open spec fn value_invfn<T: VTypeCast<SecSeqByte>>(&self) -> spec_fn(T) -> bool {
        |v: T| (self.invfn)(v.vspec_cast_to())
    }

    pub open spec fn inv<T: VTypeCast<SecSeqByte>>(&self, v: T) -> bool {
        self.value_invfn()(v)
    }

    pub proof fn lemma_inv<T: VTypeCast<SecSeqByte>>(&self)
        ensures
            forall|v: T| #[trigger] self.inv(v) == (self.invfn)(v.vspec_cast_to()),
    {
    }
}

#[derive(SpecSetter, SpecGetter)]
pub ghost struct LockPermToRaw {
    pub locked: bool,
    pub cpu: nat,
    pub points_to: SnpPointsToBytes,
    pub invfn: InvRawFn,
}

impl LockPermToRaw {
    pub open spec fn lockid(&self) -> int {
        ptrid_to_lockid(self.points_to.range().0)
    }

    pub open spec fn ptr_range(&self) -> (int, nat) {
        self.points_to.range()
    }

    pub open spec fn is_clean_lock_for(&self, ptr_range: (int, nat), cpu: nat) -> bool {
        &&& self.ptr_range() == ptr_range
        &&& !self.points_to.wf()
        &&& self.cpu == cpu
        &&& !self.locked
    }
}

impl LockPermRaw {
    pub spec fn view(&self) -> LockPermToRaw;

    #[verifier(external_body)]
    pub proof fn trusted_bind_new<T: VTypeCast<SecSeqByte>>(
        tracked &mut self,
        inv: spec_fn(T) -> bool,
        tracked points_to: SnpPointsToRaw,
    )
        requires
            inv(points_to@.value()),
            old(self)@.ptr_range() == points_to@.range(),
            !old(self)@.locked,
        ensures
            self@.invfn.value_invfn() === inv,
            self@ === old(self)@.spec_set_invfn(self@.invfn).spec_set_points_to(points_to@),
    {
        unimplemented!{}
    }

    pub proof fn tracked_bind_new<T: VTypeCast<SecSeqByte> + IsConstant + WellFormed + SpecSize>(
        tracked &mut self,
        inv: spec_fn(T) -> bool,
        tracked points_to: SnpPointsTo<T>,
    )
        requires
            points_to@.value().is_Some(),
            points_to@.wf_not_null_at(points_to@.id()) || (points_to@.wf() && spec_size::<T>()
                == 0),
            inv(points_to@.get_value()),
            old(self)@.ptr_range() == points_to@.range_id(),
            !old(self)@.locked,
        ensures
            self@.invfn.value_invfn() === inv,
            self@ === old(self)@.spec_set_invfn(self@.invfn).spec_set_points_to(
                points_to@.vspec_cast_to(),
            ),
            self@.wf(),
    {
        let tracked raw_points_to = points_to.tracked_into_raw();
        self.trusted_bind_new(inv, raw_points_to);
        self@.invfn.lemma_inv::<T>();
        assert((self@.invfn.invfn)(raw_points_to@.bytes()))
    }
}

impl LockPermToRaw {
    #[verifier(inline)]
    pub open spec fn wf(&self) -> bool {
        &&& if self.points_to.range().1 > 0 {
            // Memory access
            self.points_to.wf_not_null(self.points_to.range())
        } else {
            // Other locks (e.g., console or other devices)
            self.points_to.wf()
        }
        &&& (self.invfn.invfn)(self.points_to.bytes())
    }

    #[verifier(inline)]
    pub open spec fn wf_for(&self, lockid: int, range: (int, nat)) -> bool {
        &&& self.wf()
        &&& self.lockid() == lockid
        &&& self.points_to.range() === range
    }

    pub open spec fn is_unlocked(&self, cpu: nat, lockid: int, range: (int, nat)) -> bool {
        &&& !self.locked
        &&& self.cpu == cpu
        &&& self.wf_for(lockid, range)
    }

    pub open spec fn is_locked(&self, cpu: nat, lockid: int, range: (int, nat)) -> bool {
        &&& self.locked
        &&& self.cpu == cpu
        &&& self.wf_for(lockid, range)
    }
}

pub trait MapRawLockTrait {
    spec fn is_locked(&self, cpu: nat, lockid: int, range: (int, nat)) -> bool;

    spec fn is_unlocked(&self, cpu: nat, lockid: int, range: (int, nat)) -> bool;

    spec fn inv(&self, cpu: nat) -> bool;

    spec fn inv_locked(&self, cpu: nat, lockid: Set<int>) -> bool;

    spec fn contains_lock(&self, lockid: int, ptr_range: (int, nat)) -> bool;

    spec fn updated_lock(&self, prev: &Self, locks: Set<int>) -> bool;

    spec fn contains_clean_locks(&self, cpu: nat, locks: Map<int, (int, nat)>) -> bool;

    proof fn lemma_lock_update_auto()
        ensures
            forall|prev: &Self, cur: &Self, next: &Self, locks1: Set<int>, locks2: Set<int>|
                (#[trigger] cur.updated_lock(prev, locks1) && #[trigger] next.updated_lock(
                    cur,
                    locks2,
                )) ==> next.updated_lock(prev, locks1.union(locks2)),
    ;
}

pub trait MapLockContains<T> {
    spec fn contains_vlock(&self, lock: VSpinLock<T>) -> bool;
}

pub type LockMap = Map<int, LockPermRaw>;

impl MapRawLockTrait for LockMap {
    #[verifier(inline)]
    open spec fn is_locked(&self, cpu: nat, lockid: int, range: (int, nat)) -> bool {
        &&& self.contains_key(lockid)
        &&& (#[trigger] self[lockid])@.is_locked(cpu, lockid, range)
    }

    #[verifier(inline)]
    open spec fn is_unlocked(&self, cpu: nat, lockid: int, range: (int, nat)) -> bool {
        &&& self.contains_key(lockid)
        &&& (#[trigger] self[lockid])@.is_unlocked(cpu, lockid, range)
    }

    open spec fn inv(&self, cpu: nat) -> bool {
        self.inv_locked(cpu, set![])
    }

    open spec fn inv_locked(&self, cpu: nat, locks: Set<int>) -> bool {
        &&& forall|id: int|
            self.contains_key(id) && !locks.contains(id) ==> (#[trigger] self[id])@.is_unlocked(
                cpu,
                id,
                self[id]@.points_to.range(),
            )
        &&& forall|id: int|
            self.contains_key(id) && locks.contains(id) ==> (#[trigger] self[id])@.is_locked(
                cpu,
                id,
                self[id]@.points_to.range(),
            )
    }

    open spec fn contains_lock(&self, lockid: int, ptr_range: (int, nat)) -> bool {
        &&& self.contains_key(lockid)
        &&& self[lockid]@.points_to.range() === ptr_range
    }

    open spec fn updated_lock(&self, prev: &Self, locks: Set<int>) -> bool {
        &&& prev.dom() =~~= self.dom()
        &&& (forall|id: int|
            (locks.contains(id) && #[trigger] self.contains_key(id)) ==> (
            self[id])@.points_to.only_val_updated(prev[id]@.points_to))
        &&& (forall|id: int|
            (!locks.contains(id) && #[trigger] self.contains_key(id)) ==> (self[id]) === prev[id])
    }

    open spec fn contains_clean_locks(&self, cpu: nat, locks: Map<int, (int, nat)>) -> bool {
        &&& forall|id: int|
            locks.contains_key(id) ==> (#[trigger] self[id])@.is_clean_lock_for(locks[id], cpu)
                && self.contains_key(id)
        &&& forall|id: int| locks.contains_key(id) ==> #[trigger] self.contains_key(id)
    }

    proof fn lemma_lock_update_auto() {
        assert forall|prev: &Self, cur: &Self, next: &Self, locks1: Set<int>, locks2: Set<int>|
            (#[trigger] cur.updated_lock(prev, locks1) && #[trigger] next.updated_lock(
                cur,
                locks2,
            )) implies next.updated_lock(prev, locks1.union(locks2)) by {
            let locks = locks1.union(locks2);
            assert forall|id: int|
                locks.contains(id) && prev.contains_key(id) implies next.contains_key(id) && (
            #[trigger] next[id])@.points_to.only_val_updated(prev[id]@.points_to) by {
                if (locks1.contains(id)) {
                    assert(cur[id]@.points_to.only_val_updated(prev[id]@.points_to));
                } else {
                    assert(cur[id] === prev[id]);
                    assert(cur[id]@.points_to.only_val_updated(prev[id]@.points_to));
                }
            }
            assert forall|id: int|
                !locks.contains(id) && prev.contains_key(id) implies #[trigger] next[id]
                === prev[id] && next.contains_key(id) by {
                assert(!locks1.contains(id));
                assert(!locks2.contains(id));
                assert(cur[id] === prev[id]);
                assert(next[id] === cur[id]);
            }
            // VERUS bugs?: ASSERTIONS are required to pass verification.

            assert(forall|id: int|
                locks.contains(id) && prev.contains_key(id) ==> next.contains_key(id) && (
                #[trigger] next[id])@.points_to.only_val_updated(prev[id]@.points_to));
            assert(forall|id: int|
                !locks.contains(id) && prev.contains_key(id) ==> #[trigger] next[id] === prev[id]
                    && next.contains_key(id));
        }
    }
}

impl<T> MapLockContains<T> for Map<int, LockPermRaw> {
    #[verifier(inline)]
    open spec fn contains_vlock(&self, lock: VSpinLock<T>) -> bool {
        &&& self.contains_key(lock.lockid())
        &&& self[lock.lockid()]@.points_to.range() === lock.ptr_range()
    }
}

} // verus!

================
File: ./source/verismo/src/lock/mod.rs
================

mod spin_perm_s;
mod spin_t;
mod spincell_e;
//#[macro_use]
//mod spin_macro_t;

//pub use spin_e::*;
pub use spin_perm_s::*;
pub use spin_t::*;
pub use spincell_e::*;

use crate::addr_e::*;
use crate::ptr::*;
use crate::registers::{CoreIdPerm, CoreMode};
use crate::tspec_e::*;

//spin::def_lock!(CONSOLE_LOCK, console);

================
File: ./source/verismo/src/lock/spin_t.rs
================

use core::marker;
use core::sync::atomic::{AtomicU64, Ordering};

use super::*;
use crate::addr_e::*;
use crate::ptr::*;

verus! {

#[verifier(external_body)]
pub struct SpinLock {
    current: AtomicU64,
    holder: AtomicU64,
}

} // verus!
verus! {

impl IsConstant for SpinLock {
    open spec fn is_constant(&self) -> bool;

    open spec fn is_constant_to(&self, vmpl: nat) -> bool;
}

impl WellFormed for SpinLock {
    open spec fn wf(&self) -> bool;
}

impl VTypeCast<SecSeqByte> for SpinLock {
    open spec fn vspec_cast_to(self) -> SecSeqByte;
}

impl SpecSize for SpinLock {
    open spec fn spec_size_def() -> nat;
}

impl SpinLock {
    pub spec fn id(self) -> int;

    #[verifier(external_body)]
    pub const fn new() -> (ret: Self) {
        SpinLock { current: AtomicU64::new(1), holder: AtomicU64::new(1) }
    }

    pub open spec fn ensures_lock(
        &self,
        oldp: LockPermToRaw,
        newp: LockPermToRaw,
        ret_perm: SnpPointsToBytes,
    ) -> bool {
        &&& newp === oldp.spec_set_locked(true)
        &&& ret_perm.snp() === oldp.points_to.snp()
        &&& ret_perm.range() === oldp.points_to.range()
        &&& ret_perm.wf()
        &&& (oldp.invfn.invfn)(ret_perm.bytes())
    }

    pub open spec fn ensures_lock_value<
        T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>,
    >(&self, oldp: LockPermToRaw, newp: LockPermToRaw, ret_perm: SnpPointsToData<T>) -> bool {
        &&& newp === oldp.spec_set_locked(true)
        &&& ret_perm.snp() === oldp.points_to.snp()
        &&& ret_perm.range_id() === oldp.points_to.range()
        &&& ret_perm.wf()
        &&& ret_perm.value().is_Some()
        &&& oldp.invfn.inv(ret_perm.get_value())
    }

    pub open spec fn ensures_unlock(
        &self,
        oldp: LockPermToRaw,
        newp: LockPermToRaw,
        points_to: SnpPointsToBytes,
    ) -> bool {
        &&& newp === oldp.spec_set_locked(false).spec_set_points_to(points_to)
    }

    #[verifier(external_body)]
    pub fn trylock(
        &self,
        Tracked(lockperm): Tracked<&mut LockPermRaw>,
        Tracked(core): Tracked<&CoreIdPerm>,
    ) -> (ret: Option<Tracked<SnpPointsToRaw>>)
        requires
            old(lockperm)@.is_unlocked(core@.cpu, self.id(), old(lockperm)@.points_to.range()),
        ensures
            ret.is_Some() ==> self.ensures_lock(old(lockperm)@, lockperm@, ret.get_Some_0()@@),
            ret.is_None() ==> lockperm === old(lockperm),
    {
        if self.unverified_trylock() {
            Some(Tracked::assume_new())
        } else {
            None
        }
    }

    #[verifier(external_body)]
    pub fn unlock(
        &self,
        Tracked(lockperm): Tracked<&mut LockPermRaw>,
        Tracked(perm): Tracked<SnpPointsToRaw>,
        Tracked(core): Tracked<&CoreIdPerm>,
    )
        requires
            old(lockperm)@.is_locked(core@.cpu, self.id(), perm@.range()),
            perm@.wf(),
        ensures
            self.ensures_unlock(old(lockperm)@, lockperm@, perm@),
    {
        self.unverified_unlock();
    }

    // requires: check no deadlock
    #[verifier(external)]
    fn unverified_trylock(&self) -> bool {
        let ticket: u64 = self.current.fetch_add(1, Ordering::Relaxed);
        loop {
            let h: u64 = self.holder.load(Ordering::Acquire);
            if h == ticket {
                break ;
            }
        }
        true
    }

    #[verifier(external)]
    fn unverified_unlock(&self) {
        self.holder.fetch_add(1, Ordering::Release);
    }
}

#[verifier(external_body)]
pub fn fence() {
    core::sync::atomic::fence(Ordering::SeqCst);
}

} // verus!

================
File: ./source/verismo/src/vbox/vbox.rs
================

use alloc::boxed::Box;

use super::*;
use crate::addr_e::{AddrTrait, OnePage, PageTrait, SpecAddrTrait, SpecPageTrait};
use crate::allocator::VeriSMoAllocator;
use crate::arch::addr_s::PAGE_SIZE;
use crate::debug::VPrintAtLevel;
use crate::global::*;
use crate::lock::{LockPermRaw, MapLockContains, MapRawLockTrait};
use crate::registers::CoreIdPerm;
use crate::snp::ghcb::*;
use crate::snp::SnpCoreSharedMem;

verus! {

#[verifier(external_body)]
#[verifier::reject_recursive_types_in_ground_variants(T)]
pub struct VBox<T> {
    pub b: Box<T>,
}

impl<T: IsConstant> IsConstant for VBox<T> {
    open spec fn is_constant(&self) -> bool {
        &&& self.view().is_constant()
    }

    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        &&& self.view().is_constant_to(vmpl)
    }
}

pub closed spec fn spec_box_size() -> nat;

impl<T> SpecSize for VBox<T> {
    open spec fn spec_size_def() -> nat {
        spec_box_size()
    }
}

impl<T> VTypeCast<SecSeqByte> for VBox<T> {
    closed spec fn vspec_cast_to(self) -> SecSeqByte;
}

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> VBox<T> {
    pub fn set(self, other: Self) -> (ret: Self)
        requires
            self.wf(),
            other.wf(),
        ensures
            ret@ === other@,
            self.snp() === ret.snp(),
            ret.only_val_updated(self),
    {
        proof {
            proof_cast_from_seq_unique(other@);
        }
        let (dest, Tracked(dest_perm)) = self.into_raw();
        let (src, Tracked(src_perm)) = other.into_raw();
        let tracked mut src_perm = src_perm.tracked_into_raw();
        let tracked mut dest_perm = dest_perm.tracked_into_raw();
        mem_copy(
            src.to_usize(),
            dest.to_usize(),
            size_of::<T>(),
            Tracked(&mut src_perm),
            Tracked(&mut dest_perm),
        );
        let other = VBox::<T>::from_raw(src.to_usize(), Tracked(src_perm.tracked_into()));
        VBox::from_raw(dest.to_usize(), Tracked(dest_perm.tracked_into()))
    }

    pub fn to<T2: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(self) -> (ret: VBox<
        T2,
    >)
        requires
            spec_size::<T>() == spec_size::<T2>(),
            is_castable::<T, T2>(self@),
            self.wf(),
        ensures
            self.snp() === ret.snp(),
            self.id() == ret.id(),
            VTypeCast::<SecSeqByte>::vspec_cast_to(self@) === VTypeCast::<
                SecSeqByte,
            >::vspec_cast_to(ret@),
            ret.wf(),
    {
        let (ptr, Tracked(perm0)) = self.into_raw();
        let tracked raw = perm0.tracked_into_raw();
        let tracked perm = raw.tracked_into();
        proof {
            assert(perm@.wf());
            proof_into_is_constant::<T, SecSeqByte>(perm0@.get_value());
            proof_into_is_constant::<SecSeqByte, T2>(raw@.bytes());
        }
        VBox::from_raw(ptr.to_usize(), Tracked(perm))
    }
}

impl<T: IsConstant + WellFormed> VBox<T> {
    pub open spec fn is_page(&self) -> bool {
        &&& self.id() % (PAGE_SIZE as int) == 0
        &&& self.wf()
    }

    pub open spec fn is_shared_page(&self) -> bool {
        &&& self.is_page()
        &&& self.snp() === SwSnpMemAttr::shared()
    }

    pub open spec fn is_default_page(&self) -> bool {
        &&& self.is_page()
        &&& self.snp() === SwSnpMemAttr::spec_default()
    }

    pub open spec fn is_vmpl0_private_page(&self) -> bool {
        &&& self.is_page()
        &&& self.snp().is_vmpl0_private()
    }

    pub open spec fn is_vmsa_page(&self) -> bool {
        &&& self.is_page()
        &&& self.snp().is_vmpl0_private()
        &&& self.snp().rmp@.spec_vmsa()
    }
}

impl<T: IsConstant + WellFormed> WellFormed for VBox<T> {
    open spec fn wf(&self) -> bool {
        &&& inv_snp_value(self.snp(), self.view())
        &&& self.id().spec_valid_addr_with(spec_size::<T>())
    }
}

impl<T> VBox<T> {
    pub closed spec fn view(&self) -> T;

    pub closed spec fn id(&self) -> int;

    pub closed spec fn snp(&self) -> SwSnpMemAttr;

    pub open spec fn spec_eq(self, other: Self) -> bool {
        &&& self.id() == other.id()
        &&& self.snp() == other.snp()
        &&& self.view() === other.view()
    }

    pub open spec fn only_val_updated(self, prev: Self) -> bool {
        &&& self.id() == prev.id()
        &&& self.snp() == prev.snp()
    }
}

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> VBox<T> {
    pub fn new_aligned_uninit(align: usize, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret:
        VBox<T>)
        requires
            (*old(cs)).inv_ac(),
            spec_bit64_is_pow_of_2(align as int),
            spec_size::<T>() >= VeriSMoAllocator::spec_minsize(),
        ensures
            (*cs).inv_ac(),
            (*cs).only_lock_reg_updated((*old(cs)), set![], set![spec_ALLOCATOR_lockid()]),
            ret.id() % (align as int) == 0,
            ret.id().spec_valid_addr_with(spec_size::<T>()),
            ret.snp() === SwSnpMemAttr::spec_default(),
            ret.wf(),
            ret@.is_constant(),
    {
        let tracked perm = cs.lockperms.tracked_remove(spec_ALLOCATOR_lockid());
        let tracked mut perm0 = Map::tracked_empty();
        proof {
            assert(!perm@.locked);
            assert(perm@.lockid() === spec_ALLOCATOR_lockid());
            assert(perm@.invfn.value_invfn() === VeriSMoAllocator::invfn());
            perm0.tracked_insert(0, perm);
        }
        let size = size_of::<T>();
        let result = ALLOCATOR().alloc_aligned(
            size,
            align,
            Tracked(&mut perm0),
            Tracked(&cs.snpcore.coreid),
        );
        proof {
            cs.lockperms.tracked_insert(spec_ALLOCATOR_lockid(), perm0.tracked_remove(0));
        }
        if result.is_err() {
            new_strlit("\nfailed new_aligned_uninit\n").leak_debug();
            vc_terminate_debug(SM_TERM_MEM, Tracked(cs));
        }
        let (addr, Tracked(perm)) = result.unwrap();
        VBox::from_raw(addr, Tracked(perm.tracked_into()))
    }

    #[inline(always)]
    pub fn new_uninit(Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret: VBox<T>)
        requires
            (*old(cs)).lockperms.contains_vlock(spec_ALLOCATOR()),
            (*old(cs)).inv(),
            spec_size::<T>() >= VeriSMoAllocator::spec_minsize(),
        ensures
            (*cs).only_lock_reg_updated((*old(cs)), set![], set![spec_ALLOCATOR_lockid()]),
            (*cs).inv(),
            ret.id().spec_valid_addr_with(spec_size::<T>()),
            ret.snp() === SwSnpMemAttr::spec_default(),
            ret@.is_constant(),
    {
        VBox::new_aligned_uninit(1, Tracked(cs))
    }

    #[inline(always)]
    pub fn new_aligned(align: usize, val: T, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret:
        VBox<T>)
        requires
            (*old(cs)).inv_ac(),
            spec_bit64_is_pow_of_2(align as int),
            spec_size::<T>() >= VeriSMoAllocator::spec_minsize(),
            val.wf(),
        ensures
            (*cs).inv_ac(),
            (*cs).only_lock_reg_updated((*old(cs)), set![], set![spec_ALLOCATOR_lockid()]),
            ret.id() % (align as int) == 0,
            ret.snp() === SwSnpMemAttr::spec_default(),
            ret@ === val,
            ret.wf(),
    {
        let b = Self::new_aligned_uninit(align, Tracked(cs));
        let (ptr, Tracked(mut perm)) = b.into_raw();
        ptr.replace(Tracked(&mut perm), val);
        VBox::from_raw(ptr.to_usize(), Tracked(perm))
    }

    #[inline(always)]
    pub fn new_shared_page(
        align: usize,
        ghcb_h: GhcbHandle,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: (VBox<T>, GhcbHandle))
        requires
            (*old(cs)).inv_ac(),
            ghcb_h.ghcb_wf(),
            spec_bit64_is_pow_of_2(align as int),
            align > 0,
            align % PAGE_SIZE == 0,
            spec_size::<T>() == PAGE_SIZE,
        ensures
            (*cs).inv_ac(),
            (*cs).only_lock_reg_coremode_updated(
                (*old(cs)),
                set![],
                set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
            ),
            ret.0.is_shared_page(),
            ret.0@.is_constant(),
            ret.1.ghcb_wf(),
            ret.1.only_val_updated(ghcb_h),
    {
        let ghost cs1 = *cs;
        let onepage = VBox::<T>::new_aligned_uninit(align, Tracked(cs));
        proof {
            assert(onepage.id() % PAGE_SIZE as int == 0) by {
                proof_mod_propogate(onepage.id(), align as int, 0x1000);
            }
            assert(onepage.is_page());
        }
        let ghost cs2 = *cs;
        let ret = onepage.mk_shared_page(ghcb_h, Tracked(cs));
        proof {
            cs1.lemma_update_prop(
                cs2,
                *cs,
                set![],
                set![spec_ALLOCATOR_lockid()],
                set![],
                set![spec_PT_lockid()],
            );
            assert(set![spec_ALLOCATOR_lockid()].union(set![spec_PT_lockid()])
                =~~= set![spec_ALLOCATOR_lockid(), spec_PT_lockid()])
        }
        ret
    }

    #[inline(always)]
    pub fn new(v: T, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret: VBox<T>)
        requires
            (*old(cs)).lockperms.contains_vlock(spec_ALLOCATOR()),
            (*old(cs)).inv(),
            spec_size::<T>() >= VeriSMoAllocator::spec_minsize(),
            v.wf(),
        ensures
            cs.only_lock_reg_updated((*old(cs)), set![], set![spec_ALLOCATOR().lockid()]),
            cs.inv(),
            ret.id().spec_valid_addr_with(spec_size::<T>()),
            ret@ === v,
            ret.snp() === SwSnpMemAttr::spec_default(),
            ret.wf(),
    {
        VBox::new_aligned(1, v, Tracked(cs))
    }

    pub fn mk_shared_page(
        self,
        ghcb_h: GhcbHandle,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: (VBox<T>, GhcbHandle))
        requires
            (*old(cs)).inv_ac(),
            ghcb_h.ghcb_wf(),
            self.is_default_page(),
            self@.is_constant(),
            spec_size::<T>() == PAGE_SIZE,
        ensures
            cs.inv_ac(),
            cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![spec_PT_lockid()]),
            ret.0.is_shared_page(),
            ret.0@.is_constant(),
            ret.1.ghcb_wf(),
            ret.1.only_val_updated(ghcb_h),
    {
        let onepage = self;
        let (onepage, Tracked(mut perm)) = onepage.into_raw();
        let page: usize = onepage.to_usize().to_page();
        let tracked mut page_perms = Map::tracked_empty();
        let tracked prev_perm = perm.tracked_into_raw();
        proof {
            page_perms.tracked_insert(page as int, prev_perm);
            assert(page_perms.contains_key(page as int));
            let perm = page_perms[page as int];
            assert(perm@.wf_const_default(((page as int).to_addr(), PAGE_SIZE as nat)));
        }
        let ghcb_h = ghcb_h.mk_shared(page as u64, 1, Tracked(cs), Tracked(&mut page_perms));
        proof {
            crate::snp::mem::lemma_mk_shared_default_to_shared(
                prev_perm@,
                page_perms[page as int]@,
            );
        }
        let onepage = VBox::from_raw(
            page.to_addr(),
            Tracked(page_perms.tracked_remove(page as int).tracked_into()),
        );
        (onepage, ghcb_h)
    }
}

} // verus!
verismo_simple! {
impl<T: IsConstant + WellFormed + SpecSize> VBox<T> {
    #[verifier(external_body)]
    pub fn borrow<'a>(&'a self) -> (ret: &'a T)
    requires
        self.snp().is_vmpl0_private(),
    ensures
        self.snp().ensures_read(Some(self@), *ret)
    {
        &*self.b
    }

    #[verifier(external_body)]
    pub fn from_raw(ptr: usize, Tracked(perm): Tracked<SnpPointsTo<T>>)
        -> (res: VBox<T>)
    requires
        perm@.wf_not_null_at(ptr as int),
        perm@.value().is_Some(),
    ensures
        res@ === perm@.get_value(),
        res.id() == perm@.id(),
        res.snp() === perm@.snp(),
    {
        let mut ret: VBox<T>;
        unsafe {
            let raw = ptr as *mut T;
            ret = VBox {
                b: Box::<T>::from_raw(raw)
            }
        }
        ret
    }

    #[verifier(external_body)]
    pub fn into_raw(self) -> (ptr_perm: (SnpPPtr<T>, Tracked<SnpPointsTo<T>>))
    ensures
        ptr_perm.0.is_constant(),
        ptr_perm.1@@.get_value() === self@,
        ptr_perm.1@@.value().is_Some(),
        ptr_perm.1@@.wf_not_null_at(ptr_perm.0.id()),
        ptr_perm.1@@.snp() === self.snp(),
        self.id() == ptr_perm.0.id(),
    {
        let mut addr: usize;
        unsafe {
            let ptr = Box::into_raw(self.b);
            addr = (ptr as usize).into();
        }
        (SnpPPtr::from_usize(addr), Tracked::assume_new())
    }

    #[verifier(external_body)]
    pub fn get_const_addr(&self) -> (addr: usize_t)
    ensures
        self.id() == addr,
        self.id().spec_valid_addr_with(spec_size::<T>()),
    {
        let data_ref = &*self.b;
        unsafe {
            let addr = data_ref as *const _;
            (addr as usize)
        }
    }
}

pub trait MutFnTrait<'a, Params, Out> {
    spec fn spec_update_requires(&self, params: Params) -> bool;

    spec fn spec_update(&self, prev: &Self, params: Params, ret: Out) -> bool;

    fn box_update(&'a mut self, params: Params) -> (ret: Out)
    requires
        old(self).spec_update_requires(params)
    ensures
        self.spec_update(&*old(self), params, ret);
}

pub trait MutFnWithCSTrait<'a, T, Params, Out> {
    spec fn spec_update_cs_requires(&self, params: Params, cs: T) -> bool;

    spec fn spec_update_cs(&self, prev: &Self, params: Params, oldcs: T, ret: Out, cs: T) -> bool;

    fn box_update_cs(&'a mut self, params: Params, Tracked(cs): Tracked<&mut T>) -> (ret: Out)
    requires
        old(self).spec_update_cs_requires(params, *old(cs))
    ensures
        self.spec_update_cs(&*old(self), params, *old(cs), ret, *cs);
}

pub trait BorrowFnTrait<'a, Params, Out> {
    spec fn spec_borrow_requires(&self, params: Params) -> bool;

    spec fn spec_borrow_ensures(&self, params: Params, ret: Out) -> bool;

    fn box_borrow(&'a self, params: Params) -> (ret: Out)
    requires
        self.spec_borrow_requires(params)
    ensures
        self.spec_borrow_ensures(params, ret);
}

/*
impl MutFnTrait<'a, Out, F: Fn(T) -> F> for T {
    spec fn spec_update_requires(&self, call: F) -> bool
    {
        call.requires(*self)
    }

    spec fn spec_update(&self, prev: &Self, call: F, ret: ()) -> bool
    {
        call.ensures((*prev, ), (*self, ))
    }

    fn box_update(&'a mut self, call: F) -> (ret: ())
    {
        *self = call(*self)
    }
}
*/

impl<Params, Out, 'a, T: MutFnTrait<'a, Params, Out>> MutFnTrait<'a, Params, Out> for VBox<T> {
    open spec fn spec_update_requires(&self, params: Params) -> bool {
        self@.spec_update_requires(params)
    }

    open spec fn spec_update(&self, prev: &Self, params: Params, ret: Out) -> bool {
        &&& //self.snp().is_vmpl0_private() ==>
            self@.spec_update(&prev@, params, ret)
        &&& self.only_val_updated(*prev)
        //&&& exists |p| self@.spec_update(&p, params, ret)
        //&&& self.wf()
    }

    #[verifier(external_body)]
    fn box_update(&'a mut self, params: Params) -> (ret: Out)
    {
        self.b.box_update(params)
    }
}

impl<Params, T2, Out, 'a, T: MutFnWithCSTrait<'a, T2, Params, Out>> MutFnWithCSTrait<'a, T2, Params, Out> for VBox<T> {
    open spec fn spec_update_cs_requires(&self, params: Params, cs: T2) -> bool {
        self@.spec_update_cs_requires(params, cs)
    }

    open spec fn spec_update_cs(&self, prev: &Self, params: Params, oldcs: T2, ret: Out, cs: T2) -> bool {
        &&& //self.snp().is_vmpl0_private() ==>
            self@.spec_update_cs(&prev@, params, oldcs, ret, cs)
        &&& self.only_val_updated(*prev)
        //&&& exists |p| self@.spec_update(&p, params, ret)
        //&&& self.wf()
    }

    #[verifier(external_body)]
    fn box_update_cs(&'a mut self, params: Params, Tracked(cs): Tracked<&mut T2>) -> (ret: Out)
    {
        self.b.box_update_cs(params, Tracked(cs))
    }
}

impl<Params, Out: WellFormed + IsConstant, 'a, T: BorrowFnTrait<'a, Params, Out>> BorrowFnTrait<'a, Params, Out> for VBox<T> {
    open spec fn spec_borrow_requires(&self, params: Params) -> bool {
        self@.spec_borrow_requires(params)
    }

    open spec fn spec_borrow_ensures(&self, params: Params, ret: Out) -> bool {
        &&& (self.snp().is_vmpl0_private() ==>
            self@.spec_borrow_ensures(params, ret))
        &&& (!spec_attack() ==>
            self@.spec_borrow_ensures(params, ret))
        &&& inv_snp_value(self.snp(), ret)
    }

    #[verifier(external_body)]
    fn box_borrow(&'a self, params: Params) -> (ret: Out)
    {
        self.b.box_borrow(params)
    }
}
}

================
File: ./source/verismo/src/vbox/mod.rs
================

mod vbox;

pub use vbox::*;

use crate::arch::attack::*;
use crate::ptr::*;
use crate::tspec::*;
use crate::tspec_e::*;

================
File: ./source/verismo/src/primitives_e/sectype.rs
================

use super::*;

verus! {

pub type NoAdditional = ();

impl_secure_type! {NoAdditional, pub type}

crate::macro_const! {
    #[macro_export]

    pub const MAX_LEAK: u64 = 1u64;
}

impl<T> WellFormed for SpecSecType<T, NoAdditional> {
    #[verifier(inline)]
    open spec fn wf(&self) -> bool {
        self.wf_value()
    }
}

impl<T> WellFormed for SecType<T, NoAdditional> {
    #[verifier(inline)]
    open spec fn wf(&self) -> bool {
        self.wf_value()
    }
}

pub trait ToSecSeq {
    spec fn sec_bytes(self) -> SecSeqByte where Self: core::marker::Sized;
}

pub trait FromSecSeq<T> {
    spec fn from_sec_bytes(self) -> T where T: core::marker::Sized, Self: core::marker::Sized;
}

impl<T: VTypeCast<SecSeqByte>> ToSecSeq for T {
    #[verifier(inline)]
    open spec fn sec_bytes(self) -> SecSeqByte {
        VTypeCast::<SecSeqByte>::vspec_cast_to(self)
    }
}

#[verifier(external_body)]
pub broadcast proof fn axiom_size_from_cast_secbytes_def<T: SpecSize + VTypeCast<SecSeqByte>>(
    val: T,
)
    ensures
        T::spec_size_def() == VTypeCast::<SecSeqByte>::vspec_cast_to(val).len(),
{
}

impl<T: ToSecSeq> FromSecSeq<T> for SecSeqByte {
    open spec fn from_sec_bytes(self) -> T {
        choose|v: T| v.sec_bytes() =~~= self
    }
}

impl<T: VTypeCast<SecSeqByte>> VTypeCast<T> for SecSeqByte {
    open spec fn vspec_cast_to(self) -> T {
        self.from_sec_bytes()
    }
}

#[verifier(external_body)]
pub proof fn proof_sectype_cast_eq<T1: VTypeCast<T2>, T2: VTypeCast<T1>, M>(v: SecType<T1, M>)
    requires
        forall|basev: T1|
            VTypeCast::<T1>::vspec_cast_to(VTypeCast::<T2>::vspec_cast_to(basev)) === basev,
    ensures
        VTypeCast::<SecType<T1, M>>::vspec_cast_to(VTypeCast::<SecType<T2, M>>::vspec_cast_to(v))
            === v,
{
}

impl<T> VTypeCast<SecSeqByte> for Option<T> {
    open spec fn vspec_cast_to(self) -> SecSeqByte;
}

impl<T> VTypeCast<SecSeqByte> for Tracked<T> {
    open spec fn vspec_cast_to(self) -> SecSeqByte {
        Seq::empty()
    }
}

} // verus!

================
File: ./source/verismo/src/primitives_e/vec.rs
================

use alloc::vec::Vec;

use super::*;
use crate::vbox::*;

verus! {

impl<T: WellFormed> WellFormed for Vec<T> {
    open spec fn wf(&self) -> bool {
        &&& self@.wf()
    }
}

impl<T: IsConstant + WellFormed> IsConstant for Vec<T> {
    open spec fn is_constant(&self) -> bool {
        self@.is_constant()
    }

    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        &&& self@.is_constant_to(vmpl)
    }
}

impl<T: ToSecSeq> VTypeCast<SecSeqByte> for Vec<T> {
    open spec fn vspec_cast_to(self) -> SecSeqByte {
        self@.vspec_cast_to()
    }
}

impl<T: SpecSize> SpecSize for Vec<T> {
    open spec fn spec_size_def() -> nat;
}

pub struct PushParam<T> {
    pub val: T,
}

impl<'a, T> MutFnTrait<'a, PushParam<T>, bool> for Vec<T> {
    open spec fn spec_update_requires(&self, params: PushParam<T>) -> bool {
        true
    }

    open spec fn spec_update(&self, prev: &Self, params: PushParam<T>, ret: bool) -> bool {
        self@ === prev@.push(params.val)
    }

    fn box_update(&'a mut self, params: PushParam<T>) -> (ret: bool) {
        self.push(params.val);
        true
    }
}

struct RemoveParam {
    i: usize,
}

impl<'a, T> MutFnTrait<'a, RemoveParam, T> for Vec<T> {
    closed spec fn spec_update_requires(&self, params: RemoveParam) -> bool {
        0 <= params.i < self.len()
    }

    closed spec fn spec_update(&self, prev: &Self, params: RemoveParam, ret: T) -> bool {
        let i = params.i as int;
        &&& self@ === prev@.remove(i)
        &&& ret == prev@[i]
    }

    fn box_update(&'a mut self, params: RemoveParam) -> (ret: T) {
        self.remove(params.i)
    }
}

impl<T> VBox<Vec<T>> {
    pub fn remove(&mut self, i: usize) -> (ret: T)
        requires
            0 <= i < old(self)@.len(),
        ensures
            self.snp().is_vmpl0_private() ==> self@@ === old(self)@@.remove(i as int),
            self.only_val_updated(*old(self)),
            ret === old(self)@@[i as int],
    {
        self.box_update(RemoveParam { i })
    }

    pub fn push(&mut self, val: T)
        ensures
            self.snp().is_vmpl0_private() ==> self@@ === old(self)@@.push(val),
            self.only_val_updated(*old(self)),
    {
        self.box_update(PushParam { val });
    }
}

} // verus!

================
File: ./source/verismo/src/primitives_e/mod.rs
================

mod sectype;
mod seq;
mod vec;
pub use sectype::*;
pub use seq::ValSetSize;

use crate::arch::entities::*;
use crate::tspec::*;

================
File: ./source/verismo/src/primitives_e/seq.rs
================

use super::*;

verus! {

impl<T: WellFormed> WellFormed for Seq<T> {
    open spec fn wf(&self) -> bool {
        &&& forall|i: int| 0 <= i < self.len() ==> (#[trigger] self[i]).wf()
    }
}

impl<T: IsConstant + WellFormed> IsConstant for Seq<T> {
    open spec fn is_constant(&self) -> bool {
        &&& forall|i: int| 0 <= i < self.len() ==> (#[trigger] self[i]).is_constant()
        &&& self.wf()
    }

    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        &&& forall|i: int| 0 <= i < self.len() ==> (#[trigger] self[i]).is_constant_to(vmpl)
        &&& self.wf()
    }
}

pub open spec fn recursive_sec_bytes<T: ToSecSeq>(s: Seq<T>) -> SecSeqByte
    decreases s.len(),
{
    if s.len() > 0 {
        let prevs = s.subrange(0, s.len() - 1);
        if prevs.len() < s.len() {
            recursive_sec_bytes(prevs) + s.last().sec_bytes()
        } else {
            Seq::empty()
        }
    } else {
        Seq::empty()
    }
}

impl<T: ToSecSeq> VTypeCast<SecSeqByte> for Seq<T> {
    open spec fn vspec_cast_to(self) -> SecSeqByte {
        recursive_sec_bytes(self)
    }
}

#[macro_use]
macro_rules! def_basic_tosecseq {
($basetype: ty) => {
    verus!{
    impl VTypeCast<SecSeqByte> for $basetype {
        open spec fn vspec_cast_to(self) -> SecSeqByte {
            let seq: Seq<u8> = self.vspec_cast_to();
            Seq::new(
                seq.len(),
                |i| SpecSecType::constant(seq[i])
            )
        }
    }
    }
}
}

def_basic_tosecseq!{u8}

def_basic_tosecseq!{usize}

def_basic_tosecseq!{u16}

def_basic_tosecseq!{u32}

def_basic_tosecseq!{u64}

pub trait ValSetSize {
    spec fn valset_size(self, vmpl: nat) -> nat where Self: core::marker::Sized
        recommends
            1 <= vmpl <= 4,
    ;
}

pub open spec fn valset_size(s: SecSeqByte, vmpl: nat) -> nat
    decreases s.len(),
{
    if s.len() == 0 {
        1
    } else {
        valset_size(s.subrange(0, s.len() - 1), vmpl) * s.last().valsets[vmpl].len()
    }
}

impl ValSetSize for SecSeqByte {
    open spec fn valset_size(self, vmpl: nat) -> nat {
        valset_size(self, vmpl)
    }
}

impl<T: IsFullSecret> IsFullSecret for Seq<T> {
    open spec fn is_fullsecret_to(&self, vmpl: nat) -> bool {
        forall|i| 0 <= i < self.len() ==> self[i].is_fullsecret_to(vmpl)
    }
}

} // verus!

================
File: ./source/verismo/src/registers/core_perm_s.rs
================

use super::*;
use crate::ptr::SnpPointsToRaw;

verus! {

#[derive(SpecGetter, SpecSetter)]
pub ghost struct CoreMode {
    pub cpu: nat,  // CPU index
    pub run: bool,
    pub vmpl: nat,  // VMPL index
    pub count: nat,  // number of VM exits
    pub sent_ghcb_msrs: Seq<(nat, nat)>,  // req, resp,
    pub sent_mem: Seq<((int, nat), SecSeqByte, SecSeqByte)>,  // memrange, bytes
}

impl CoreMode {
    pub open spec fn spec_vmgexit(
        self,
        ghcbperm: RegisterPermValue<u64_s>,
        memperm: Option<SnpPointsToRaw>,
        prev: Self,
        prev_ghcbperm: RegisterPermValue<u64_s>,
        prev_memperm: Option<SnpPointsToRaw>,
    ) -> bool {
        &&& prev.spec_set_count(prev.count + 1).spec_set_sent_ghcb_msrs(
            prev.sent_ghcb_msrs.push(
                (prev_ghcbperm.value.vspec_cast_to(), ghcbperm.value.vspec_cast_to()),
            ),
        ).spec_set_sent_mem(
            if memperm.is_Some() {
                prev.sent_mem.push(
                    (
                        memperm.get_Some_0()@.range(),
                        prev_memperm.get_Some_0()@.bytes(),
                        memperm.get_Some_0()@.bytes(),
                    ),
                )
            } else {
                prev.sent_mem
            },
        ) === self
    }
}

#[verifier(external_body)]
pub tracked struct CoreIdPerm {
    no_copy: NoCopy,
}

impl CoreIdPerm {
    pub spec fn view(&self) -> CoreMode;
}

pub open spec fn spec_cpumap_contains_cpu(ap_ids: Map<int, CoreIdPerm>, i: int, vmpl: nat) -> bool {
    ap_ids.contains_key(i) && (ap_ids[i])@.vmpl == vmpl && ap_ids[i]@.cpu == i
}

pub open spec fn spec_ap_ids_wf(ap_ids: Map<int, CoreIdPerm>, bsp: int) -> bool {
    forall|i| (i != bsp) ==> #[trigger] spec_cpumap_contains_cpu(ap_ids, i, 0)
}

pub open spec fn spec_ap_ids_wf_lowerbound(
    ap_ids: Map<int, CoreIdPerm>,
    bsp: int,
    min_cpu: int,
) -> bool {
    forall|i: int| (i != bsp) && i >= min_cpu ==> #[trigger] spec_cpumap_contains_cpu(ap_ids, i, 0)
}

} // verus!

================
File: ./source/verismo/src/registers/core_exit_t.rs
================

use super::*;
use crate::ptr::*;

verus! {

pub open spec fn is_none_or_sharedmem(memperm: Option<SnpPointsToRaw>) -> bool {
    &&& memperm.is_Some() ==> (memperm.get_Some_0()@.snp().is_hv_shared()
        || memperm.get_Some_0()@.size() == 0)
    &&& memperm.is_Some() ==> memperm.get_Some_0()@.wf()
}

pub open spec fn hvupdate_none_or_sharedmem(
    memperm: Option<SnpPointsToRaw>,
    prevmemperm: Option<SnpPointsToRaw>,
) -> bool {
    &&& prevmemperm.is_Some() == memperm.is_Some()
    &&& prevmemperm.is_Some() ==> memperm.get_Some_0()@.spec_write_rel(
        prevmemperm.get_Some_0()@,
        memperm.get_Some_0()@.snp_bytes,
    )
}

} // verus!
verus! {

/// Only shared memory or non-mem is passed.
#[inline(always)]
#[verifier(external_body)]
pub fn vmgexit(
    Tracked(ghcbperm): Tracked<&mut RegisterPerm>,
    Tracked(coreperm): Tracked<&mut CoreIdPerm>,
    Tracked(memperm): Tracked<&mut Option<SnpPointsToRaw>>,
)
    requires
        old(ghcbperm).is_ghcb(),
        is_none_or_sharedmem(*old(memperm)),
    ensures
        coreperm@.spec_vmgexit(
            ghcbperm.view::<u64_s>(),
            *memperm,
            old(coreperm)@,
            old(ghcbperm)@,
            *old(memperm),
        ),
        (ghcbperm).view::<u64_s>().spec_write_value(old(ghcbperm)@, old(ghcbperm)@.value()),
        hvupdate_none_or_sharedmem(*memperm, *old(memperm)),
        ghcbperm.view::<u64_s>().spec_write_value(
            old(ghcbperm).view::<u64_s>(),
            ghcbperm.view::<u64_s>().value,
        ),
{
    unsafe {
        asm!(
            "rep vmmcall",
            options(nostack, preserves_flags),
        );
    }
}

} // verus!

================
File: ./source/verismo/src/registers/mod.rs
================

use crate::arch::attack::*;
use crate::arch::reg::*;
use crate::tspec::*;
use crate::tspec_e::*;

mod core_exit_t;
mod core_perm_s;
mod msr_perm_s;
#[macro_use]
mod msr_t;
mod reg_trait_t;
mod trackedcore;
use core::arch::asm;

pub use core_exit_t::*;
pub use core_perm_s::{
    spec_ap_ids_wf, spec_ap_ids_wf_lowerbound, spec_cpumap_contains_cpu, CoreIdPerm, CoreMode,
};
pub use msr_perm_s::*;
pub use msr_t::*;
pub use reg_trait_t::*;
pub use trackedcore::SnpCore;

use crate::ptr::*;

================
File: ./source/verismo/src/registers/trackedcore/mod.rs
================

mod snpcore;
pub use snpcore::*;

use crate::arch::reg::*;
use crate::registers::*;
use crate::tspec::*;
use crate::tspec_e::*;

================
File: ./source/verismo/src/registers/trackedcore/snpcore.rs
================

use super::*;
use crate::pgtable_e::{static_cr3_value, PTE};

verus! {

pub type RegMap = Map<RegName, RegisterPerm>;

#[derive(SpecGetter, SpecSetter)]
pub tracked struct SnpCore {
    pub coreid: CoreIdPerm,
    pub vmpl: nat,
    pub cpu: nat,
    pub regs: Map<RegName, RegisterPerm>,
}

impl SnpCore {
    #[verifier(inline)]
    pub open spec fn cpu(&self) -> nat {
        self.coreid@.cpu
    }

    pub open spec fn update_reg_coremode(self, prev: Self) -> bool {
        self === prev.spec_set_coreid(self.coreid).spec_set_regs(self.regs)
    }

    pub open spec fn inv_reg_cpu(&self) -> bool {
        let regs = self.regs;
        let coreid = self.coreid;
        let cr3_pte: u64 = regs[RegName::Cr3].val::<u64_s>().vspec_cast_to();
        &&& coreid@.vmpl == self.vmpl
        &&& coreid@.run
        &&& coreid@.cpu == self.cpu
        &&& forall|id: RegName| regs.contains_key(id)
        &&& forall|id: RegName| (#[trigger] regs[id]).cpu() == coreid@.cpu
        &&& forall|id: RegName| (#[trigger] regs[id]).id() === id
        &&& forall|id: RegName| (#[trigger] regs[id]).wf()
        &&& regs[RegName::MSR(MSR_GHCB_BASE)].shared() == true
        &&& forall|id: RegName|
            id !== RegName::MSR(MSR_GHCB_BASE) ==> !(#[trigger] regs[id]).shared()
        &&& regs[RegName::GdtrBaseLimit].val::<crate::snp::cpu::Gdtr>().is_constant()
        &&& regs[RegName::IdtrBaseLimit].val::<crate::boot::idt::Idtr>().is_constant()
        &&& regs[RegName::XCr0].val::<u64_s>().is_constant()
        &&& regs[RegName::Cr0].val::<u64_s>().is_constant()
        &&& regs[RegName::Cr1].val::<u64_s>().is_constant()
        &&& regs[RegName::Cr2].val::<u64_s>().is_constant()
        &&& regs[RegName::Cr3].val::<u64_s>().is_constant()
        &&& regs[RegName::Cr4].val::<u64_s>().is_constant()
        &&& regs[RegName::Cs].val::<u16_s>().is_constant()
        &&& regs[RegName::MSR(MSR_EFER_BASE)].val::<u64_s>().is_constant()
        &&& cr3_pte == static_cr3_value()
    }

    pub open spec fn ghcb_value(&self) -> u64 {
        self.regs[RegName::MSR(MSR_GHCB_BASE)].val::<u64_s>().vspec_cast_to()
    }

    pub open spec fn inv(&self) -> bool {
        &&& self.inv_reg_cpu()
        &&& self.vmpl == 0
    }

    #[verifier(inline)]
    pub open spec fn last_ghcb_req(&self) -> nat {
        (*self).ghcbmsr_msgs().last().0
    }

    #[verifier(inline)]
    pub open spec fn last_ghcb_resp(&self) -> nat {
        (*self).ghcbmsr_msgs().last().1
    }

    #[verifier(inline)]
    pub open spec fn last_ghcbmem_req(&self) -> crate::snp::ghcb::GhcbPage {
        (*self).ghcbmem_msgs().last().1.vspec_cast_to()
    }

    #[verifier(inline)]
    pub open spec fn last_ghcbmem_resp(&self) -> crate::snp::ghcb::GhcbPage {
        (*self).ghcbmem_msgs().last().2.vspec_cast_to()
    }

    #[verifier(inline)]
    pub open spec fn ghcbmsr_msgs(&self) -> Seq<(nat, nat)> {
        (*self).coreid@.sent_ghcb_msrs
    }

    #[verifier(inline)]
    pub open spec fn ghcbmem_msgs(&self) -> Seq<((int, nat), SecSeqByte, SecSeqByte)> {
        (*self).coreid@.sent_mem
    }

    pub open spec fn reg_updated(self, prev: Self, regs: Set<RegName>) -> bool {
        &&& forall|i| !regs.contains(i) ==> #[trigger] self.regs[i] === prev.regs[i]
    }

    #[verifier(inline)]
    pub open spec fn only_reg_updated(self, prev: Self, regs: Set<RegName>) -> bool {
        &&& self.reg_updated(prev, regs)
        &&& (regs.is_empty()) ==> self.regs === prev.regs
        &&& self === prev.spec_set_regs(self.regs)
    }

    #[verifier(inline)]
    pub open spec fn only_reg_coremode_updated(self, prev: Self, regs: Set<RegName>) -> bool {
        &&& self === prev.spec_set_coreid(self.coreid).spec_set_regs(self.regs)
        &&& self.reg_updated(prev, regs)
    }

    pub proof fn lemma_regs_update_auto()
        ensures
            forall|prev: Self, cur: Self, next: Self, regs1, regs2|
                (#[trigger] cur.reg_updated(prev, regs1) && #[trigger] next.reg_updated(cur, regs2))
                    ==> next.reg_updated(prev, regs1.union(regs2)),
    {
        assert forall|prev: Self, cur: Self, next: Self, regs1, regs2|
            (#[trigger] cur.reg_updated(prev, regs1) && #[trigger] next.reg_updated(
                cur,
                regs2,
            )) implies next.reg_updated(prev, regs1.union(regs2)) by {
            let regs = regs1.union(regs2);
            assert forall|i| !regs.contains(i) implies #[trigger] next.regs[i] === prev.regs[i] by {
                assert(!regs1.contains(i));
                assert(cur.regs[i] === prev.regs[i]);
            }
        }
    }
}

} // verus!

================
File: ./source/verismo/src/registers/msr_t.rs
================

use super::*;

verismo! {
pub struct Msr {
    pub reg: u32,
}

/*
#[inline(always)]
#[verifier(external_body)]
pub fn ghcb_vmgexit(value: u64_t) {
    let low = 0xff03u32;
    let high = 0u32;
    let reg: u32 = 0xc0010130u32;
    unsafe {
        asm!(
            "wrmsr",
            "rep",
            "vmmcall",
            in("ecx") reg,
            in("eax") low, in("edx") high,
            options(nostack, preserves_flags),
        );
    }
}
*/

impl Msr {
    #[inline(always)]
    #[verifier(external_body)]
    pub fn write_vmgexit(&self, value: u64_s,
        Tracked(perm): Tracked<&mut RegisterPerm>,
        Tracked(coreperm): Tracked<&mut CoreIdPerm>,
        Tracked(memperm): Tracked<&mut Option<SnpPointsToRaw>>)
    {
        let low: u32 = value.reveal_value() as u32;
        let high: u32 = (value.reveal_value() >> 32u64) as u32;
        let reg = self.reg.reveal_value();
        unsafe {
            asm!(
                "wrmsr",
                "rep vmmcall",
                in("ecx") reg,
                in("eax") low, in("edx") high,
                options(nostack, preserves_flags),
            );
        }
    }
}
impl AnyRegTrait<u64_s> for Msr {
    open spec fn reg_id(&self) -> RegName {
        RegName::MSR(self.reg as u32)
    }

    open spec fn write_extra_requires(&self, val: u64_s, perm: RegisterPermValue<u64_s>) -> bool {
        true
    }
    open spec fn read_extra_requires(&self,perm: RegisterPermValue<u64_s>) -> bool {
        true
    }

    #[inline(always)]
    #[verifier(external_body)]
    fn read(&self, Tracked(perm): Tracked<&RegisterPerm>) -> (ret: u64_s)
    {
        let (high, low): (u32, u32);
        let reg = self.reg.reveal_value();
        unsafe {
            asm!(
                "rdmsr",
                in("ecx") reg,
                out("eax") low, out("edx") high,
                options(nomem, nostack, preserves_flags),
            );
        }

        u64_s::constant((high as u64) << 32u64 | (low as u64))
    }

    #[inline(always)]
    #[verifier(external_body)]
    fn write(&self, value: u64_s, Tracked(perm): Tracked<&mut RegisterPerm>)
    {
        let low: u32 = value.reveal_value() as u32;
        let high: u32 = (value.reveal_value() >> 32u64) as u32;
        let reg = self.reg.reveal_value();
        unsafe {
            asm!(
                "wrmsr",
                in("ecx") reg,
                in("eax") low, in("edx") high,
                options(nostack, preserves_flags),
            );
        }
    }
}
}

pub type SegmentSelector = u16;

macro_rules! readreg_u64 {
    ($name:literal, $rname: ident) => {
        paste::paste! {
            verismo! {
                #[inline(always)]
                #[verifier(external_body)]
                fn read(&self, Tracked(perm): Tracked<&RegisterPerm>) -> (ret: u64_s)
                {
                    let value: u64;
                    unsafe {
                        asm!(
                            concat!("mov {},", $name),
                            out(reg) value,
                            options(nomem, nostack, preserves_flags)
                        );
                    }
                    u64_s::constant(value)
                }
            }
        }
    };
}

macro_rules! writereg_u64 {
    ($name:literal, $rname: ident) => {
        paste::paste! {
            verismo! {
                #[inline(always)]
                #[verifier(external_body)]
                fn write(&self, value: u64_s, Tracked(perm): Tracked<&mut RegisterPerm>)
                {
                    let val = value.reveal_value();
                    unsafe {
                        asm!(
                            concat!("mov ", $name, ", {}"),
                            in(reg) val,
                            options(nomem, nostack, preserves_flags)
                        );
                    }
                }
            }
        }
    };
}

macro_rules! impl_reg64 {
    ($($classname: ident, $name:literal, $rname: ident),*$(,)*) => {
    $(
        verismo_simple!{
        pub struct $classname;
        //pub const $rname: $classname = $classname;
        impl AnyRegTrait<u64_s> for $classname {
            open spec fn reg_id(&self) -> RegName {
                crate::arch::reg::RegName::$rname
            }
            open spec fn write_extra_requires(&self, val: u64_s, perm: RegisterPermValue<u64_s>) -> bool {
                true
            }
            open spec fn read_extra_requires(&self,perm: RegisterPermValue<u64_s>) -> bool {
                true
            }
            readreg_u64!{$name, $rname}
            writereg_u64!{$name, $rname}
        }}
    )*
    }
}

macro_rules! impl_reg16 {
    ($($classname: ident, $name:literal, $rname: ident),*$(,)*) => {
    $(
        verus!{
        pub struct $classname;
        }
        verismo!{
        impl AnyRegTrait<u16_s> for $classname {
            #[verifier(inline)]
            open spec fn reg_id(&self) -> RegName {
                RegName::$rname
            }

            open spec fn write_extra_requires(&self, val: u16_s, perm: RegisterPermValue<u16_s>) -> bool {
                true
            }
            open spec fn read_extra_requires(&self,perm: RegisterPermValue<u16_s>) -> bool {
                true
            }

            #[verifier(external_body)]
            #[inline(always)]
            fn read(&self, Tracked(perm): Tracked<&RegisterPerm>) -> (ret: u16_s)
            {
                let segment: u16;
                unsafe {
                    asm!(
                        concat!("mov {0:x}, ", $name),
                        out(reg) segment,
                        options(nomem, nostack, preserves_flags));
                }
                u16_s::constant(segment)
            }

            #[verifier(external_body)]
            #[inline(always)]
            fn write(&self, value: u16_s, Tracked(perm): Tracked<&mut RegisterPerm>)
            {
                let val = value.reveal_value();
                unsafe {
                    asm!(
                        concat!("mov ", $name, ", {0:x}"),
                        in(reg) val,
                        options(nomem, nostack, preserves_flags)
                    );
                }
            }
        }}
    )*
    }
}

verus! {

pub struct XCR0;

} // verus!
verismo! {
impl AnyRegTrait<u64_s> for XCR0 {
    #[verifier(inline)]
    open spec fn reg_id(&self) -> RegName {
        RegName::XCr0
    }

    open spec fn write_extra_requires(&self, val: u64_s, perm: RegisterPermValue<u64_s>) -> bool {
        true
    }

    open spec fn read_extra_requires(&self,perm: RegisterPermValue<u64_s>) -> bool {
        true
    }

    #[inline]
    #[verifier(external_body)]
    fn read(&self, Tracked(perm): Tracked<&RegisterPerm>) -> (ret: u64_s)
    {
        let (low, high): (u32, u32);
        unsafe {
            asm!(
                "xgetbv",
                in("ecx") 0,
                out("rax") low, out("rdx") high,
                options(nomem, nostack, preserves_flags),
            );
        }
        u64_s::constant((high as u64) << 32u64 | (low as u64))
    }

    #[inline]
    #[verifier(external_body)]
    fn write(&self, value: u64_s, Tracked(perm): Tracked<&mut RegisterPerm>) {
        let low = value.reveal_value() as u32;
        let high = (value.reveal_value() >> 32u64) as u32;

        unsafe {
            asm!(
                "xsetbv",
                in("ecx") 0,
                in("rax") low, in("rdx") high,
                options(nomem, nostack, preserves_flags),
            );
        }
    }
}
}

use crate::BIT64;
crate::macro_const! {
pub const CR4_OSFXSR: u64 = BIT64!(9u64);
pub const CR4_OSXMMEXCPT: u64 = BIT64!(10u64);
pub const CR4_OSXSAVE: u64 = BIT64!(18u64);
pub const CR4_PAE: u64 = BIT64!(5u64);

pub const XCR0_X87: u64 = BIT64!(0);
pub const XCR0_SSE: u64 = BIT64!(1);
pub const XCR0_AVX: u64 = BIT64!(2);
pub const XCR0_YMM: u64 = BIT64!(2);
pub const XCR0_BNDREG: u64 = BIT64!(3);
pub const XCR0_BNDCSR: u64 = BIT64!(4);
pub const XCR0_OPMASK: u64 = BIT64!(5);
pub const XCR0_ZMM_HI256: u64 = BIT64!(6);
pub const XCR0_HI16_ZMM: u64 = BIT64!(7);
}
impl_reg64! {
    CR0, "cr0", Cr0,
    CR3, "cr3", Cr3,
    CR4, "cr4", Cr4,
}

impl_reg16! {
    CS, "cs", Cs,
}

#[macro_export]
macro_rules! impl_dpr {
    ($($typename: ident, $point_type: ty,$name:literal, $rname: ident),*$(,)*) => {
    $(verismo_simple!{
    pub struct $typename;
    impl<'a> AnyRegTrait<$point_type> for $typename {
        open spec fn reg_id(&self) -> RegName {
            crate::arch::reg::RegName::$rname
        }

        open spec fn write_extra_requires(
            &self,
            val: $point_type,
            perm: RegisterPermValue<$point_type>) -> bool
        {
            &&& val.is_constant()
        }

        // Unused read.
        open spec fn read_extra_requires(
            &self,
            perm: RegisterPermValue<$point_type>) -> bool
        {
            true
        }

        // Dead code.
        #[verifier(external_body)]
        fn read(&self, Tracked(perm): Tracked<&RegisterPerm>) -> (ret: $point_type)
        {
            let mut dtp = $point_type::default();
            unsafe {
                //asm!("sidt [{}]", in(reg) &idt, options(nostack, preserves_flags));
                unsafe {
                    core::arch::asm!(
                        concat!("s", $name, " [{}]"),
                        in(reg) &dtp,
                        options(nostack, preserves_flags)
                    );
                }
            }
            dtp
        }

        #[verifier(external_body)]
        fn write(&self, value: $point_type, Tracked(perm): Tracked<&mut RegisterPerm>)
        {
            // asm!("lidt [{}]", in(reg) &value, options(readonly, nostack, preserves_flags));
            unsafe {
                core::arch::asm!(
                    concat!("l", $name, " [{}]"),
                    in(reg) &value,
                    options(readonly, nostack, preserves_flags)
                );
            }
        }
    }
    })*
}
}

================
File: ./source/verismo/src/registers/msr_perm_s.rs
================

use super::*;

verus! {

pub ghost struct RegisterPermValue<T> {
    // Identifier
    pub cpu: nat,
    pub id: RegName,
    pub shared: bool,
    // Verismo software tracked value
    pub value: T,
}

impl<T> RegisterPermValue<T> {
    pub open spec fn shared(&self) -> bool {
        self.shared
    }

    pub open spec fn value(&self) -> T {
        self.value
    }
}

impl<T: WellFormed + IsConstant> RegisterPermValue<T> {
    pub open spec fn spec_write_value(self, prev: Self, val: T) -> bool {
        &&& prev.cpu == self.cpu
        &&& prev.id == self.id
        &&& prev.shared() == self.shared()
        &&& self.value() === val
    }

    pub open spec fn wf(&self) -> bool {
        &&& self.shared() ==> self.value().is_constant()
        &&& self.value().wf()
    }
}

#[verifier(external_body)]
pub tracked struct RegisterPerm {
    no_copy: NoCopy,
}

impl RegisterPerm {
    pub open spec fn view<T>(&self) -> RegisterPermValue<T> {
        RegisterPermValue {
            value: self.val(),
            cpu: self.cpu(),
            id: self.id(),
            shared: self.shared(),
        }
    }

    #[verifier(external_body)]
    pub broadcast proof fn axiom_eq<T>(x: Self, y: Self)
        requires
            x.view::<T>() === y.view::<T>(),
        ensures
            x === y,
    {
    }

    pub spec fn cpu(&self) -> nat;

    pub spec fn id(&self) -> RegName;

    pub spec fn shared(&self) -> bool;

    pub spec fn val<T>(&self) -> T where T: core::marker::Sized;

    pub spec fn wf(&self) -> bool;

    #[verifier(inline)]
    pub open spec fn wf_notshared(&self) -> bool {
        &&& self.wf()
        &&& !self.shared()
    }

    #[verifier(external_body)]
    pub broadcast proof fn axiom_wf<T: WellFormed + IsConstant>(&self)
        ensures
            self.wf() == self.view::<T>().wf(),
    {
    }

    pub open spec fn is_ghcb(&self) -> bool {
        let ghcb_perm_view: RegisterPermValue<u64> = self@;
        &&& ghcb_perm_view.value.is_constant()
        &&& ghcb_perm_view.id === RegName::MSR(MSR_GHCB_BASE)
        &&& ghcb_perm_view.wf()
        &&& ghcb_perm_view.shared()
    }
}

} // verus!

================
File: ./source/verismo/src/registers/reg_trait_t.rs
================

use super::*;

verus! {

pub trait AnyRegTrait<T: IsConstant + WellFormed> {
    spec fn reg_id(&self) -> RegName;

    spec fn write_extra_requires(&self, val: T, perm: RegisterPermValue<T>) -> bool;

    spec fn read_extra_requires(&self, perm: RegisterPermValue<T>) -> bool;

    fn read<'a>(&self, Tracked(perm): Tracked<&RegisterPerm>) -> (ret: T)
        requires
            perm.id() === self.reg_id(),
            perm.wf(),
            self.read_extra_requires(perm.view::<T>()),
        ensures
            ((!perm.view::<T>().shared()) || (perm.view::<T>().shared() && !spec_attack())
                ==> perm.view::<T>().value() === ret),
            (perm.view::<T>().shared() ==> (ret.wf() && ret.is_constant())),
    ;

    fn write(&self, value: T, Tracked(perm): Tracked<&mut RegisterPerm>)
        requires
            old(perm).view::<T>().id === self.reg_id(),
            old(perm).wf(),
            self.write_extra_requires(value, old(perm)@),
        ensures
            (perm).view::<T>().spec_write_value(old(perm)@, value),
            perm.wf(),
    ;
}

} // verus!

================
File: ./source/verismo/src/tspec_e/size_e.rs
================

use super::*;

verismo! {
pub fn sizeof<T: SpecSize>() -> (ret: usize)
ensures
    ret == spec_size::<T>()
{
    usize_s::constant(core::mem::size_of::<T>())
}

#[verifier(external_fn_specification)]
pub fn ex_size_of<T>() -> (size: usize)
    ensures size == spec_size::<T>()
{
    core::mem::size_of::<T>()
}

}

================
File: ./source/verismo/src/tspec_e/array/sort.rs
================

use vstd::multiset::Multiset;
use vstd::seq_lib::*;

use super::array_utils::*;
use super::*;

verus! {

proof fn lemma_seq_to_multiset_swap<T>(input: Seq<T>, i: int, j: int) -> (ret: Seq<T>)
    requires
        0 <= i < input.len(),
        0 <= j < input.len(),
    ensures
        ret === spec_swap(input, i, j),
        ret.to_multiset() =~~= input.to_multiset(),
{
    let ret = spec_swap(input, i, j);
    input.to_multiset_ensures();
    lemma_seq_to_multiset_update(input, i, input[j]);
    lemma_seq_to_multiset_update(input.update(i, input[j]), j, input[i]);
    input.update(i, input[j]).to_multiset_ensures();
    input.to_multiset().remove(input[i]).insert(input[j]).remove(input[j]).insert(input[i]);
    ret
}

proof fn lemma_seq_to_multiset_update<T>(input: Seq<T>, i: int, v: T)
    requires
        0 <= i < input.len(),
    ensures
        input.update(i, v).to_multiset() =~~= input.to_multiset().remove(input[i]).insert(v),
{
    let ret = input.update(i, v);
    let left = input.subrange(0, i);
    let u = input[i];
    let right = input.subrange(i + 1, input.len() as int);
    assert(ret =~~= left.push(v) + right);
    assert(input =~~= left.push(u) + right);
    lemma_multiset_commutative(left.push(v), right);
    lemma_multiset_commutative(left.push(u), right);
    left.to_multiset_ensures();
    assert(left.push(v).to_multiset() =~~= left.to_multiset().add(Multiset::singleton(v)));
    assert(left.push(u).to_multiset() =~~= left.to_multiset().add(Multiset::singleton(u)));
    assert(ret.to_multiset() =~~= left.push(v).to_multiset().add(right.to_multiset()));
    assert(input.to_multiset() =~~= left.push(u).to_multiset().add(right.to_multiset()));
    assert(ret.to_multiset() =~~= left.to_multiset().add(Multiset::singleton(u)).add(
        right.to_multiset(),
    ).remove(u).add(Multiset::singleton(v)));
}

proof fn proof_seq_to_multiset_update<T>(input: Seq<T>, i: int, v: T)
    requires
        0 <= i < input.len(),
    ensures
        input.to_multiset().contains(input[i]),
        input.update(i, v).to_multiset() =~~= (input.to_multiset().remove(input[i]).insert(v)),
{
    lemma_seq_to_multiset_update(input, i, v);
    input.to_multiset_ensures();
}

proof fn proof_seq_update_in_subrange<T>(input: Seq<T>, start: int, end: int, i: int, v: T)
    requires
        0 <= start <= i < end <= input.len(),
    ensures
        input.update(i, v).subrange(start, end) =~~= input.subrange(start, end).update(
            i - start,
            v,
        ),
{
    assert(input.update(i, v).subrange(start, end)[i - start] === v);
}

proof fn proof_seq_swap_in_subrange<T>(input: Seq<T>, start: int, end: int, i: int, j: int)
    requires
        0 <= start <= i < end <= input.len(),
        0 <= start <= j < end <= input.len(),
    ensures
        spec_swap(input, i, j).subrange(start, end) =~~= spec_swap(
            input.subrange(start, end),
            i - start,
            j - start,
        ),
{
    let v = input[i];
    let u = input[j];
    assert(input.subrange(start, end)[i - start] === v);
    assert(input.subrange(start, end)[j - start] === u);
    proof_seq_update_in_subrange(input, start, end, i, input[i]);
    assert(input.update(i, v).subrange(start, end) =~~= input.subrange(start, end).update(
        i - start,
        v,
    ));
    proof_seq_update_in_subrange(input.update(i, v), start, end, j, u);
    assert(input.update(i, v).update(j, u).subrange(start, end) =~~= input.update(i, v).subrange(
        start,
        end,
    ).update(j - start, u));
    assert(input.update(i, v).subrange(start, end).update(j - start, u) =~~= input.subrange(
        start,
        end,
    ).update(i - start, v).update(j - start, u));
}

pub open spec fn spec_less_fn_requires<T, F: FnOnce(T, T) -> bool>(
    less: F,
    speclt: spec_fn(T, T) -> bool,
    s: Seq<T>,
) -> bool {
    // all elements meets the requires
    &&& forall|x, y|
        s.contains(x) && s.contains(y) ==> #[trigger] less.requires(
            (x, y),
        )
    // less and speclt relationship

    &&& forall|x, y, r|
        less.ensures((x, y), r) ==> (speclt(x, y)
            == r)
        // spec_lt properties

    &&& forall|x, y| #[trigger] speclt(x, y) ==> !speclt(y, x)
    &&& forall|x| !#[trigger] speclt(x, x)
    &&& forall|x, y, z| #[trigger] speclt(x, y) && !#[trigger] speclt(z, y) ==> speclt(x, z)
    &&& forall|x, y, z| !#[trigger] speclt(x, y) && !#[trigger] speclt(y, z) ==> !speclt(x, z)
}

} // verus!
verus! {

impl<T: Copy, const N: IndexType> Array<T, N> {
    fn partition<F>(
        &mut self,
        start: usize_t,
        end: usize_t,
        less: F,
        Ghost(speclt): Ghost<spec_fn(T, T) -> bool>,
    ) -> (ret: usize_t) where F: FnOnce(T, T) -> bool + core::marker::Copy
        requires
            spec_less_fn_requires(less, speclt, old(self)@.subrange(start as int, end as int)),
            0 <= (start as int) < (end as int) <= Self::spec_len(),
            start.is_constant(),
            end.is_constant(),
        ensures
            forall|k: int|
                (ret as int) <= k < end as int ==> !speclt(#[trigger] self@[k], self@[ret as int]),
            forall|k: int|
                (start as int) <= k < ret as int ==> speclt(#[trigger] self@[k], self@[ret as int]),
            forall|k: int|
                0 <= k < (start as int) || (end as int) <= k < Self::spec_len() ==> self@[k]
                    === old(self)@[k],  // unchanged elements
            old(self)@.to_multiset() =~~= self@.to_multiset(),
            old(self)@.subrange(start as int, end as int).to_multiset() =~~= self@.subrange(
                start as int,
                end as int,
            ).to_multiset(),
            start as int <= (ret as int) < (end as int),
            ret.is_constant(),
    {
        let pivot_index = end - 1;
        proof {
            lemma_seq_to_multiset_swap(self@, pivot_index as int, (end as int - 1));
            proof_seq_swap_in_subrange(
                self@,
                start as int,
                end as int,
                pivot_index as int,
                end as int - 1,
            );
            lemma_seq_to_multiset_swap(
                self@.subrange(start as int, end as int),
                pivot_index as int - start as int,
                (end as int - 1 - start as int),
            );
        }
        self.swap(pivot_index, end - 1);
        let mut i = start;
        let mut j = start;
        let last = end - 1;
        while j < last
            invariant
                i.is_constant(),
                j.is_constant(),
                start.is_constant(),
                last.is_constant(),
                last as int == end as int - 1,
                0 <= (start as int) <= (last as int) < (end as int),
                0 <= (start as int) < (end as int) <= Self::spec_len(),
                start as int <= (j as int) <= (last as int),
                (start as int) <= (i as int) <= (j as int),
                spec_less_fn_requires(less, speclt, self@.subrange(start as int, end as int)),
                forall|k: int|
                    (start as int) <= k < i as int ==> speclt(
                        #[trigger] self@[k],
                        self@[last as int],
                    ),
                forall|k: int|
                    (i as int) <= k < (j as int) ==> !speclt(
                        #[trigger] self@[k],
                        self@[last as int],
                    ),
                forall|k: int|
                    0 <= k < (start as int) || (end as int) <= k < Self::spec_len() ==> self@[k]
                        === old(self)@[k],
                old(self)@.to_multiset() =~~= self@.to_multiset(),
                old(self)@.subrange(start as int, end as int).to_multiset() =~~= self@.subrange(
                    start as int,
                    end as int,
                ).to_multiset(),
        {
            proof {
                assert(self@.len() == Self::spec_len());
                self@.to_multiset_ensures();
                let targets = self@.subrange(start as int, end as int);
                targets.to_multiset_ensures();
                assert(self[j as int] === targets[j as int - (start as int)]);
                assert(self[last as int] === targets[last as int - (start as int)]);
                assert(targets.contains(self[j as int]));
                assert(targets.contains(self[last as int]));
            }
            if less(*self.index(j), *self.index(last)) {
                proof {
                    lemma_seq_to_multiset_swap(self@, i as int, j as int);
                    proof_seq_swap_in_subrange(self@, start as int, end as int, i as int, j as int);
                    lemma_seq_to_multiset_swap(
                        self@.subrange(start as int, end as int),
                        i as int - start as int,
                        j as int - start as int,
                    );
                }
                self.swap(i, j);
                i = i + 1;
            }
            j = j + 1;
        }
        proof {
            lemma_seq_to_multiset_swap(self@, i as int, j as int);
            proof_seq_swap_in_subrange(self@, start as int, end as int, i as int, last as int);
            lemma_seq_to_multiset_swap(
                self@.subrange(start as int, end as int),
                i as int - start as int,
                last as int - start as int,
            );
        }
        self.swap(i, last);
        i
    }

    pub fn sort<F>(
        &mut self,
        start: usize,
        end: usize,
        less: F,
        Ghost(speclt): Ghost<spec_fn(T, T) -> bool>,
    ) where F: FnOnce(T, T) -> bool + core::marker::Copy
        requires
            spec_less_fn_requires(less, speclt, old(self)@.subrange(start as int, end as int)),
            0 <= (start as int) <= (end as int) <= Self::spec_len(),
            start.is_constant(),
            end.is_constant(),
        ensures
            seq_is_sorted(self@.subrange(start as int, end as int), speclt),
            forall|i: int, j: int|
                start as int <= i < j < end as int ==> !speclt(
                    #[trigger] self@[j],
                    #[trigger] self@[i],
                ),
            forall|k: int|
                0 <= k < (start as int) || (end as int) <= k < Self::spec_len() ==> self@[k]
                    === old(self)@[k],
            old(self)@.subrange(start as int, end as int).to_multiset() =~~= self@.subrange(
                start as int,
                end as int,
            ).to_multiset(),
            old(self)@.to_multiset() =~~= self@.to_multiset(),
        decreases end as int - start as int,
    {
        if start >= end || start + 1 >= end {
            return ;
        }
        let ghost s0 = self@;
        let ghost ss0 = s0.subrange(start as int, end as int);
        proof {
            self@.to_multiset_ensures();
            self@.subrange(start as int, end as int).to_multiset_ensures();
        }
        let pivot_index = self.partition(start, end, less, Ghost(speclt));
        let ghost s1 = self@;
        let ghost ss1 = s1.subrange(start as int, end as int);
        let ghost (s1_left, s1_right) = (
            s1.subrange(start as int, pivot_index as int),
            s1.subrange(pivot_index as int + 1, end as int),
        );
        let ghost s1_right2 = s1.subrange(pivot_index as int, end as int);
        proof {
            s1.to_multiset_ensures();
            ss1.to_multiset_ensures();
            s1_left.to_multiset_ensures();
            s1_right.to_multiset_ensures();
            assert forall|x, y| ss1.contains(x) && ss1.contains(y) implies #[trigger] less.requires(
                (x, y),
            ) by {
                assert(ss1.to_multiset().count(x) > 0);
                assert(ss1.to_multiset().count(y) > 0);
                assert(ss0.contains(x));
                assert(ss0.contains(y));
            }
            assert forall|x| s1_left.contains(x) implies ss1.contains(x) by {
                let i = choose|i: int| 0 <= i < s1_left.len() && s1_left[i] === x;
                assert(ss1[i] === s1_left[i]);
                assert(ss1.contains(x));
            }
            assert forall|x, y|
                s1_left.contains(x) && s1_left.contains(y) implies #[trigger] less.requires(
                (x, y),
            ) by {
                assert(ss1.contains(x));
                assert(ss1.contains(y));
            }
        }
        if start + 1 < pivot_index {
            self.sort(start, pivot_index, less, Ghost(speclt));
        }
        let ghost s2 = self@;
        let ghost ss2 = s2.subrange(start as int, end as int);
        let ghost (s2_left, s2_right) = (
            s2.subrange(start as int, pivot_index as int),
            s2.subrange(pivot_index as int + 1, end as int),
        );
        let ghost s2_right2 = s2.subrange(pivot_index as int, end as int);
        proof {
            s2.to_multiset_ensures();
            ss2.to_multiset_ensures();
            s2_left.to_multiset_ensures();
            s2_right.to_multiset_ensures();
            assert(ss1 =~~= s1_left + s1_right2);
            assert(ss2 =~~= s2_left + s2_right2);
            assert(s1_right2 =~~= s2_right2);
            lemma_multiset_commutative(s1_left, s1_right2);
            lemma_multiset_commutative(s2_left, s2_right2);
            assert forall|x, y| ss2.contains(x) && ss2.contains(y) implies #[trigger] less.requires(
                (x, y),
            ) by {
                assert(ss2.to_multiset().count(x) > 0);
                assert(ss2.to_multiset().count(y) > 0);
                assert(ss1.contains(x));
                assert(ss1.contains(y));
            }
            assert forall|x| s2_right.contains(x) implies ss2.contains(x) by {
                let i = choose|i: int| 0 <= i < s2_right.len() && s2_right[i] === x;
                assert(s2[i + (pivot_index as int) + 1] === s2_right[i]);
                assert(s2[i + (pivot_index as int) + 1] === ss2[i + (pivot_index as int) + 1
                    - start as int]);
                assert(ss2.contains(x));
            }
            assert forall|x, y|
                s2_right.contains(x) && s2_right.contains(y) implies #[trigger] less.requires(
                (x, y),
            ) by {
                assert(ss2.contains(x));
                assert(ss2.contains(y));
            }
        }
        if pivot_index + 1 < end - 1 {
            self.sort(pivot_index + 1, end, less, Ghost(speclt));
        }
        let ghost s3 = self@;
        let ghost ss3 = s3.subrange(start as int, end as int);
        let ghost (s3_left, s3_right) = (
            s3.subrange(start as int, pivot_index as int),
            s3.subrange(pivot_index as int + 1, end as int),
        );
        proof {
            s3.to_multiset_ensures();
            ss3.to_multiset_ensures();
            s3_left.to_multiset_ensures();
            s3_right.to_multiset_ensures();
            // prove multiset
            lemma_multiset_commutative(s3_left.push(s3[pivot_index as int]), s3_right);
            lemma_multiset_commutative(s2_left.push(s2[pivot_index as int]), s2_right);
            lemma_multiset_commutative(s1_left.push(s1[pivot_index as int]), s1_right);
            assert(s3.subrange(start as int, end as int) =~~= s3_left.push(s3[pivot_index as int])
                + s3_right);
            assert(s2_left =~~= s3_left);
            assert(s2.subrange(start as int, end as int) =~~= s2_left.push(s1[pivot_index as int])
                + s1_right);
            assert(s1_right =~~= s2_right);
            assert(s1.subrange(start as int, end as int) =~~= s1_left.push(s1[pivot_index as int])
                + s1_right);
            assert(s3.subrange(start as int, end as int).to_multiset() =~~= s1.subrange(
                start as int,
                end as int,
            ).to_multiset());
            // prove order
            assert forall|i: int, j: int| start as int <= i < j < end as int implies !speclt(
                #[trigger] self@[j],
                #[trigger] self@[i],
            ) by {
                let v1 = self@[i];
                let v2 = self@[j];
                if i <= (pivot_index as int) <= j {
                    if j > pivot_index as int {
                        assert(pivot_index as int + 1 <= j < (end as int));
                        assert(v2 === s3_right[j - 1 - pivot_index as int]);
                        assert(s3_right.contains(v2));
                        assert(s3_right.to_multiset().contains(v2));
                        assert(s2_right.to_multiset().contains(v2));
                        assert(s2_right.contains(v2));
                        assert(s2.contains(v2));
                        let jj = choose|jj|
                            s2[jj] === v2 && ((pivot_index as int) < jj < (end as int));
                        assert(s2[jj] === v2);
                        assert((pivot_index as int) < jj < (end as int));
                        assert(s2[jj] === s1[jj]);
                        assert(!speclt(s1[jj], s1[pivot_index as int]));
                    } else {
                        assert(!speclt(self@[pivot_index as int], v2));
                    }
                    if i != pivot_index as int {
                        assert(s2[i] === v1);
                        assert(s2[i] === s2_left[i - start as int]);
                        assert(s2_left.contains(v1));
                        assert(s2_left.to_multiset().contains(v1));
                        assert(s1_left.to_multiset().contains(v1));
                        assert(s1_left.contains(v1));
                        assert(speclt(v1, self@[pivot_index as int]));
                    }
                    assert(!speclt(v2, v1));
                }
            }
            let target = self@.subrange(start as int, end as int);
            assert forall|i: int, j: int| 0 <= i < j < target.len() implies !speclt(
                #[trigger] target[j],
                #[trigger] target[i],
            ) by {
                assert(target[i] === self[i + start as int]);
                assert(target[j] === self[j + start as int]);
            }
            assert(seq_is_sorted(target, speclt));
        }
    }
}

} // verus!

================
File: ./source/verismo/src/tspec_e/array/array_t.rs
================

//mod s;
//mod sec;
//pub use s::*;
use super::*;

verus! {

impl<T: Default + core::marker::Copy, const N: IndexType> Default for Array<T, N> {
    #[verifier::external_body]
    fn default() -> Self {
        Array { array: [Default::default();N] }
    }
}

} // verus!
verus! {

impl<T: core::marker::Copy, const N: usize> Clone for Array<T, N> {
    #[verifier(external_body)]
    fn clone(&self) -> (ret: Self)
        ensures
            ret === *self,
    {
        let new = self.array;
        Array { array: new }
    }
}

} // verus!
verus! {

impl<T, const N: IndexType> Array<T, N> {
    pub spec fn _spec_index(&self, i: int) -> T;

    pub open spec fn view(&self) -> Seq<T> {
        Seq::new(Self::spec_len(), |i| self._spec_index(i))
    }

    #[verifier(inline)]
    pub open spec fn spec_index(&self, i: int) -> T {
        self.view().index(i)
    }
}

impl<T: core::marker::Copy, const N: IndexType> Array<T, N> {
    #[verifier(external_body)]
    pub const fn new(elem: T) -> (ret: Self)
        ensures
            ret@ =~~= Seq::new(N as nat, |i| elem),
    {
        let new = [elem;N];
        Array { array: new }
    }
}

} // verus!
use core::ops::Index;
verus! {

impl<T, const N: IndexType> Array<T, N> {
    pub open spec fn spec_len() -> nat {
        N as nat
    }

    #[verifier(external_body)]
    pub fn trusted_len() -> (ret: IndexType)
        ensures
            ret as nat == Self::spec_len(),
    {
        N
    }

    #[verifier(external_body)]
    pub fn trusted_index(&self, index: usize) -> (ret: &T)
        requires
            index < self@.len(),
        ensures
            *ret === self.spec_index(index as int),
    {
        &self.array[index]
    }

    #[verifier(external_body)]
    pub fn trusted_update(&mut self, index: usize, elem: T) -> (ret: T)
        requires
            0 <= index < old(self)@.len(),
        ensures
            self@ === old(self)@.update(index as int, elem),
            ret === old(self)@[index as int],
    {
        core::mem::replace(&mut self.array[index], elem)
    }
}

impl<T: IsConstant + WellFormed, const N: IndexType> Array<T, N> {
    #[verifier(external_body)]
    pub fn as_slice<'a>(&'a self) -> (ret: &'a [T])
        ensures
            self@ === ret@,
    {
        &self.array
    }
}

/*impl<T: Default + SpecDefault, const N: IndexType> Array<T, N>
    {
        // User must update back to help verus get the updated element back
        pub fn trusted_take(&mut self, index: usize) -> (ret: T)
            requires
                0<= index < old(self)@.len(),
            ensures
                ret === old(self).spec_index(index as int),
                self@ === old(self)@.update(index as int, T::spec_default())
        {
            self.trusted_update(index, crate::tspec_e::default::default())
        }
    }*/
} // verus!

================
File: ./source/verismo/src/tspec_e/array/array_utils.rs
================

use super::*;

verus! {

#[verifier(inline)]
pub open spec fn spec_swap<T>(s: Seq<T>, i: int, j: int) -> Seq<T> {
    s.update(i, s[j]).update(j, s[i])
}

} // verus!
verus! {

impl<T: Copy, const N: IndexType> Array<T, N> {
    pub fn swap(&mut self, i: usize, j: usize)
        requires
            i < Self::spec_len(),
            j < Self::spec_len(),
            i.is_constant(),
            j.is_constant(),
        ensures
            forall|k: int|
                (0 <= k < self@.len() && k != i as int && k != j as int) ==> self@[k] === old(
                    self,
                )@[k],
            self@[i as int] === old(self)@[j as int],
            self@[j as int] === old(self)@[i as int],
            self@ =~~= spec_swap(old(self)@, i as int, j as int),
    {
        if i == j {
            assert(i as int == j as int);
            assert(self@ === old(self)@);
            return ;
        }
        let x1 = *self.index(i);
        let x2 = *self.index(j);
        proof {
            assert(x1 === old(self)@[i as int]);
            assert(x2 === old(self)@[j as int]);
        }
        self.update(j, x1);
        self.update(i, x2);
        proof {
            assert forall|k: int|
                (0 <= k < self@.len() && k != i as int && k != j as int) implies self@[k] === old(
                self,
            )@[k] by {}
        }
    }

    pub fn reverse(&mut self, start: usize, end: usize)
        requires
            start.is_constant(),
            end.is_constant(),
            0 <= start as int <= end as int <= Self::spec_len(),
        ensures
            forall|k: int|
                start as int <= k < end as int ==> self@[k] === old(self)@[end as int + start as int
                    - 1 - k],
            forall|k: int|
                0 <= k < start as int || end as int <= k < Self::spec_len() ==> self@[k] === old(
                    self,
                )@[k],
    {
        if end < 1 || start >= end - 1 {
            return ;
        }
        let mut i: usize = start;
        let mut j: usize = end - 1;
        while i < j
            invariant
                i as int <= j as int + 1,
                start as int <= i as int,
                j as int <= end as int - 1,
                end as int - 1 - j as int == i as int - start as int,
                0 <= (start as int) < (end as int) <= Self::spec_len(),
                forall|k: int|
                    start as int <= k < (i as int) || (j as int) < k < end as int ==> self@[k]
                        === old(self)@[end as int + start as int - 1 - k],
                forall|k: int|
                    i as int <= k <= j as int || 0 <= k < start as int || end as int <= k
                        < Self::spec_len() ==> self@[k] === old(self)@[k],
                i.is_constant(),
                j.is_constant(),
        {
            assert(i <= j);
            assert(j < Self::spec_len());
            self.swap(i, j);
            i = i + 1;
            j = j - 1;
        }
    }

    pub fn memset(&mut self, elem: T)
        requires
            old(self)@.len() > 0,
        ensures
            forall|j| 0 <= j < self@.len() ==> self@[j] === elem,
    {
        let mut i = 0;
        while i < self.len()
            invariant
                i.is_constant(),
                0 <= (i as int) <= self@.len(),
                forall|j: int| 0 <= j < (i as int) ==> self@[j] === elem,
        {
            self.set(i, elem);
            i = i + 1;
        }
    }
}

} // verus!

================
File: ./source/verismo/src/tspec_e/array/array_e.rs
================

use core::ops::Index;

use super::*;
use crate::vbox::MutFnTrait;

verus! {

// DO not use secret as index.
impl<T, const N: IndexType> Array<T, N> {
    verus! {
        #[inline(always)]
        pub fn const_len() -> (ret: usize_t)
        ensures
            ret == Self::spec_len(),
        {
            Self::trusted_len()
        }

        #[inline(always)]
        pub fn len(&self) -> (ret: usize)
        ensures
            ret == Self::spec_len(),
            ret.is_constant(),
        {
            Self::trusted_len()
        }

        pub fn index(&self, index: usize) -> (ret: &T)
        requires
            index < self@.len(),
            index.is_constant(),
        ensures
            ret === &self.spec_index(index as int)
        {
            &self.trusted_index(index)
        }
    }

    verus! {
        pub fn index2(&self, index: usize) -> (ret: &T)
        requires
            index < self@.len(),
        ensures
            ret === &self.spec_index(index as int)
        {
            &self.trusted_index(index)
        }
    }
}

impl<T, const N: IndexType> Array<T, N> {
    verus! {
        pub fn update(&mut self, index: usize, elem: T) -> (ret: T)
        requires
            index < old(self).view().len(),
            index.is_constant(),
            //old(self).wf(),
            //elem.wf(),
        ensures
            self@ === old(self)@.update(index as int, self@[index as int]),
            self@[index as int] === elem,
            ret === old(self).spec_index(index as int),
            //self.wf(),
        {
            self.trusted_update(index, elem)
            //assert(self@[index as int].wf());
            /*assert(self@.wf()) by {
                assert forall |i: int| 0 <= i < self@.len()
                implies (#[trigger]self@[i]).wf()
                by {}
            }*/
        }
    }
}

impl<T, const N: IndexType> Array<T, N> {
    verus! {
        pub fn set(&mut self, index: usize_t, elem: T)
        requires
            index < old(self).view().len(),
            index.is_constant(),
        ensures
            self@ === old(self)@.update(index as int, elem),
        {
            self.trusted_update(index, elem);
        }
    }

    verus! {
        pub fn set2(&mut self, index: usize, elem: T)
        requires
            index < old(self).view().len(),
        ensures
            self@ === old(self)@.update(index as int, elem),
        {
            self.trusted_update(index, elem);
        }
    }
}

verus! {
    pub struct ArrayUpdate<T> {
        pub index: usize,
        pub val: T
    }
    impl<T, const N: usize_t, 'a> MutFnTrait<'a, ArrayUpdate<T>, T> for Array<T, N>
    {
        open spec fn spec_update_requires(&self, params: ArrayUpdate<T>) -> bool {
            let ArrayUpdate{index, val} = params;
            &&& index < N
        }

        open spec fn spec_update(&self, prev: &Self, params: ArrayUpdate<T>, ret: T) -> bool {
            let ArrayUpdate{index, val} = params;
            &&& self@[index as int] === val
            &&& ret === prev@[index as int]
            &&& self@ === prev@.update(index as int, self@[index as int])
        }

        fn box_update(&'a mut self, params: ArrayUpdate<T>) -> (ret: T)
        {
            self.trusted_update(params.index, params.val)
        }
    }
}

verus! {
    impl<T: Default + SpecDefault, const N: IndexType> Array<T, N>
    {
        // User must update back to help verus get the updated element back
        pub fn take(&mut self, index: usize) -> (ret: T)
            requires
                index < old(self)@.len(),
                index.is_constant(),
            ensures
                ret === old(self).spec_index(index as int),
                self@ === old(self)@.update(index as int, self@[index as int]),
                self@[index as int] === T::spec_default(),
        {
            let ret = self.update(index, crate::tspec_e::default::default());
            ret

        }

        pub fn take_end(&mut self, index: usize, elem: T)
        requires
            (index as int) < old(self)@.len(),
            index.is_constant(),
        ensures
            self@ === old(self)@.update(index as int, self@[index as int]),
            self@[index as int] === elem,
        {
            self.update(index, elem);
        }
    }
}

} // verus!

================
File: ./source/verismo/src/tspec_e/array/mod.rs
================

mod array_e;
mod array_s;
mod array_t;
pub mod array_utils;
mod sort;

//pub use s::*;
pub use array_e::*;
use array_t::*;

use super::*;

pub type IndexType = usize;

crate::macro_const_int! {
    #[macro_export]
    pub const U32_MAX: u32 = 0xffff_ffff;//4294967295u32;
}

verismo! {
#[derive(Copy)]
#[repr(C)]
#[verifier(external_body)]
#[verifier::accept_recursive_types(T)]
pub struct Array<T, const N: usize> {
    pub array: [T; N],
}
}

================
File: ./source/verismo/src/tspec_e/array/array_s.rs
================

use super::*;

verismo! {
    impl<T: WellFormed, const N: IndexType> WellFormed for Array<T, N> {
        open spec fn wf(&self) -> bool {
           self@.wf()
        }
    }

    impl<T: IsConstant + WellFormed> IsConstant for [T] {
        open spec fn is_constant(&self) -> bool {
            self@.is_constant()
        }

        open spec fn is_constant_to(&self, vmpl: nat) -> bool {
            self@.is_constant_to(vmpl)
        }
    }

    impl<T: IsConstant + WellFormed, const N: IndexType> IsConstant for Array<T, N> {
        open spec fn is_constant(&self) -> bool {
            self@.is_constant()
        }

        open spec fn is_constant_to(&self, vmpl: nat) -> bool {
            self@.is_constant_to(vmpl)
        }
    }

    impl<T: ToSecSeq, const N: IndexType> VTypeCast<SecSeqByte> for Array<T, N>
    {
        open spec fn vspec_cast_to(self) -> SecSeqByte {
            self@.vspec_cast_to()
        }
    }

    // Use reveal_N to reveal the size relationship.
    impl<T: SpecSize, const N: IndexType> SpecSize for Array<T, N>
    {
        #[verifier(inline)]
        open spec fn spec_size_def() -> nat {
            (N * T::spec_size_def()) as nat
        }
    }


    impl<T: SpecSize, const N: IndexType> Array<T, N>
    {
        #[verifier(external_body)]
        pub proof fn reveal_N(n: nat)
        requires
            N == n
        ensures
            spec_size::<Array<T, N>>() ==  n * T::spec_size_def(),
            spec_size::<Array<T, N>>() ==  N * T::spec_size_def()
        {}
    }
}

================
File: ./source/verismo/src/tspec_e/math/pow_e.rs
================

use super::*;

verus! {

pub fn pow2_to_bits(val: u64) -> (ret: u64)
    requires
        spec_bit64_is_pow_of_2(val as int),
    ensures
        1u64 << (ret as u64) == val as int,
        0 <= (ret as int) < 64,
        ret as u64 == spec_pow2_to_bits(val as u64),
        ret as u64 == spec_pow2_to_bits_exe(val as nat),
{
    proof {
        bit_shl64_auto();
        proof_pow2_to_bits(val as nat);
    }
    if val > 1 {
        let prev = pow2_to_bits(val >> 1u64);
        prev + 1
    } else {
        0
    }
}

} // verus!

================
File: ./source/verismo/src/tspec_e/math/mod.rs
================

mod align_e;
mod bits_e;
mod minmax;
mod pow_e;
pub use align_e::*;
pub use bits_e::*;
pub use minmax::*;
pub use pow_e::*;

use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

================
File: ./source/verismo/src/tspec_e/math/align_e.rs
================

use super::align_s::*;
use super::*;

verus! {

pub fn align_up_by(val: u64, align: u64) -> (ret: u64)
    requires
        spec_bit64_is_pow_of_2(align as int),
        val + align <= MAXU64!(),
    ensures
        (ret as int) % (align as int) == 0,
        ret as int == spec_align_up(val as int, align as int),
        spec_is_align_up_by_int(val as int, align as int, ret as int),
{
    proof {
        proof_align_up(val as nat, align as nat);
        proof_align_is_aligned(val as int, align as int);
    }
    let mask = align - 1;
    if val & mask == 0 {
        val
    } else {
        (val | mask) + 1
    }
}

} // verus!
verismo_simple! {
    pub fn align_down_by(val: u64, align: u64) -> (ret: u64)
        requires
            spec_bit64_is_pow_of_2(align as int),
            val.wf(),
            align.wf(),
        ensures
            ret == spec_align_down(val as int, align as int),
            spec_is_align_down_by_int(val as int, align as int, ret as int),
            (ret as int) == val as int - val as int % align as int,
            ret as int == (val as u64 >> spec_pow2_to_bits(align as u64)) << spec_pow2_to_bits(align as u64),
            (ret as int) % (align as int) == 0,
            val.is_constant() && align.is_constant() ==> ret.is_constant(),
            ret.wf(),
    {
        proof {
            proof_align_down(val as nat, align as nat);
            proof_align_is_aligned(val as int, align as int);
        }
        val & (!(align - 1))
    }
}

================
File: ./source/verismo/src/tspec_e/math/minmax.rs
================

use super::*;

verus! {

pub fn min(x: u64, y: u64) -> (res: u64)
    ensures
        res as int === spec_min(x as int, y as int),
{
    if x < y {
        x
    } else {
        y
    }
}

pub fn max(x: u64, y: u64) -> (res: u64)
    ensures
        res as int === spec_max(x as int, y as int),
{
    if x > y {
        x
    } else {
        y
    }
}

} // verus!

================
File: ./source/verismo/src/tspec_e/math/bits_e.rs
================

use super::bits_p::*;
use super::*;
use crate::tspec::*;

verus! {

pub fn bit_check(val: u64, bit: u64) -> (ret: bool)
    requires
        bit < 64,
    ensures
        ret == spec_has_bit_set(val as u64, bit as u64),
{
    proof {
        lemma_bits64!();
    }
    val & (1u64 << bit) != 0
}

} // verus!
verismo! {
    pub fn bit_set(val: &mut u64, bit: u64)
    requires
        bit < 64,
    ensures
        *val == spec_bit_set(*old(val) as u64, bit as u64),
        spec_has_bit_set(*val as u64, bit as u64),
    {
        proof{
            proof_bit_check(*val as u64, bit as u64);
        }
        *val = *val | (1u64 << bit)
    }

    pub fn bit_clear(val: &mut u64, bit: u64)
    requires
        bit < 64,
    ensures
        *val == spec_bit_clear((*old(val)) as u64, bit as u64),
        !spec_has_bit_set((*val) as u64, bit as u64),
        //(*val)@ === ((*old(val)) & (!(1u64_s << bit)))@
    {
        proof{
            proof_bit_check((*val) as u64, bit as u64);
        }
        *val = (*val) & (!(1u64_s << bit))
    }

    pub open spec fn spec_u32_to_u64(low: int, high: int) -> int
    {
        low + high * 0x1_0000_0000
    }

    pub open spec fn spec_u64_highu32(value: int) -> int
    {
        value / 0x1_0000_0000
    }

    #[inline]
    pub fn u32_to_u64(low: u32, high: u32) -> (ret: u64)
    ensures
        ret as int == spec_u32_to_u64(low as int, high as int)
    {
        proof {
            bit_shl64_pow2_auto();
            bit_lsh64_mul_rel(high as u64, 32);
        }
        let low = low as u64;
        let high = high as u64;
        low  + (high << 32u64)
    }

    #[inline]
    pub fn u64_highu32(value: u64) -> (ret: u32)
    ensures
        ret as int == spec_u64_highu32(value as int)
    {
        proof {
            bit_rsh64_div_rel(value as u64, 32);
            bit_shl64_pow2_auto();
        }
        (value >> 32u64) as u32
    }

    pub fn fill_tailing_ones(input: u64) -> (ret: u64)
    ensures
        ret == spec_fill_ones_exe(input as u64),
    {
        let mut ret = input;
        ret = ret | (ret >> 1u64);
        ret = ret | (ret >> 2u64);
        ret = ret | (ret >> 4u64);
        ret = ret | (ret >> 8u64);
        ret = ret | (ret >> 16u64);
        ret = ret | (ret >> 32u64);
        ret
    }
}

verus! {

pub fn prev_power_of_two(input: u64) -> (ret: u64)
    ensures
        spec_is_prev_power_of_two(input as nat, ret as nat),
{
    proof {
        lemma_prev_power_of_two(input as u64);
    }
    if input == 0 {
        0
    } else {
        proof {
            lemma_fill_ones(input as u64);
        }
        let mut ret: u64 = fill_tailing_ones(input.into()).into();
        ret = (ret >> 1u64) + 1u64;
        ret
    }
}

pub fn next_power_of_two(input: u64) -> (ret: u64)
    requires
        input <= POW2!(63),
    ensures
        spec_is_next_power_of_two(input as nat, ret as nat),
{
    proof {
        lemma_next_power_of_two(input as u64);
    }
    if input <= 1 {
        1
    } else {
        let v = input - 1;
        proof {
            lemma_fill_ones(v as u64);
        }
        fill_tailing_ones(v.into()).reveal_value() + 1
    }
}

} // verus!

================
File: ./source/verismo/src/tspec_e/mod.rs
================

// math -> size
// size -> array
// size -> stream
#[macro_use]
mod math;
mod array;
mod default;
mod size_e;
mod type_test;

pub use core::mem::*;
pub use core::ops::{Add, BitAnd, BitOr, BitXor, Div, Mul, Neg, Rem, Shl, Shr, Sub};

pub use array::*;
pub use math::*;
pub use size_e::*;
pub use vops::*;

pub use crate::primitives_e::*;
pub use crate::tspec::*;

/*mod array;
mod cast;
mod default;
mod size_e;


//use builtin::*;
pub use cast::*;
pub use default::*;
pub use math::*;
pub use size_e::*;
pub use crate::tspec::*;
*/

/*
pub use crate::pervasive::map::Map;
pub use crate::pervasive::option::*;
pub use crate::pervasive::seq::Seq;
pub use crate::pervasive::set::Set;
pub use crate::pervasive::set_lib;
*/

================
File: ./source/verismo/src/tspec_e/default.rs
================

use super::*;
verismo! {
    #[verifier(external_body)]
    pub fn default<T: Default + SpecDefault>() -> (ret: T)
    ensures
        ret === T::spec_default(),
    {
        T::default()
    }
}

================
File: ./source/verismo/src/tspec_e/type_test.rs
================

use super::*;

impl_secure_type! {(), type}
use vops::VEq;

verismo! {
    // Automatically Add derive(VTypeCast)
    #[repr(C, align(1))]
    struct S1 {
        pub a: (u64, u8),
        pub c: u8,
        pub b: u8,
    }// 9 + 1 + 1 = 11

    #[repr(C, align(1))]
    struct S2 {
        pub a: u64,
        pub b: usize,
        pub c: S1,
    }// 8 + 8 + 11 = 27

    #[repr(C, align(1))]
    struct S3 {
        pub c: S2,
    }// 8 + 8 + 11 = 27

    #[repr(C, align(1))]
    struct S4 {
        pub c: S3,
    }// 8 + 8 + 11 = 27

    proof fn test_type_size2()
    ensures
        S4::spec_size_def() == 27,
        //S4::spec_size_def() == 2,
    {
    }

    #[repr(C, align(1))]
    #[derive(VDefault)]
    pub struct S5 {
        pub a: Ghost<u64>,
        pub t: Tracked<u32>,
        pub c: u64,
    }

    proof fn test_default()
    ensures
        S5::spec_default().a === arbitrary(),
        S5::spec_default().c == 0,
    {}

    fn test_exe_default() -> (ret: S5)
    ensures
        ret.c == 0,
        ret.c@ == u64::spec_default(),
    {
        S5::default()
    }
}

================
File: ./source/verismo/src/entry.s
================

.globl boot_stack
.globl boot_stack_end
.globl stack_guard_page
.globl verismo_start
.globl verismo_end
.globl _start
.globl ap_entry
_start:
verismo_start:
	cld
	cli
	mov %rsi, %rdi
	call bsp_call
.align 0x1000
ap_entry:
	cld
	cli
	call ap_call
ap_loop:
	jmp ap_loop

.section .bss
.align 8
monitor_data_addr:
.space 8
verismo_end:
================
File: ./source/verismo/src/snp/cpuid.rs
================

use vstd::slice::slice_index_get;

use super::ghcb::*;
use crate::arch::reg::RegName;
use crate::debug::VPrintAtLevel;
use crate::global::*;
use crate::registers::*;
use crate::snp::SnpCoreSharedMem;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::{BIT32, BIT64};

verus! {

const SNP_CPUID_COUNT_MAX: usize = 64;

} // verus!
verismo_simple! {

#[repr(C, align(1))]
#[derive(VClone, Copy)]
pub struct RegABCD {
    pub eax: u32,
    pub ebx: u32,
    pub ecx: u32,
    pub edx: u32,
}

#[repr(C, align(1))]
#[derive(VClone, Copy)]
pub struct SnpCpuidFn {
    pub eax_in: u32,
    pub ecx_in: u32,
    pub xcr0_in: u64,
    pub xss_in: u64,
    pub rets: RegABCD,
    pub reserved: u64,
}

#[repr(C, align(1))]
#[derive(VClone, Copy)]
pub struct SnpCpuidTable {
    pub count: u32,
    pub reserved_1: u32,
    pub reserved_2: u64,
    pub fn_: Array<SnpCpuidFn, SNP_CPUID_COUNT_MAX>,
    pub reserved_3: [u8; 1008],
}
}

verus! {

impl SnpCpuidTable {
    pub proof fn lemma_size() -> (ret: nat)
        ensures
            (ret == spec_size::<SnpCpuidTable>()),
            ret == 0x1000,
    {
        let ret = spec_size::<SnpCpuidTable>();
        ret
    }
}

} // verus!
verus! {

//AMD 11.3.2 Enabling Extended SSE Instruction Execution
/*See linux/latest/source/arch/x86/include/asm/cpufeatures.h*//* CPUID level 0x00000001 (EDX), word 0 */
pub const X86_FEATURE_FXSR: u32 = BIT32!(24);

pub const X86_FEATURE_XMM: u32 = BIT32!(25);

//sse
pub const X86_FEATURE_XMM2: u32 = BIT32!(26);

//sse2
/* Intel-defined CPU features, CPUID level 0x00000001 (ECX), word 4 */

pub const X86_FEATURE_XMM3: u32 = BIT32!(0);

//sse3
pub const X86_FEATURE_PCLMULQDQ: u32 = BIT32!(1);

pub const X86_FEATURE_MOVBE: u32 = BIT32!(22);

pub const X86_FEATURE_AES: u32 = BIT32!(25);

pub const X86_FEATURE_XSAVE: u32 = BIT32!(26);

// XSAVE instruction enabled in the OS
pub const X86_FEATURE_OSXSAVE: u32 = BIT32!(27);

pub const X86_FEATURE_AVX: u32 = BIT32!(28);

//has_aesni && has_pclmulqdq && has_avx && has_sse && has_movbe
pub const EVERCRYPT_USED_FEATURES: u32 = X86_FEATURE_AES | X86_FEATURE_PCLMULQDQ | X86_FEATURE_AVX
    | X86_FEATURE_XMM3;

/* Intel-defined CPU features, CPUID level 0x00000007 (ECX), word 4 */

pub const X86_FEATURE_VPCLMULQDQ: u32 = BIT32!(10);

} // verus!
// return regflag if feature is set
macro_rules! feature {
    ($reg: ident, $feature: ident, $regflag: expr) => {
        if $reg & $feature == $feature {
            $regflag
        } else {
            0
        }
    };
}

verus! {

pub fn process_cpuid(eax: u32, ecx: u32, xcr0: u64, xss: u64, cpuid_table: &[SnpCpuidFn]) -> (ret:
    Option<RegABCD>)
    requires
        cpuid_table@.is_constant(),
    ensures
        ret.is_constant(),
        ret.is_Some() ==> exists|i|
            0 <= i < cpuid_table@.len() && cpuid_table@[i].eax_in@.val == eax
                && cpuid_table@[i].ecx_in@.val == ecx && ret.get_Some_0() === cpuid_table@[i].rets,
{
    let mut i: usize = 0;
    let mut ret = None;
    let n = cpuid_table.len();
    while i < n
        invariant_except_break
            cpuid_table@.is_constant(),
            n == cpuid_table@.len(),
            i.is_constant(),
            0 <= (i as int) <= n,
            forall|k: int|
                0 <= k < (i as int) ==> !(cpuid_table@[k].eax_in@.val == eax
                    && cpuid_table@[k].ecx_in@.val == ecx),
            ret.is_None(),
        ensures
            (0 <= (i as int) < n) == ret.is_Some(),
            (i == n) == ret.is_None(),
            ret.is_Some() ==> cpuid_table@[i as int].eax_in@.val == eax
                && cpuid_table@[i as int].ecx_in@.val == ecx && ret.get_Some_0()
                === cpuid_table@[i as int].rets,
    {
        let leaf = slice_index_get(cpuid_table, i);
        if (eax == leaf.eax_in.into()) && (ecx == leaf.ecx_in.into()) {
            ret = Some(leaf.rets);
            break ;
        }
        i = i + 1;
    }
    return ret;
}

} // verus!
verismo_simple! {
pub trait CryptoFeatures {
    spec fn has_avx_sse_features(&self) -> bool;
}

// hidden bit op
pub closed spec fn cr4_wf_for_crypto(cr4: u64) -> bool {
    true
    //cr4 & CR4_OSFXSR == CR4_OSFXSR
}

// hidden bit op
pub closed spec fn xcr0_wf_for_crypto(xcr0: u64) -> bool {
    xcr0 == (XCR0_X87 | XCR0_AVX | XCR0_SSE)
}

impl CryptoFeatures for Map<RegName, RegisterPerm> {
    // Reveal regs but hide bit ops.
    open spec fn has_avx_sse_features(&self) -> bool {
        let cr4: u64_s = self[RegName::Cr4]@.value();
        let xcr0: u64_s = self[RegName::XCr0]@.value();
        &&& cr4_wf_for_crypto(cr4 as u64_t)
        &&& xcr0_wf_for_crypto(xcr0 as u64_t)
    }
}
}

verus! {

/// Initialize AVX and SSE features for crypto
pub fn init_cpu_for_crypto(cpuid_page: &SnpCpuidTable, Tracked(cs): Tracked<&mut SnpCoreSharedMem>)
    requires
        old(cs).inv(),
        cpuid_page.is_constant(),
    ensures
        cs.inv(),
        cs.snpcore.regs.has_avx_sse_features(),
        cs.only_lock_reg_coremode_updated(
            *old(cs),
            set![RegName::Cr4, RegName::XCr0, GHCB_REGID()],
            set![spec_CONSOLE_lockid()],
        ),
{
    let cpuid_table = cpuid_page.fn_.as_slice();
    process_cpuid(0x8000_001f, 0, 0, 0, cpuid_table);
    let ret = process_cpuid(0x1, 0, 0, 0, cpuid_table);
    let mut feature_flg: u32 = 0;
    if let Some(RegABCD { ecx, .. }) = ret {
        let ecx: u32 = ecx.into();
        if (ecx & EVERCRYPT_USED_FEATURES) != EVERCRYPT_USED_FEATURES {
            proof {
                reveal_strlit("!EVERCRYPT_USED_FEATURES: ");
            }
            (new_strlit("!EVERCRYPT_USED_FEATURES: "), u32_s::new(ecx)).err(Tracked(cs));
            vc_terminate_debug(SM_EVERCRYPT_EXIT, Tracked(cs));
        }
        feature_flg = ecx;
    } else {
        vc_terminate_debug(SM_EVERCRYPT_EXIT, Tracked(cs));
    }
    let tracked mut cr4perm = cs.snpcore.regs.tracked_remove(RegName::Cr4);
    let mut reg = CR4.read(Tracked(&cr4perm));
    let fxsr: u64 = feature!(feature_flg, X86_FEATURE_FXSR, CR4_OSFXSR | CR4_OSXMMEXCPT);
    let xsave: u64 = feature!(feature_flg, X86_FEATURE_XSAVE, CR4_OSXSAVE);
    reg = reg.bitor(fxsr).bitor(xsave);
    CR4.write(reg, Tracked(&mut cr4perm));
    proof {
        cs.snpcore.regs.tracked_insert(RegName::Cr4, cr4perm);
    }
    // Check hardware support
    //let (eax, _, _, _)
    process_cpuid(0xd, 0, 0, 0, cpuid_table);
    let ret = process_cpuid(0x7, 0, 0, 0, cpuid_table);
    if let Some(RegABCD { ecx, .. }) = ret {
        let ecx: u32 = ecx.into();
        if ecx & X86_FEATURE_VPCLMULQDQ != X86_FEATURE_VPCLMULQDQ {
            proof {
                reveal_strlit("!X86_FEATURE_VPCLMULQDQ: ");
            }
            (new_strlit("!X86_FEATURE_VPCLMULQDQ: "), u32_s::new(ecx)).err(Tracked(cs));
            vc_terminate_debug(SM_EVERCRYPT_EXIT, Tracked(cs));
        }
    } else {
        vc_terminate_debug(SM_EVERCRYPT_EXIT, Tracked(cs));
    }
    let tracked mut xcr0perm = cs.snpcore.regs.tracked_remove(RegName::XCr0);
    // set up XFEATURE_MASK in xcr0
    let xcr0 = u64_s::new(XCR0_X87 | XCR0_AVX | XCR0_SSE);
    XCR0.write(xcr0, Tracked(&mut xcr0perm));
    proof {
        cs.snpcore.regs.tracked_insert(RegName::XCr0, xcr0perm);
        reveal_strlit("Crypto CPU init\n");
    }
    new_strlit("Crypto CPU init\n").info(Tracked(cs));
}

} // verus!

================
File: ./source/verismo/src/snp/cpu/vmsa.rs
================

use registers::*;

use super::*;
use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::arch::reg::*;
use crate::debug::VPrint;
use crate::global::spec_ALLOCATOR_lockid;
use crate::ptr::*;
use crate::registers::*;
use crate::security::SnpSecretsPageLayout;
use crate::snp::percpu::StackPages;
use crate::snp::SnpCoreSharedMem;
use crate::vbox::{BorrowFnTrait, MutFnTrait, MutFnWithCSTrait, VBox};

verus! {

#[vbit_struct(SevFeatures, u64)]
pub struct SevFeaturesSpec {
    #[vbits(0, 0)]
    snp: u64,
    #[vbits(1, 1)]
    vtom: u64,
    #[vbits(2, 2)]
    reflectvc: u64,
    #[vbits(3, 3)]
    restrict_inj: u64,
    #[vbits(4, 4)]
    alternate_inj: u64,
    #[vbits(7, 7)]
    btb_isolation: u64,
    #[vbits(9, 9)]
    secure_tsc: u64,
}

} // verus!
verismo_simple! {
#[derive(VPrint)]
#[repr(C, align(1))]
pub struct VmsaSegmentRegister {
    pub selector: u16,
    pub attr: u16,
    pub limit: u32,
    pub base: u64,
}

#[derive(SpecGetter, SpecSetter, VPrint)]
#[repr(C, align(1))]
/// Virtual Machine Saving Area for world switches
pub struct Vmsa {
    pub es: VmsaSegmentRegister,
    pub cs: VmsaSegmentRegister,
    pub ss: VmsaSegmentRegister,
    pub ds: VmsaSegmentRegister,
    pub fs: VmsaSegmentRegister,
    pub gs: VmsaSegmentRegister,
    pub gdtr: VmsaSegmentRegister,
    pub reserved_ldtr_idtr_tr: Array<u8, 90>,

    #[def_offset]
    pub vmpl: u8,
    pub cpl: u8,

    pub reserved2: Array<u8, 4>,

    pub efer: u64,

    pub reserved3: Array<u8, 112>,

    pub cr4: u64,
    pub cr3: u64,
    pub cr0: u64,
    pub reserved_dr7_6: Array<u8, 16>,
    pub rflags: u64,
    pub rip: u64,

    pub reserved4: Array<u8, 88>,

    pub rsp: u64,

    pub reserved5: Array<u8, 24>,

    #[def_offset]
    pub rax: u64,

    pub reserved6: Array<u8, 104>,

    pub gpat: u64,

    pub reserved7: Array<u8, 152>,

    #[def_offset]
    pub rcx: u64,
    #[def_offset]
    pub rdx: u64,
    #[def_offset]
    pub rbx: u64,

    pub reserved8: Array<u8, 8>,

    pub rbp: u64,
    pub rsi: u64,
    pub rdi: u64,
    //pub r8: u64,

    pub reserved_9_r9_15_exits_scratch: Array<u8, 112>,

    pub sev_features: u64,
    pub vintr_ctrl: u64,

    #[def_offset]
    pub guest_error_code: u64,

    pub virtual_tom: u64,

    pub reserved_12: Array<u8, 24>,
    pub xcr0: u64,

    pub reserved13: [u8; 3088],
}
}

pub type VmsaPage = Vmsa;

verus! {

proof fn proof_vmsa_size()
    ensures
        spec_size::<VmsaPage>() == spec_cast_integer::<_, nat>(PAGE_SIZE!()),
{
}

} // verus!
def_asm_addr_for! {
    ap_entry_addr = ap_entry
}

verus! {

#[derive(IsConstant, WellFormed, SpecSize, VTypeCastSec)]
pub struct PerCpuData<'a> {
    pub secret: &'a SnpSecretsPageLayout,
    pub cpu: u32,
    pub resvd: u32,
}

} // verus!
verus! {

impl<'a> PerCpuData<'a> {
    pub open spec fn inv(&self) -> bool {
        &&& self.wf()
        &&& self.secret.wf_mastersecret()
    }
}

} // verus!
verus! {

pub open spec fn ensures_init_ap_vmsa(
    vmsa: Vmsa,
    new_vmsa: Vmsa,
    cpu: VBox<PerCpuData>,
    cs: SnpCoreSharedMem,
    newcs: SnpCoreSharedMem,
) -> bool {
    &&& newcs.inv_ac()
    &&& newcs.only_lock_reg_updated(cs, set![], set![spec_ALLOCATOR_lockid()])
    &&& vmsa.is_constant() ==> new_vmsa.is_constant()
    &&& (new_vmsa.rdi@.val) == cpu.id()
    &&& (new_vmsa.rsp@.val) != 0
    &&& (new_vmsa.vmpl@.val == 0)
}

pub open spec fn requires_init_ap_vmsa(
    cpu: VBox<PerCpuData>,
    gdt: &GDT,
    cs: SnpCoreSharedMem,
) -> bool {
    &&& cs.inv_ac()
    &&& gdt@.is_constant()
    &&& cpu.wf()
    &&& cpu@.inv()
}

fn init_ap_vmsa<'a>(
    vmsa: &mut Vmsa,
    cpu: VBox<PerCpuData>,
    gdt: &GDT,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        requires_init_ap_vmsa(cpu, gdt, *old(cs)),
    ensures
        ensures_init_ap_vmsa(*old(vmsa), *vmsa, cpu, *old(cs), *cs),
{
    let stack = VBox::<StackPages>::new_aligned_uninit(PAGE_SIZE, Tracked(cs));
    // Copy control registers
    vmsa.efer = Msr { reg: MSR_EFER_BASE.into() }.read(
        Tracked(cs.snpcore.regs.tracked_borrow(RegName::MSR(MSR_EFER_BASE))),
    );
    vmsa.xcr0 = XCR0.read(Tracked(cs.snpcore.regs.tracked_borrow(RegName::XCr0)));
    vmsa.cr0 = CR0.read(Tracked(cs.snpcore.regs.tracked_borrow(RegName::Cr0)));
    vmsa.cr3 = CR3.read(Tracked(cs.snpcore.regs.tracked_borrow(RegName::Cr3)));
    vmsa.cr4 = CR4.read(Tracked(cs.snpcore.regs.tracked_borrow(RegName::Cr4)));
    // Set AP entry address
    vmsa.rip = ap_entry_addr().into();
    // Default PAT
    vmsa.gpat = crate::pgtable_e::PAT_RESET_VAL.into();
    vmsa.rdi = cpu.into_raw().0.as_u64().into();
    // Set stack end addr
    vmsa.rsp = (stack.into_raw().0.to_usize() + size_of::<StackPages>()).into();
    // Enable SNP
    // Use restricted injection
    let sev_features = SevFeatures::empty().set_snp(1u64).set_vtom(0u64).set_reflectvc(
        0u64,
    ).set_restrict_inj(1u64).set_alternate_inj(0u64);
    vmsa.sev_features = sev_features.value.into();
    vmsa.vmpl = 0u64.into();
    // Reuse GDT
    let gdtr = GdtBaseLimit.read(Tracked(cs.snpcore.regs.tracked_borrow(RegName::GdtrBaseLimit)));
    vmsa.gdtr.base = gdtr.base;
    vmsa.gdtr.limit = gdtr.limit.into();
    //let gdt_ptr = SnpPPtr::from_usize(gdtr.base as usize);
    //let gdt = gdt_ptr.borrow(Tracked(gdt_perm));
    fill_seg(&mut vmsa.cs, gdt, GDT_KERNEL_CS, GDTR_LIMIT, GDTR_BASE);
    fill_seg(&mut vmsa.es, gdt, GDT_KERNEL_DS, GDTR_LIMIT, GDTR_BASE);
    fill_seg(&mut vmsa.ss, gdt, GDT_KERNEL_DS, GDTR_LIMIT, GDTR_BASE);
    fill_seg(&mut vmsa.ds, gdt, GDT_KERNEL_DS, GDTR_LIMIT, GDTR_BASE);
}

} // verus!
verus! {

pub struct InitApVmsa;

pub struct InitAPParams<'a> {
    pub fun: InitApVmsa,
    pub cpu: VBox<PerCpuData<'a>>,
    pub gdt: &'a GDT,
}

pub type InitAPOut = u8;

impl<'a, 'b> MutFnWithCSTrait<'a, SnpCoreSharedMem, InitAPParams<'b>, InitAPOut> for VmsaPage {
    open spec fn spec_update_cs_requires(
        &self,
        params: InitAPParams<'b>,
        cs: SnpCoreSharedMem,
    ) -> bool {
        requires_init_ap_vmsa(params.cpu, params.gdt, cs)
    }

    open spec fn spec_update_cs(
        &self,
        prev: &Self,
        params: InitAPParams<'b>,
        oldcs: SnpCoreSharedMem,
        ret: InitAPOut,
        cs: SnpCoreSharedMem,
    ) -> bool {
        ensures_init_ap_vmsa(*prev, *self, params.cpu, oldcs, cs)
    }

    fn box_update_cs(
        &'a mut self,
        params: InitAPParams<'b>,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: u8) {
        init_ap_vmsa(self, params.cpu, params.gdt, Tracked(cs));
        0
    }
}

} // verus!
verus! {

pub fn fill_seg(
    seg: &mut VmsaSegmentRegister,
    gdt: &GDT,
    gdt_idx: usize_t,
    limit: u32_t,
    base: u64_t,
)
    requires
        gdt_idx < gdt@.len(),
        gdt@.is_constant(),
    ensures
        seg.is_constant(),
{
    assert(gdt_idx < 32);
    let selector = (gdt_idx * 8usize);
    let entry = Descriptor { value: (*gdt.index(gdt_idx)).into() };
    let attr: u16 = (entry.get_attr_0_7() as u16) | (entry.get_attr_8_11() << 8u64) as u16;
    seg.selector = selector.into();
    seg.attr = attr.into();
    seg.limit = limit.into();
    seg.base = base.into();
}

} // verus!
verus! {

pub struct UpdateRichOSVmsa<'a> {
    pub gdt: &'a GDT,
    pub gdtr_addr: u64,
    pub bp_addr: u64,
    pub kernel_addr: u64,
    pub vmpl: u8,
}

} // verus!
verus! {

pub struct UpdateVMPL {
    pub vmpl: u8,
}

impl<'a> MutFnTrait<'a, UpdateVMPL, u8> for VmsaPage {
    open spec fn spec_update_requires(&self, params: UpdateVMPL) -> bool {
        &&& 0 <= params.vmpl < 4
    }

    open spec fn spec_update(&self, prev: &Self, params: UpdateVMPL, ret: u8) -> bool {
        &&& self.vmpl.spec_eq(params.vmpl)
        &&& *self === prev.spec_set_vmpl(self.vmpl)
        &&& self.vmpl.is_constant()
    }

    fn box_update(&'a mut self, params: UpdateVMPL) -> (ret: u8) {
        self.vmpl = params.vmpl.into();
        0
    }
}

} // verus!
verismo_simple! {
pub struct UpdateRichOSVmsaOut;
impl<'a, 'b> MutFnTrait<'a, UpdateRichOSVmsa<'b>, UpdateRichOSVmsaOut> for VmsaPage {
    open spec fn spec_update_requires(&self, params: UpdateRichOSVmsa<'b>) -> bool {
        &&& params.gdt.is_constant()
    }

    open spec fn spec_update(&self, prev: &Self, params: UpdateRichOSVmsa<'b>, ret: UpdateRichOSVmsaOut) -> bool {
        let UpdateRichOSVmsa{gdt, vmpl, gdtr_addr, bp_addr, kernel_addr} = params;
        &&& prev.is_constant() ==> self.is_constant()
        &&& (self.gdtr.base as int) == gdtr_addr
        &&& self.vmpl == vmpl
        &&& self.rip == kernel_addr
        &&& self.rsi == bp_addr
    }

    fn box_update(&'a mut self, params: UpdateRichOSVmsa<'b>) -> (ret: UpdateRichOSVmsaOut)
    {
        let UpdateRichOSVmsa{gdt, vmpl, gdtr_addr, bp_addr, kernel_addr} = params;

        self.guest_error_code = 0xfff;
        self.gpat = crate::pgtable_e::PAT_RESET_VAL.into();
        self.rsi= bp_addr.into();
        self.cr0 = 1; // enable protected mode
        self.rflags = 2;
        self.xcr0 = 1;
        self.efer = 0x1000;
        self.rip = kernel_addr.into();

        let gdt_size: u32 = size_of::<GDT>() as u32;
        self.gdtr.base = gdtr_addr.into();
        self.gdtr.limit = gdt_size - 1;
        self.gdtr.selector = 0;
        self.gdtr.attr = 0;
        fill_seg(&mut self.cs, gdt, GDT_KERNEL_CS, GDTR_LIMIT, GDTR_BASE);
        fill_seg(&mut self.es, gdt, GDT_KERNEL_DS, GDTR_LIMIT, GDTR_BASE);
        fill_seg(&mut self.ss, gdt, GDT_KERNEL_DS, GDTR_LIMIT, GDTR_BASE);
        fill_seg(&mut self.ds, gdt, GDT_KERNEL_DS, GDTR_LIMIT, GDTR_BASE);

        let sev_features = SevFeatures::empty().set_snp(1u64_t)
            .set_vtom(0u64_t)
            .set_reflectvc(0u64_t)
            .set_restrict_inj(1u64_t)
            .set_alternate_inj(0u64_t);
        self.sev_features = sev_features.value.into();
        self.vmpl = vmpl.into();
        UpdateRichOSVmsaOut
    }
}
}

================
File: ./source/verismo/src/snp/cpu/gdt.rs
================

use super::*;
use crate::arch::reg::*;
use crate::registers::*;

verus! {

pub const GDT_KERNEL_CS: usize = 1;

pub const GDT_KERNEL_DS: usize = 3;

pub const GDTR_LIMIT: u32 = 0xffff_ffff;

pub const GDTR_BASE: u64 = 0;

} // verus!
verus! {

#[vbit_struct(DescriptorAttr0_7, u64)]
pub struct DescriptorAttr0_7Spec {
    #[vbits(0, 0)]
    pub accessed: u64,
    #[vbits(1, 1)]
    pub write: u64,
    #[vbits(2, 2)]
    pub conform: u64,
    #[vbits(3, 3)]
    pub exe: u64,
    #[vbits(4, 4)]
    pub sys: u64,
    #[vbits(5, 6)]
    pub dpl: u64,
    #[vbits(7, 7)]
    pub present: u64,
}

#[vbit_struct(DescriptorAttr8_11, u64)]
pub struct DescriptorAttr8_11Spec {
    #[vbits(0, 0)]
    pub avl: u64,
    #[vbits(1, 1)]
    pub long: u64,
    #[vbits(2, 2)]
    pub size32_or_16: u64,
    #[vbits(3, 3)]
    pub granularity: u64,
}

#[vbit_struct(Descriptor, u64)]
pub struct DescriptorSpec {
    #[vbits(0, 15)]
    pub limit0_15: u64,
    #[vbits(16, 39)]
    pub base0_23: u64,
    #[vbits(40, 47)]
    pub attr_0_7: u64,
    #[vbits(48, 51)]
    pub limit16_19: u64,
    #[vbits(52, 55)]
    pub attr_8_11: u64,
    #[vbits(56, 64)]
    pub base24_31: u64,
}

} // verus!
verus! {

impl Descriptor {
    pub const fn entry_cs_sys() -> Self {
        Self::empty().set_limit0_15(0xffff).set_limit16_19(0xf).set_base0_23(0).set_base24_31(
            0,
        ).set_attr_0_7(
            DescriptorAttr0_7::empty().set_dpl(0).set_sys(1).set_write(1).set_exe(1).set_present(
                1,
            ).value(),
        ).set_attr_8_11(
            DescriptorAttr8_11::empty().set_long(0).set_size32_or_16(1).set_granularity(1).value(),
        )
        //Descriptor { value: 0xcf9b000000ffff }

    }

    pub const fn entry_cs_user() -> Self {
        Self::entry_cs_sys().set_attr_0_7(
            DescriptorAttr0_7::new(Self::entry_cs_sys().get_attr_0_7()).set_dpl(3).value(),
        )
        //Descriptor { value: 0xcffb000000ffff }

    }

    pub const fn entry_ds_sys() -> Self {
        Self::entry_cs_sys().set_attr_0_7(
            DescriptorAttr0_7::new(Self::entry_cs_sys().get_attr_0_7()).set_exe(0).value(),
        )
        //Descriptor { value: 0xcf93000000ffff }

    }

    pub const fn entry_ds_user() -> Self {
        Self::entry_ds_sys().set_attr_0_7(
            DescriptorAttr0_7::new(Self::entry_cs_sys().get_attr_0_7()).set_dpl(3).value(),
        )
    }
}

} // verus!
verus! {

pub type GDT = Array<u64_s, 32>;

} // verus!
verismo_simple! {
    #[repr(C, packed)]
    #[derive(VDefault)]
    pub struct Gdtr {
        /// Size of the DT.
        pub limit: u16,
        /// Pointer to the memory region containing the DT.
        pub base: u64,
    }
}

crate::impl_dpr! {
    GdtBaseLimit, Gdtr, "gdt", GdtrBaseLimit,
}

================
File: ./source/verismo/src/snp/cpu/mod.rs
================

mod gdt;
mod vmsa;

pub use gdt::*;
pub use vmsa::*;

use crate::snp::ghcb::{vc_terminate_debug, SM_TERM_INVALID_PARAM};
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

================
File: ./source/verismo/src/snp/ghcb/def_s.rs
================

use super::*;

verus! {

pub const GHCB_BUFFER_SIZE: usize = 0x7f0;

#[derive(PartialEq, Eq, SpecIntEnum)]
pub enum SvmStatus {
    Ok = 0,
    Unsupported,  /* Requested operation not supported */
    VmmError,  /* Unexpected state from the VMM */
    DecodeFailed,  /* Instruction decoding failed */
    Exception,  /* Instruction caused exception */
    Retry,  /* Retry instruction emulation */
}

} // verus!
verismo! {
pub struct GHCBProto;
}

================
File: ./source/verismo/src/snp/ghcb/proto_impl.rs
================

use super::proto_s::SM_TERM_GHCB_RESP_INVALID;
use super::*;
use crate::debug::{VPrint, VPrintAtLevel};
use crate::global::*;
use crate::pgtable_e::va_to_pa;
use crate::vbox::*;

verus! {

#[verifier(external_body)]
proof fn trusted_ghcb_change_pages_state_via_pg(
    ppage: int,
    npages: nat,
    tracked page_perms: &mut Map<int, SnpPointsToRaw>,
    op: PageOps,
    tracked snpcore: &SnpCore,
)
    requires
        requires_pages_perms(*old(page_perms), ppage, npages),
    ensures
        ensure_pages_perm_change_state(*old(page_perms), *page_perms, ppage, npages, op),
{
}

pub fn ghcb_change_page_state_via_pg(
    ghcb_ptr: SnpPPtr<GhcbPage>,
    ppage: u64,
    npages: u64,
    op: PageOps,
    Tracked(page_perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
    Tracked(ghcbpage_perm0): Tracked<&mut Map<int, SnpPointsTo<GhcbPage>>>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        old(cs).inv(),
        old(ghcbpage_perm0).contains_key(0),
        old(ghcbpage_perm0)[0]@.wf_shared(ghcb_ptr.id()),
        ghcb_ptr.is_constant(),
        spec_valid_page_state_change(ppage, npages as nat),
        requires_pages_perms(*old(page_perms), ppage as int, npages as nat),
        forall|i|
            ppage <= i < (ppage + npages) ==> old(page_perms).contains_key(i) && old(
                page_perms,
            )[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)),
    ensures
        ghcbpage_perm0.contains_key(0),
        ghcbpage_perm0[0]@.only_val_updated(old(ghcbpage_perm0)[0]@),
        ghcbpage_perm0[0]@.wf_shared(ghcb_ptr.id()),
        cs.inv(),
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![]),
        ensure_pages_perm_change_state(
            *old(page_perms),
            *page_perms,
            ppage as int,
            npages as nat,
            op,
        ),
        forall|i|
            ppage <= i < (ppage + npages) ==> page_perms.contains_key(i)
                && ensure_page_perm_change_state(
                old(page_perms)[i],
                #[trigger] page_perms[i],
                i as int,
                op,
            ),
{
    let mut offset = 0;
    let tracked mut ghcbpage_perm = ghcbpage_perm0.tracked_remove(0);
    let ghost old_page_perms = *page_perms;
    let ghost oldcs = *cs;
    while offset < npages
        invariant
            spec_valid_page_state_change(ppage, npages as nat),
            0 <= offset <= npages,
            ghcbpage_perm@.wf_shared(ghcb_ptr.id()),
            ghcb_ptr.is_constant(),
            cs.inv(),
            cs.only_lock_reg_coremode_updated(oldcs, set![], set![]),
            ensure_pages_perm_change_state(
                old_page_perms,
                *page_perms,
                ppage as int,
                offset as nat,
                op,
            ),
            forall|i| (ppage + offset) <= i < (ppage + npages) ==> page_perms.contains_key(i),
            forall|i|
                (ppage + offset) <= i < (ppage + npages) ==> old_page_perms[i] === page_perms[i],
            requires_pages_perms(old_page_perms, ppage as int, npages as nat),
    {
        let ghost prevcs = *cs;
        let n = if (npages - offset) < SNP_PAGE_STATE_CHANGE_MAX_ENTRY as u64 {
            (npages - offset) as u16
        } else {
            SNP_PAGE_STATE_CHANGE_MAX_ENTRY as u16
        };
        let tracked mut ghcbpage_perm0 = Map::tracked_empty();
        proof {
            ghcbpage_perm0.tracked_insert(0, ghcbpage_perm);
        }
        let ghost subdom = Set::new(|i| ppage + offset <= i < ppage + offset + n);
        assert forall|i| subdom.contains(i) implies page_perms.contains_key(i) by {
            assert(old_page_perms[i] === page_perms[i]);
            assert(ppage + offset <= i < ppage + npages);
        }
        assert(subdom.subset_of(page_perms.dom()));
        let ghost prev_page_perms = *page_perms;
        let tracked mut sub_page_perms = page_perms.tracked_remove_keys(subdom);
        let ghost removed_page_perms = *page_perms;
        assert forall|i|
            !subdom.contains(i) && prev_page_perms.contains_key(i) implies page_perms.contains_key(
            i,
        ) && page_perms[i] === prev_page_perms[i] by {}
        internal::ghcb_change_page_state_via_pg_internal(
            ghcb_ptr.clone(),
            ppage + offset as u64,
            n,
            op,
            Tracked(&mut sub_page_perms),
            Tracked(&mut ghcbpage_perm0),
            Tracked(cs),
        );
        offset = offset + n as u64;
        proof {
            let tracked sub_page_perms = sub_page_perms.tracked_remove_keys(subdom);
            page_perms.tracked_union_prefer_right(sub_page_perms);
            assert forall|i|
                !sub_page_perms.contains_key(i) && prev_page_perms.contains_key(
                    i,
                ) implies page_perms.contains_key(i) && page_perms[i] === prev_page_perms[i] by {
                assert(removed_page_perms.contains_key(i));
                assert(removed_page_perms[i] === prev_page_perms[i]);
                assert(removed_page_perms[i] === page_perms[i]);
            }
            assert forall|i|
                ppage <= i < (ppage + offset) implies #[trigger] page_perms.contains_key(i)
                && ensure_page_perm_change_state(
                old_page_perms[i],
                page_perms[i],
                i as int,
                op,
            ) by {
                if i < ppage + offset - n {
                    assert(prev_page_perms.contains_key(i));
                    assert(!subdom.contains(i));
                    assert(page_perms.contains_key(i));
                    assert(page_perms[i] === prev_page_perms[i]);
                } else {
                    assert(subdom.contains(i));
                    assert(sub_page_perms.contains_key(i));
                    assert(page_perms[i] === sub_page_perms[i]);
                }
            }
            assert forall|i| (ppage + offset) <= i < (ppage + npages) implies (
            page_perms.contains_key(i) && old_page_perms[i] === page_perms[i]) by {
                assert(!subdom.contains(i));
                assert(!sub_page_perms.contains_key(i));
                assert(page_perms[i] === prev_page_perms[i]);
                assert(prev_page_perms.contains_key(i));
            }
            ghcbpage_perm = ghcbpage_perm0.tracked_remove(0);
            oldcs.lemma_update_prop(prevcs, *cs, set![], set![], set![], set![]);
        }
    }
    proof {
        ghcbpage_perm0.tracked_insert(0, ghcbpage_perm);
    }
}

} // verus!
mod internal {
    use super::*;
    verus! {

pub fn ghcb_change_page_state_via_pg_internal(
    ghcb_ptr: SnpPPtr<GhcbPage>,
    ppage: u64,
    npages: u16,
    op: PageOps,
    Tracked(page_perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
    Tracked(ghcbpage_perm0): Tracked<&mut Map<int, SnpPointsTo<GhcbPage>>>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
)
    requires
        old(cs).inv(),
        old(ghcbpage_perm0).contains_key(0),
        old(ghcbpage_perm0)[0]@.wf_shared(ghcb_ptr.id()),
        ghcb_ptr.is_constant(),
        spec_valid_page_state_change(ppage, npages as nat),
        npages <= SNP_PAGE_STATE_CHANGE_MAX_ENTRY,
        requires_pages_perms(*old(page_perms), ppage as int, npages as nat),
        forall|i|
            ppage <= i < (ppage + npages) ==> old(page_perms).contains_key(i) && old(
                page_perms,
            )[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)),
    ensures
        ghcbpage_perm0.contains_key(0),
        ghcbpage_perm0[0]@.only_val_updated(old(ghcbpage_perm0)[0]@),
        ghcbpage_perm0[0]@.wf_shared(ghcb_ptr.id()),
        cs.inv(),
        cs.only_lock_reg_coremode_updated(*old(cs), set![], set![]),
        ensure_pages_perm_change_state(
            *old(page_perms),
            *page_perms,
            ppage as int,
            npages as nat,
            op,
        ),
{
    if npages == 0 {
        return ;
    }
    let tracked mut ghcbpage_perm = ghcbpage_perm0.tracked_remove(0);
    let scratch_ptr = ghcb_ptr.shared_buffer();
    let scratch_paddr = scratch_ptr.as_u64();
    let mut ghcb = VBox::<GhcbPage>::from_raw(ghcb_ptr.to_usize(), Tracked(ghcbpage_perm));
    ghcb.box_update(GhcbClear);
    let (ghcb_ptr, Tracked(mut ghcbpage_perm)) = ghcb.into_raw();
    let tracked (left, right) = ghcbpage_perm.tracked_into_raw().trusted_split(
        GhcbPage::spec_shared_buffer_offset(),
    );
    let tracked (scratch_perm, right) = right.trusted_split(spec_size::<SnpPageStateChange>());
    let mut scratch: VBox<SnpPageStateChange> = VBox::from_raw(
        scratch_ptr.to_usize(),
        Tracked(scratch_perm.trusted_into()),
    );
    // Clear the buffer
    scratch.box_update((FillPageStateChangeFn, ppage, npages, op));
    let (scratch_ptr, Tracked(scratch_perm)) = scratch.into_raw();
    let mut exit_code = SVM_EXIT_PAGE_STATE_CHANGE;
    let mut exit_info1 = 0;
    let mut exit_info2 = 0;
    let tracked mut ghcbpage_perm = left.trusted_join(scratch_perm.tracked_into_raw()).trusted_join(
        right,
    ).tracked_into();
    let (mut header, Tracked(mut ghcbpage_perm)) = scratch_ptr.header().copy_with::<GhcbPage>(
        Tracked(ghcbpage_perm),
    );
    let ghost oldcs = *cs;
    let ghost old_ghcbpage_perm = ghcbpage_perm;
    while header.cur_entry.le(&header.end_entry)
        invariant
            header.is_constant(),
            ghcbpage_perm@.wf_shared(ghcb_ptr.id()),
            ghcb_ptr.is_constant(),
            ghcbpage_perm@.only_val_updated(old_ghcbpage_perm@),
            cs.inv(),
            cs.only_lock_reg_coremode_updated(oldcs, set![], set![]),
            page_perms === old(page_perms),
    {
        let mut ghcb = VBox::<GhcbPage>::from_raw(ghcb_ptr.to_usize(), Tracked(ghcbpage_perm));
        ghcb.box_update((GhcbSetSwScratchFn, scratch_paddr));
        let (_, Tracked(mut tmp_ghcbpage_perm)) = ghcb.into_raw();
        let tracked mut ghcbpage_perm0 = Map::tracked_empty();
        let ghost prevcs = *cs;
        let mut exit_code = SVM_EXIT_PAGE_STATE_CHANGE;
        let mut exit_info1 = 0;
        let mut exit_info2 = 0;
        proof {
            ghcbpage_perm0.tracked_insert(0, tmp_ghcbpage_perm);
        }
        let resp = ghcb_page_proto(
            ghcb_ptr.clone(),
            &mut exit_code,
            &mut exit_info1,
            &mut exit_info2,
            Tracked(&mut ghcbpage_perm0),
            Tracked(cs),
        );
        proof {
            oldcs.lemma_update_prop(prevcs, *cs, set![], set![], set![], set![]);
            assert(cs.only_lock_reg_coremode_updated(oldcs, set![], set![]));
        }
        match resp {
            SvmStatus::Ok => {},
            _ => {
                proof {
                    reveal_strlit("Bad change page state");
                }
                new_strlit("Bad change page state").err(Tracked(cs));
                vc_terminate(SM_TERM_GHCB_RESP_INVALID, Tracked(&mut cs.snpcore));
            },
        }
        let scratch_ptr: SnpPPtr<SnpPageStateChange> = ghcb_ptr.shared_buffer().to();
        let (tmpheader, Tracked(mut tmp_ghcb_perm)) = scratch_ptr.header().copy_with::<GhcbPage>(
            Tracked(ghcbpage_perm0.tracked_remove(0)),
        );
        header = tmpheader;
        //header.leak_debug();
        proof {
            // TODO: Add page_perm updates
            ghcbpage_perm = tmp_ghcb_perm;
        }
    }
    proof {
        trusted_ghcb_change_pages_state_via_pg(
            ppage as int,
            npages as nat,
            page_perms,
            op,
            &cs.snpcore,
        );
        ghcbpage_perm0.tracked_insert(0, ghcbpage_perm);
    }
}

} // verus!
}

verus! {

pub const SNP_PAGE_STATE_CHANGE_MAX_ENTRY: usize = 253;

} // verus!
#[vbit_struct(SnpPageStateChangeEntry, u64)]
pub struct SpecSnpPageStateChangeEntry {
    #[vbits(0, 11)]
    pub cur_page: u64,
    #[vbits(12, 51)]
    pub gpn: u64,
    #[vbits(52, 55)]
    pub operation: u64,
    #[vbits(56, 56)]
    pub psize: u64,
}

verus! {

impl SpecSnpPageStateChangeEntry {
    pub open spec fn req_entry(gpn: u64, opval: u64, psize: u64) -> SpecSnpPageStateChangeEntry {
        SpecSnpPageStateChangeEntry::empty().spec_set_gpn(gpn).spec_set_operation(
            opval,
        ).spec_set_psize(psize)
    }
}

} // verus!
verismo_simple! {
#[repr(C, align(1))]
#[derive(Copy, VClone, VPrint, SpecGetter, SpecSetter)]
pub struct SnpPageStateChangeHeader {
    pub cur_entry: u16,
    pub end_entry: u16,
    pub reserved: u32,
}

#[repr(C, align(1))]
#[derive(SpecGetter, SpecSetter)]
pub struct SnpPageStateChange {
    #[def_offset]
    pub header: SnpPageStateChangeHeader,
    pub entries: [u64; SNP_PAGE_STATE_CHANGE_MAX_ENTRY],
}
}

verus! {

use crate::vbox::MutFnTrait;

pub struct FillPageStateChangeFn;

pub type FillPageStateChange = (FillPageStateChangeFn, u64, u16, PageOps);

impl<'a> MutFnTrait<'a, FillPageStateChange, bool> for SnpPageStateChange {
    open spec fn spec_update_requires(&self, params: FillPageStateChange) -> bool {
        let (_, ppage, npages, op) = params;
        &&& self.is_constant()
        &&& spec_valid_page_state_change(ppage, npages as nat)
        &&& npages <= SNP_PAGE_STATE_CHANGE_MAX_ENTRY
        &&& npages > 0
    }

    open spec fn spec_update(&self, prev: &Self, params: FillPageStateChange, ret: bool) -> bool {
        let (_, ppage, npages, op) = params;
        let opval = op.as_int() as u64;
        &&& self.is_constant()
        &&& forall|k: int|
            0 <= k < npages ==> SnpPageStateChangeEntry::spec_new(self.entries@[k].vspec_cast_to())@
                === SpecSnpPageStateChangeEntry::req_entry((ppage + k) as u64, opval, 0)
    }

    fn box_update(&'a mut self, params: FillPageStateChange) -> (ret: bool) {
        let (_, ppage, npages, op) = params;
        self.header.cur_entry = 0u64.into();
        self.header.end_entry = (npages - 1).into();
        let mut i: u16 = 0;
        let opval = op.as_u64();
        let ghost prev = *self;
        while i < npages
            invariant
                0 <= i <= npages,
                self.header === prev.header,
                self.is_constant(),
                spec_valid_page_state_change(ppage, npages as nat),
                opval == op.as_int(),
                npages <= SNP_PAGE_STATE_CHANGE_MAX_ENTRY,
                forall|k: int|
                    0 <= k < i ==> SnpPageStateChangeEntry::spec_new(
                        self.entries@[k].vspec_cast_to(),
                    )@ === SpecSnpPageStateChangeEntry::req_entry((ppage + k) as u64, opval, 0),
        {
            let gpn = ppage + i as u64;
            let entry = SnpPageStateChangeEntry::empty().set_gpn(gpn).set_operation(
                opval,
            ).set_psize(0);
            self.entries.update((i as usize), entry.value.into());
            i = i + 1;
        }
        true
    }
}

} // verus!

================
File: ./source/verismo/src/snp/ghcb/mod.rs
================

mod def_s;
#[macro_use]
mod proto_s;
mod proto_e;
mod proto_impl;
mod proto_page;

pub use def_s::*;
pub use proto_e::*;
pub use proto_impl::*;
pub use proto_page::*;
pub use proto_s::*;

use super::cpu::Vmsa;
use super::trackedcore::*;
use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::arch::attack::*;
use crate::arch::reg::*;
use crate::ptr::*;
use crate::registers::*;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

pub type GhcbHandle = crate::vbox::VBox<GhcbPage>;
#[macro_export]
macro_rules! SVM_EVTINJ_IS_VALID {
    ($x: expr) => {
        bit_check($x, SVM_EVTINJ_VALID_BIT)
    };
}

#[macro_export]
macro_rules! GHCB_MSR_PROTOCOL_MIN {
    ($x: expr) => {
        (($x) >> 32) & 0xffff
    };
}

#[macro_export]
macro_rules! GHCB_MSR_PROTOCOL_MAX {
    ($x: expr) => {
        (($x) >> 48) & 0xffff
    };
}

================
File: ./source/verismo/src/snp/ghcb/proto_e.rs
================

use super::*;
use crate::arch::reg::MSR_GHCB_BASE;
use crate::debug::VPrintAtLevel;
use crate::registers::*;

verus! {

#[inline]
pub fn SM_EVERCRYPT_ERR(subcode: u64_t) -> (ret: u64_t)
    requires
        subcode < 0x100,
{
    proof {
        assert(subcode << SUBCODE_OFFSET!() < 0x10000) by (bit_vector)
            requires
                subcode < 0x100,
        ;
    }
    (SM_EVERCRYPT_EXIT + (subcode << SUBCODE_OFFSET!()))
}

#[inline]
pub fn SM_TERM_RICHOS_ERR(subcode: u64_t) -> (ret: u64_t)
    requires
        subcode < 0x100,
{
    proof {
        assert(subcode << SUBCODE_OFFSET!() < 0x10000) by (bit_vector)
            requires
                subcode < 0x100,
        ;
    }
    (SM_TERM_RICHOS + (subcode << SUBCODE_OFFSET!()))
}

} // verus!
verus! {

pub const GHCB_HV_DEBUG: u64 = 0xf03;

} // verus!
/*
#[verifier::external]
pub mod trust {
    use alloc::fmt;

    use super::*;
    impl fmt::Write for GHCBProto {
        fn write_str(&mut self, s: &str) -> fmt::Result {
            GHCBProto::print_str(s);
            Ok(())
        }
    }
}*/
verus! {

pub open spec fn GHCB_REGID() -> RegName {
    RegName::MSR(MSR_GHCB_BASE)
}

} // verus!
verismo! {
    pub const fn MSR_GHCB() -> (ret: Msr)
    ensures
        ret.reg_id() === GHCB_REGID()
    {
        Msr {
            reg: u32_s::constant(MSR_GHCB_BASE),
        }
    }
}

verus! {

impl GHCBProto {
    #[inline]
    pub fn exit_value(reason_code: u64) -> (ret: u64)
        ensures
            ret == GHCBProto::spec_exit_value(reason_code as u64),
    {
        let mut value: u64 = GHCB_MSR_TERMINATE_REQ;
        value = value | (SM_SEV_TERM_SET << 12u64);
        value = value | (reason_code << 16u64);
        assert(value == GHCBProto::spec_exit_value(reason_code as u64));
        value
    }

    #[inline]
    fn msr_page_state_change_req(ppage: usize_t, op: PageOps) -> (ret: u64)
        ensures
            (ret as int) == GHCBProto::spec_msr_page_state_req(ppage as usize, op),
    {
        GHCB_SNP_PAGE_STATE_CHANGE_REQ as u64 | (ppage as u64 & 0xfff_ffff_ffff) << 12u64 | (
        op.as_u64() as u64) << 52u64
    }

    #[inline(always)]
    pub fn msr_data(value: u64) -> (ret: u64)
        ensures
            ret == value as u64 & !GHCB_MSR_INFO_MASK,
    {
        value & !GHCB_MSR_INFO_MASK
    }

    #[inline(always)]
    pub fn msr_info(value: u64) -> (ret: u64)
        ensures
            ret == value as u64 & GHCB_MSR_INFO_MASK,
    {
        value & GHCB_MSR_INFO_MASK
    }

    #[inline(always)]
    pub fn msr_register_ghcb(page: u64) -> (ret: u64)
        ensures
            ret == page as u64 | GHCB_MSR_REGISTER_GHCB_REQ,
    {
        page | GHCB_MSR_REGISTER_GHCB_REQ
    }

    pub open spec fn restored_ghcb(new: SnpCore, prev: SnpCore) -> bool {
        !spec_attack() ==> new.regs[GHCB_REGID()].view::<usize_s>().value
            == prev.regs[GHCB_REGID()].view::<usize_s>().value
    }
}

} // verus!
verus! {

pub open spec fn spec_eq_shared(expected: nat, ret: nat) -> bool {
    &&& !spec_attack() ==> expected == ret
}

pub open spec fn spec_ghcb_send_core_update(
    oldsnpcore: SnpCore,
    snpcore: SnpCore,
    req_resp: (nat, nat),
) -> bool {
    &&& snpcore.ghcbmsr_msgs() === oldsnpcore.ghcbmsr_msgs().push(req_resp)
    &&& snpcore.reg_updated(oldsnpcore, set![GHCB_REGID()])
    &&& snpcore.update_reg_coremode(oldsnpcore)
}

pub fn ghcb_msr_send(
    val: u64,
    Tracked(memperm): Tracked<&mut Option<SnpPointsToRaw>>,
    Tracked(snpcore): Tracked<&mut SnpCore>,
) -> (ret: u64)
    requires
        (*old(snpcore)).inv_reg_cpu(),
        val.is_constant(),
        is_none_or_sharedmem(*old(memperm)),
    ensures
        (*snpcore).inv_reg_cpu(),
        spec_ghcb_send_core_update(*old(snpcore), *snpcore, (val as nat, snpcore.last_ghcb_resp())),
        snpcore.regs[GHCB_REGID()].val::<u64_s>()@.val == snpcore.last_ghcb_resp(),
        hvupdate_none_or_sharedmem(*memperm, *old(memperm)),
        ret.is_constant(),
        spec_eq_shared(snpcore.last_ghcb_resp(), ret as nat),
{
    //let tracked SnpCore{regs, coreid} = snpcore;
    let tracked mut ghcbperm = snpcore.regs.tracked_remove(GHCB_REGID());
    let ghcb_msr = MSR_GHCB();
    ghcb_msr.write(val.into(), Tracked(&mut ghcbperm));
    //ghcb_msr.write_vmgexit(val, Tracked(&mut ghcbperm), Tracked(&mut snpcore.coreid), Tracked(memperm));
    proof {
        let ghcb_write_val: nat = ghcbperm.val::<u64_s>().vspec_cast_to();
        assert(ghcb_write_val == val as nat);
    }
    vmgexit(Tracked(&mut ghcbperm), Tracked(&mut snpcore.coreid), Tracked(memperm));
    let ret = ghcb_msr.read(Tracked(&ghcbperm)).reveal_value();
    proof {
        snpcore.regs.tracked_insert(GHCB_REGID(), ghcbperm);
    }
    ret
}

} // verus!
verus! {

pub fn ghcb_proto(
    val: u64,
    Tracked(memperm): Tracked<&mut Option<SnpPointsToRaw>>,
    Tracked(snpcore): Tracked<&mut SnpCore>,
) -> (ret: u64)
    requires
        old(snpcore).inv_reg_cpu(),
        val.is_constant(),
        is_none_or_sharedmem(*old(memperm)),
    ensures
        (*snpcore).inv_reg_cpu(),
        GHCBProto::restored_ghcb(*snpcore, *old(snpcore)),
        spec_ghcb_send_core_update(
            *old(snpcore),
            *snpcore,
            (val as nat, (*snpcore).last_ghcb_resp()),
        ),
        ret.is_constant(),
        spec_eq_shared((*snpcore).last_ghcb_resp(), ret as nat),
        hvupdate_none_or_sharedmem(*memperm, *old(memperm)),
{
    let tracked perm = snpcore.regs.tracked_borrow(GHCB_REGID());
    let ghcb_msr = MSR_GHCB();
    let oldval = ghcb_msr.read(Tracked(perm));
    let ret = ghcb_msr_send(val, Tracked(memperm), Tracked(snpcore));
    let tracked mut perm = snpcore.regs.tracked_remove(GHCB_REGID());
    ghcb_msr.write(oldval, Tracked(&mut perm));
    proof {
        snpcore.regs.tracked_insert(GHCB_REGID(), perm);
    }
    ret
}

} // verus!
verus! {

pub fn ghcb_msr_proto(val: u64, Tracked(snpcore): Tracked<&mut SnpCore>) -> (ret: u64)
    requires
        old(snpcore).inv_reg_cpu(),
        val.is_constant(),
    ensures
        (*snpcore).inv_reg_cpu(),
        GHCBProto::restored_ghcb(*snpcore, *old(snpcore)),
        spec_ghcb_send_core_update(
            *old(snpcore),
            (*snpcore),
            (val as nat, (*snpcore).last_ghcb_resp()),
        ),
        spec_eq_shared((*snpcore).last_ghcb_resp(), ret as nat),
        ret.is_constant(),
{
    let tracked mut nonmem = None;
    ghcb_proto(val, Tracked(&mut nonmem), Tracked(snpcore))
}

} // verus!
verus! {

fn vc_terminate_s(reason_code: u64, Tracked(snpcore): Tracked<&mut SnpCore>) -> (ret: !)
    requires
        (*old(snpcore)).inv_reg_cpu(),
        reason_code.is_constant(),
    ensures
        false,
{
    let value = GHCBProto::exit_value(reason_code);
    ghcb_msr_proto(value, Tracked(snpcore));
    loop {
    }
}

pub fn vc_terminate(reason_code: u64_t, Tracked(snpcore): Tracked<&mut SnpCore>) -> (ret: !)
    requires
        (*old(snpcore)).inv_reg_cpu(),
    ensures
        false,
{
    (new_strlit("terminate: "), reason_code).leak_debug();
    vc_terminate_s(reason_code, Tracked(snpcore))
}

pub fn early_vc_terminate_debug(
    reason_code: u64_t,
    Tracked(cc): Tracked<&mut SnpCoreConsole>,
) -> (ret: !)
    requires
        old(cc).wf(),
    ensures
        false,
{
    vc_terminate_s(reason_code, Tracked(&mut cc.snpcore))
}

pub fn vc_terminate_debug(reason_code: u64_t, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret:
    !)
    requires
        old(cs).inv(),
    ensures
        false,
{
    proof {
        reveal_strlit("terminate: ");
    }
    (new_strlit("terminate: "), reason_code).debug(Tracked(cs));
    vc_terminate_s(reason_code, Tracked(&mut cs.snpcore))
}

// If the previous ghcb message is to request page state change,
// we allow safe update for assigned, asid and page size field in the corresponding page.
#[verifier(external_body)]
proof fn trusted_ghcb_change_page_state(
    tracked perm: &mut SnpPointsToRaw,
    op: PageOps,
    tracked snpcore: &SnpCore,
)
    requires
        snpcore.last_ghcb_req() == GHCBProto::spec_msr_page_state_req(
            old(perm)@.ppage() as usize,
            op,
        ) as nat,
        old(perm)@.range().0 % PAGE_SIZE!() == 0,
        old(perm)@.range().1 == PAGE_SIZE!(),
        old(perm)@.wf(),
    ensures
        perm@.range() === old(perm)@.range(),
        perm@.snp().ensures_rmpupdate(old(perm)@.snp(), op.is_Shared(), op.is_Unsmash()),
        perm@.wf(),
{
}

// the memperm should be mapped to the target physical page.
pub fn ghcb_change_page_state_via_msr(
    ppage: usize_t,
    op: PageOps,
    Tracked(memperm): Tracked<&mut SnpPointsToRaw>,
    Tracked(snpcore): Tracked<&mut SnpCore>,
)
    requires
        old(snpcore).inv_reg_cpu(),
        old(memperm)@.wf(),
        old(memperm)@.range().1 == PAGE_SIZE,
        ppage == old(memperm)@.ppage(),
        old(memperm)@.range().0 % PAGE_SIZE!() == 0,
        ppage.is_constant(),
    ensures
        snpcore.inv_reg_cpu(),
        spec_ghcb_send_core_update(
            *old(snpcore),
            *snpcore,
            (
                GHCBProto::spec_msr_page_state_req(ppage as usize, op) as nat,
                snpcore.last_ghcb_resp(),
            ),
        ),
        ensure_page_perm_change_state(*old(memperm), *memperm, ppage as int, op),
{
    let value = GHCBProto::msr_page_state_change_req(ppage, op);
    let resp = ghcb_msr_proto(value, Tracked(snpcore));
    proof {
        trusted_ghcb_change_page_state(memperm, op, snpcore);
    }
}

pub fn ghcb_register_ghcb(ppage: usize, Tracked(snpcore): Tracked<&mut SnpCore>)
    requires
        old(snpcore).inv_reg_cpu(),
        ppage.is_constant(),
        ppage.spec_valid_pn_with(1),
    ensures
        snpcore.inv_reg_cpu(),
        spec_ghcb_send_core_update(*old(snpcore), *snpcore, snpcore.ghcbmsr_msgs().last()),
        (snpcore.last_ghcb_resp() as u64 & GHCB_MSR_INFO_MASK) != GHCB_MSR_REGISTER_GHCB_RES
            ==> spec_attack(),
        (snpcore.last_ghcb_resp() as u64 & !GHCB_MSR_INFO_MASK) != GVN::new(
            ppage as int,
        ).to_addr().to_u64() ==> spec_attack(),
        snpcore.regs[GHCB_REGID()].val::<u64_s>() === (ppage.spec_to_addr().vspec_cast_to()),
        snpcore.ghcb_value() == ppage.spec_to_addr(),
{
    let pa: u64 = ppage.to_addr() as u64;
    let resp = ghcb_msr_proto(GHCBProto::msr_register_ghcb(pa), Tracked(snpcore));
    // Validate the response
    if GHCBProto::msr_info(resp) != GHCB_MSR_REGISTER_GHCB_RES {
        vc_terminate(SM_TERM_GHCB_RESP_INVALID, Tracked(snpcore));
    }
    if GHCBProto::msr_data(resp) != pa {
        vc_terminate(SM_TERM_GHCB_RESP_INVALID, Tracked(snpcore));
    }
    let tracked mut perm = snpcore.regs.tracked_remove(GHCB_REGID());
    MSR_GHCB().write(pa.into(), Tracked(&mut perm));
    proof {
        snpcore.regs.tracked_insert(GHCB_REGID(), perm);
    }
}

pub fn negotiate_protocol(Tracked(snpcore): Tracked<&mut SnpCore>)
    requires
        old(snpcore).inv_reg_cpu(),
    ensures
        snpcore.inv_reg_cpu(),
        spec_ghcb_send_core_update(
            *old(snpcore),
            *snpcore,
            (GHCB_MSR_SEV_INFO_REQ as nat, snpcore.last_ghcb_resp()),
        ),
        (snpcore.last_ghcb_resp() as u64 & GHCB_MSR_INFO_MASK) != GHCB_MSR_SEV_INFO_RES
            ==> spec_attack(),
        (snpcore).update_reg_coremode(*old(snpcore)),
{
    let resp = ghcb_msr_proto(GHCB_MSR_SEV_INFO_REQ, Tracked(snpcore));
    if GHCBProto::msr_info(resp) != GHCB_MSR_SEV_INFO_RES {
        vc_terminate(SM_TERM_GHCB_RESP_INVALID, Tracked(snpcore));
    }
}

} // verus!

================
File: ./source/verismo/src/snp/ghcb/proto_s.rs
================

use super::*;

verus! {

pub const GHCB_VERSION_1: u16 = 1;

#[is_variant]
#[derive(SpecIntEnum, Copy, Clone)]
pub enum PageOps {
    Private = 1,
    Shared = 2,
    Smash = 3,
    Unsmash = 4,
}

impl WellFormed for PageOps {
    open spec fn wf(&self) -> bool {
        true
    }
}

impl IsConstant for PageOps {
    open spec fn is_constant(&self) -> bool {
        true
    }

    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        true
    }
}

} // verus!
verus! {

pub const GHCB_MSR_SEV_INFO_REQ: u64 = 0x002;

pub const GHCB_MSR_SEV_INFO_RES: u64 = 0x001;

pub const GHCB_MSR_REGISTER_GHCB_REQ: u64 = 0x012;

pub const GHCB_MSR_REGISTER_GHCB_RES: u64 = 0x013;

pub const GHCB_SNP_PAGE_STATE_CHANGE_REQ: u64 = 0x0014;

pub const GHCB_SNP_PAGE_STATE_CHANGE_RESP: u64 = 0x0015;

pub const GHCB_MSR_TERMINATE_REQ: u64 = 0x100;

pub const GHCB_MSR_INFO_MASK: u64 = 0xfff;

pub const GHCB_DEFAULT_USAGE: u32 = 0;

pub const GHCB_VTL_RETURN_USAGE: u32 = 2;

// GHCB exit code
pub const SVM_EXIT_VMGEXIT: u64 = 0x403;

pub const SVM_EXIT_MSR: u64 = 0x07c;

pub const SVM_EXIT_VMMCALL: u64 = 0x081;

pub const SVM_EXIT_PAGE_STATE_CHANGE: u64 = 0x80000010;

pub const SVM_EXIT_SNP_GUEST_REQUEST: u64 = 0x80000011;

#[derive(SpecIntEnum)]
pub enum SVMExitCode {
    VmgExit = 0x403,
    MSR = 0x07c,
    VmmCall = 0x081,
    MaskInt = 0xfff,
    PageStateChange = 0x80000010,
    SnpGuestRequest = 0x80000011,
}

} // verus!
#[macro_export]
macro_rules! SVM_EVTINJ_VEC_MASK {
    ($x: expr) => {
        ($x & 0xff)
    };
}

verus! {

pub const SVM_EVTINJ_VEC_X86_TRAP_GP: u64 = 13;

pub const SVM_EVTINJ_VEC_X86_TRAP_UD: u64 = 6;

} // verus!
#[macro_export]
macro_rules! SVM_EVTINJ_TYPE_MASK {
    ($x: expr) => {
        ($x & 0b111_0000_0000)
    };
}

verus! {

pub const SVM_EVTINJ_TYPE_EXEPT: u64 = 0x300;

pub const SVM_EVTINJ_VALID_BIT: u64 = 31;

// VeriSMo termination constants
/// 15
pub const SM_SEV_TERM_SET: u64 = 0x3;

/// 0
pub const SM_TERM_GENERAL: u64 = 0;

/// 1
pub const SM_TERM_NOT_VMPL0: u64 = 1;

/// 2
pub const SM_TERM_UNHANDLED_VC: u64 = 2;

/// 3
pub const SM_TERM_PSC_ERROR: u64 = 3;

/// 4
pub const SM_TERM_SET_PAGE_ERROR: u64 = 4;

/// 5
pub const SM_TERM_NO_GHCB: u64 = 5;

/// 6
pub const SM_TERM_GHCB_RESP_INVALID: u64 = 6;

/// 7
pub const SM_TERM_INVALID_PARAM: u64 = 7;

/// 8
pub const SM_TERM_PVALIDATE: u64 = 8;

/// 8
pub const SM_TERM_MEM: u64 = 0x11;

pub const SM_EVERCRYPT_EXIT: u64 = 0xa;

pub const SM_TERM_TIMEOUT: u64 = 0xb;

pub const SM_TERM_VMM_ERR: u64 = 0xc;

pub const SM_TERM_GHCB_EXCEPTION: u64 = 0xd;

pub const SM_TERM_UNSUPPORTED: u64 = 0xe;

pub const SM_TERM_RICHOS: u64 = 0xf;

pub const SM_TERM_PERMS: u64 = 0x10;

} // verus!
macro_const! {

pub const SUBCODE_OFFSET: u64 = 0x8u64;
}

verus! {

pub open spec fn spec_valid_page_state_change(ppage: u64, npages: nat) -> bool {
    &&& ppage + npages <= 0x100_0000_0000
}

pub open spec fn ensure_page_perm_change_state(
    prev_memperm: SnpPointsToRaw,
    memperm: SnpPointsToRaw,
    ppage: int,
    op: PageOps,
) -> bool {
    &&& memperm@.snp().ensures_rmpupdate(prev_memperm@.snp(), op.is_Shared(), op.is_Unsmash())
    &&& (memperm)@.wf_range(prev_memperm@.range())
    &&& ppage == (memperm)@.ppage()
}

pub open spec fn requires_pages_perms(
    page_perms: Map<int, SnpPointsToRaw>,
    ppage: int,
    npages: nat,
) -> bool {
    &&& wf_page_range(page_perms, ppage, npages)
}

pub open spec fn ensure_pages_perm_change_state(
    prev_pageperms: Map<int, SnpPointsToRaw>,
    page_perms: Map<int, SnpPointsToRaw>,
    ppage: int,
    npages: nat,
    op: PageOps,
) -> bool {
    &&& prev_pageperms.dom() =~~= page_perms.dom()
    &&& forall|i| ppage <= i < (ppage + npages) ==> page_perms.contains_key(i)
    &&& forall|i|
        ppage <= i < (ppage + npages) ==> ensure_page_perm_change_state(
            prev_pageperms[i],
            #[trigger] page_perms[i],
            i,
            op,
        )
}

impl GHCBProto {
    /// only works on 4k page
    pub open spec fn spec_msr_page_state_req(page: usize, op: PageOps) -> u64 {
        let opu64 = op.as_int() as u64;
        GHCB_SNP_PAGE_STATE_CHANGE_REQ | (page as u64 & 0xfff_ffff_ffff) << 12 | (opu64 << 52)
    }

    pub open spec fn spec_exit_value(reason_code: u64) -> u64 {
        GHCB_MSR_TERMINATE_REQ | (SM_SEV_TERM_SET << 12u64) | (reason_code << 16u64)
    }
}

} // verus!

================
File: ./source/verismo/src/snp/ghcb/proto_page.rs
================

use super::*;
use crate::debug::VPrintAtLevel;
use crate::global::{spec_ALLOCATOR, spec_CONSOLE, spec_PT};
use crate::pgtable_e::*;
use crate::snp::mem::*;

verismo_simple! {

#[derive(SpecGetter, SpecSetter, Copy, VClone)]
#[repr(C, align(1))]
pub struct GhcbVmsa {
    pub reserved_0: Array<u8, 0x0CB>,
    #[def_offset]
    pub cpl: u8, // 0x0CBh
    pub reserved_2: Array<u8, 0x12c>, // 0x1F8 - 0xcb - 1

    #[def_offset]
    pub rax: u64, // 0x1f8

    pub reserved2: Array<u8, 0x108>, // (0x308 - 0x1F8 - 8)

    #[def_offset]
    pub rcx: u64, // 0x308
    #[def_offset]
    pub rdx: u64,
    #[def_offset]
    pub rbx: u64,

    pub reserved3: Array<u8, 32>,
    #[def_offset]
    pub r8: u64,
    pub reserved4: Array<u8, 0x48>, // (0x390 - 0x308 - 24 - 40)

    #[def_offset]
    pub sw_exit_code: u64,
    #[def_offset]
    pub sw_exit_info_1: u64,
    #[def_offset]
    pub sw_exit_info_2: u64,
    #[def_offset]
    pub sw_scratch: u64,

    pub reserved5: Array<u8, 16>,

    #[def_offset]
    pub guest_error_code: u64,

    pub reserved_6: Array<u8, 32>,
    #[def_offset]
    pub xcr0: u64,
}

#[repr(C, align(1))]
#[derive(SpecSetter, SpecGetter)]
pub struct GhcbPage {
    #[def_offset]
    vmsa: GhcbVmsa,
    valid_bitmap: [u8; 16],
    reserved6: [u8; 1024],
    #[def_offset]
    shared_buffer: [u8; GHCB_BUFFER_SIZE],
    reserved7: [u8; 10],
    version: u16,
    #[def_offset]
    usage: u32,
}

impl VBox<GhcbPage> {
    pub fn set_usage_ext(&mut self, usage: u32)
    requires
        old(self).is_constant(),
        usage.is_constant(),
        old(self).ghcb_wf(),
    ensures
        self.is_constant(),
        self.only_val_updated(*old(self)),
        self@ === old(self)@.spec_set_usage(usage),
        self.ghcb_wf(),
    {
        self.set_usage(usage);
    }
}
}

verus! {

proof fn proof_ghcb_size()
    ensures
        spec_size::<GhcbPage>() == PAGE_SIZE!(),
        spec_size::<GhcbVmsa>() == 0x03f0,
        GhcbVmsa::spec_rax_offset() < 0x400,
        GhcbVmsa::spec_rcx_offset() < 0x400,
        GhcbVmsa::spec_rdx_offset() < 0x400,
        GhcbVmsa::spec_sw_exit_code_offset() < 0x400,
{
}

} // verus!
macro_rules! ghcb_fns {
    ($name: ident, $valty: tt) => {
        paste::paste! {verus!{
        #[inline]
        pub fn [<set_bitmap_ $name>](&mut self)
        requires
            old(self).is_constant(),
        ensures
            self.is_constant(),
            *self === old(self).spec_set_valid_bitmap(self.spec_valid_bitmap()),
            ensure_set_bitmap(self.spec_valid_bitmap()@, old(self).spec_valid_bitmap()@, GhcbVmsa::[<spec_ $name _offset>]()),
        {
            proof {proof_ghcb_size();}
            assert(GhcbVmsa::[<spec_ $name _offset>]() < 0x400);
            self.set_offset_valid(GhcbVmsa::[<_ $name _offset>]());
        }

        #[inline]
        pub fn [<is_ $name _valid>](&self) -> (ret: bool)
        requires
            self.is_constant(),
        ensures
            ret == ensures_offset_valid(self.spec_valid_bitmap()@, GhcbVmsa::[<spec_ $name _offset>]())
        {
            proof {proof_ghcb_size();}
            assert(GhcbVmsa::[<spec_ $name _offset>]() < 0x400);
            self.is_offset_valid(GhcbVmsa::[<_ $name _offset>]())
        }
        }
}
    };
}

macro_rules! ghcb_fns_u64 {
    ($name: ident) => {
        ghcb_fns! {$name, u64}
    };
}

macro_rules! ghcb_box_fn {
($fnname: ident, $inputty: ident, $checkname: ident, $fieldt: ty, $fieldname: ident) => {
    paste::paste! {
    verus!{
        pub struct $fnname;
        pub type $inputty = ($fnname, $fieldt);
    }
    verus!{
    impl VBox<GhcbPage> {
        pub fn $fieldname(self) -> (ret: ($fieldt, Self))
        requires
            self.ghcb_wf(),
        ensures
            ret.1.spec_eq(self),
        {
            let (ghcb_ptr, Tracked(mut ghcb_perm)) = self.into_raw();
            assert(ghcb_perm@.snp() === SwSnpMemAttr::shared());
            let (val, Tracked(ghcb_perm)) = ghcb_ptr.vmsa().$fieldname().copy_with::<GhcbPage>(Tracked(ghcb_perm));
            let ghcb = VBox::from_raw(ghcb_ptr.to_usize(), Tracked(ghcb_perm));
            (val.into(), ghcb)
        }
    }}
    verismo_simple! {
    impl GhcbPage {
        pub closed spec fn [<ensures_set_ $fieldname>](&self, prev: Self, params: $inputty) -> bool {
            &&& self.vmsa.$fieldname == params.1
            &&& *self === prev.spec_set_valid_bitmap(self.valid_bitmap).spec_set_vmsa(self.vmsa)
            &&& ensures_offset_valid(self.valid_bitmap@, GhcbVmsa::[<spec_ $fieldname _offset>]())
        }
    }
    impl<'a> MutFnTrait<'a, $inputty, bool> for GhcbPage {
        open spec fn spec_update_requires(&self, params: $inputty) -> bool {
            &&&self.is_constant()
            &&&params.1.is_constant()
        }

        open spec fn spec_update(&self, prev: &Self, params: $inputty, ret: bool) -> bool {
            &&& self.is_constant()
            &&& self.[<ensures_set_ $fieldname>](*prev, params)
            &&& ret
        }

        #[inline]
        fn box_update(&'a mut self, params: $inputty) -> (ret: bool)
        {
            self.[<set_bitmap_ $fieldname>]();
            self.vmsa.$fieldname = params.1.into();
            true
        }
    }

    pub struct $checkname;

    impl<'a> BorrowFnTrait<'a, $checkname, bool> for GhcbPage {
        fn box_borrow(&'a self, params: $checkname) -> (ret: bool)
        {
            proof {proof_ghcb_size();}
            assert(GhcbVmsa::[<spec_ $fieldname _offset>]() < 0x400);
            self.is_offset_valid(GhcbVmsa::[<_ $fieldname _offset>]())
        }

        open spec fn spec_borrow_requires(&self, params: $checkname) -> bool {
            self.is_constant()
        }

        closed spec fn spec_borrow_ensures(&self, params: $checkname, ret: bool) -> bool {
            ret == ensures_offset_valid(self.spec_valid_bitmap()@, GhcbVmsa::[<spec_ $fieldname _offset>]())
        }
    }
    }
}
}
}

verus! {

pub closed spec fn ensures_offset_valid(cur: Seq<u8_s>, offset: nat) -> bool {
    let i: int = ((offset / 8) / 8) as int;
    let bit: u8 = ((offset / 8) % 8) as u8;
    let bits: u8 = cur[i].vspec_cast_to();
    bits & BIT8!(bit) != 0
}

pub closed spec fn ensure_set_bitmap(cur: Seq<u8_s>, prev: Seq<u8_s>, offset: nat) -> bool {
    let i: int = ((offset / 8) / 8) as int;
    let bit: u8 = ((offset / 8) % 8) as u8;
    let bits: u8 = cur[i].vspec_cast_to();
    let prev_bits: u8 = prev[i].vspec_cast_to();
    &&& forall|j| 0 <= j < 16 && j != i ==> cur[j] === prev[j]
    &&& bits == (prev_bits | BIT8!(bit))
    &&& ensures_offset_valid(cur, offset)
}

#[verifier(bit_vector)]
proof fn lemma_bit8_set2(val: u8, b1: u8, b2: u8)
    requires
        0 <= b1 < 8,
        0 <= b2 < 8,
    ensures
        b1 != b2 ==> (val & (1 << b1) != 0) == ((val | (1 << b2)) & (1 << b1) != 0),
        (val | (1 << b2)) & (1 << b2) != 0,
{
}

#[verifier(bit_vector)]
proof fn lemma_bit8_setbit(val: u8, b1: u8)
    requires
        0 <= b1 < 8,
    ensures
        (val | (1 << b1)) & (1 << b1) != 0,
{
}

proof fn proof_bit8_setbit()
    ensures
        forall|val: u8, b1: u8| 0 <= b1 < 8 ==> (val | (1 << b1)) & (1 << b1) != 0,
        forall|val: u8, b1: u8, b2: u8|
            0 <= b1 < 8 && 0 <= b2 < 8 ==> ((val | (1 << b2)) & (1 << b1) != 0) == ((val & (1 << b1)
                != 0) || (b1 == b2)),
{
    assert forall|val: u8, b1: u8| 0 <= b1 < 8 implies (val | (1 << b1)) & (1 << b1) != 0 by {
        lemma_bit8_setbit(val, b1)
    }
    assert forall|val: u8, b1: u8, b2: u8| 0 <= b1 < 8 && 0 <= b2 < 8 implies ((val | (1 << b2)) & (
    1 << b1) != 0) == ((val & (1 << b1) != 0) || b1 == b2) by {
        lemma_bit8_set2(val, b1, b2);
    }
}

} // verus!
verus! {

impl GhcbPage {
    pub closed spec fn ensure_clear(&self) -> bool {
        forall|i| 0 <= i < 16 ==> self.valid_bitmap@[i] === u8_s::spec_constant(0)
    }

    pub fn clear(&mut self)
        requires
            old(self).is_constant(),
        ensures
            self.is_constant(),
            self.ensure_clear(),
            *self === old(self).spec_set_valid_bitmap(self.spec_valid_bitmap()),
    {
        // Clear valid bitmap
        self.valid_bitmap.memset(0u8.into());
    }

    pub fn set_offset_valid(&mut self, offset: usize)
        requires
            old(self).is_constant(),
            offset < 0x400,
        ensures
            self.is_constant(),
            ensure_set_bitmap(
                self.spec_valid_bitmap()@,
                old(self).spec_valid_bitmap()@,
                offset as nat,
            ),
            ensures_offset_valid(self.spec_valid_bitmap()@, offset as nat),
            *self === old(self).spec_set_valid_bitmap(self.spec_valid_bitmap()),
    {
        let i: usize = ((offset / 8) / 8) as u8 as usize;
        assert(i == (offset / 8) / 8);
        let bit: u8 = ((offset / 8) % 8) as u8;
        let oldv: u8 = (*self.valid_bitmap.index(i)).into();
        let newv = oldv | BIT8!(bit);
        proof {
            lemma_bit8_setbit(oldv, bit);
        }
        self.valid_bitmap.set(i, newv.into());
    }

    pub fn is_offset_valid(&self, offset: usize) -> (ret: bool)
        requires
            self.is_constant(),
            offset < 0x400,
        ensures
            ret == ensures_offset_valid(self.spec_valid_bitmap()@, offset as nat),
    {
        let idx: usize = ((offset / 8) / 8) as usize;
        let bit: u8 = ((offset / 8) % 8) as u8;
        let v: u8 = (*self.valid_bitmap.index(idx)).into();
        (v & BIT8!(bit)) != 0
    }
}

} // verus!
verus! {

impl GhcbPage {
    ghcb_fns_u64!{rax}

    ghcb_fns_u64!{rbx}

    ghcb_fns_u64!{rcx}

    ghcb_fns_u64!{rdx}

    ghcb_fns_u64!{r8}

    ghcb_fns!{cpl, u8}

    ghcb_fns_u64!{sw_exit_code}

    ghcb_fns_u64!{sw_exit_info_1}

    ghcb_fns_u64!{sw_exit_info_2}

    ghcb_fns_u64!{sw_scratch}
}

} // verus!
use crate::vbox::MutFnTrait;

verus! {

pub struct FillGhcbFn;

pub type FillGhcb = (FillGhcbFn, u64, u64, u64);

impl GhcbPage {
    closed spec fn ensure_fill_ghcb(&self, params: FillGhcb) -> bool {
        &&& self.vmsa.sw_exit_code@.val == params.1
        &&& self.vmsa.sw_exit_info_1@.val == params.2
        &&& self.vmsa.sw_exit_info_2@.val == params.3
        &&& ensures_offset_valid(self.spec_valid_bitmap()@, GhcbVmsa::spec_sw_exit_code_offset())
        &&& ensures_offset_valid(self.spec_valid_bitmap()@, GhcbVmsa::spec_sw_exit_info_1_offset())
        &&& ensures_offset_valid(self.spec_valid_bitmap()@, GhcbVmsa::spec_sw_exit_info_2_offset())
    }
}

impl<'a> MutFnTrait<'a, FillGhcb, bool> for GhcbPage {
    open spec fn spec_update_requires(&self, params: FillGhcb) -> bool {
        self.is_constant()
    }

    closed spec fn spec_update(&self, prev: &Self, params: FillGhcb, ret: bool) -> bool {
        &&& self.is_constant()
        &&& self.ensure_fill_ghcb(params)
        &&& ret
    }

    fn box_update(&'a mut self, params: FillGhcb) -> (ret: bool) {
        self.usage = GHCB_DEFAULT_USAGE.into();
        self.version = GHCB_VERSION_1.into();
        self.set_bitmap_sw_exit_code();
        self.vmsa.sw_exit_code = params.1.into();
        self.set_bitmap_sw_exit_info_1();
        self.vmsa.sw_exit_info_1 = params.2.into();
        self.set_bitmap_sw_exit_info_2();
        self.vmsa.sw_exit_info_2 = params.3.into();
        proof {
            proof_bit8_setbit();
        }
        true
    }
}

} // verus!
verus! {

pub struct GhcbClear;

impl<'a> MutFnTrait<'a, GhcbClear, bool> for GhcbPage {
    open spec fn spec_update_requires(&self, params: GhcbClear) -> bool {
        self.is_constant()
    }

    open spec fn spec_update(&self, prev: &Self, params: GhcbClear, ret: bool) -> bool {
        &&& self.is_constant()
        &&& forall|i| 0 <= i < 16 ==> self.spec_valid_bitmap()@[i] === u8_s::spec_constant(0)
        &&& *self === prev.spec_set_valid_bitmap(self.spec_valid_bitmap())
        &&& ret
    }

    fn box_update(&'a mut self, params: GhcbClear) -> (ret: bool) {
        self.clear();
        true
    }
}

} // verus!
//($fnname: ident, $inputty: ident, $fieldt: ty, $fieldname: ident)
ghcb_box_fn! {GhcbSetRcxFn, GhcbSetRcx, GhcbCheckRcx, u64 ,rcx}
ghcb_box_fn! {GhcbSetRaxFn, GhcbSetRax, GhcbCheckRax, u64, rax}
ghcb_box_fn! {GhcbSetRdxFn, GhcbSetRdx, GhcbCheckRdx, u64, rdx}
ghcb_box_fn! {GhcbSetSwScratchFn, GhcbSetSwScratch, GhcbCheckSwScratch, u64, sw_scratch}
ghcb_box_fn! {GhcbSetR8Fn, GhcbSetR8, GhcbCheckR8x, u64, r8}
ghcb_box_fn! {GhcbSetCplFn, GhcbSetCpl, GhcbCheckCpl, u8, cpl}

verus! {

pub closed spec fn _ghcb_wf(id: int, snp: SwSnpMemAttr) -> bool {
    open_ghcb_wf(id, snp)
}

pub open spec fn open_ghcb_wf(id: int, snp: SwSnpMemAttr) -> bool {
    &&& snp === SwSnpMemAttr::shared()
    &&& id % PAGE_SIZE!() == 0
}

impl VBox<GhcbPage> {
    pub open spec fn ghcb_wf(&self) -> bool {
        &&& self@.is_constant()
        &&& _ghcb_wf(self.id(), self.snp())
    }

    pub proof fn proof_ghcb_wf(&self)
        ensures
            self.ghcb_wf() == (open_ghcb_wf(self.id(), self.snp()) && self@.is_constant()),
    {
    }

    pub fn ghcb_change_page_state_via_pg(
        self,
        ppage: u64,
        npages: u64,
        op: PageOps,
        Tracked(page_perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: VBox<GhcbPage>)
        requires
            self.ghcb_wf(),
            (*old(cs)).inv(),
            spec_valid_page_state_change(ppage, npages as nat),
            requires_pages_perms(*old(page_perms), ppage as int, npages as nat),
            forall|i|
                ppage <= i < (ppage + npages) ==> old(page_perms).contains_key(i) && (
                #[trigger] old(page_perms)[i])@.wf_range((i.to_addr(), PAGE_SIZE as nat)),
        ensures
            ret.only_val_updated(self),
            ret.ghcb_wf(),
            cs.inv(),
            cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            ensure_pages_perm_change_state(
                *old(page_perms),
                *page_perms,
                ppage as int,
                npages as nat,
                op,
            ),
            forall|i|
                ppage <= i < (ppage + npages) ==> #[trigger] page_perms.contains_key(i)
                    && ensure_page_perm_change_state(old(page_perms)[i], page_perms[i], i, op),
    {
        let (ghcb_ptr, Tracked(ghcbpage_perm)) = self.into_raw();
        let tracked mut ghcbpage_perm0 = Map::tracked_empty();
        proof {
            ghcbpage_perm0.tracked_insert(0, ghcbpage_perm);
        }
        ghcb_change_page_state_via_pg(
            ghcb_ptr.clone(),
            ppage,
            npages,
            op,
            Tracked(page_perms),
            Tracked(&mut ghcbpage_perm0),
            Tracked(cs),
        );
        VBox::from_raw(ghcb_ptr.to_usize(), Tracked(ghcbpage_perm0.tracked_remove(0)))
    }

    pub fn alloc_ghcb_handle(Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ghcb: Self)
        requires
            (*old(cs)).inv(),
        ensures
            ghcb.ghcb_wf(),
            cs.inv(),
            cs.only_lock_reg_coremode_updated(
                (*old(cs)),
                set![GHCB_REGID()],
                set![spec_ALLOCATOR().lockid(), spec_PT().lockid()],
            ),
    {
        let ghost cs1 = *cs;
        let ghcb = GhcbHandle::new_aligned_uninit(PAGE_SIZE, Tracked(cs));
        let ghost cs2 = *cs;
        let (ghcb_ptr, Tracked(ghcb_perm)) = ghcb.into_raw();
        let tracked mut ghcb_perm = ghcb_perm.tracked_into_raw();
        let Tracked(ghcb_perm) = init_ghcb_page(ghcb_ptr.clone(), Tracked(ghcb_perm), Tracked(cs));
        proof {
            let ghost cs3 = *cs;
            cs1.lemma_update_prop(
                cs2,
                cs3,
                set![],
                set![spec_ALLOCATOR().lockid()],
                set![GHCB_REGID()],
                set![spec_PT().lockid()],
            );
            assert(set![spec_ALLOCATOR().lockid()].union(set![spec_PT().lockid()])
                =~~= set![spec_ALLOCATOR().lockid(), spec_PT().lockid()]);
        }
        VBox::from_raw(ghcb_ptr.to_usize(), Tracked(ghcb_perm.trusted_into()))
    }

    pub fn new_ghcb_handle(
        ghcb_ptr: SnpPPtr<GhcbPage>,
        Tracked(page_perm): Tracked<SnpPointsToRaw>,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ghcb: Self)
        requires
            page_perm@.wf_const_default((ghcb_ptr.id(), PAGE_SIZE as nat)),
            ghcb_ptr.is_constant(),
            ghcb_ptr.id() % PAGE_SIZE!() == 0,
            (*old(cs)).inv(),
        ensures
            ghcb.ghcb_wf(),
    {
        let Tracked(ghcb_perm) = init_ghcb_page(ghcb_ptr.clone(), Tracked(page_perm), Tracked(cs));
        VBox::from_raw(ghcb_ptr.to_usize(), Tracked(ghcb_perm.trusted_into()))
    }

    pub fn ghcb_page_proto(
        self,
        exit: &mut u64,
        exit1: &mut u64,
        exit2: &mut u64,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: (SvmStatus, Self))
        requires
            (*old(cs)).inv(),
            self.ghcb_wf(),
        ensures
            ret.1.only_val_updated(self),
            ret.1.ghcb_wf(),
            cs.inv(),
            cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
    {
        let ghost oldcs = *cs;
        let tracked mut ghcb_msr_perm = cs.snpcore.regs.tracked_remove(GHCB_REGID());
        let ghost oldghcb_msr_perm = ghcb_msr_perm;
        let mut ghcb = self;
        let ghcb_addr = ghcb.get_const_addr() as u64;
        if !MSR_GHCB().read(Tracked(&ghcb_msr_perm)).eq(&ghcb_addr) {
            proof {
                cs.snpcore.regs.tracked_insert(GHCB_REGID(), ghcb_msr_perm);
            }
            vc_terminate(SM_TERM_NO_GHCB, Tracked(&mut cs.snpcore));
        }
        ghcb.box_update((FillGhcbFn, *exit, *exit1, *exit2));
        let (ghcb_ptr, Tracked(mut ghcbpage_perm)) = ghcb.into_raw();
        let tracked mut op_ghcbpage_perm = Some(ghcbpage_perm.tracked_into_raw());
        vmgexit(
            Tracked(&mut ghcb_msr_perm),
            Tracked(&mut cs.snpcore.coreid),
            Tracked(&mut op_ghcbpage_perm),
        );
        if !MSR_GHCB().read(Tracked(&ghcb_msr_perm)).eq(&ghcb_ptr.as_u64()) {
            proof {
                cs.snpcore.regs.tracked_insert(GHCB_REGID(), ghcb_msr_perm);
            }
            vc_terminate(SM_TERM_NO_GHCB, Tracked(&mut cs.snpcore));
        }
        proof {
            cs.snpcore.regs.tracked_insert(GHCB_REGID(), ghcb_msr_perm);
            assert(cs.snpcore.regs[GHCB_REGID()] === (*old(cs)).snpcore.regs[GHCB_REGID()]);
            assert(cs.snpcore.regs =~~= (*old(cs)).snpcore.regs);
            assert(cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]));
            ghcbpage_perm = op_ghcbpage_perm.tracked_unwrap().tracked_into();
        }
        let ghost prevcs = *cs;
        // HV-shared memory should be copied or moved and cannot be borrowed.
        let (exit_info_1, Tracked(ghcbpage_perm)) = ghcb_ptr.vmsa().sw_exit_info_1().copy_with::<
            GhcbPage,
        >(Tracked(ghcbpage_perm));
        let (exit_info_2, Tracked(ghcbpage_perm)) = ghcb_ptr.vmsa().sw_exit_info_2().copy_with::<
            GhcbPage,
        >(Tracked(ghcbpage_perm));
        let (sw_exit_code, Tracked(ghcbpage_perm)) = ghcb_ptr.vmsa().sw_exit_code().copy_with::<
            GhcbPage,
        >(Tracked(ghcbpage_perm));
        let exit_info_1: u64 = exit_info_1.into();
        let exit_info_2: u64 = exit_info_2.into();
        let ghcb = VBox::from_raw(ghcb_addr as usize, Tracked(ghcbpage_perm));
        let err = if (exit_info_1 & 0xffffffff) == 1 {
            let vec = SVM_EVTINJ_VEC_MASK!(exit_info_2);
            let ty = SVM_EVTINJ_TYPE_MASK!(exit_info_2);
            let valid = SVM_EVTINJ_IS_VALID!(exit_info_2);
            if valid && (ty == SVM_EVTINJ_TYPE_EXEPT) && (vec == SVM_EVTINJ_VEC_X86_TRAP_GP || vec
                == SVM_EVTINJ_VEC_X86_TRAP_UD) {
                SvmStatus::Exception
            } else {
                SvmStatus::VmmError
            }
        } else {
            *exit = sw_exit_code.into();
            *exit1 = exit_info_1;
            *exit2 = exit_info_2;
            SvmStatus::Ok
        };
        new_strlit("ghcb_page_proto\n").leak_debug();
        (err, ghcb)
    }

    pub fn ghcb_read_msr(self, reg: u32, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) -> (ret: (
        u64,
        Self,
    ))
        requires
            self.ghcb_wf(),
            (*old(cs)).inv(),
        ensures
            cs.inv(),
            cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
            ret.1.only_val_updated(self),
            ret.1.ghcb_wf(),
    {
        let mut ghcb = self;
        ghcb.box_update(GhcbClear);
        ghcb.box_update((GhcbSetRcxFn, reg as u64));
        let mut exit_code = SVM_EXIT_MSR;
        let mut exit_info1 = 0;
        let mut exit_info2 = 0;
        let (resp, mut ghcb) = ghcb.ghcb_page_proto(
            &mut exit_code,
            &mut exit_info1,
            &mut exit_info2,
            Tracked(cs),
        );
        let mut rax: u64 = 0;
        let mut rdx: u64 = 0;
        match resp {
            SvmStatus::Ok => {
                // Cannot borrow Hv-shared memory. Copy to private.
                let (ghcb_ptr, Tracked(ghcb_perm)) = ghcb.into_raw();
                let (ax, Tracked(ghcb_perm)) = ghcb_ptr.vmsa().rax().copy_with::<GhcbPage>(
                    Tracked(ghcb_perm),
                );
                let (dx, Tracked(ghcb_perm)) = ghcb_ptr.vmsa().rax().copy_with::<GhcbPage>(
                    Tracked(ghcb_perm),
                );
                rax = ax.into();
                rdx = dx.into();
                ghcb = VBox::from_raw(ghcb_ptr.to_usize(), Tracked(ghcb_perm));
            },
            _ => {
                vc_terminate(SM_TERM_GHCB_RESP_INVALID, Tracked(&mut cs.snpcore));
            },
        }
        return ((rax as u32 as u64) | ((rdx as u32 as u64) << 32u64), ghcb);
    }

    pub fn ghcb_write_msr(
        self,
        reg: u32,
        val: u64,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: Self)
        requires
            self.ghcb_wf(),
            (*old(cs)).inv(),
        ensures
            ret.only_val_updated(self),
            ret.ghcb_wf(),
            cs.inv(),
            cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
    {
        let mut ghcb = self;
        ghcb.box_update(GhcbClear);
        ghcb.box_update((GhcbSetRcxFn, reg as u64));
        ghcb.box_update((GhcbSetRaxFn, val as u32 as u64));
        ghcb.box_update((GhcbSetRdxFn, (val >> 32u64)));
        let mut exit_code = SVM_EXIT_MSR;
        let mut exit_info1 = 1;
        let mut exit_info2 = 0;
        let (resp, ghcb) = ghcb.ghcb_page_proto(
            &mut exit_code,
            &mut exit_info1,
            &mut exit_info2,
            Tracked(cs),
        );
        match resp {
            SvmStatus::Ok => {},
            _ => {
                vc_terminate(SM_TERM_GHCB_RESP_INVALID, Tracked(&mut cs.snpcore));
            },
        }
        ghcb
    }

    pub fn ghcb_guest_request(
        self,
        req_gpa: u64,
        resp_gpa: u64,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: Self)
        requires
            (*old(cs)).inv(),
            self.ghcb_wf(),
        ensures
            ret.only_val_updated(self),
            ret.ghcb_wf(),
            cs.inv(),
            cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
    {
        let mut ghcb = self;
        ghcb.box_update(GhcbClear);
        let (ghcb_ptr, Tracked(ghcbpage_perm)) = ghcb.into_raw();
        let mut exit_code = SVM_EXIT_SNP_GUEST_REQUEST;
        let mut exit_info1 = req_gpa;
        let mut exit_info2 = resp_gpa;
        let tracked mut ghcbpage_perm0 = Map::tracked_empty();
        proof {
            ghcbpage_perm0.tracked_insert(0, ghcbpage_perm);
        }
        let resp = ghcb_page_proto(
            ghcb_ptr.clone(),
            &mut exit_code,
            &mut exit_info1,
            &mut exit_info2,
            Tracked(&mut ghcbpage_perm0),
            Tracked(cs),
        );
        match resp {
            SvmStatus::Ok => {
                if exit_code != SVM_EXIT_SNP_GUEST_REQUEST {
                    vc_terminate(SM_TERM_GHCB_RESP_INVALID, Tracked(&mut cs.snpcore));
                }
            },
            _ => {
                vc_terminate(SM_TERM_GHCB_RESP_INVALID, Tracked(&mut cs.snpcore));
            },
        }
        VBox::from_raw(ghcb_ptr.to_usize(), Tracked(ghcbpage_perm0.tracked_remove(0)))
    }
}

fn init_ghcb_page(
    ghcb_ptr: SnpPPtr<GhcbPage>,
    Tracked(page_perm): Tracked<SnpPointsToRaw>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ghcb_perm: Tracked<SnpPointsToRaw>)
    requires
        page_perm@.wf_const_default((ghcb_ptr.id(), PAGE_SIZE as nat)),
        ghcb_ptr.is_constant(),
        ghcb_ptr.id() % PAGE_SIZE!() == 0,
        (*old(cs)).inv(),
    ensures
        ghcb_perm@@.wf_shared((ghcb_ptr.id(), PAGE_SIZE as nat)),
        cs.inv(),
        cs.snpcore.ghcb_value() == ghcb_ptr.id(),
        cs.only_lock_reg_coremode_updated((*old(cs)), set![GHCB_REGID()], set![spec_PT().lockid()]),
{
    let ghcb_page: u64 = ghcb_ptr.as_u64().to_page();
    assert((ghcb_page as int).to_addr() == ghcb_ptr.id());
    let tracked mut page_perm0 = Map::tracked_empty();
    let ghost old_page_perm = page_perm;
    proof {
        page_perm0.tracked_insert(0, page_perm);
    }
    early_mk_shared(ghcb_page, Tracked(cs), Tracked(&mut page_perm0));
    let tracked mut ghcb_perm = page_perm0.tracked_remove(0);
    mem_set_zeros(ghcb_ptr.to_usize(), PAGE_SIZE, Tracked(&mut ghcb_perm));
    ghcb_register_ghcb(vn_to_pn(ghcb_page, Tracked(&ghcb_perm)) as usize, Tracked(&mut cs.snpcore));
    proof {
        crate::snp::mem::lemma_mk_shared_default_to_shared(old_page_perm@, ghcb_perm@);
    }
    Tracked(ghcb_perm)
}

} // verus!
use crate::vbox::*;
verus! {

pub fn ghcb_page_proto(
    ghcb_ptr: SnpPPtr<GhcbPage>,
    exit: &mut u64,
    exit1: &mut u64,
    exit2: &mut u64,
    Tracked(ghcbpage_perm0): Tracked<&mut Map<int, SnpPointsTo<GhcbPage>>>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: SvmStatus)
    requires
        (*old(cs)).inv(),
        old(ghcbpage_perm0).contains_key(0),
        old(ghcbpage_perm0)[0]@.wf_shared(ghcb_ptr.id()),
        ghcb_ptr.is_constant(),
    ensures
        ghcbpage_perm0.contains_key(0),
        ghcbpage_perm0[0]@.only_val_updated(old(ghcbpage_perm0)[0]@),
        ghcbpage_perm0[0]@.wf_shared(ghcb_ptr.id()),
        cs.inv(),
        cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]),
{
    let ghost oldcs = *cs;
    let tracked mut ghcb_msr_perm = cs.snpcore.regs.tracked_remove(GHCB_REGID());
    let ghost oldghcb_msr_perm = ghcb_msr_perm;
    let tracked ghcbpage_perm = ghcbpage_perm0.tracked_remove(0);
    if !MSR_GHCB().read(Tracked(&ghcb_msr_perm)).eq(&ghcb_ptr.as_u64()) {
        proof {
            cs.snpcore.regs.tracked_insert(GHCB_REGID(), ghcb_msr_perm);
        }
        vc_terminate(SM_TERM_NO_GHCB, Tracked(&mut cs.snpcore));
    }
    let mut ghcb: VBox<GhcbPage> = VBox::from_raw(ghcb_ptr.to_usize(), Tracked(ghcbpage_perm));
    ghcb.box_update((FillGhcbFn, *exit, *exit1, *exit2));
    let (ghcb_ptr, Tracked(mut ghcbpage_perm)) = ghcb.into_raw();
    let tracked mut op_ghcbpage_perm = Some(ghcbpage_perm.tracked_into_raw());
    vmgexit(
        Tracked(&mut ghcb_msr_perm),
        Tracked(&mut cs.snpcore.coreid),
        Tracked(&mut op_ghcbpage_perm),
    );
    if !MSR_GHCB().read(Tracked(&ghcb_msr_perm)).eq(&ghcb_ptr.as_u64()) {
        proof {
            cs.snpcore.regs.tracked_insert(GHCB_REGID(), ghcb_msr_perm);
        }
        vc_terminate(SM_TERM_NO_GHCB, Tracked(&mut cs.snpcore));
    }
    proof {
        assert(ghcb_msr_perm.val::<u64_s>() === oldghcb_msr_perm.val::<u64_s>());
        assert(ghcb_msr_perm.view::<u64_s>() === oldghcb_msr_perm.view::<u64_s>());
        cs.snpcore.regs.tracked_insert(GHCB_REGID(), ghcb_msr_perm);
        assert(cs.snpcore.regs[GHCB_REGID()] === (*old(cs)).snpcore.regs[GHCB_REGID()]);
        assert(cs.snpcore.regs =~~= (*old(cs)).snpcore.regs);
        assert(cs.only_lock_reg_coremode_updated((*old(cs)), set![], set![]));
        ghcbpage_perm = op_ghcbpage_perm.tracked_unwrap().tracked_into();
    }
    let ghost prevcs = *cs;
    // HV-shared memory should be copied or moved and cannot be borrowed.
    let (exit_info_1, Tracked(ghcbpage_perm)) = ghcb_ptr.vmsa().sw_exit_info_1().copy_with::<
        GhcbPage,
    >(Tracked(ghcbpage_perm));
    let (exit_info_2, Tracked(ghcbpage_perm)) = ghcb_ptr.vmsa().sw_exit_info_2().copy_with::<
        GhcbPage,
    >(Tracked(ghcbpage_perm));
    let (sw_exit_code, Tracked(ghcbpage_perm)) = ghcb_ptr.vmsa().sw_exit_code().copy_with::<
        GhcbPage,
    >(Tracked(ghcbpage_perm));
    let exit_info_1: u64 = exit_info_1.into();
    let exit_info_2: u64 = exit_info_2.into();
    proof {
        ghcbpage_perm0.tracked_insert(0, ghcbpage_perm);
    }
    if (exit_info_1 & 0xffffffff) == 1 {
        let vec = SVM_EVTINJ_VEC_MASK!(exit_info_2);
        let ty = SVM_EVTINJ_TYPE_MASK!(exit_info_2);
        let valid = SVM_EVTINJ_IS_VALID!(exit_info_2);
        if valid && (ty == SVM_EVTINJ_TYPE_EXEPT) && (vec == SVM_EVTINJ_VEC_X86_TRAP_GP || vec
            == SVM_EVTINJ_VEC_X86_TRAP_UD) {
            SvmStatus::Exception
        } else {
            SvmStatus::VmmError
        }
    } else {
        *exit = sw_exit_code.into();
        *exit1 = exit_info_1;
        *exit2 = exit_info_2;
        SvmStatus::Ok
    }
}

} // verus!

================
File: ./source/verismo/src/snp/mod.rs
================

pub mod cpu;
pub mod cpuid;
pub mod ghcb;
pub mod mem;
pub mod percpu;
mod trackedcore;
pub use trackedcore::*;

================
File: ./source/verismo/src/snp/percpu/def_s.rs
================

use super::*;

verus! {

pub const VMPL_COUNT: usize = 4;

pub const BSP: u64 = 0;

} // verus!
#[macro_export]
macro_rules! RICHOS_VMPL {
    () => {
        crate::arch::VMPL::VMPL1
    };
}

verismo_simple! {
pub type StackPages = [u8; 0x10000];
}

================
File: ./source/verismo/src/snp/percpu/mod.rs
================

use super::cpu::*;
use super::ghcb::*;
use crate::addr_e::OnePage;
use crate::ptr::*;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

mod def_s;

pub use def_s::*;

================
File: ./source/verismo/src/snp/trackedcore/snpmulticore.rs
================

use super::*;
use crate::global::{spec_CONSOLE, IsConsole, *};
use crate::lock::*;
use crate::pgtable_e::*;
use crate::ptr::SnpPointsToRaw;

verus! {

#[derive(SpecGetter, SpecSetter)]
pub tracked struct SnpCoreSharedMem {
    pub snpcore: SnpCore,
    pub lockperms: Map<int, LockPermRaw>,
}

impl SnpCoreSharedMem {
    pub open spec fn inv(&self) -> bool {
        &&& self.snpcore.inv()
        &&& self.lockperms.inv(self.snpcore.cpu())
        &&& contains_CONSOLE(self.lockperms)
        &&& contains_ALLOCATOR(self.lockperms)
        &&& contains_PT(self.lockperms)
        &&& self.wf_pt()
    }

    pub open spec fn inv_stage_common(&self) -> bool {
        &&& self.inv()
    }

    pub open spec fn inv_stage_verismo(&self) -> bool {
        &&& self.inv()
        &&& contains_OSMEM(self.lockperms)
        &&& contains_RICHOS_VMSA(self.lockperms)
        &&& contains_PCR(self.lockperms)
    }

    pub open spec fn inv_stage_ap_wait(&self) -> bool {
        self.inv_stage_verismo()
    }

    pub open spec fn inv_stage_pcr(&self) -> bool {
        &&& self.inv()
        &&& contains_PCR(self.lockperms)
    }

    pub open spec fn wf_top_pt(&self) -> bool {
        let cr3_u64: u64 = self.snpcore.regs[RegName::Cr3].val::<u64_s>().vspec_cast_to();
        let top_pte = self.pte_perms()[top_lvl_idx()].val();
        &&& top_pte.value == static_cr3_value()
        &&& cr3_u64 == static_cr3_value()
    }

    pub open spec fn wf_pt(&self) -> bool {
        &&& self.wf_top_pt()
        &&& wf_ptes(self.pte_perms())
    }

    pub open spec fn pte_perms(&self) -> PtePerms {
        self.lockperms[spec_PT_lockid()]@.points_to.value::<TrackedPTEPerms>()@
    }

    pub open spec fn inv_core(&self, core: nat) -> bool {
        &&& self.inv()
        &&& self.snpcore.cpu() == core
    }

    pub open spec fn only_lock_reg_updated(
        &self,
        prev: Self,
        regs: Set<RegName>,
        locks: Set<int>,
    ) -> bool {
        &&& self.lockperms.updated_lock(&prev.lockperms, locks)
        &&& self.snpcore.only_reg_updated(prev.snpcore, regs)
    }

    //#[verifier(inline)]
    pub open spec fn only_lock_reg_coremode_updated(
        &self,
        prev: Self,
        regs: Set<RegName>,
        locks: Set<int>,
    ) -> bool {
        &&& self.lockperms.updated_lock(&prev.lockperms, locks)
        &&& self.snpcore.only_reg_coremode_updated(prev.snpcore, regs)
    }

    pub proof fn lemma_update_prop_auto()
        ensures
            forall|prev: Self, cur: Self, next: Self, regs1, regs2|
                (#[trigger] cur.snpcore.reg_updated(prev.snpcore, regs1)
                    && #[trigger] next.snpcore.reg_updated(cur.snpcore, regs2))
                    ==> next.snpcore.reg_updated(prev.snpcore, regs1.union(regs2)),
            forall|prev: Self, cur: Self, next: Self, locks1: Set<int>, locks2: Set<int>|
                (#[trigger] cur.lockperms.updated_lock(&prev.lockperms, locks1)
                    && #[trigger] next.lockperms.updated_lock(&cur.lockperms, locks2))
                    ==> next.lockperms.updated_lock(&prev.lockperms, locks1.union(locks2)),
    {
        LockMap::lemma_lock_update_auto();
        SnpCore::lemma_regs_update_auto();
    }

    pub proof fn lemma_update_prop(
        &self,
        cur: Self,
        next: Self,
        regs1: Set<RegName>,
        locks1: Set<int>,
        regs2: Set<RegName>,
        locks2: Set<int>,
    )
        requires
            #[trigger] cur.only_lock_reg_coremode_updated(*self, regs1, locks1),
            #[trigger] next.only_lock_reg_coremode_updated(cur, regs2, locks2),
        ensures
            next.only_lock_reg_coremode_updated(*self, regs1.union(regs2), locks1.union(locks2)),
            Set::<int>::empty().union(set![]) =~~= set![],
            regs1.union(regs2) =~~= regs2.union(regs1),
            locks1.union(locks2) =~~= locks2.union(locks1),
            forall|s: Set<RegName>| s.union(s) =~~= s,
            forall|s: Set<int>| s.union(s) =~~= s,
            forall|s: Set<RegName>| s.union(set![]) =~~= s,
            forall|s: Set<int>| s.union(set![]) =~~= s,
            forall|s: Set<RegName>| set![].union(s) =~~= s,
            forall|s: Set<int>| set![].union(s) =~~= s,
    {
        Self::lemma_update_prop_auto();
    }
}

} // verus!
verus! {

pub tracked struct SnpCoreConsole {
    pub snpcore: SnpCore,
    pub console: Map<int, SnpPointsToRaw>,
}

pub open spec fn snpcore_console_wf(snpcore: SnpCore, console: SnpPointsToRaw) -> bool {
    &&& snpcore.inv()
    &&& console.is_console()
}

impl SnpCoreConsole {
    pub open spec fn console(&self) -> SnpPointsToRaw {
        self.console[0]
    }

    pub open spec fn wf(&self) -> bool {
        &&& self.console.contains_key(0)
        &&& snpcore_console_wf(self.snpcore, self.console[0])
    }

    pub open spec fn wf_core(&self, core: nat) -> bool {
        &&& self.wf()
        &&& self.snpcore.cpu() == core
    }

    #[verifier(inline)]
    pub open spec fn only_console_reg_coremode_updated(
        &self,
        prev: Self,
        regs: Set<RegName>,
    ) -> bool {
        &&& self.console.contains_key(0)
        &&& self.console[0]@.only_val_updated(prev.console[0]@)
        &&& self.snpcore.only_reg_coremode_updated(prev.snpcore, regs)
    }
}

} // verus!

================
File: ./source/verismo/src/snp/trackedcore/mod.rs
================

mod snpmulticore;
pub use snpmulticore::*;

use crate::arch::reg::*;
use crate::ptr::*;
use crate::registers::*;
use crate::tspec::*;
use crate::tspec_e::*;

================
File: ./source/verismo/src/snp/mem.rs
================

use crate::addr_e::*;
use crate::arch::addr_s::PAGE_SIZE;
use crate::debug::VPrintAtLevel;
use crate::global::*;
use crate::pgtable_e::*;
use crate::ptr::*;
use crate::snp::ghcb::*;
use crate::snp::SnpCoreSharedMem;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::vbox::VBox;

verus! {

pub open spec fn mk_shared_ensures_pageperm(
    prev_page_perm: SnpPointsToBytes,
    page_perm: SnpPointsToBytes,
) -> bool {
    &&& page_perm.wf_range(prev_page_perm.range())
    &&& page_perm.snp().is_shared_from(prev_page_perm.snp())
}

pub proof fn lemma_mk_shared_default_to_shared(
    prev_page_perm: SnpPointsToBytes,
    page_perm: SnpPointsToBytes,
)
    requires
        mk_shared_ensures_pageperm(prev_page_perm, page_perm),
        prev_page_perm.snp() === SwSnpMemAttr::spec_default(),
        prev_page_perm.wf(),
    ensures
        page_perm.snp() === SwSnpMemAttr::shared(),
{
    assert(page_perm.snp().pte =~~= SwSnpMemAttr::shared().pte);
}

proof fn lemma_mk_private_shared_to_default(
    prev_page_perm: SnpPointsToBytes,
    page_perm: SnpPointsToBytes,
)
    requires
        mk_private_ensures_pageperm(prev_page_perm, page_perm),
        prev_page_perm.snp() === SwSnpMemAttr::shared(),
        prev_page_perm.wf(),
    ensures
        page_perm.snp() === SwSnpMemAttr::spec_default(),
{
    assert(page_perm.snp().pte =~~= SwSnpMemAttr::spec_default().pte);
}

pub open spec fn mk_private_ensures_pageperm(
    prev_page_perm: SnpPointsToBytes,
    page_perm: SnpPointsToBytes,
) -> bool {
    &&& page_perm.wf_range(prev_page_perm.range())
    &&& page_perm.snp().is_vmpl0_private()
    &&& page_perm.snp().pte() === prev_page_perm.snp().pte().spec_set_encrypted(true)
    &&& page_perm.snp() === prev_page_perm.snp().spec_set_pte(page_perm.snp().pte).spec_set_rmp(
        page_perm.snp().rmp,
    )
    &&& page_perm.snp().rmp === SwSnpMemAttr::spec_default().rmp
}

pub open spec fn spec_is_shared_page_perms(
    page_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    npages: nat,
) -> bool {
    forall|i|
        start_page <= i < (start_page + npages) ==> #[trigger] page_perms.contains_key(i)
            && page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)) && page_perms[i]@.snp()
            === SwSnpMemAttr::shared()
}

pub open spec fn spec_is_vmprivate_const_perms(
    page_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    npages: nat,
) -> bool {
    forall|i|
        start_page <= i < (start_page + npages) ==> #[trigger] page_perms.contains_key(i)
            && page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat))
            && page_perms[i]@.snp().requires_pvalidate(i.to_addr(), 0, 0)
            && page_perms[i]@.bytes().is_constant()
}

// Make a page as shared via GHCB MSR
pub fn early_mk_shared(
    start_page: u64,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    Tracked(page_perm0): Tracked<&mut Map<int, SnpPointsToRaw>>,
)
    requires
        (*old(cs)).inv(),
        old(page_perm0).contains_key(0),
        old(page_perm0)[0]@.wf_const_default(((start_page as int).to_addr(), PAGE_SIZE as nat)),
    ensures
        (*cs).inv(),
        (*cs).only_lock_reg_coremode_updated(
            (*old(cs)),
            set![GHCB_REGID()],
            set![spec_PT().lockid()],
        ),
        page_perm0.contains_key(0),
        mk_shared_ensures_pageperm(old(page_perm0)[0]@, page_perm0[0]@),
{
    let page_op = PageOps::Shared;
    let tracked mut page_perm = page_perm0.tracked_remove(0);
    let ghost old_page_perm = page_perm;
    ghcb_change_page_state_via_msr(
        vn_to_pn(start_page, Tracked(&page_perm)) as usize,
        page_op,
        Tracked(&mut page_perm),
        Tracked(&mut cs.snpcore),
    );
    let ghost perm1 = page_perm;
    proof {
        page_perm0.tracked_insert(0, page_perm);
    }
    let ok = set_page_enc_dec(start_page.to_addr(), false, Tracked(cs), Tracked(page_perm0));
    if !ok {
        new_strlit("\nfailed early_mk_shared\n").leak_debug();
        vc_terminate(SM_TERM_MEM, Tracked(&mut cs.snpcore));
    }
}

} // verus!
verus! {

impl GhcbHandle {
    // start_page is virtual starting page;
    pub fn new_shared_vpage<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(
        self,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    ) -> (ret: (VBox<T>, Self))
        requires
            self.ghcb_wf(),
            (*old(cs)).inv_ac(),
            spec_size::<T>() == PAGE_SIZE,
        ensures
            (*cs).inv_ac(),
            (*cs).only_lock_reg_coremode_updated(
                (*old(cs)),
                set![],
                set![spec_ALLOCATOR_lockid(), spec_PT_lockid()],
            ),
            ret.0.snp() === SwSnpMemAttr::shared(),
            ret.0.is_shared_page(),
            ret.1.ghcb_wf(),
    {
        let ghost oldcs = (*cs);
        let ret: VBox<T> = VBox::new_aligned_uninit(PAGE_SIZE, Tracked(cs));
        let (ptr, Tracked(perm)) = ret.into_raw();
        let page: u64 = ptr.as_u64().to_page();
        let tracked mut page_perms = Map::tracked_empty();
        proof {
            assert(ptr.id().spec_valid_addr_with(spec_size::<T>()));
            page_perms.tracked_insert(page as int, perm.tracked_into_raw());
        }
        let ghost old_page_perms = page_perms;
        let ghost prevcs = (*cs);
        let handle = self.mk_shared(page, 1, Tracked(cs), Tracked(&mut page_perms));
        proof {
            assert(old_page_perms.contains_key(page as int));
            assert(page_perms.contains_key(page as int));
            oldcs.lemma_update_prop(
                prevcs,
                (*cs),
                set![],
                set![spec_ALLOCATOR_lockid()],
                set![],
                set![spec_PT_lockid()],
            );
            assert(set![spec_ALLOCATOR_lockid()].union(set![spec_PT_lockid()])
                =~~= set![spec_ALLOCATOR_lockid(), spec_PT_lockid()]);
            assert(mk_shared_ensures_pageperm(
                old_page_perms[page as int]@,
                page_perms[page as int]@,
            ));
            lemma_mk_shared_default_to_shared(
                old_page_perms[page as int]@,
                page_perms[page as int]@,
            );
            assert(page_perms[page as int]@.snp() === SwSnpMemAttr::shared());
        }
        let tracked perm = page_perms.tracked_remove(page as int).tracked_into();
        (VBox::from_raw(ptr.to_usize(), Tracked(perm)), handle)
    }

    pub fn mk_shared(
        self,
        start_page: u64,
        npages: u64,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
        Tracked(page_perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
    ) -> (ret: Self)
        requires
            self.ghcb_wf(),
            (*old(cs)).inv(),
            npages > 0,
            spec_valid_page_state_change(start_page, npages as nat),
            wf_page_range(*old(page_perms), start_page as int, npages as nat),
        ensures
            ret.ghcb_wf(),
            ret.only_val_updated(self),
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![spec_PT_lockid()]),
            page_perms.dom() =~~= old(page_perms).dom(),
            forall|i|
                start_page <= i < (start_page + npages) ==> #[trigger] page_perms.contains_key(i)
                    && mk_shared_ensures_pageperm(old(page_perms)[i]@, page_perms[i]@),
    {
        let ghost oldcs = (*cs);
        let ghost old_page_perms = *page_perms;
        proof {
            assert(requires_pages_perms(*page_perms, start_page as int, npages as nat));
            assert(page_perms.contains_key(start_page as int));
        }
        let ret = self.ghcb_change_page_state_via_pg(
            vn_to_pn(start_page, Tracked(page_perms.tracked_borrow(start_page as int))),
            npages,
            PageOps::Shared,
            Tracked(page_perms),
            Tracked(cs),
        );
        let ghost prevcs = (*cs);
        proof {
            assert(forall|i|
                start_page <= i < (start_page + npages) ==> #[trigger] page_perms.contains_key(i)
                    && ensure_page_perm_change_state(
                    old_page_perms[i],
                    page_perms[i],
                    i,
                    PageOps::Shared,
                ));
            assert forall|i|
                start_page <= i < (start_page + npages) implies #[trigger] page_perms.contains_key(
                i,
            ) && ensure_page_perm_change_state(
                old_page_perms[i],
                page_perms[i],
                i,
                PageOps::Shared,
            ) by {
                assert(old_page_perms.contains_key(i));
                assert(page_perms.contains_key(i));
            }
            assert forall|i: int|
                start_page <= i < start_page + npages implies page_perms.contains_key(i)
                && page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)) by {
                assert(page_perms.contains_key(i));
                assert(old_page_perms.contains_key(i));
                assert(old_page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)));
            }
        }
        let ghost prev_page_perms = *page_perms;
        set_pages_enc_dec(start_page, npages as u64, false, Tracked(cs), Tracked(page_perms));
        proof {
            oldcs.lemma_update_prop(
                prevcs,
                (*cs),
                set![],
                set![],
                set![],
                set![spec_PT().lockid()],
            );
            assert(set![].union(set![spec_PT().lockid()]) =~~= set![spec_PT().lockid()]);
            assert forall|i|
                start_page <= i < (start_page + npages) implies #[trigger] page_perms.contains_key(
                i,
            ) && mk_shared_ensures_pageperm(old_page_perms[i]@, page_perms[i]@) by {
                assert(old_page_perms.contains_key(i));
                assert(prev_page_perms.contains_key(i));
                assert(page_perms.contains_key(i));
                let oldp = old_page_perms[i];
                let prevp = prev_page_perms[i];
                let p = page_perms[i];
                assert(oldp@.wf_range((i.to_addr(), PAGE_SIZE as nat)));
                assert(ensure_page_perm_change_state(oldp, prevp, i, PageOps::Shared));
                assert(ensures_mem_enc_dec_memperm(false, prevp, p));
                assert(p@.wf_range(oldp@.range()));
                assert(p@.snp().is_shared_from(oldp@.snp()));
                assert(p@.snp().is_shared());
            }
        }
        ret
    }

    pub fn mk_private(
        self,
        start_page: u64,
        npages: u64,
        Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
        Tracked(page_perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
    ) -> (ret: Self)
        requires
            self.ghcb_wf(),
            (*old(cs)).inv(),
            npages > 0,
            spec_valid_page_state_change(start_page, npages as nat),
            wf_page_range(*old(page_perms), start_page as int, npages as nat),
        ensures
            ret.ghcb_wf(),
            ret.only_val_updated(self),
            (*cs).inv(),
            (*cs).only_lock_reg_coremode_updated((*old(cs)), set![], set![spec_PT_lockid()]),
            page_perms.dom() =~~= old(page_perms).dom(),
            forall|i|
                start_page <= i < (start_page + npages) ==> #[trigger] page_perms.contains_key(i)
                    && mk_private_ensures_pageperm(old(page_perms)[i]@, page_perms[i]@),
    {
        let end_page = start_page + npages;
        let ghost oldcs = (*cs);
        let ghost old_page_perms = *page_perms;
        proof {
            assert(page_perms.contains_key(start_page as int));
            assert forall|i|
                start_page <= i < start_page
                    + npages implies #[trigger] old_page_perms.contains_key(i) by {}
        }
        let ret = self.ghcb_change_page_state_via_pg(
            vn_to_pn(start_page, Tracked(page_perms.tracked_borrow(start_page as int))),
            npages,
            PageOps::Private,
            Tracked(page_perms),
            Tracked(cs),
        );
        let ghost prevcs = (*cs);
        proof {
            assert forall|i: int|
                start_page <= i < start_page + npages implies page_perms.contains_key(i)
                && page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)) by {
                assert(page_perms.contains_key(i));
                assert(old_page_perms.contains_key(i));
                assert(old_page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)));
            }
        }
        let ghost prev_page_perms = *page_perms;
        set_pages_enc_dec(start_page, npages as u64, true, Tracked(cs), Tracked(page_perms));
        proof {
            oldcs.lemma_update_prop(
                prevcs,
                (*cs),
                set![],
                set![],
                set![],
                set![spec_PT().lockid()],
            );
            assert(set![].union(set![spec_PT().lockid()]) =~~= set![spec_PT().lockid()]);
            assert forall|i| start_page <= i < start_page + npages implies (
            #[trigger] page_perms.contains_key(i) && spec_perm_requires_pvalidate(
                page_perms[i],
                i.to_addr(),
                PAGE_SIZE as nat,
                true,
            )) by {
                assert(prev_page_perms.contains_key(i));
                assert(page_perms.contains_key(i));
                assert(prev_page_perms[i]@.snp().wf());
                assert(!prev_page_perms[i]@.snp().pte().spec_encrypted());
                assert(page_perms[i]@.snp_wf_range((i.to_addr(), PAGE_SIZE as nat)));
                assert(!page_perms[i]@.snp().rmp@.spec_validated());
                assert(page_perms[i]@.snp().deterministic_pte());
            }
        }
        pvalmem2(
            start_page.to_addr(),
            end_page.to_addr(),
            true,
            Tracked(page_perms),
            Tracked(&mut cs.snpcore),
        );
        proof {
            assert forall|i| (start_page <= i < (start_page + npages)) implies ((
            #[trigger] page_perms.contains_key(i)) && old_page_perms.contains_key(i)
                && mk_private_ensures_pageperm(old_page_perms[i]@, page_perms[i]@)) by {
                let page_perm = page_perms[i]@;
                let prev_page_perm = old_page_perms[i]@;
                assert(old_page_perms.contains_key(i));
                assert(page_perms.contains_key(i));
                assert(prev_page_perms.contains_key(i));
                assert(page_perm.snp().is_vmpl0_private());
                assert(page_perm.snp().pte() === prev_page_perm.snp().pte().spec_set_encrypted(
                    true,
                ));
                assert(page_perm.snp() === prev_page_perm.snp().spec_set_pte(
                    page_perm.snp().pte,
                ).spec_set_rmp(page_perm.snp().rmp));
                assert(page_perm.snp().rmp === SwSnpMemAttr::spec_default().rmp);
            }
            assert(forall|i|
                (start_page <= i < (start_page + npages)) ==> ((#[trigger] page_perms.contains_key(
                    i,
                )) && mk_private_ensures_pageperm(old_page_perms[i]@, page_perms[i]@)));
        }
        ret
    }
}

} // verus!

================
File: ./source/verismo/src/pgtable_e/def.rs
================

use super::*;
use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::debug::VPrintAtLevel;
use crate::global::*;
use crate::registers::{AnyRegTrait, RegisterPerm, RegisterPermValue};
use crate::snp::SnpCoreSharedMem;
use crate::*;

verus! {

pub type PageTable = Array<u64_s, 512>;

pub const PAT_RESET_VAL: u64 = 0x7040600070406;

pub const PAGE_TABLE_LEVELS: u8 = 4;

#[vbit_struct(PTE, u64)]
pub struct SpecPTE {
    #[vbits(0, 0)]
    pub present: u64,
    #[vbits(1, 1)]
    pub write: u64,
    #[vbits(2, 2)]
    pub supervisor: u64,
    #[vbits(3, 3)]
    pub pwt: u64,
    #[vbits(4, 4)]
    pub pcd: u64,
    #[vbits(5, 5)]
    pub accessed: u64,
    #[vbits(6, 6)]
    pub dirty: u64,
    #[vbits(7, 7)]
    pub psize: u64,
    #[vbits(8, 8)]
    pub global: u64,
    #[vbits(12, 12)]
    pub bit12: u64,
    #[vbits(51, 51)]
    pub encrypted: u64,
    #[vbits(12, 50)]
    pub page: u64,
    #[vbits(63, 63)]
    pub nx: u64,
}

// Private perms to prevent
pub struct PtePerm {
    pub lvl: nat,
    pub val: PTE,  // current lvl, entry value
    pub range: (int, nat),
    pub perm: Option<SnpPointsTo<PageTable>>,  // memory perms for next table
}

pub type PtePerms = Map<(nat, int), PtePerm>;

} // verus!
verus! {

pub closed spec fn static_cr3_value() -> int;

pub open spec fn top_lvl_idx() -> (nat, int) {
    (PAGE_TABLE_LEVELS as nat, 0)
}

pub trait PtePermsTrait {
    spec fn pte(&self, vaddr: int) -> Option<PtePerm>;

    spec fn pde(&self, vaddr: int) -> Option<PtePerm>;

    spec fn pdpe(&self, vaddr: int) -> Option<PtePerm>;

    spec fn pml4e(&self, vaddr: int) -> Option<PtePerm>;

    spec fn entry(&self, vaddr: int, lvl: nat) -> Option<PtePerm>;
}

} // verus!
verismo_simple! {
pub struct TrackedPTEPerms {
    pub perms: Tracked<PtePerms>
}
}

================
File: ./source/verismo/src/pgtable_e/tlb.rs
================

use core::arch::asm;

use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::ptr::*;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

verus! {

pub open spec fn ensure_invlpg(prev_mem_perm: SnpPointsToRaw, mem_perm: SnpPointsToRaw) -> bool {
    let snp = mem_perm@.snp();
    let prev_snp = prev_mem_perm@.snp();
    &&& snp === prev_snp.spec_set_pte(snp.pte)
    &&& snp.pte =~~= seq![prev_snp.pte.last()]
    &&& snp.pte() === prev_snp.pte.last()
    &&& mem_perm@.bytes() === prev_mem_perm@.bytes()
    &&& mem_perm@.range() === prev_mem_perm@.range()
}

/// Invalidate TLB for a given address using the `invlpg` instruction.
/// The PTE will be updated to apply the recent pte change.
#[inline]
#[verifier(external_body)]
pub fn invlpg(vaddr: u64, Tracked(mem_perm): Tracked<&mut SnpPointsToRaw>)
    requires
        (vaddr as int).spec_valid_addr_with(PAGE_SIZE as nat),
        (vaddr as int) % (PAGE_SIZE as int) == 0,
    ensures
        ensure_invlpg(*old(mem_perm), *mem_perm),
{
    unsafe {
        asm!("invlpg [{}]", in(reg) vaddr, options(nostack, preserves_flags));
    }
}

} // verus!

================
File: ./source/verismo/src/pgtable_e/mod.rs
================

mod def;
mod pte;
mod tlb;

pub use def::*;
pub use pte::*;
pub use tlb::*;

use crate::arch::pgtable::*;
use crate::arch::reg::RegName;
use crate::ptr::*;
use crate::registers::CR3;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::DEFINE_BIT_FIELD_GET;

================
File: ./source/verismo/src/pgtable_e/pte.rs
================

use super::*;
use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::debug::VPrintAtLevel;
use crate::global::*;
use crate::registers::{AnyRegTrait, RegisterPerm, RegisterPermValue};
use crate::snp::ghcb::*;
use crate::snp::SnpCoreSharedMem;
use crate::*;

crate::macro_const_int! {
    #[macro_export]
    pub const L4_MAX_ADDR: usize = 0x1000_0000_0000_0000usize;
}

verus! {

pub fn vn_to_pn(page: u64, Tracked(page_perm): Tracked<&SnpPointsToRaw>) -> (ret: u64)
    requires
        page_perm@.range().0 <= (page as int).to_addr(),
        page_perm@.range().end() >= (page as int).to_addr(),
        page_perm@.snp().guestmap[page as int] == spec_vn_to_pn(page as int),
    ensures
        ret == spec_vn_to_pn(page as int),
{
    page
}

pub fn va_to_pa(vaddr: u64, Tracked(page_perm): Tracked<&SnpPointsToRaw>) -> (ret: u64)
    requires
        page_perm@.range().0 <= vaddr,
        page_perm@.range().end() >= vaddr,
        page_perm@.snp().guestmap[(vaddr as int).to_page()] == spec_va_to_pa(
            vaddr as int,
        ).to_page(),
    ensures
        ret == spec_va_to_pa(vaddr as int),
{
    vaddr
}

pub fn pa_to_va(paddr: u64, Tracked(page_perm): Tracked<&SnpPointsToRaw>) -> (ret: u64)
    requires
        page_perm@.range().0 <= spec_pa_to_va(paddr as int),
        page_perm@.range().end() >= spec_pa_to_va(paddr as int),
        page_perm@.snp().guestmap[spec_pa_to_va(paddr as int).to_page()] == (
        paddr as int).to_page(),
    ensures
        ret == spec_pa_to_va(paddr as int),
{
    paddr
}

impl PTE {
    pub open spec fn has_next(&self) -> bool {
        &&& self.spec_psize() == 0
        &&& self.spec_present() == 1
    }

    pub open spec fn to_attr(&self) -> PTAttr {
        PTAttr {
            encrypted: self.spec_encrypted() == 1,
            x: self.spec_nx() == 0,
            w: self.spec_write() == 1,
        }
    }
}

} // verus!
verus! {

#[verifier(inline)]
pub open spec fn lvl_to_size(lvl: nat) -> nat {
    if lvl == 0 {
        0x1000
    } else if lvl == 1 {
        0x20_0000
    } else if lvl == 2 {
        0x10_0000_0000
    } else if lvl == 3 {
        0x1_0000_0000_0000
    } else if lvl == 4 {
        0x1000_0000_0000_0000
    } else {
        0
    }
}

impl PtePerm {
    pub closed spec fn perm(&self) -> Option<SnpPointsTo<PageTable>> {
        self.perm
    }

    pub closed spec fn next_tb(&self) -> PageTable {
        self.perm().get_Some_0()@.get_value()
    }

    pub closed spec fn next_tbptr(&self) -> int {
        let ppage: int = self.val.spec_page() as int;
        spec_page_table_pa_to_va(ppage.to_addr())
    }

    pub closed spec fn val(&self) -> PTE {
        self.val
    }

    spec fn wf_current_psize(&self) -> bool {
        if self.lvl != 1 && self.lvl != 2 {
            &&& self.val.spec_psize() == 0
        } else {
            true
        }
    }

    spec fn wf_range(&self) -> bool {
        lvl_to_size(self.lvl) == self.range.1
    }

    spec fn wf_current_perm(&self) -> bool {
        // The last level does not carry any mem perm
        // since those are used as non-pt memory.
        if self.has_next() {
            self.wf_current_perm_with_mem()
        } else {
            self.perm.is_None()
        }
    }

    // When write pte val, we provide both pt and non-pt memory perm.
    spec fn wf_current_perm_with_mem(&self) -> bool {
        let perm = self.perm.get_Some_0();
        &&& self.perm.is_Some()
        &&& perm@.value().is_Some()
        &&& perm@.is_wf_pte(self.next_tbptr())
    }

    pub closed spec fn wf(&self, lvl_idx: (nat, int)) -> bool {
        &&& self.wf_current_psize()
        &&& self.wf_current_perm()
        &&& self.wf_range()
        &&& self.lvl == lvl_idx.0
        &&& self.range.0 == lvl_idx.1
    }

    pub closed spec fn has_next(&self) -> bool {
        if self.lvl != PAGE_TABLE_LEVELS {
            &&& self.val.has_next()
            &&& self.lvl != 0
        } else {
            true
        }
    }
}

} // verus!
verus! {

impl TrackedPTEPerms {
    pub open spec fn invfn() -> spec_fn(TrackedPTEPerms) -> bool {
        |v: TrackedPTEPerms| wf_ptes(v@)
    }

    pub open spec fn view(&self) -> PtePerms {
        self.perms@
    }
}

} // verus!
verus! {

impl PtePermsTrait for PtePerms {
    open spec fn pte(&self, vaddr: int) -> Option<PtePerm> {
        self.entry(vaddr, 0)
    }

    open spec fn pde(&self, vaddr: int) -> Option<PtePerm> {
        self.entry(vaddr, 1)
    }

    open spec fn pdpe(&self, vaddr: int) -> Option<PtePerm> {
        self.entry(vaddr, 2)
    }

    open spec fn pml4e(&self, vaddr: int) -> Option<PtePerm> {
        self.entry(vaddr, 3)
    }

    closed spec fn entry(&self, vaddr: int, lvl: nat) -> Option<PtePerm> {
        let lvl_idx = (lvl, vaddr / PAGE_SIZE!());
        if self.contains_key(lvl_idx) {
            Some(self[lvl_idx])
        } else {
            None
        }
    }
}

pub open spec fn lvl_index(lvl: nat, addr: int) -> (nat, int) {
    (lvl, spec_align_down(addr, lvl_to_size(lvl) as int))
}

pub proof fn lemma_max_lvl_index(lvl: nat, addr: int) -> (ret: (nat, int))
    requires
        addr.spec_valid_addr_with(0x1000),
        lvl == PAGE_TABLE_LEVELS,
    ensures
        ret.1 == 0,
        ret === lvl_index(lvl, addr),
{
    let ret = lvl_index(lvl, addr);
    assert(spec_align_down(addr, lvl_to_size(lvl) as int) == 0) by {
        assert(lvl_to_size(lvl) == L4_MAX_ADDR!());
        assert(0 <= addr < L4_MAX_ADDR!());
        assert(addr / L4_MAX_ADDR!() * L4_MAX_ADDR!() == 0);
    }
    ret
}

pub closed spec fn pte_perms_wf_prev(perms: PtePerms, lvl: nat, addr: int) -> bool {
    let prev_idx = lvl_index(lvl + 1, addr);
    let idx = lvl_index(lvl, addr);
    &&& (perms.contains_key(prev_idx) && perms[prev_idx].has_next()) == perms.contains_key(idx)
    &&& perms[idx].val().value === perms[prev_idx].next_tb()[spec_table_index(
        addr as u64,
        lvl,
    )].vspec_cast_to()
}

pub open spec fn wf_ptes(m: Map<(nat, int), PtePerm>) -> bool {
    &&& m[(PAGE_TABLE_LEVELS as nat, 0)].val().value == static_cr3_value()
    &&& m.contains_key(top_lvl_idx())
    // All existed entries are valid

    &&& forall|i|
        m.contains_key(i) ==> #[trigger] m[i].wf(
            i,
        )
    // When an entry exists

    &&& forall|lvl: nat, addr: int| 0 <= lvl < 4 ==> pte_perms_wf_prev(m, lvl, addr)
}

spec fn pte_perms_contains(perms: PtePerms, lvl: nat, addr: int) -> bool {
    {
        &&& forall|i| perms.contains_key(i) ==> #[trigger] perms[i].wf(i)
        &&& perms.contains_key(top_lvl_idx())
        &&& forall|l: nat|
            lvl <= l < 4 ==> #[trigger] perms.contains_key(lvl_index(l, addr)) == (
            perms.contains_key(lvl_index(l + 1, addr)) && perms[lvl_index(l + 1, addr)].has_next())
    }
}

} // verus!
#[vbit_struct(VAddrIndex, u64)]
#[derive(IsConstant)]
pub struct SpecVAddrIndex {
    #[vbits(12, 20)]
    pub index0: u64,
    #[vbits(21, 29)]
    pub index1: u64,
    #[vbits(30, 38)]
    pub index2: u64,
    #[vbits(39, 47)]
    pub index3: u64,
}

verus! {

spec fn spec_page_table_pa_to_va(pa: int) -> int {
    pa
}

pub closed spec fn spec_table_index(vaddr: u64, lvl: nat) -> int {
    let ret = if lvl == 0 {
        VAddrIndex::spec_new(vaddr).spec_index0()
    } else if lvl == 1 {
        VAddrIndex::spec_new(vaddr).spec_index1()
    } else if lvl == 2 {
        VAddrIndex::spec_new(vaddr).spec_index2()
    } else if lvl == 3 {
        VAddrIndex::spec_new(vaddr).spec_index3()
    } else {
        0
    };
    ret as int
}

pub proof fn proof_table_index(vaddr: u64, lvl: nat)
    ensures
        0 <= spec_table_index(vaddr, lvl) < PT_ENTRY_NUM!(),
        PT_ENTRY_NUM == 512,
        PT_ENTRY_NUM!() == 512,
{
    assert(PT_ENTRY_NUM!() == 512) by {
        assert(PT_ENTRY_NUM!() == 512) by (bit_vector);
    }
    let addr_index = VAddrIndex { value: vaddr };
    addr_index.lemma_bound_index0();
    assert(0 <= addr_index.spec_index0() < PT_ENTRY_NUM!());
    addr_index.lemma_bound_index1();
    assert(0 <= addr_index.spec_index1() < PT_ENTRY_NUM!());
    addr_index.lemma_bound_index2();
    assert(0 <= addr_index.spec_index2() < PT_ENTRY_NUM!());
    addr_index.lemma_bound_index3();
    assert(0 <= addr_index.spec_index3() < PT_ENTRY_NUM!());
}

} // verus!
verus! {

fn next_addr(vaddr: u64) -> (ret: u64)
    ensures
        ret == vaddr / 0x200 * 0x200,
{
    vaddr / 0x200 * 0x200
}

fn table_index(vaddr: u64, lvl: u8) -> (ret: usize)
    ensures
        0 <= ret < PT_ENTRY_NUM!(),
        PT_ENTRY_NUM!() == PT_ENTRY_NUM,
        PT_ENTRY_NUM == 512,
        (ret as int) == spec_table_index(vaddr, lvl as nat),
{
    proof {
        proof_table_index(vaddr as u64, lvl as nat);
    }
    let ret = if lvl == 0 {
        VAddrIndex::new(vaddr).get_index0()
    } else if lvl == 1 {
        VAddrIndex::new(vaddr).get_index1()
    } else if lvl == 2 {
        VAddrIndex::new(vaddr).get_index2()
    } else if lvl == 3 {
        VAddrIndex::new(vaddr).get_index3()
    } else {
        0
    };
    ret as usize
}

} // verus!
verus! {

#[derive(IsConstant)]
pub struct PTEWithPtr {
    pte: PTE,
    ptr: Option<SnpPPtr<PageTable>>,
    index: usize_t,
}

pub fn check_is_encrypted(
    vaddr: usize,
    Tracked(mem_perm): Tracked<&SnpPointsToRaw>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: Option<bool>)
    requires
        (*old(cs)).inv(),
        (vaddr as int).spec_valid_addr_with(0x1000),
        mem_perm@.wf_range((vaddr as int, PAGE_SIZE as nat)),
    ensures
        (*cs).inv(),
        (*cs).only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
        ret.is_Some() ==> ret.get_Some_0() == mem_perm@.snp().encrypted(),
{
    match borrow_entry(vaddr as u64, 0, Tracked(mem_perm), Tracked(cs)) {
        Some(pte_with_ptr) => {
            let PTEWithPtr { pte, ptr, index } = pte_with_ptr;
            Some(pte.get_encrypted() == 1)
        },
        _ => { None },
    }
}

fn borrow_entry(
    vaddr: u64,
    lvl: u8,
    Tracked(mem_perm): Tracked<&SnpPointsToRaw>,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
) -> (ret: Option<PTEWithPtr>)
    requires
        (*old(cs)).inv(),
        (vaddr as int).spec_valid_addr_with(0x1000),
        lvl <= PAGE_TABLE_LEVELS,
        mem_perm@.wf_range((vaddr as int, PAGE_SIZE as nat)),
    ensures
        cs.inv(),
        ret.is_Some() ==> ret.is_constant(),
        cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
        (ret.is_Some() && (lvl == 0)) ==> wf_pte_mem_perm(ret.get_Some_0().pte, mem_perm),
{
    assert(cs.wf_top_pt());
    let ghost cr3_u64: u64 = cs.snpcore.regs[RegName::Cr3].val::<u64_s>().vspec_cast_to();
    assert(cr3_u64 == static_cr3_value());
    let tracked cr3perm = cs.snpcore.regs.tracked_borrow(RegName::Cr3);
    assert(contains_PT(cs.lockperms));
    let tracked mut pt_lock = cs.lockperms.tracked_remove(spec_PT_lockid());
    let (tracked_ptr, Tracked(mut ptperm_perm), Tracked(pt_lock)) = PT().acquire(
        Tracked(pt_lock),
        Tracked(&cs.snpcore.coreid),
    );
    let TrackedPTEPerms { perms } = tracked_ptr.take(Tracked(&mut ptperm_perm));
    let Tracked(mut pt_perms) = perms;
    assert(wf_ptes(pt_perms));
    let ghost cr3_u64: u64 = cr3perm.val::<u64_s>().vspec_cast_to();
    assert(cr3_u64 == static_cr3_value());
    assert(pt_perms[top_lvl_idx()].val().value == static_cr3_value());
    let ret = _borrow_entry(
        vaddr,
        lvl,
        Tracked(mem_perm),
        Tracked(cr3perm),
        Tracked(&mut pt_perms),
    );
    tracked_ptr.put(Tracked(&mut ptperm_perm), TrackedPTEPerms { perms: Tracked(pt_perms) });
    PT().release(Tracked(&mut pt_lock), Tracked(ptperm_perm), Tracked(&cs.snpcore.coreid));
    proof {
        cs.lockperms.tracked_insert(spec_PT_lockid(), pt_lock);
    }
    ret
}

pub closed spec fn cr3_to_pte_ptr(cr3perm: RegisterPermValue<u64_s>) -> PTEWithPtr {
    PTEWithPtr { pte: PTE::spec_new(cr3perm.value.vspec_cast_to()), ptr: None, index: 0 }
}

fn _borrow_entry(
    vaddr: u64,
    lvl: u8,
    Tracked(mem_perm): Tracked<&SnpPointsToRaw>,
    Tracked(cr3perm): Tracked<&RegisterPerm>,
    Tracked(pt_perms): Tracked<&mut PtePerms>,
) -> (ret: Option<PTEWithPtr>)
    requires
        (vaddr as int).spec_valid_addr_with(0x1000),
        lvl <= PAGE_TABLE_LEVELS,
        cr3perm.id() == CR3.reg_id(),
        cr3perm.wf_notshared(),
        cr3perm.val::<u64_s>().is_constant(),
        cr3perm.val::<u64_s>().vspec_cast_to() === old(pt_perms)[top_lvl_idx()].val().value,
        wf_ptes(*old(pt_perms)),
        mem_perm@.wf_range((vaddr as int, PAGE_SIZE as nat)),
    ensures
        *pt_perms =~~= *old(pt_perms),
        ret.is_Some() ==> ret.is_constant(),
        ret.is_Some() ==> pt_perms[lvl_index(lvl as nat, vaddr as int)].val()
            === ret.get_Some_0().pte,
        ret.is_Some() ==> ret.get_Some_0().index == spec_table_index(vaddr, lvl as nat),
        ret.is_Some() ==> ret.get_Some_0().ptr.is_Some() == (lvl != PAGE_TABLE_LEVELS),
        ret.is_Some() && ret.get_Some_0().ptr.is_Some() ==> ret.get_Some_0().ptr.get_Some_0().id()
            == pt_perms[lvl_index(lvl as nat + 1, vaddr as int)].next_tbptr(),
        ret.is_Some() ==> pt_perms.contains_key(lvl_index(lvl as nat, vaddr as int)),
        (ret.is_Some() && (lvl == 0)) ==> wf_pte_mem_perm(ret.get_Some_0().pte, mem_perm),
        lvl == PAGE_TABLE_LEVELS ==> ret === Some(cr3_to_pte_ptr(cr3perm@)),
{
    let ghost old_pt_perms = *pt_perms;
    proof {
        axiom_rel_pt_perm_mem_perm(pt_perms, mem_perm);
    }
    let ghost mut lvl_idx;
    let ghost glvl = lvl as nat;
    let ghost gaddr = vaddr as int;
    proof {
        lvl_idx = lvl_index(glvl, gaddr);
    }
    if lvl == PAGE_TABLE_LEVELS {
        let cr3 = CR3.read(Tracked(cr3perm));
        proof {
            assert(lvl_idx.1 == 0) by {
                lemma_max_lvl_index(glvl, gaddr);
            }
            assert(lvl_idx === top_lvl_idx());
            assert(pt_perms[lvl_idx].val().value === cr3perm.val::<u64_s>().vspec_cast_to());
            assert(cr3 === cr3perm@.value());
        }
        //(new_strlit("cr3"), cr3).leak_debug();
        return Some(PTEWithPtr { pte: PTE::new(cr3.into()), ptr: None, index: 0 });
    }
    let tracked mut prev_pt_perms = pt_perms.tracked_remove_keys(pt_perms.dom());
    proof {
        assert(prev_pt_perms =~~= old_pt_perms);
    }
    let prev = _borrow_entry(
        vaddr,
        lvl + 1,
        Tracked(mem_perm),
        Tracked(cr3perm),
        Tracked(&mut prev_pt_perms),
    );
    let ghost prev_lvl_idx = lvl_index(glvl + 1, gaddr);
    proof {
        pt_perms.tracked_union_prefer_right(prev_pt_perms);
        assert(*pt_perms =~~= old_pt_perms);
    }
    let ret = if let Some(tmp) = prev {
        let prev_entry = tmp.pte;
        if (lvl + 1) == PAGE_TABLE_LEVELS || (prev_entry.get_psize() == 0
            && prev_entry.get_present() == 1) {
            proof {
                assert(pt_perms.contains_key(prev_lvl_idx));
                assert(prev_pt_perms[prev_lvl_idx] === pt_perms[prev_lvl_idx]);
                assert(pt_perms[prev_lvl_idx].wf(prev_lvl_idx));
                assert(PTE::spec_new(prev.get_Some_0().pte.value.vspec_cast_to())
                    === pt_perms[prev_lvl_idx].val());
                //assert(prev.get_Some_0().pte === pt_perms[prev_lvl_idx].val());
                prev.get_Some_0().pte.lemma_new_eq();
                assert(pt_perms[prev_lvl_idx].has_next());
                assert(pte_perms_wf_prev(*pt_perms, glvl, gaddr));
                assert(pt_perms.contains_key(lvl_idx));
            }
            let tracked pte_perm = pt_perms.tracked_remove(prev_lvl_idx);
            let tracked PtePerm { lvl: lvl_nat, val, range, perm } = pte_perm;
            let ppage: usize = prev_entry.get_page() as usize;  // ppage == start_page.
            let page_table_ptr = SnpPPtr::<PageTable>::from_usize(ppage.to_addr());
            assert(perm.is_Some());
            let tracked perm = perm.tracked_unwrap();
            let page_table = page_table_ptr.borrow(Tracked(&perm));
            let idx = table_index(vaddr, lvl);
            assert(page_table@.len() == PT_ENTRY_NUM);
            assert(idx < PT_ENTRY_NUM);
            let pte_val = *page_table.index(idx);
            let ret = PTEWithPtr {
                pte: PTE::new(pte_val.into()),
                ptr: Some(page_table_ptr),
                index: idx,
            };
            proof {
                ret.pte.lemma_new_eq();
                pt_perms[lvl_index(lvl as nat, vaddr as int)].val().lemma_new_eq();
                assert(ret.index == spec_table_index(vaddr, lvl as nat));
                let tracked pte_perm = PtePerm { lvl: lvl_nat, val, range, perm: Some(perm) };
                pt_perms.tracked_insert(prev_lvl_idx, pte_perm);
            }
            //(new_strlit("pte_val"), pte_val).leak_debug();
            Some(ret)
        } else {
            None
        }
    } else {
        None
    };
    return ret;
}

} // verus!
verus! {

// Push a new pte into the mem perm.
// When there are more than one pte in the queue,
// we do not guarantee the actual pte used by the perm token.
// To enforce the use of the recent pte,
// tlb flush is required to remove any other potentials.
pub open spec fn write_pte_ensures_memperm(
    prev_pte: PTE,
    pte: PTE,
    prev_mem_perm: SnpPointsToBytes,
    mem_perm: SnpPointsToBytes,
) -> bool {
    let same_map = prev_pte.spec_page() == pte.spec_page();
    let same_enc = prev_pte.spec_encrypted() == pte.spec_encrypted();
    &&& mem_perm.wf()
    &&& mem_perm.snp().pte == prev_mem_perm.snp().pte.push(pte.to_attr())
    &&& mem_perm.snp() === prev_mem_perm.snp().spec_set_pte(mem_perm.snp().pte)
    &&& mem_perm.snp.hw_rmp_wf()
    &&& mem_perm.range() === prev_mem_perm.range()
    &&& same_map && same_enc ==> mem_perm.bytes() === prev_mem_perm.bytes()
}

pub open spec fn write_pte_requires_memperm(
    val: PTE,
    addr: int,
    lvl: nat,
    mem_perms: Map<int, SnpPointsToRaw>,
) -> bool {
    let start = addr;
    let end = addr + lvl_to_size(lvl);
    forall|i: int|
        start <= i < end && i % PAGE_SIZE!() == 0 ==> #[trigger] mem_perms.contains_key(i)
            && mem_perms[i]@.wf_range((i, PAGE_SIZE as nat))
}

pub open spec fn write_pte_ensures(
    addr: int,
    lvl: nat,
    prev_pte: PTE,
    pte: PTE,
    prev_mem_perms: Map<int, SnpPointsToRaw>,
    mem_perms: Map<int, SnpPointsToRaw>,
) -> bool {
    let start = addr;
    let end = addr + lvl_to_size(lvl);
    forall|i: int|
        start <= i < end && i % PAGE_SIZE!() == 0 ==> #[trigger] mem_perms.contains_key(i)
            && write_pte_ensures_memperm(prev_pte, pte, prev_mem_perms[i]@, mem_perms[i]@)
}

impl SnpPPtr<u64_s> {
    // Requires to use permissions of all virtual memory related to the pte
    // mem_perms: page idx => memory perm.
    // Returns new memory permission tokens reflecting the updated pte.
    #[verifier(external_body)]
    fn write_pte(
        &self,
        val: PTE,
        Ghost(lvl): Ghost<(nat)>,
        Ghost(addr): Ghost<(int)>,
        Tracked(pte_perms): Tracked<&mut PtePerms>,
        Tracked(mem_perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
    )
        requires
            wf_ptes(*old(pte_perms)),
            old(pte_perms).contains_key(lvl_index(lvl, addr)),
            old(pte_perms)[lvl_index(lvl + 1, addr)].next_tbptr() + spec_table_index(
                addr as u64,
                lvl,
            ) * 8 == self.id(),
            addr % (lvl_to_size(lvl) as int) == 0,
            write_pte_requires_memperm(val, addr, lvl, *old(mem_perms)),
        ensures
            pte_perms[lvl_index(lvl, addr)].val === val,
            write_pte_ensures(
                addr,
                lvl,
                old(pte_perms)[lvl_index(lvl, addr)].val,
                val,
                *old(mem_perms),
                *mem_perms,
            ),
    {
        self.replace(Tracked::assume_new(), val.value.into());
    }
}

} // verus!
verus! {

// Hold mem perm to ensure that no concurrent use of the memory.
// The mem perm will be updated to reflect the new PTE value.
// Return true if succeeds.
pub open spec fn ensures_mem_enc_dec_memperm(
    enc: bool,
    prev_mem_perm: SnpPointsToRaw,
    mem_perm: SnpPointsToRaw,
) -> bool {
    &&& prev_mem_perm@.snp().spec_set_pte(seq![mem_perm@.snp().pte()]) === mem_perm@.snp()
    &&& mem_perm@.snp().pte() === prev_mem_perm@.snp().pte().spec_set_encrypted(enc)
    &&& prev_mem_perm@.snp().pte().spec_encrypted() != enc
    &&& prev_mem_perm@.bytes() === mem_perm@.bytes()
    &&& prev_mem_perm@.range() === mem_perm@.range()
}

pub fn set_page_enc_dec(
    vaddr: u64,
    enc: bool,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    Tracked(mem_perm0): Tracked<&mut Map<int, SnpPointsToRaw>>,
) -> (ret: bool)
    requires
        (*old(cs)).inv(),
        (vaddr as int).spec_valid_addr_with(PAGE_SIZE as nat),
        (vaddr as int) % PAGE_SIZE!() == 0,
        old(mem_perm0).contains_key(0),
        old(mem_perm0)[0]@.wf_range((vaddr as int, PAGE_SIZE as nat)),
    ensures
        cs.inv(),
        cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
        ret ==> ensures_mem_enc_dec_memperm(enc, old(mem_perm0)[0], mem_perm0[0]),
        !ret ==> mem_perm0[0] === old(mem_perm0)[0],
        mem_perm0.contains_key(0),
        mem_perm0[0]@.wf_range((vaddr as int, PAGE_SIZE as nat)),
{
    let ghost addr_id: int = vaddr.vspec_cast_to();
    let ghost old_mem_perm0 = *mem_perm0;
    let tracked cr3perm = cs.snpcore.regs.tracked_borrow(RegName::Cr3);
    assert(contains_PT(cs.lockperms));
    let tracked mut pt_lock = cs.lockperms.tracked_remove(spec_PT_lockid());
    let (tracked_ptr, Tracked(mut ptperm_perm), Tracked(pt_lock)) = PT().acquire(
        Tracked(pt_lock),
        Tracked(&cs.snpcore.coreid),
    );
    // Dummy take to get PTE permission.
    let TrackedPTEPerms { perms } = tracked_ptr.take(Tracked(&mut ptperm_perm));
    let Tracked(mut pt_perms) = perms;
    let lvl = 0;
    assert(wf_ptes(pt_perms));
    let pte_val_opt = _borrow_entry(
        vaddr,
        lvl,
        Tracked(mem_perm0.tracked_borrow(0)),
        Tracked(cr3perm),
        Tracked(&mut pt_perms),
    );
    let ret = if let Option::Some(pte_with_ptr) = pte_val_opt {
        let PTEWithPtr { pte, ptr, index } = pte_with_ptr;
        assert(ptr.is_Some());
        let mut pte_addr: usize = ptr.unwrap().to_usize();
        let encryption = if enc {
            1
        } else {
            0
        };
        if pte.get_encrypted() != encryption {
            let new_pte = pte.set_encrypted(encryption);
            let tracked mut memmap_perm = Map::<int, SnpPointsToRaw>::tracked_empty();
            proof {
                let tracked mem_perm = mem_perm0.tracked_remove(0);
                memmap_perm.tracked_insert(addr_id, mem_perm);
                proof_table_index(vaddr, lvl as nat);
            }
            pte_addr = pte_addr + index * 8;
            let pte_ptr = SnpPPtr::<u64_s>::from_usize(pte_addr);
            proof {
                assert(memmap_perm.contains_key(addr_id));
            }
            pte_ptr.write_pte(
                new_pte,
                Ghost(lvl as nat),
                Ghost(addr_id),
                Tracked(&mut pt_perms),
                Tracked(&mut memmap_perm),
            );
            proof {
                assert(memmap_perm.contains_key(addr_id));
            }
            let tracked mut mem_perm = memmap_perm.tracked_remove(addr_id);
            invlpg(vaddr, Tracked(&mut mem_perm));
            proof {
                mem_perm0.tracked_insert(0, mem_perm);
            }
            true
        } else {
            (new_strlit("double private/share from richos\n"), pte.get_encrypted()).leak_debug();
            false
        }
    } else {
        false
    };
    tracked_ptr.put(Tracked(&mut ptperm_perm), TrackedPTEPerms { perms: Tracked(pt_perms) });
    PT().release(Tracked(&mut pt_lock), Tracked(ptperm_perm), Tracked(&cs.snpcore.coreid));
    proof {
        cs.lockperms.tracked_insert(spec_PT_lockid(), pt_lock);
    }
    return ret;
}

pub fn set_pages_enc_dec(
    start_page: u64,
    size: u64,
    enc: bool,
    Tracked(cs): Tracked<&mut SnpCoreSharedMem>,
    Tracked(mem_perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
)
    requires
        (*old(cs)).inv(),
        start_page.spec_valid_pn_with(size as nat),
        forall|page: int|
            start_page <= page < start_page + size ==> old(mem_perms).contains_key(page) && old(
                mem_perms,
            )[page]@.wf_range((page.to_addr(), PAGE_SIZE as nat)),
    ensures
        cs.inv(),
        cs.only_lock_reg_updated((*old(cs)), set![], set![spec_PT().lockid()]),
        mem_perms.dom() =~~= old(mem_perms).dom(),
        forall|page: int|
            start_page <= page < start_page + size ==> #[trigger] mem_perms.contains_key(page)
                && mem_perms[page]@.wf_range((page.to_addr(), PAGE_SIZE as nat))
                && ensures_mem_enc_dec_memperm(enc, old(mem_perms)[page], mem_perms[page]),
{
    let mut i = 0;
    let ghost old_mem_perms = *mem_perms;
    let ghost old_cs = (*cs);
    while i < size
        invariant
            i <= size,
            (*cs).inv(),
            (*cs).only_lock_reg_updated(old_cs, set![], set![spec_PT().lockid()]),
            start_page.spec_valid_pn_with(size as nat),
            mem_perms.dom() =~~= old_mem_perms.dom(),
            forall|page: int|
                start_page <= page < start_page + size ==> #[trigger] mem_perms.contains_key(page)
                    && mem_perms[page]@.wf_range((page.to_addr(), PAGE_SIZE as nat)),
            forall|page: int|
                start_page + i <= page < start_page + size ==> old_mem_perms[page]
                    === #[trigger] mem_perms[page],
            forall|page: int|
                start_page <= page < start_page + i ==> #[trigger] mem_perms.contains_key(page)
                    && ensures_mem_enc_dec_memperm(enc, old_mem_perms[page], mem_perms[page]),
    {
        let tracked mut mem_perm0 = Map::tracked_empty();
        let page = start_page + i;
        let ghost oldperm = mem_perms[page as int];
        let ghost prev_cs = (*cs);
        proof {
            mem_perm0.tracked_insert(0, mem_perms.tracked_remove(page as int));
        }
        let ok = set_page_enc_dec(page.to_addr(), enc, Tracked(cs), Tracked(&mut mem_perm0));
        if !ok {
            new_strlit("\nfailed set_pages_enc_dec\n").leak_debug();
            vc_terminate(SM_TERM_MEM, Tracked(&mut cs.snpcore));
        }
        i = i + 1;
        proof {
            let ghost newperm = mem_perm0[0];
            old_cs.lemma_update_prop(
                prev_cs,
                (*cs),
                set![],
                set![spec_PT().lockid()],
                set![],
                set![spec_PT().lockid()],
            );
            mem_perms.tracked_insert(page as int, mem_perm0.tracked_remove(0));
            assert(old_mem_perms[page as int] === oldperm);
            assert(mem_perms[page as int] === newperm);
            assert(ensures_mem_enc_dec_memperm(enc, oldperm, newperm));
        }
    }
}

pub open spec fn wf_pte_mem_perm(pte: PTE, perm: &SnpPointsToRaw) -> bool {
    &&& (pte@.spec_encrypted() == 1)
        == perm@.snp().pte().spec_encrypted()/*&&& (pte@.spec_nx() == 0)== perm@.snp().pte().spec_x()
    &&& (pte@.spec_write() == 1) == perm@.snp().pte().spec_w()*/

}

// trusted hardware spec indicating the page table defines the access permission.
#[verifier(external_body)]
pub proof fn axiom_rel_pt_perm_mem_perm(tracked pte_perms: &PtePerms, tracked perm: &SnpPointsToRaw)
    ensures
        wf_pte_mem_perm(pte_perms[lvl_index(0, perm@.range().0)].val, perm),
{
}

} // verus!

================
File: ./source/verismo/src/ptr/ptr_t.rs
================

use super::*;

verismo! {
    /// SNP safe pointer usage
    impl<V: IsConstant + WellFormed + SpecSize> SnpPPtr<V> {
        /// Moves `v` out of the location pointed to by the pointer `self`
        /// and returns it.
        /// Requires the memory to be initialized, and leaves it uninitialized.
        ///
        /// In the ghost perspective, this updates `perm.value`
        /// from `Some(v)` to `None`,
        /// while returning the `v` as an `exec` value.
        #[inline(always)]
        #[verifier(external_body)]
        pub fn _take(&self, Tracked(perm): Tracked<&mut SnpPointsTo<V>>) -> (v: V)
            requires
                old(perm)@.ptr_not_null_wf(*self),
                old(perm)@.value.is_Some(),
            ensures
                perm@.spec_write_rel(old(perm)@, None),
                old(perm)@.spec_read_rel(v),
        {
            unsafe {
                let mut m = MaybeUninit::uninit();
                mem::swap(&mut m, &mut *(self.uptr as *mut MaybeUninit<V>));
                m.assume_init()
            }
        }

        #[inline(always)]
        #[verifier(external_body)]
        pub fn _put(&self, Tracked(perm): Tracked<&mut SnpPointsTo<V>>, in_v: V)
            requires
                old(perm)@.ptr_not_null_wf(*self),
                old(perm)@.value().is_None(),
            ensures
                perm@.spec_write_rel(old(perm)@, Some(in_v)),
        {
            unsafe {
                *(self.uptr as *mut MaybeUninit<V>) = MaybeUninit::new(in_v);
            }
        }

        /// Swaps the `in_v: V` passed in as an argument with the value in memory.
        #[inline(always)]
        #[verifier(external_body)]
        pub fn _replace(&self, Tracked(perm): Tracked<&mut SnpPointsTo<V>>, in_v: V) -> (out_v: V)
            requires
                old(perm)@.ptr_not_null_wf(*self),
                old(perm)@.is_assigned(),
            ensures
                perm@.spec_write_rel(old(perm)@, Some(in_v)),
                old(perm)@.spec_read_rel(out_v),
            opens_invariants none
        {
            unsafe {
                let mut m = MaybeUninit::new(in_v);
                mem::swap(&mut m, &mut *(self.uptr as *mut MaybeUninit<V>));
                m.assume_init()
            }
        }

        /// Given a shared borrow of the `PointsTo<V>`, obtain a shared borrow of `V`.
        // Note that `self` is just a pointer, so it doesn't need to outlive
        // the returned borrow.
        // A non-private mem should not be borrowed since its value can change anytime
        #[inline(always)]
        #[verifier(external_body)]
        pub fn _borrow<'a>(&self, Tracked(perm): Tracked<&'a SnpPointsTo<V>>) -> (v: &'a V)
            requires
                perm@.wf_not_null_at(self.id()) || perm@.is_wf_pte(self.id()),
                perm@.value.is_Some(),
                perm@.snp().is_vmpl0_private()
            ensures
                perm@.spec_read_rel(*v),
        {
            unsafe {
                (*(self.uptr as *mut MaybeUninit<V>)).assume_init_ref()
            }
        }
    }

    impl<V: IsConstant + WellFormed + SpecSize + Clone> SnpPPtr<V> {
        #[inline(always)]
        #[verifier(external_body)]
        pub fn _copy<'a>(&self, Tracked(perm): Tracked<&'a SnpPointsTo<V>>) -> (v: V)
            requires
                perm@.wf_not_null_at(self.id()) || perm@.is_wf_pte(self.id()),
                perm@.value.is_Some(),
            ensures
                perm@.spec_read_rel(v),
        {
            unsafe {
                (*(self.uptr as *mut V)).clone()
            }
        }
    }
}

================
File: ./source/verismo/src/ptr/raw_ptr_s.rs
================

use super::*;
use crate::*;
verus! {

#[derive(SpecSetter, SpecGetter)]
pub ghost struct SnpPointsToBytes {
    pub pptr: int,
    pub snp_bytes: SecSeqByte,
    pub snp: SnpMemAttr,
}

impl IsSnpPPtr for SnpPointsToBytes {

}

impl SnpPointsToRaw {
    pub open spec fn view(&self) -> SnpPointsToBytes;
}

impl SnpPointsToBytes {
    #[verifier(external_body)]
    broadcast proof fn axiom_map_ext_equal(self, other: Self)
        ensures
            #[trigger] (self =~= other) == (self.bytes() =~~= other.bytes() && self.snp()
                === other.snp() && self.range() === other.range()),
    {
    }

    #[verifier(external_body)]
    broadcast proof fn axiom_map_ext_equal_deep(self, other: Self)
        ensures
            #[trigger] (self =~~= other) == (self.bytes() =~~= other.bytes() && self.snp
                === other.snp && self.range() === other.range()),
    {
    }

    #[verifier(inline)]
    pub open spec fn sw_eq(self, rhs: Self) -> bool {
        &&& self.snp() === rhs.snp()
        &&& self.bytes() =~~= rhs.bytes()
        &&& self.range() === rhs.range()
    }

    pub open spec fn only_val_updated(self, rhs: Self) -> bool {
        &&& self.snp() === rhs.snp()
        &&& self.range() === rhs.range()
        &&& rhs.wf() ==> self.wf()
    }

    pub open spec fn size(&self) -> nat {
        self.snp_bytes.len()
    }

    pub open spec fn range(&self) -> (int, nat) {
        (self.pptr, self.size())
    }

    pub open spec fn is_assign_with(&self, val: SecSeqByte) -> bool {
        self.snp().is_vmpl0_private() ==> self.bytes() =~~= val
    }

    pub open spec fn wf_range(&self, range: (int, nat)) -> bool {
        &&& self.range() == range
        &&& self.wf()
    }

    pub open spec fn snp_wf_range(&self, range: (int, nat)) -> bool {
        &&& self.range() == range
        &&& self.snp.wf()
    }

    pub open spec fn wf_const_default(&self, range: (int, nat)) -> bool {
        &&& self.wf_not_null(range)
        &&& self.snp() === SwSnpMemAttr::spec_default()
        &&& self.bytes().is_constant()
    }

    pub open spec fn wf_shared(&self, range: (int, nat)) -> bool {
        &&& self.wf_not_null(range)
        &&& self.snp() === SwSnpMemAttr::shared()
        &&& self.bytes().is_constant()
    }

    pub open spec fn wf_default(&self, range: (int, nat)) -> bool {
        &&& self.wf_not_null(range)
        &&& self.snp() === SwSnpMemAttr::spec_default()
    }

    pub open spec fn wf_secret_default(&self, range: (int, nat)) -> bool {
        &&& self.wf_range(range)
        &&& self.snp() === SwSnpMemAttr::spec_default()
        &&& self.bytes().is_fullsecret()
        &&& range.0.spec_valid_addr_with(range.1)
    }

    pub open spec fn wf_not_null(&self, range: (int, nat)) -> bool {
        &&& range.0.spec_valid_addr_with(range.1)
        &&& self.wf_range(range)
    }

    pub open spec fn wf_freemem(&self, range: (int, nat)) -> bool {
        &&& self.wf_const_default(range)
    }

    pub open spec fn ptr_not_null_wf(&self, addr: int, len: nat) -> bool {
        &&& self.wf_not_null((addr, len))
    }

    pub open spec fn snp_wf_not_null_range(&self, range: (int, nat)) -> bool {
        &&& range.0.spec_valid_addr_with(range.1)
        &&& self.range() == range
        &&& self.snp.wf()
    }

    pub open spec fn ptr_validated_low(&self, addr: int, len: nat) -> bool {
        &&& self.wf_not_null((addr, len))
        &&& self.snp() === SwSnpMemAttr::spec_default()
    }

    pub open spec fn wf(&self) -> bool {
        &&& self.snp.wf()
        &&& inv_snp_value(self.snp(), self.bytes())
    }

    pub open spec fn inv_snp_value(&self) -> bool {
        inv_snp_value(self.snp(), self.bytes())
    }

    pub open spec fn bytes(&self) -> SecSeqByte
        recommends
            self.snp().is_vmpl0_private(),
    {
        self.snp_bytes
    }

    pub open spec fn spec_write_rel(self, prev: Self, bytes: SecSeqByte) -> bool {
        &&& self.snp_bytes === bytes  // equal value without memattr

        &&& self.snp_bytes.is_constant() == prev.snp_bytes.is_constant()
        &&& self.range() === prev.range()
        &&& self.snp == prev.snp
        &&& self.wf()
    }

    pub open spec fn value<T: VTypeCast<SecSeqByte>>(&self) -> T {
        self.snp_bytes.vspec_cast_to()
    }

    pub open spec fn ppage(&self) -> int {
        self.snp().guestmap[self.range().0.to_page()]
    }
}

impl<T: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> VTypeCast<
    SnpPointsToData<T>,
> for SnpPointsToBytes {
    open spec fn vspec_cast_to(self) -> SnpPointsToData<T> {
        SnpPointsToData {
            ptr: self.pptr,
            value: Some(self.snp_bytes.vspec_cast_to()),
            snp: self.snp,
        }
    }
}

impl SnpPointsToRaw {
    #[verifier(external_body)]
    pub proof fn tracked_empty(ptr: int, snp: SwSnpMemAttr) -> (tracked ret: SnpPointsToRaw)
        ensures
            ret@.snp() === snp,
            ret@.range() === (ptr, 0),
            ret@.wf(),
    {
        unimplemented!{}
    }

    // spec_size must be equal to byte.size(), otherwise, some axiom related to vpsec_cast_to may not hold.
    #[verifier(external_body)]
    pub proof fn trusted_into<V: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(
        tracked self,
    ) -> (tracked points_to: SnpPointsTo<V>)
        requires
            self@.wf(),
            spec_size::<V>() == self@.size(),
        ensures
            points_to@ === self@.vspec_cast_to(),
            self@ === points_to@.vspec_cast_to(),
            points_to@.wf(),
    {
        unimplemented!{}
    }

    pub proof fn tracked_into<V: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(
        tracked self,
    ) -> (tracked points_to: SnpPointsTo<V>)
        requires
            self@.wf(),
            spec_size::<V>() == self@.size(),
        ensures
            points_to@ === self@.vspec_cast_to(),
            self@ === points_to@.vspec_cast_to(),
            points_to@.wf(),
            self@.bytes().is_constant() ==> points_to@.get_value().is_constant(),
    {
        proof_into_is_constant::<SecSeqByte, V>(self@.bytes());
        proof_into_is_constant_to::<SecSeqByte, V>(self@.bytes(), 1);
        proof_into_is_constant_to::<SecSeqByte, V>(self@.bytes(), 2);
        proof_into_is_constant_to::<SecSeqByte, V>(self@.bytes(), 3);
        proof_into_is_constant_to::<SecSeqByte, V>(self@.bytes(), 4);
        self.trusted_into()
    }

    #[verifier(external_body)]
    pub proof fn trusted_split(tracked self, len1: nat) -> (tracked res: (Self, Self))
        requires
            self@.snp.wf(),
            len1 <= self@.size(),
            self@.size() > 0,
        ensures
            res.0@.snp_wf_range((self@.pptr, len1)),
            res.1@.snp_wf_range((self@.pptr + len1 as int, (self@.size() - len1) as nat)),
            self@.snp === res.0@.snp,
            self@.snp === res.1@.snp,
            res.0@.bytes() =~~= self@.bytes().take(len1 as int),
            res.1@.bytes() =~~= self@.bytes().skip(len1 as int),
    {
        unimplemented!{}
    }

    pub proof fn tracked_split(tracked self, len1: nat) -> (tracked res: (Self, Self))
        requires
            self@.snp.wf(),
            len1 <= self@.size(),
            self@.size() > 0,
        ensures
            res.0@.snp_wf_range((self@.pptr, len1)),
            res.1@.snp_wf_range((self@.pptr + len1 as int, (self@.size() - len1) as nat)),
            self@.snp === res.0@.snp,
            self@.snp === res.1@.snp,
            res.0@.bytes() =~~= self@.bytes().take(len1 as int),
            res.1@.bytes() =~~= self@.bytes().skip(len1 as int),
            (res.0@.bytes().is_constant_to(1) && res.1@.bytes().is_constant_to(1))
                <==> self@.bytes().is_constant_to(1),
            (res.0@.bytes().is_constant_to(2) && res.1@.bytes().is_constant_to(2))
                <==> self@.bytes().is_constant_to(2),
            (res.0@.bytes().is_constant_to(3) && res.1@.bytes().is_constant_to(3))
                <==> self@.bytes().is_constant_to(3),
            (res.0@.bytes().is_constant_to(4) && res.1@.bytes().is_constant_to(4))
                <==> self@.bytes().is_constant_to(4),
    {
        let tracked ret = self.trusted_split(len1);
        assert(self@.bytes() =~~= ret.0@.bytes() + ret.1@.bytes());
        proof_bytes_add_is_constant_to(ret.0@.bytes(), ret.1@.bytes(), 1);
        proof_bytes_add_is_constant_to(ret.0@.bytes(), ret.1@.bytes(), 2);
        proof_bytes_add_is_constant_to(ret.0@.bytes(), ret.1@.bytes(), 3);
        proof_bytes_add_is_constant_to(ret.0@.bytes(), ret.1@.bytes(), 4);
        ret
    }

    #[verifier(external_body)]
    pub proof fn trusted_join(tracked self, tracked other: Self) -> (tracked res: Self)
        requires
            self@.range().end() == other@.range().0,
            self@.snp() === other@.snp(),
            self@.snp.wf(),
            other@.snp.wf(),
        ensures
            res@.pptr === self@.pptr,
            res@.snp === self@.snp,
            res@.size() == self@.size() + other@.size(),
            res@.snp.wf(),
            res@.bytes() =~~= self@.bytes() + other@.bytes(),
    {
        unimplemented!{}
    }

    pub proof fn tracked_join(tracked self, tracked other: Self) -> (tracked res: Self)
        requires
            self@.range().end() == other@.range().0,
            self@.snp() === other@.snp(),
            self@.snp.wf(),
            other@.snp.wf(),
        ensures
            res@.pptr === self@.pptr,
            res@.snp === self@.snp,
            res@.size() == self@.size() + other@.size(),
            res@.snp.wf(),
            res@.bytes() =~~= self@.bytes() + other@.bytes(),
            (self@.bytes().is_constant_to(1) && other@.bytes().is_constant_to(1))
                == res@.bytes().is_constant_to(1),
            (self@.bytes().is_constant_to(2) && other@.bytes().is_constant_to(2))
                == res@.bytes().is_constant_to(2),
            (self@.bytes().is_constant_to(3) && other@.bytes().is_constant_to(3))
                == res@.bytes().is_constant_to(3),
            (self@.bytes().is_constant_to(4) && other@.bytes().is_constant_to(4))
                == res@.bytes().is_constant_to(4),
    {
        let tracked ret = self.trusted_join(other);
        proof_bytes_add_is_constant_to(self@.bytes(), other@.bytes(), 1);
        proof_bytes_add_is_constant_to(self@.bytes(), other@.bytes(), 2);
        proof_bytes_add_is_constant_to(self@.bytes(), other@.bytes(), 3);
        proof_bytes_add_is_constant_to(self@.bytes(), other@.bytes(), 4);
        ret
    }

    #[verifier(external_body)]
    pub proof fn tracked_to_pages(tracked self) -> (tracked s: Map<int, Self>)
        requires
            self@.range().0 % PAGE_SIZE!() == 0,
            (self@.size() as int) % PAGE_SIZE!() == 0,
            self@.wf(),
        ensures
            forall|i|
                self@.range().0 / PAGE_SIZE!() <= i < self@.range().end() / PAGE_SIZE!()
                    ==> #[trigger] s.contains_key(i) && s[i]@.snp() === self@.snp()
                    && s[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat)) && s[i]@.bytes()
                    =~~= self@.bytes().subrange(
                    i.to_addr() - self@.range().0,
                    i.to_addr() - self@.range().0 + PAGE_SIZE as int,
                ),
    {
        unimplemented!{}
    }

    pub open spec fn wf_seq_perms(s: Map<int, Self>, i: int) -> bool {
        &&& s[i]@.wf()
        &&& s.contains_key(i)
        &&& i > 0 ==> s[i]@.range().0 == s[i - 1]@.range().end()
    }

    pub open spec fn merge_perm_ensures(s: Map<int, Self>, n: nat, res: Self) -> bool {
        &&& res@.wf()
        &&& res@.range().0 == s[0]@.range().0
        &&& res@.range().end() == s[n - 1]@.range().end()
        &&& res@.snp() === s[0]@.snp()
    }

    /*
        pub proof fn tracked_joins(tracked s: Map<int, Self>, n: nat) -> (tracked merged: Self)
        requires
            forall |i: int| 0 <= i < n ==> Self::wf_seq_perms(s, i),
            forall |i: int, j: int| 0 <= i < n && 0 <= j < n ==> s[i]@.snp() === s[j]@.snp(),
            n >= 1,
        ensures
            Self::merge_perm_ensures(s, n, merged)
        decreases
            n
        {
            let ghost olds = s;
            let tracked mut s = s;
            assert(Self::wf_seq_perms(s, n - 1));
            if n > 1 {
                assert(Self::wf_seq_perms(s, n - 2));
                let tracked last = s.tracked_remove(n - 1);
                assert forall |i: int| 0 <= i < (n - 1)
                implies Self::wf_seq_perms(s, i) by {
                    assert(Self::wf_seq_perms(olds, i));
                    assert(olds[i] === s[i]);
                    if i > 0 {
                        assert(Self::wf_seq_perms(olds, i - 1));
                        assert(olds[i - 1] === s[i - 1]);
                    }
                }
                let tracked prev = Self::tracked_joins(s, (n - 1) as nat);
                prev.trusted_join(last)
            } else {
                assert(Self::wf_seq_perms(s, 0));
                let tracked ret = s.tracked_remove(0);
                Self::merge_perm_ensures(olds, n, ret);
                ret
            }
        }*/
    pub proof fn tracked_splits(tracked self, offsets: Seq<nat>) -> (tracked s: Map<int, Self>)
        requires
            offsets[0] == 0,
            forall|i: int| 0 <= i < offsets.len() ==> offsets[i] < self@.size(),
            forall|i, j|
                0 <= i < offsets.len() && 0 <= j < offsets.len() ==> offsets[i] < offsets[j],
            offsets.len() > 0,
        ensures
            forall|i: int| 0 <= i < offsets.len() ==> Self::wf_seq_perms(s, i),
            forall|i: int| 0 <= i < offsets.len() ==> s[i]@.range().0 == offsets[i],
            Self::merge_perm_ensures(s, offsets.len(), self),
        decreases offsets.len(),
    {
        if offsets.len() == 0 {
            let tracked mut ret = Map::tracked_empty();
            ret.tracked_insert(0, self);
            ret
        } else {
            let tracked (left, last) = self.trusted_split(offsets.last());
            let tracked mut ret = left.tracked_splits(offsets.take((offsets.len() - 1)));
            ret.tracked_insert(offsets.len() as int, last);
            ret
        }
    }
}

impl SnpMemAttrTrait for SnpPointsToBytes {
    open spec fn snp(&self) -> SwSnpMemAttr {
        self.snp.sw.spec_set_rmpmap(Map::empty()).spec_set_sysmap(Map::empty())
    }

    open spec fn hw_snp(&self) -> HwSnpMemAttr {
        self.snp.hw
    }
}

pub open spec fn wf_page_range(
    page_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    npages: nat,
) -> bool {
    forall|i|
        start_page <= i < (start_page + npages) ==> #[trigger] page_perms.contains_key(i)
            && page_perms[i]@.wf_range((i.to_addr(), PAGE_SIZE as nat))
}

} // verus!

================
File: ./source/verismo/src/ptr/def_s.rs
================

use verismo_macro::*;

use super::snp::{SnpMemAttr, SwSnpMemAttr};
use super::*;
use crate::debug::VPrint;

verismo_simple! {
    #[repr(C)]
    #[derive(VPrint, Copy)]
    pub struct SnpPPtr<V: IsConstant + WellFormed + SpecSize> {
        pub uptr: usize_t,
        pub dummy: Ghost<V>,
    }

    pub struct SnpPPtrWithPerm<V: IsConstant + WellFormed + SpecSize> {
        pub ptr: SnpPPtr<V>,
        pub perm: Tracked<SnpPointsTo<V>>,
    }
}

verus! {

impl<T: IsConstant + WellFormed + SpecSize> SnpPPtrWithPerm<T> {
    pub open spec fn wf(&self) -> bool {
        &&& self.perm@@.wf_at(self.ptr.id())
        &&& self.ptr.not_null()
    }
}

impl<V: IsConstant + WellFormed + SpecSize> core::marker::Copy for SnpPPtr<V> {

}

impl<V: IsConstant + WellFormed + SpecSize> Clone for SnpPPtr<V> {
    #[verifier(external_body)]
    fn clone(&self) -> (ret: Self)
        ensures
            *self === ret,
    {
        SnpPPtr { uptr: self.uptr, dummy: self.dummy }
    }
}

impl<V: IsConstant + WellFormed + SpecSize> SnpPPtr<V> {
    pub open spec fn id(&self) -> int {
        self.uptr as int
    }

    pub open spec fn range_id(&self) -> (int, nat) {
        (self.id(), spec_size::<V>())
    }
}

} // verus!
verismo_simple! {
    #[verifier(external_body)]
    #[verifier::reject_recursive_types_in_ground_variants(V)]
    pub tracked struct SnpPointsTo<V> {
        phantom: marker::PhantomData<V>,
        no_copy: NoCopy,
    }

    pub trait IsSnpPPtr{}

    /// Represents the meaning of a [`PointsTo`] object.
    #[derive(SpecGetter, SpecSetter)]
    pub ghost struct SnpPointsToData<V: IsConstant + WellFormed + SpecSize> {
        pub ptr: int,
        pub value: Option<V>,
        pub snp: SnpMemAttr,
    }

    impl<V: IsConstant + WellFormed + SpecSize> IsSnpPPtr for SnpPointsToData<V> {}

    pub ghost struct SnpPPtrWithSnp<T: IsConstant + WellFormed + SpecSize> {
        pub ptr: SnpPPtr<T>,
        pub snp: SwSnpMemAttr,
    }

    #[verifier(external_body)]
    pub tracked struct SnpPointsToRaw {
        no_copy: NoCopy,
    }
}

verus! {

impl<T: IsConstant + WellFormed + SpecSize> SnpPointsTo<T> {
    pub open spec fn view(&self) -> SnpPointsToData<T>;
}

} // verus!

================
File: ./source/verismo/src/ptr/ptr_u.rs
================

use super::*;

verus! {

impl<V: IsConstant + WellFormed + SpecSize> WellFormed for SnpPointsToData<V> {
    open spec fn wf(&self) -> bool {
        &&& self.snp.wf()
        &&& !self.snp().is_pte
        &&& self.value().is_Some() ==> self.wf_value(self.value().get_Some_0())
    }
}

impl<V: IsConstant + WellFormed + SpecSize> SnpPPtr<V> {
    pub open spec fn not_null(&self) -> bool {
        &&& self.id().spec_valid_addr_with(spec_size::<V>())
        &&& self.wf()
    }

    pub open spec fn is_null(&self) -> bool {
        &&& self.wf()
        &&& !self.id().spec_valid_addr_with(spec_size::<V>())
    }
}

impl<V: IsConstant + WellFormed + SpecSize> SnpPointsToData<V> {
    pub open spec fn id(&self) -> int {
        self.ptr
    }

    pub open spec fn range_id(&self) -> (int, nat) {
        (self.ptr, spec_size::<V>())
    }

    pub open spec fn pptr(&self) -> int {
        self.ptr
    }

    /// None if the value is never set or if the memory is hv shared.
    /// If snp.is_vmpl0_private() ==> value() represents the content in mem.
    /// If !snp.is_vmpl0_private() ==> value() is not usable.
    pub open spec fn value(&self) -> Option<V>
        recommends
            self.hw_snp().is_vmpl0_private(),
    {
        self.value
    }

    #[verifier(inline)]
    pub open spec fn get_value(&self) -> V {
        self.value().get_Some_0()
    }

    pub open spec fn is_assigned(&self) -> bool {
        self.snp().is_vmpl0_private() ==> self.value().is_Some()
    }

    pub open spec fn is_valid_private(&self) -> bool {
        &&& self.snp().is_vmpl0_private()
        &&& self.value().is_Some()
    }

    // hv-shared memory only holds non-secret data (guessing space = 1)
    pub open spec fn wf_value(&self, val: V) -> bool {
        &&& (!self.snp().is_vm_confidential() ==> val.is_constant())
        &&& val.wf()
    }

    pub open spec fn wf_at(&self, ptr: int) -> bool {
        &&& self.pptr() === ptr
        &&& self.wf()
    }

    pub open spec fn wf_const_default(&self, ptr: int) -> bool {
        &&& self.wf_not_null_at(ptr)
        &&& self.snp() === SwSnpMemAttr::spec_default()
        &&& self.get_value().is_constant()
        &&& self.value().is_Some()
    }

    pub open spec fn wf_shared(&self, ptr: int) -> bool {
        &&& self.wf_not_null_at(ptr)
        &&& self.snp() === SwSnpMemAttr::shared()
        &&& self.get_value().is_constant()
        &&& self.value().is_Some()
    }

    // wf_pte cannot use SnpPPtr::replace or put.
    pub open spec fn is_wf_pte(&self, ptr: int) -> bool {
        &&& self.snp.wf()
        &&& ptr.spec_valid_addr_with(spec_size::<V>())
        &&& self.pptr() === ptr
        &&& self.snp() === SwSnpMemAttr::spec_default_pte()
        &&& self.get_value().is_constant()
        &&& self.value().is_Some()
        &&& self.wf_value(self.value().get_Some_0())
    }

    pub open spec fn wf_not_null_at(&self, ptr: int) -> bool {
        &&& spec_size::<V>() > 0 ==> ptr.spec_valid_addr_with(spec_size::<V>())
        &&& self.wf_at(ptr)
    }

    #[verifier(inline)]
    pub open spec fn ptr_not_null_wf(&self, ptr: SnpPPtr<V>) -> bool {
        &&& self.wf_not_null_at(ptr.id())
    }

    pub open spec fn ptr_not_null_private_wf(&self, ptr: SnpPPtr<V>) -> bool {
        &&& self.ptr_not_null_wf(ptr)
        &&& self.is_valid_private()
    }

    pub open spec fn ensures_read(self, val: V) -> bool {
        self.snp().ensures_read(self.value(), val)
    }
}

} // verus!

================
File: ./source/verismo/src/ptr/mod.rs
================

use core::mem::MaybeUninit;
use core::{marker, mem};

use crate::addr_e::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::tspec::*;
extern crate alloc;

mod def_s;
mod ptr_s;
mod ptr_t;
mod ptr_u;
mod snp;

mod ptr_e;
mod raw_ptr_s;
mod raw_ptr_t;

pub use def_s::*;
pub use ptr_s::inv_snp_value;
pub use ptr_t::*;
pub use raw_ptr_s::*;
pub use raw_ptr_t::*;
pub use snp::*;

use crate::tspec_e::*;

================
File: ./source/verismo/src/ptr/snp/snp_u.rs
================

use super::*;
use crate::arch::entities::*;
use crate::arch::errors::MemError;
use crate::arch::memop::*;
use crate::arch::reg::*;
use crate::arch::rmp::perm_s::*;
use crate::arch::rmp::*;
use crate::arch::x64::*;
use crate::registers::SnpCore;

verus! {

#[repr(C, align(1))]
#[vbit_struct(RmpAttr, u64)]
pub struct RmpAttrSpec {
    #[vbits(0, 7)]
    pub vmpl: u64,
    #[vbits(8, 15)]
    pub perms: u64,
    #[vbits(16, 16)]
    pub vmsa: u64,
}

#[derive(SpecSetter, SpecGetter)]
pub ghost struct PTAttr {
    pub encrypted: bool,
    pub w: bool,
    pub x: bool,
}

impl PTAttr {
    pub open spec fn code() -> Self {
        PTAttr { encrypted: true, w: false, x: true }
    }
}

impl SpecDefault for PTAttr {
    open spec fn spec_default() -> Self {
        PTAttr { encrypted: true, w: true, x: false }
    }
}

// software tracked memory attr
#[derive(SpecSetter, SpecGetter)]
pub ghost struct SwSnpMemAttr {
    // RMP related
    pub rmp: RmpEntry,
    // PTE related
    pub guestmap: Map<int, int>,
    pub sysmap: Map<int, int>,  // unused in SW state
    pub rmpmap: Map<int, int>,  // SPA -> GPA
    // When there are more than one pte in the queue,
    // we do not guarantee the actual pte used by the perm token.
    pub pte: Seq<PTAttr>,
    pub is_pte: bool,
}

pub type HwSnpMemAttr = SwSnpMemAttr;

impl RmpAttrSpec {
    pub open spec fn perms(&self) -> PagePerm {
        PagePerm::from_int(self.spec_perms() as int)
    }

    pub open spec fn is_vmsa(&self) -> bool {
        self.spec_vmsa() == 1
    }

    pub open spec fn vmpl(&self) -> VMPL
        recommends
            self.valid_vmpl(),
    {
        VMPL::from_int(self.spec_vmpl() as int)
    }

    pub open spec fn valid_vmpl(&self) -> bool {
        VMPL::spec_from_int(self.spec_vmpl() as int).is_Some()
    }

    pub open spec fn from(vmpl: VMPL, vmsa: bool, perms: u8) -> Self {
        RmpAttrSpec::empty().spec_set_vmpl(vmpl.as_int() as u64).spec_set_vmsa(
            if vmsa {
                1
            } else {
                0
            },
        ).spec_set_perms(perms as u64)
    }
}

impl RmpAttr {
    pub fn from(vmpl: VMPL, vmsa: bool, perms: u8) -> (ret: Self)
        ensures
            ret@ === RmpAttrSpec::from(vmpl, vmsa, perms),
            ret@.valid_vmpl(),
    {
        RmpAttr::empty().set_vmpl(vmpl.as_u64()).set_vmsa(
            if vmsa {
                1
            } else {
                0
            },
        ).set_perms(perms as u64)
    }
}

impl SwSnpMemAttr {
    pub spec fn pte(&self) -> PTAttr;

    #[verifier(external_body)]
    pub broadcast proof fn axiom_pte(&self)
        ensures
            self.pte.len() == 1 ==> #[trigger] self.pte() === self.pte.last(),
    {
    }

    pub open spec fn encrypted(&self) -> bool {
        self.pte().encrypted
    }

    #[verifier(inline)]
    pub open spec fn deterministic_pte(&self) -> bool {
        &&& self.pte.len() == 1
        &&& self.pte() === self.pte.last()
        &&& self.guestmap =~~= Map::new(|gva: int| true, |gva: int| spec_va_to_pa(gva))
    }

    pub open spec fn is_vm_confidential(&self) -> bool {
        &&& self.encrypted()
        &&& self.rmp@.spec_validated()
    }

    pub open spec fn is_vmpl_private(&self, vmpl: int) -> bool {
        &&& self.is_vm_confidential()
        &&& self.rmp@.is_vmpl_private(vmpl)
    }

    pub open spec fn is_confidential_to(&self, vmpl: int) -> bool {
        &&& self.is_vm_confidential()
        &&& self.rmp@.is_confidential_to(vmpl)
    }

    pub open spec fn is_vmpl0_private(&self) -> bool {
        &&& self.is_vmpl_private(0)
    }

    #[verifier(inline)]
    pub open spec fn is_hv_shared(&self) -> bool {
        !self.is_confidential_to(4)
    }

    pub open spec fn is_vmpl0_confidential(&self) -> bool {
        &&& self.is_confidential_to(1)
        &&& self.is_confidential_to(2)
        &&& self.is_confidential_to(3)
        &&& self.is_confidential_to(4)
    }

    pub open spec fn inv_confidential(&self) -> bool {
        &&& self.is_confidential_to(4) ==> self.encrypted()
        &&& (self.is_confidential_to(1) || self.is_confidential_to(2) || self.is_confidential_to(3))
            ==> self.is_confidential_to(4)
    }

    pub open spec fn ensures_read<T: WellFormed + IsConstant>(
        self,
        val: Option<T>,
        ret: T,
    ) -> bool {
        &&& val.is_Some() && self.is_vmpl0_private() ==> { val === Some(ret) }
        &&& ret.wf()
        &&& !self.is_confidential_to(1) ==> ret.is_constant_to(1)
        &&& !self.is_confidential_to(2) ==> ret.is_constant_to(2)
        &&& !self.is_confidential_to(3) ==> ret.is_constant_to(3)
        &&& !self.is_confidential_to(4) ==> ret.is_constant_to(4)
    }
}

impl HwSnpMemAttr {
    pub open spec fn hvupdate_rel(self, prev: Self) -> bool {
        &&& self.rmp@.inv_hvupdate_rel(prev.rmp@)
        &&& self.pte == prev.pte  // proved by arch pgtable
        //&&& self.rmp@.validated ==> self.rmpmap === prev.rmpmap

    }

    pub proof fn proof_hvupdate_rel_propograte(next: Self, current: Self, prev: Self)
        requires
            next.hvupdate_rel(current),
            current.hvupdate_rel(prev),
        ensures
            next.hvupdate_rel(prev),
    {
    }
}

pub ghost struct SnpMemAttr {
    pub hw: HwSnpMemAttr,  // hardware rmp state
    pub sw: SwSnpMemAttr,
}

impl SnpMemAttr {
    pub proof fn proof_valid_access(self, vaddr: int, size: nat, p: Perm)
        requires
            p.is_Write() || p.is_Read(),
            self.valid_access(vaddr, size, p),
            self.wf(),
        ensures
            self.sw.is_vmpl0_private()
                ==> self.hw.is_vmpl0_private(),
    //self.hw.is_vmpl0_private() ==> self.sw.is_vmpl0_private(),

    {
        assert(self.hw.rmp@.inv_hvupdate_rel(self.sw.rmp@));
        if self.hw.is_vmpl0_private() {
            assert(self.sw.is_vm_confidential());
        }
    }
}

pub trait SnpMemAttrTrait {
    spec fn snp(&self) -> SwSnpMemAttr;

    spec fn hw_snp(&self) -> HwSnpMemAttr;
}

impl SnpMemAttrTrait for SnpMemAttr {
    open spec fn snp(&self) -> SwSnpMemAttr {
        self.sw
    }

    open spec fn hw_snp(&self) -> HwSnpMemAttr {
        self.hw
    }
}

impl SwSnpMemAttr {
    pub open spec fn wf(&self) -> bool {
        &&& self.rmp.inv()
        &&& self.inv_confidential()
        &&& self.deterministic_pte()
        &&& (*self === SwSnpMemAttr::spec_default().spec_set_rmp(self.rmp).spec_set_pte(self.pte))
        &&& self.rmp@.spec_asid() == SwSnpMemAttr::spec_default().rmp@.spec_asid()
        &&& self.rmp@.spec_gpn() == SwSnpMemAttr::spec_default().rmp@.spec_gpn()
        &&& self.rmp@.spec_size() == SwSnpMemAttr::spec_default().rmp@.spec_size()
        &&& !self.pte().spec_encrypted() ==> !self.rmp@.spec_validated()
    }

    pub open spec fn init() -> Self {
        let rmp_psp = HiddenRmpEntryForPSP {
            validated: false,
            vmsa: false,
            perms: crate::arch::rmp::perm_s::rmp_perm_init(),
            immutable: false,
            assigned: true,
            asid: arbitrary::<nat>() + 1,
            gpn: arbitrary(),  // unused
            size: PageSize::Size4k,  // unused
        };
        SwSnpMemAttr {
            rmp: arbitrary::<RmpEntry>().spec_set_val(rmp_psp),
            pte: seq![PTAttr::spec_default()],
            is_pte: false,
            guestmap: Map::new(|gva: int| true, |gva: int| gva),
            rmpmap: Map::empty(),
            sysmap: Map::empty(),
        }
    }

    pub open spec fn allocator_default() -> Self {
        let rmp_psp = HiddenRmpEntryForPSP {
            validated: true,
            vmsa: false,
            perms: crate::arch::rmp::perm_s::rmp_perm_init(),
            immutable: false,
            assigned: true,
            asid: arbitrary::<nat>() + 1,
            gpn: arbitrary(),  // unused
            size: PageSize::Size4k,  // unused
        };
        Self::init().spec_set_rmp(RmpEntry { val: rmp_psp })
    }

    pub open spec fn spec_default_pte() -> Self {
        Self::spec_default().spec_set_is_pte(true)
    }

    pub open spec fn executable() -> Self {
        Self::spec_default().spec_set_pte(seq![PTAttr::code()])
    }

    pub open spec fn shared() -> Self {
        let rmp_psp = HiddenRmpEntryForPSP {
            validated: false,
            vmsa: false,
            perms: crate::arch::rmp::perm_s::rmp_perm_init(),
            immutable: false,
            assigned: false,
            asid: ASID_FOR_HV!(),
            gpn: arbitrary(),  // unused
            size: PageSize::Size4k,  // unused
        };
        Self::spec_default().spec_set_rmp(
            arbitrary::<RmpEntry>().spec_set_val(rmp_psp),
        ).spec_set_pte(seq![PTAttr::spec_default().spec_set_encrypted(false)])
    }

    pub open spec fn vmsa() -> Self {
        let base = Self::spec_default();
        base.spec_set_rmp(RmpEntry { val: base.rmp.val.spec_set_vmsa(true) })
    }/*pub open spec fn vmsa2(attr: RmpAttrSpec) -> Self {
        let base  = Self::spec_default();
        let rmp = base.rmp.val;
        let rmpperms = rmp.perms.insert(attr.vmpl(), attr.perms());
        base.spec_set_rmp(
            RmpEntry {
                val: rmp.spec_set_perms(rmpperms).spec_set_vmsa(true)
            }
        )
    }*/

}

impl SpecDefault for SwSnpMemAttr {
    open spec fn spec_default() -> Self {
        SwSnpMemAttr::allocator_default()
    }
}

impl SwSnpMemAttr {
    #[verifier(inline)]
    pub open spec fn is_default(&self) -> bool {
        &&& *self === SwSnpMemAttr::spec_default()
    }

    pub open spec fn is_shared_from(&self, old: Self) -> bool {
        &&& self.is_shared()
        &&& self.rmp@.gpn === old.rmp@.gpn
        &&& old.pte().spec_set_encrypted(false) === self.pte()
    }

    #[verifier(inline)]
    pub open spec fn is_shared(&self) -> bool {
        &&& self.rmp@ === SwSnpMemAttr::shared().rmp@.spec_set_gpn(self.rmp@.gpn)
        &&& !self.pte().spec_encrypted()
    }
}

// Partial Eq on software parts
impl VSpecEq<SnpMemAttr> for SnpMemAttr {
    #[verifier(inline)]
    open spec fn spec_eq(self, rhs: Self) -> bool {
        &&& self.sw_eq(rhs)
    }
}

impl SnpMemAttr {
    #[verifier(inline)]
    pub open spec fn sw_eq(self, rhs: Self) -> bool {
        &&& self.sw === rhs.sw
    }

    #[verifier(inline)]
    pub open spec fn sw_default(&self) -> bool {
        &&& self.sw === SwSnpMemAttr::spec_default()
    }

    pub closed spec fn map_wf(&self) -> bool {
        &&& self.hw.guestmap === self.sw.guestmap
    }

    pub open spec fn hw_rmp_wf(&self) -> bool {
        // software-hardware relationship
        &&& self.hw.hvupdate_rel(self.sw)
        &&& self.hw.rmp.inv()
    }

    pub open spec fn wf(&self) -> bool {
        &&& self.hw_rmp_wf()
        // rmp status is valid

        &&& self.snp().wf()
        // vmpl-x secret must be stored in vmpl-x's confidential memory.

    }
}

} // verus!

================
File: ./source/verismo/src/ptr/snp/rmp/rmp_e.rs
================

use super::rmp_t::{pvalidate, rmpadjust};
use super::*;
use crate::addr_e::*;
use crate::arch::reg::RflagBit;
use crate::arch::rmp::*;
use crate::registers::{CoreIdPerm, SnpCore};
use crate::snp::ghcb::*;

verus! {

pub open spec fn spec_perm_requires_pvalidate(
    perm: SnpPointsToRaw,
    addr: int,
    size: nat,
    val: bool,
) -> bool {
    &&& perm@.snp().requires_pvalidate(
        addr,
        RMP_4K as int,
        if val {
            1
        } else {
            0
        },
    )
    &&& perm@.snp_wf_range((addr, size as nat))
}

pub open spec fn spec_perms_requires_pvalidate(
    page_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    end_page: int,
    val: bool,
) -> bool {
    forall|i|
        start_page <= i < end_page ==> #[trigger] page_perms.contains_key(i)
            && spec_perm_requires_pvalidate(page_perms[i], i.to_addr(), PAGE_SIZE as nat, val)
}

pub open spec fn spec_perm_ensures_pvalidate(
    perm: SnpPointsToRaw,
    old_perm: SnpPointsToRaw,
    start: int,
    size: nat,
    val: bool,
) -> bool {
    &&& perm@.snp().ensures_pvalidated(old_perm@.snp(), val)
    &&& perm@.snp_wf_range((start, size))
    &&& old_perm@.bytes().wf() ==> perm@.bytes().wf()
}

pub open spec fn spec_perms_ensures_pvalidate(
    page_perms: Map<int, SnpPointsToRaw>,
    old_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    end_page: int,
    val: bool,
) -> bool {
    &&& old_perms.dom() =~~= page_perms.dom()
    &&& forall|i|
        start_page <= i < end_page ==> #[trigger] page_perms.contains_key(i)
            && spec_perm_ensures_pvalidate(
            page_perms[i],
            old_perms[i],
            i.to_addr(),
            PAGE_SIZE as nat,
            val,
        )
}

pub fn pvalmem2(
    start: u64,
    end: u64,
    val: bool,
    Tracked(perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
    Tracked(snpcore): Tracked<&mut SnpCore>,
)
    requires
        end as int % PAGE_SIZE!() == 0,
        start as int % PAGE_SIZE!() == 0,
        start < end,
        spec_perms_requires_pvalidate(
            *old(perms),
            (start as int).to_page(),
            (end as int).to_page(),
            val,
        ),
        old(snpcore).inv(),
        old(snpcore).coreid@.vmpl == 0,
    ensures
        spec_perms_ensures_pvalidate(
            *perms,
            *old(perms),
            (start as int).to_page(),
            (end as int).to_page(),
            val,
        ),
        snpcore.inv(),
        *old(snpcore) === *snpcore,
{
    let ghost old_perms = *perms;
    let ghost old_snpcore = *snpcore;
    let mut vaddr = start;
    while vaddr < end
        invariant
            vaddr.is_constant(),
            end.is_constant(),
            end as int % PAGE_SIZE!() == 0,
            vaddr as int % PAGE_SIZE!() == 0,
            (start as int) <= (vaddr as int) <= (end as int),
            forall|i|
                (start as int).to_page() <= i < (end as int).to_page()
                    ==> #[trigger] old_perms.contains_key(i),
            forall|i|
                (vaddr as int).to_page() <= i < (end as int).to_page()
                    ==> #[trigger] perms.contains_key(i) && old_perms.contains_key(i) && perms[i]
                    === old_perms[i],
            spec_perms_requires_pvalidate(
                old_perms,
                (start as int).to_page(),
                (end as int).to_page(),
                val,
            ),
            spec_perms_requires_pvalidate(
                *perms,
                (vaddr as int).to_page(),
                (end as int).to_page(),
                val,
            ),
            spec_perms_ensures_pvalidate(
                *perms,
                old_perms,
                (start as int).to_page(),
                (vaddr as int).to_page(),
                val,
            ),
            snpcore.inv(),
            snpcore.coreid@.vmpl == 0,
            old_snpcore === *snpcore,
    {
        let ghost pn = (vaddr as int).to_page();
        proof {
            assert(perms.contains_key(pn));
        }
        let tracked current_perm = perms.tracked_remove(pn);
        let next_vaddr = vaddr + PAGE_SIZE as u64;
        let Tracked(current_perm) = pvalmem(
            vaddr,
            next_vaddr,
            val,
            Tracked(current_perm),
            Tracked(snpcore),
        );
        assert((vaddr as int).to_page() + 1 == (next_vaddr as int).to_page());
        vaddr = next_vaddr;
        proof {
            perms.tracked_insert(pn, current_perm);
            assert(perms.contains_key(pn));
            assert forall|i|
                (vaddr as int).to_page() <= i < (
                end as int).to_page() implies #[trigger] perms.contains_key(i)
                && old_perms.contains_key(i) && perms[i] === old_perms[i] by {
                assert(old_perms.contains_key(i));
                assert(perms.contains_key(i));
            }
            assert forall|i|
                (start as int).to_page() <= i < (
                vaddr as int).to_page() implies #[trigger] perms.contains_key(i)
                && spec_perm_ensures_pvalidate(
                perms[i],
                old_perms[i],
                i.to_addr(),
                PAGE_SIZE as nat,
                val,
            ) by {
                assert(old_perms.contains_key(i));
                assert(perms.contains_key(i));
            }
            assert forall|i|
                (vaddr as int).to_page() <= i < (
                end as int).to_page() implies #[trigger] perms.contains_key(i)
                && spec_perm_requires_pvalidate(perms[i], i.to_addr(), PAGE_SIZE as nat, val) by {
                assert(old_perms.contains_key(i));
                assert(perms.contains_key(i));
            }
            assert(spec_perms_ensures_pvalidate(
                *perms,
                old_perms,
                (start as int).to_page(),
                (vaddr as int).to_page(),
                val,
            ));
        }
    }
}

pub fn pvalmem(
    start: u64,
    end: u64,
    val: bool,
    Tracked(perm): Tracked<SnpPointsToRaw>,
    Tracked(snpcore): Tracked<&mut SnpCore>,
) -> (ret: Tracked<SnpPointsToRaw>)
    requires
        spec_perm_requires_pvalidate(perm, start as int, (end - start) as nat, val),
        end as int % PAGE_SIZE!() == 0,
        start as int % PAGE_SIZE!() == 0,
        start < end,
        old(snpcore).inv(),
        old(snpcore).coreid@.vmpl == 0,
    ensures
        ret@@.snp_wf_range((start as int, (end - start) as nat)),
        ret@@.snp().ensures_pvalidated(perm@.snp(), val),
        spec_perm_ensures_pvalidate(ret@, perm, start as int, (end - start) as nat, val),
        snpcore.inv(),
        *old(snpcore) === *snpcore,
{
    let tracked mut perm = perm;
    let ghost old_perm = perm;
    let ghost mut expected_snp = perm@.snp();
    proof {
        expected_snp.rmp.val.validated = val;
        // = RmpEntry::new(old_perm@.snp().rmp@.spec_set_validated(val));
    }
    let tracked mut retp = SnpPointsToRaw::tracked_empty(start as int, expected_snp);
    let ghost gvalidate: int = if val {
        1
    } else {
        0
    };
    let ghost oldsnpcore = *snpcore;
    let mut vaddr = start;
    while vaddr < end
        invariant
            vaddr.is_constant(),
            end.is_constant(),
            end as int % PAGE_SIZE!() == 0,
            vaddr as int % PAGE_SIZE!() == 0,
            (start as int) <= (vaddr as int) <= (end as int),
            gvalidate == if val {
                1int
            } else {
                0int
            },
            perm@.snp() === old_perm@.snp(),
            perm@.snp().requires_pvalidate(vaddr as int, RMP_4K as int, gvalidate),
            perm@.snp_wf_range((vaddr as int, (end - vaddr) as nat)),
            retp@.snp().ensures_pvalidated(perm@.snp(), val),
            retp@.snp_wf_range((start as int, (vaddr - start) as nat)),
            snpcore.inv(),
            old_perm@.bytes().wf() ==> perm@.bytes().wf(),
            old_perm@.bytes().wf() ==> retp@.bytes().wf(),
            snpcore.coreid@.vmpl == 0,
            oldsnpcore === *snpcore,
    {
        let ghost prev_perm = perm;
        let tracked (mut current_perm, next_perm) = perm.trusted_split(PAGE_SIZE as nat);
        let ghost old_current_perm = current_perm;
        let mut rflags: u64 = 0;
        let validate = if val {
            1
        } else {
            0
        };
        let rc = pvalidate(
            vaddr,
            RMP_4K,
            validate,
            &mut rflags,
            Tracked(snpcore),
            Tracked(&mut current_perm),
        );
        if bit_check(rflags, RflagBit::CF.as_u64()) || rc != 0 {
            // failed validation ==> possible attack
            vc_terminate(SM_TERM_INVALID_PARAM, Tracked(snpcore));
        }
        vaddr = vaddr + PAGE_SIZE as u64;
        proof {
            assert(retp@.snp().ensures_pvalidated(perm@.snp(), val));
            assert(current_perm@.snp().ensures_pvalidated(perm@.snp(), val));
            assert(current_perm@.size() == PAGE_SIZE);
            // rmp status is valid
            retp@.snp().rmp.proof_eq(current_perm@.snp().rmp);
            assert(retp@.snp() === current_perm@.snp());
            HwSnpMemAttr::reveal_use_rflags();
            assert(retp@.snp.wf());
            if old_perm@.bytes().wf() {
                assert(retp@.bytes().wf());
                assert(old_current_perm@.bytes() =~~= prev_perm@.bytes().take(PAGE_SIZE as int));
                assert(old_current_perm@.bytes().wf());
                assert(current_perm@.bytes().wf());
                assert((retp@.bytes() + current_perm@.bytes()).wf());
            }
            retp = retp.tracked_join(current_perm);
            assert(old_perm@.bytes().wf() ==> retp@.bytes().wf());
            perm = next_perm;
            assert(retp@.range() === (start as int, (vaddr - start) as nat));
        }
    }
    Tracked(retp)
}

pub open spec fn spec_rmpadjmem_requires_at(
    page_perm: SnpPointsToRaw,
    i: int,
    attr: RmpAttrSpec,
) -> bool {
    &&& page_perm@.snp().requires_rmpadjust_mem((i as int).to_addr(), RMP_4K as int, attr, None)
    &&& page_perm@.wf_range((i.to_addr(), PAGE_SIZE as nat))
    &&& attr.spec_perms() > 0 ==> page_perm@.bytes().is_constant_to(attr.spec_vmpl() as nat)
    &&& 0 < attr.spec_vmpl() < 4
}

pub open spec fn spec_rmpadjmem_requires(
    page_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    npages: nat,
    attr: RmpAttrSpec,
) -> bool {
    &&& start_page.spec_valid_pn_with(npages)
    &&& forall|i|
        start_page <= i < (start_page + npages) ==> #[trigger] page_perms.contains_key(i)
            && spec_rmpadjmem_requires_at(page_perms[i], i, attr)
}

pub open spec fn spec_ensures_rmpadjust(
    prev_perm: SnpPointsToRaw,
    perm: SnpPointsToRaw,
    page: int,
    attr: RmpAttrSpec,
) -> bool {
    &&& perm@.snp().ensures_rmpadjust(prev_perm@.snp(), attr)
    &&& perm@.wf_range((page.to_addr(), PAGE_SIZE as nat))
    &&& perm@.range() === prev_perm@.range()
    &&& perm@.bytes() =~~= prev_perm@.bytes()
}

pub open spec fn spec_ensures_rmpadjmem(
    prev_perms: Map<int, SnpPointsToRaw>,
    page_perms: Map<int, SnpPointsToRaw>,
    start_page: int,
    npages: nat,
    attr: RmpAttrSpec,
) -> bool {
    &&& forall|i|
        start_page <= i < (start_page + npages) ==> #[trigger] page_perms.contains_key(i)
            && spec_ensures_rmpadjust(prev_perms[i], page_perms[i], i, attr)
    &&& page_perms.dom() =~~= prev_perms.dom()
}

pub fn rmpadjmem(
    start_page: usize,
    npages: usize,
    attr: RmpAttr,
    Tracked(snpcore): Tracked<&mut SnpCore>,
    Tracked(perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
)
    requires
        !attr@.is_vmsa(),
        spec_rmpadjmem_requires(*old(perms), start_page as int, npages as nat, attr@),
        old(snpcore).inv(),
        old(snpcore).coreid@.vmpl == 0,
    ensures
        spec_ensures_rmpadjmem(*old(perms), *perms, start_page as int, npages as nat, attr@),
        *old(snpcore) === *snpcore,
{
    let ghost old_perms = *perms;
    let ghost old_snpcore = *snpcore;
    let tracked mut page_perms = perms.tracked_remove_keys(perms.dom());
    proof {
        assert(page_perms =~~= old_perms);
    }
    let mut i = 0;
    while i < npages
        invariant
            0 <= i <= npages,
            forall|k|
                start_page as int + i <= k < start_page as int + npages ==> old_perms[k]
                    === page_perms[k],
            spec_rmpadjmem_requires(old_perms, start_page as int, npages as nat, attr@),
            spec_rmpadjmem_requires(page_perms, start_page as int + i, (npages - i) as nat, attr@),
            spec_ensures_rmpadjmem(old_perms, page_perms, start_page as int, i as nat, attr@),
            !attr@.is_vmsa(),
            snpcore.inv(),
            snpcore.coreid@.vmpl == 0,
            *snpcore === old_snpcore,
    {
        let page = start_page + i;
        let vaddr = page.to_addr() as u64;
        proof {
            assert(page_perms.contains_key(page as int));
        }
        let tracked mut current_perm = page_perms.tracked_remove(page as int);
        let ghost prev_perm = current_perm;
        let rc = rmpadjust(
            vaddr,
            RMP_4K,
            attr,
            Tracked(snpcore),
            Tracked(None),
            Tracked(&mut current_perm),
        );
        if rc != 0 {
            // failed validation ==> possible attack
            vc_terminate(SM_TERM_INVALID_PARAM, Tracked(snpcore));
        }
        i = i + 1;
        proof {
            // rmp status is valid
            assert(current_perm@.snp().ensures_rmpadjust(prev_perm@.snp(), attr@));
            assert(spec_ensures_rmpadjust(prev_perm, current_perm, page as int, attr@));
            assert(old_perms[page as int] === prev_perm);
            page_perms.tracked_insert(page as int, current_perm);
            assert(page_perms.contains_key(page as int));
            assert forall|k|
                start_page as int + i <= k < start_page as int + npages implies old_perms[k]
                === page_perms[k] by {
                assert(old_perms.contains_key(k));
                assert(page_perms.contains_key(k));
            }
            assert forall|k| start_page as int + i <= k < start_page as int + npages implies (
            #[trigger] page_perms.contains_key(k)) && spec_rmpadjmem_requires_at(
                page_perms[k],
                k,
                attr@,
            ) by {
                assert(old_perms.contains_key(k));
                assert(page_perms.contains_key(k));
                assert(page_perms[k] === old_perms[k]);
                assert(spec_rmpadjmem_requires_at(old_perms[k], k, attr@));
                assert(spec_rmpadjmem_requires_at(page_perms[k], k, attr@));
            }
            assert forall|k|
                start_page <= k < (start_page + i) implies #[trigger] page_perms.contains_key(k)
                && spec_ensures_rmpadjust(old_perms[k], page_perms[k], k, attr@) by {
                assert(page_perms.contains_key(k));
                assert(old_perms.contains_key(k));
            }
        }
    }
    proof {
        perms.tracked_union_prefer_right(page_perms);
    }
}

pub fn rmpadjust_check(
    vaddr: u64,
    attr: RmpAttr,
    Tracked(snpcore): Tracked<&mut SnpCore>,
    Tracked(perm): Tracked<&mut SnpPointsToRaw>,
)
    requires
        spec_rmpadjmem_requires_at(*old(perm), (vaddr as int).to_page(), attr@),
        old(snpcore).inv(),
    ensures
        spec_ensures_rmpadjust(*old(perm), *perm, (vaddr as int).to_page(), attr@),
        old(perm)@.range() === perm@.range(),
        old(perm)@.snp_bytes === perm@.snp_bytes,
        old(perm)@.wf() ==> perm@.wf(),
        *snpcore === *old(snpcore),
{
    let rc = rmpadjust(vaddr, RMP_4K, attr, Tracked(snpcore), Tracked(None), Tracked(perm));
    if rc != 0 {
        // failed validation ==> possible attack
        vc_terminate(SM_TERM_INVALID_PARAM, Tracked(snpcore));
    }
}

} // verus!

================
File: ./source/verismo/src/ptr/snp/rmp/mod.rs
================

mod rmp_e;
mod rmp_reset;
mod rmp_t;

pub use rmp_e::*;
pub use rmp_reset::*;
pub use rmp_t::*;

use crate::addr_e::*;
use crate::ptr::*;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

================
File: ./source/verismo/src/ptr/snp/rmp/rmp_t.rs
================

use core::arch::asm;

use super::*;
use crate::registers::{CoreIdPerm, SnpCore};

verus! {

// PVALIDATE and RMPADJUST related
/// 0
pub const RMP_4K: u64 = 0;

/// 1
pub const RMP_2M: u64 = 1;

pub const RMP_READ: u8 = 1;

pub const RMP_WRITE: u8 = 2;

pub const RMP_USER_EXE: u8 = 4;

pub const RMP_KERN_EXE: u8 = 8;

pub const RMP_NO_WRITE: u8 = RMP_READ | RMP_USER_EXE | RMP_KERN_EXE;

pub const RMP_RWX: u8 = RMP_NO_WRITE | RMP_WRITE;

} // verus!
verus! {

// After validation, SPA may change and thus, software-tracked value will not be retained anymore.
// The actual value could be any value including secret values.
#[verifier(external_body)]
pub fn pvalidate(
    vaddr: u64,
    psize: u64,
    validate: u64,
    rflags: &mut u64,
    Tracked(mycore): Tracked<&SnpCore>,
    Tracked(perm): Tracked<&mut SnpPointsToRaw>,
) -> (ret: u64)
    requires
        old(perm)@.snp().requires_pvalidate(vaddr as int, psize as int, validate as int),
        mycore.coreid@.vmpl == 0,
        mycore.inv(),
    ensures
        old(perm)@.snp.pvalidate_ret(
            perm@.snp,
            ret,
            *rflags,
            vaddr as int,
            psize as int,
            validate as int,
        ),
        old(perm)@.range() === perm@.range(),
        old(perm)@.bytes().wf() ==> perm@.bytes().wf(),
{
    let flags: u64;
    let rc: u64;
    unsafe {
        asm!(
                ".byte 0xF2, 0x0F, 0x01, 0xFF",
                "pushf; pop {out}\n\t",
                out = out(reg) flags,
                lateout("eax") rc,
                in("eax") vaddr,
                in("ecx") psize,
                in("edx") validate,
            );
    }
    *rflags = flags;
    rc
}

// If the rmpadjust is for vmsa page, newcore perm is required.
// After rmpadjust, the current context my lose the ownership of the page.
//  * If creating vmsa page, the memory perm will be taken into the function if the vmsa is for a different cpu.
#[verifier(external_body)]
pub fn rmpadjust(
    vaddr: u64,
    psize: u64,
    attr: RmpAttr,
    Tracked(mycore): Tracked<&SnpCore>,
    Tracked(newcore): Tracked<Option<CoreIdPerm>>,
    Tracked(perm): Tracked<&mut SnpPointsToRaw>,
) -> (ret: u64)
    requires
        old(perm)@.snp().requires_rmpadjust(vaddr as int, psize as int, attr@, newcore, old(perm)@),
        mycore.coreid@.vmpl == 0,
        attr.spec_vmpl() > mycore.coreid@.vmpl,
    ensures
        old(perm)@.snp.rmpadjust_ret(perm@.snp, ret, vaddr as int, psize as int, attr@),
        old(perm)@.range() === perm@.range(),
        old(perm)@.snp_bytes === perm@.snp_bytes,
{
    let ret: u64;
    unsafe {
        asm!(".byte 0xf3,0x0f,0x01,0xfe",
                in("rax") vaddr, in("rcx") psize, in("rdx") attr.value,
                lateout("rax") ret,
                options(nostack));
    }
    ret
}

} // verus!

================
File: ./source/verismo/src/ptr/snp/rmp/rmp_reset.rs
================

use super::*;
use crate::arch::rmp::*;
use crate::registers::SnpCore;
verus! {

pub open spec fn spec_reset_perm_at(
    perm: SnpPointsToRaw,
    old_perm: SnpPointsToRaw,
    vmpl: nat,
) -> bool {
    let vmpl = VMPL::from_int(vmpl as int);
    let rmp = perm@.snp().rmp@;
    let old_rmp = old_perm@.snp().rmp@;
    &&& rmp === old_rmp.spec_set_perms(rmp.perms).spec_set_vmsa(false)
    &&& rmp.perms[vmpl] =~~= PagePerm::empty()
    &&& perm@.bytes() =~~= old_perm@.bytes()
    &&& perm@.range() === old_perm@.range()
    &&& perm@.wf()
}

pub open spec fn spec_reset_perm(perm: SnpPointsToRaw, old_perm: SnpPointsToRaw) -> bool {
    let rmp = perm@.snp().rmp@;
    let old_rmp = old_perm@.snp().rmp@;
    &&& rmp === old_rmp.spec_set_perms(rmp.perms)
    &&& rmp.perms =~~= rmp_perm_init()
    &&& perm@.bytes() === old_perm@.bytes()
    &&& perm@.range() === old_perm@.range()
    &&& perm@.wf()
}

#[verifier(external_body)]
pub fn rmp_reset_vmpl_perm(
    page: usize,
    Tracked(snpcore): Tracked<&mut SnpCore>,
    Tracked(perm): Tracked<&mut SnpPointsToRaw>,
)
    requires
        old(perm)@.wf_range(((page as int).to_addr(), PAGE_SIZE as nat)),
        old(perm)@.snp().rmp@.spec_validated(),
        old(snpcore).inv(),
        page.spec_valid_pn_with(1),
    ensures
        perm@.wf_range(((page as int).to_addr(), PAGE_SIZE as nat)),
        spec_reset_perm(*perm, *old(perm)),
        *snpcore === *old(snpcore),
{
    let mut vmpl = 1;
    let ghost old_perm = *perm;
    while vmpl < 4
        invariant
            1 <= vmpl <= 4,
            perm@.wf_range(((page as int).to_addr(), PAGE_SIZE as nat)),
            perm@.bytes() === old_perm@.bytes(),
            perm@.range() === old_perm@.range(),
            perm@.snp().rmp@.spec_validated(),
            snpcore.inv(),
            *snpcore === *old(snpcore),
            forall|i| 1 <= i < vmpl ==> #[trigger] spec_reset_perm_at(*perm, old_perm, i),
            page.spec_valid_pn_with(1),
    {
        let ghost prev_perm = *perm;
        let rmp_attr = RmpAttr::empty().set_vmpl(vmpl as u64).set_perms(0);
        rmpadjust_check(page.to_addr() as u64, rmp_attr, Tracked(snpcore), Tracked(perm));
        proof {
            assert(rmp_attr@.perms() =~~= PagePerm::empty());
            assert(rmp_attr@.vmpl() == VMPL::from_int(vmpl as int));
            assert forall|i: int| 1 <= i < (vmpl + 1) implies #[trigger] spec_reset_perm_at(
                *perm,
                old_perm,
                i as nat,
            ) by {
                if i < vmpl {
                    assert(spec_reset_perm_at(prev_perm, old_perm, i as nat));
                    let i = VMPL::from_int(i);
                    assert(perm@.snp().rmp@.perms[i] === prev_perm@.snp().rmp@.perms[i]);
                    assert(perm@.snp().rmp@.perms.contains_key(i));
                    assert(perm@.snp().rmp@.perms[i] =~~= PagePerm::empty());
                } else {
                    let i = VMPL::from_int(i);
                    assert(perm@.snp().rmp@.perms.contains_key(i));
                    assert(perm@.snp().rmp@.perms[i] =~~= PagePerm::empty());
                }
            }
            assert(forall|i: int|
                1 <= i < (vmpl + 1) ==> #[trigger] spec_reset_perm_at(*perm, old_perm, i as nat));
        }
        vmpl = vmpl + 1;
    }
    proof {
        assert(spec_reset_perm_at(*perm, old_perm, 1));
        assert(spec_reset_perm_at(*perm, old_perm, 2));
        assert(spec_reset_perm_at(*perm, old_perm, 3));
    }
}

} // verus!

================
File: ./source/verismo/src/ptr/snp/snp_s.rs
================

use super::*;
use crate::arch::entities::*;
use crate::arch::errors::MemError;
use crate::arch::memop::*;
use crate::arch::reg::*;
use crate::arch::rmp::perm_s::*;
use crate::arch::rmp::*;
use crate::arch::x64::*;
use crate::registers::{CoreIdPerm, SnpCore};
use crate::snp::cpu::VmsaPage;

verus! {

impl SwSnpMemAttr {
    pub open spec fn ensures_rmpupdate(&self, prev: Self, shared: bool, page_2m: bool) -> bool {
        let rmp = prev.rmp@;
        let asid = if shared {
            ASID_FOR_HV!()
        } else {
            rmp.spec_asid()
        };
        let hidden = HiddenRmpEntryForPSP {
            immutable: rmp.spec_immutable(),
            assigned: !shared,
            validated: false,
            vmsa: false,
            asid,
            gpn: rmp.spec_gpn(),
            size: if page_2m {
                PageSize::Size2m
            } else {
                PageSize::Size4k
            },
            perms: rmp_perm_init(),
        };
        let newrmp = RmpEntry { val: hidden };
        &&& prev.rmp.rmpupdate(newrmp).is_Ok()
        &&& self.rmp === prev.rmp.rmpupdate(newrmp).get_Ok_0()
        &&& *self === prev.spec_set_rmp(self.rmp)
    }

    pub open spec fn requires_pvalidate(&self, vaddr: int, is_2m: int, val: int) -> bool {
        &&& is_2m % 2 == 0  // Only support 4k page

        &&& (val % 2 == 0)
            == self.rmp@.spec_validated()
        //&&& (val % 2 == 1) ==>  self.rmp@.perms =~~= rmp_perm_init()

        &&& self.deterministic_pte()
        &&& self.encrypted()
    }

    pub open spec fn ensures_pvalidated(&self, prev: Self, val: bool) -> bool {
        &&& self.rmp@ === prev.rmp@.spec_set_validated(val)
        &&& *self === prev.spec_set_rmp(self.rmp)
    }

    // VMSA vmpl is determined only by vmpl field in vmsa page
    // attr.vmpl only decide which vmpl the permission assigned to.
    // attr.vmpl must be higher than the current vmpl
    pub open spec fn requires_rmpadjust(
        &self,
        vaddr: int,
        is_2m: int,
        attr: RmpAttrSpec,
        newcore: Option<CoreIdPerm>,
        memperm: SnpPointsToBytes,
    ) -> bool {
        let vmsa: VmsaPage = memperm.bytes().vspec_cast_to();
        let vmpl = vmsa.vmpl;
        &&& self.requires_rmpadjust_mem(vaddr, is_2m, attr, newcore)
        &&& attr.is_vmsa() ==> vmpl.spec_eq(newcore.get_Some_0()@.vmpl)
    }

    pub open spec fn requires_rmpadjust_mem(
        &self,
        vaddr: int,
        is_2m: int,
        attr: RmpAttrSpec,
        newcore: Option<CoreIdPerm>,
    ) -> bool {
        &&& is_2m % 2 == 0  // Only support 4k page

        &&& 1 <= attr.spec_vmpl() <= 3
        &&& attr.is_vmsa() ==> newcore.is_Some()
        &&& !attr.is_vmsa() ==> newcore.is_None()
        &&& self.rmp@.spec_validated()  // need to be validated before rmpadjust, otherwises it will never return.

        &&& self.wf()
    }

    pub open spec fn ensures_rmpadjust(&self, prev: Self, attr: RmpAttrSpec) -> bool {
        &&& self.rmp@ === prev.rmp@.spec_set_perms(
            prev.rmp@.perms.insert(attr.vmpl(), attr.perms()),
        ).spec_set_vmsa(attr.is_vmsa())
        &&& *self === prev.spec_set_rmp(self.rmp)
    }
}

impl HwSnpMemAttr {
    // True -> Return
    // False -> Never return
    pub open spec fn valid_memmap(self, start: int, size: nat) -> bool {
        forall|vaddr|
            start <= vaddr < start + size ==> self.guestmap[vaddr]
                == self.rmpmap[self.sysmap[self.guestmap[vaddr]]]
    }

    // True -> Return
    // False -> Never return
    pub open spec fn valid_access(self, vmid: nat, vaddr: int, size: nat, perm: Perm) -> bool
        recommends
            vmid >= 1,
    {
        let memid = MemID::Guest((vmid - 1) as nat, VMPL::VMPL0);
        &&& self.rmp.check_access_no_addr_check(memid, self.encrypted(), perm).is_Ok()
        &&& self.encrypted() ==> self.valid_memmap(vaddr, size)
    }

    // True -> Return
    // False -> Never return
    pub open spec fn rmpadjust_ret(
        self,
        new: Self,
        rax: u64,
        vmid: nat,
        vaddr: int,
        psize: PageSize,
        vmpl: VMPL,
        is_vmsa: bool,
        perms: PagePerm,
    ) -> bool
        recommends
            vmid > 0,
    {
        let memid = MemID::Guest((vmid - 1) as nat, VMPL::VMPL0);
        let ret = self.rmp.rmpadjust(
            memid,
            vmpl,
            psize,
            GPA::new(self.guestmap[vaddr]).to_page(),
            is_vmsa,
            perms,
        );
        let size = (if psize.is_Size4k() {
            PAGE_SIZE!()
        } else {
            0x2000_000
        }) as nat;
        let map_ok = new.valid_memmap(vaddr, size);
        if map_ok {
            if let ResultWithErr::Error(_, memerr) = ret {
                let arch: Archx64 = arbitrary();
                let memop: MemOp<GuestVir> = choose|memop: MemOp<GuestVir>|
                    memop.is_RmpOp() && memop.get_RmpOp_0().is_RmpAdjust();
                let op = choose|op: Archx64Op|
                    op.to_memid() === memid && op.is_MemOp() && op.get_MemOp_0() === memop;
                let (trap, trans) = Archx64::handle_mem_err_fn(MemError::from_err(memerr, memop));
                if !trap {
                    &&& rax == trans(arch, op).spec_regdb()[op.cpu_memid()].spec_index(RegName::Rax)
                    &&& new === self
                    &&& arch.is_run(op.cpu_memid())
                } else {
                    false
                }
            } else {
                new === self.spec_set_rmp(ret.to_result())
            }
        } else {
            false
        }
    }

    // True -> Return
    // False -> Never return
    pub open spec fn pvalidate_ret(
        self,
        new: Self,
        rax: u64,
        rflags: u64,
        vmid: nat,
        vaddr: int,
        psize: PageSize,
        val: bool,
    ) -> bool
        recommends
            vmid > 0,
    {
        let memid = MemID::Guest((vmid - 1) as nat, VMPL::VMPL0);
        let page = GVA::new(vaddr).to_page();
        let ret = self.rmp.pvalidate(memid, psize, self.rmp@.gpn, val);
        let size = if psize.is_Size4k() {
            PAGE_SIZE!()
        } else {
            0x2000_000
        };
        let map_ok = self.valid_memmap(vaddr, size as nat);
        if map_ok {
            if let ResultWithErr::Error(_, memerr) = ret {
                let arch: Archx64 = arbitrary();
                let memop: MemOp<GuestVir> = choose|memop: MemOp<GuestVir>|
                    memop.is_RmpOp() && memop.get_RmpOp_0().is_Pvalidate();
                let op = choose|op: Archx64Op|
                    op.to_memid() === memid && op.is_MemOp() && op.get_MemOp_0() === memop;
                let (trap, trans) = Archx64::handle_mem_err_fn(MemError::from_err(memerr, memop));
                if !trap {
                    &&& rax == trans(arch, op).spec_regdb()[op.cpu_memid()].spec_index(RegName::Rax)
                    &&& rflags == trans(arch, op).spec_regdb()[op.cpu_memid()].spec_index(
                        RegName::Rflags,
                    )
                    &&& self === new
                    &&& arch.is_run(op.cpu_memid())
                } else {
                    false
                }
            } else {
                new === self.spec_set_rmp(ret.to_result())
            }
        } else {
            false
        }
    }

    pub proof fn reveal_use_rflags()
        ensures
            forall|rflags: u64| bits_p::spec_bit_set(rflags, RflagBit::CF.as_int() as u64) != 0,
    {
        assert forall|rflags: u64|
            bits_p::spec_bit_set(rflags, RflagBit::CF.as_int() as u64) != 0 by {
            let b = RflagBit::CF.as_int() as u64;
            bit_set_non_zero(rflags, b);
        }
    }
}

impl SnpMemAttr {
    pub open spec fn valid_access(self, vaddr: int, size: nat, perm: Perm) -> bool {
        &&& self.hw.valid_access(self.sw.rmp@.asid, vaddr, size, perm)
    }

    pub open spec fn pvalidate_ret(
        self,
        new: Self,
        rax: u64,
        rflags: u64,
        vaddr: int,
        is_2m: int,
        val: int,
    ) -> bool {
        let psize = if is_2m % 2 == 0 {
            PageSize::Size4k
        } else {
            PageSize::Size4k
        };
        let validated = val % 2 == 1;
        &&& self.hw.pvalidate_ret(new.hw, rax, rflags, self.sw.rmp@.asid, vaddr, psize, validated)
        &&& if (rax == 0 && !spec_has_bit_set(rflags, RflagBit::CF.as_int() as u64)) {
            &&& new.sw.ensures_pvalidated(self.sw, validated)
        } else {
            new.sw === self.sw
        }
    }

    /*pub proof fn proof_pvalidate_ret_wf(self, new: Self, rax: u64, rflags: u64, vaddr: int, is_2m: int, val: int)
        requires
            self.wf(),
            self.pvalidate_ret(new, rax, rflags, vaddr, is_2m, val)
        ensures
            (rax == 0 && !bits_p::spec_has_bit_set(rflags, RflagBit::CF.as_int() as u64)) ==> new.rmp_wf()
        {
            let memid = MemID::Guest((self.sw.rmp@.asid - 1) as nat, VMPL::VMPL0);
            assert forall |rflags: u64| bits_p::spec_bit_set(rflags, RflagBit::CF.as_int() as u64) != 0 by
            {
                let b =  RflagBit::CF.as_int() as u64;
                bit_set_non_zero(rflags, b);
            }
            if (rax == 0 && bits_p::spec_bit_set(rflags, RflagBit::CF.as_int() as u64) == 0) {
                let psize = if is_2m % 2 == 0 {PageSize::Size4k} else {PageSize::Size4k};
                let validated = val % 2 == 1;
                assert(new.hw.rmp === self.hw.rmp.pvalidate(memid, psize, self.hw.rmp@.gpn, validated).to_result());
                assert(self.hw.rmp.pvalidate(memid, psize, self.hw.rmp@.gpn, validated).is_Ok());
                assert(new.hw.rmp !== self.hw.rmp);
                assert(self.hw.hvupdate_rel(self.sw));
                assert(new.hw.rmp@.is_valid());
                assert(new.hw.rmp@.spec_validated() == validated);
                assert(new.sw.rmp@.spec_validated() == validated);
                assert(new.hw.hvupdate_rel(new.sw));
                assert(new.sw.pte() === new.sw.pte.last());
            }
        }*/
    pub open spec fn rmpadjust_ret(
        self,
        new: Self,
        rax: u64,
        vaddr: int,
        is_2m: int,
        attr: RmpAttrSpec,
    ) -> bool {
        let psize = if is_2m % 2 == 0 {
            PageSize::Size4k
        } else {
            PageSize::Size4k
        };
        &&& self.hw.rmpadjust_ret(
            new.hw,
            rax,
            self.sw.rmp@.asid,
            vaddr,
            psize,
            attr.vmpl(),
            attr.is_vmsa(),
            attr.perms(),
        )
        &&& if (rax == 0) {
            new.sw.ensures_rmpadjust(self.sw, attr)
        } else {
            new.sw === self.sw
        }
    }
}

} // verus!

================
File: ./source/verismo/src/ptr/snp/mod.rs
================

pub mod rmp;
mod snp_s;
mod snp_u;

pub use rmp::*;
pub use snp_s::*;
pub use snp_u::*;

use crate::arch::addr_s::*;
use crate::ptr::*;
use crate::tspec::*;
use crate::*;

================
File: ./source/verismo/src/ptr/raw_ptr_t.rs
================

use super::*;

verus! {

pub open spec fn spec_set_zeros(old_perm: SnpPointsToBytes, perm: SnpPointsToBytes) -> bool {
    &&& perm.range() === old_perm.range()
    &&& perm.snp() === old_perm.snp()
    &&& perm.wf()
    &&& perm.is_assign_with(Seq::new(perm.range().1, |i| u8_s::spec_constant(0)@))
    &&& perm.bytes().is_constant()
}

pub open spec fn spec_mem_copy(
    bytes: SecSeqByte,
    dst_perm: SnpPointsToBytes,
    old_dst_perm: SnpPointsToBytes,
) -> bool {
    &&& dst_perm.range() === old_dst_perm.range()
    &&& dst_perm.snp() === old_dst_perm.snp()
    &&& dst_perm.wf()
    &&& bytes.len() == dst_perm.range().1 ==> dst_perm.bytes() =~~= bytes
    &&& bytes.len() < dst_perm.range().1 ==> dst_perm.bytes() =~~= bytes
        + old_dst_perm.bytes().skip(bytes.len() as int)
    &&& bytes.len() > dst_perm.range().1 ==> dst_perm.bytes() =~~= bytes.take(
        dst_perm.range().1 as int,
    )
}

pub open spec fn spec_mem_copy_onepage_start(i: int, dst_addr: int, size: nat) -> int {
    let start = i.to_addr() - dst_addr;
    if start < size {
        start
    } else {
        size as int
    }
}

pub open spec fn spec_mem_copy_onepage_end(i: int, dst_addr: int, size: nat) -> int {
    let start = i.to_addr() - dst_addr;
    let end = if (i + 1).to_addr() - dst_addr > size {
        size as int
    } else {
        (i + 1).to_addr() - dst_addr
    };
    end
}

pub open spec fn spec_mem_copy_page(
    i: int,
    dst_addr: int,
    size: nat,
    bytes: SecSeqByte,
    dst_perm: SnpPointsToBytes,
    old_dst_perm: SnpPointsToBytes,
) -> bool {
    let start = spec_mem_copy_onepage_start(i, dst_addr, size);
    let end = spec_mem_copy_onepage_end(i, dst_addr, size);
    spec_mem_copy(bytes.subrange(start, end), dst_perm, old_dst_perm)
}

/// mem_set_zeros will leave the data as non-secret.
#[inline(always)]
#[verifier(external_body)]
pub fn mem_set_zeros(addr: usize, size: usize, Tracked(perm): Tracked<&mut SnpPointsToRaw>)
    requires
        old(perm)@.snp_wf_range((addr as int, size as nat)),
    ensures
        spec_set_zeros(old(perm)@, perm@),
{
    unsafe {
        core::intrinsics::volatile_set_memory(addr as *mut u8, 0, size);
    }
}

#[inline(always)]
#[verifier(external_body)]
pub fn mem_set_zeros2(
    addr: usize,
    size: usize,
    Tracked(perms): Tracked<&mut Map<int, SnpPointsToRaw>>,
)
    requires
        forall|i|
            (addr as int).to_page() <= i < ((addr + size) as int).to_page() ==> #[trigger] old(
                perms,
            ).contains_key(i) && old(perms)[i]@.snp_wf_range(
                ((i as int).to_addr(), PAGE_SIZE as nat),
            ),
    ensures
        perms.dom() =~~= old(perms).dom(),
        forall|i|
            (addr as int).to_page() <= i < ((addr + size) as int).to_page() ==> spec_set_zeros(
                old(perms)[i]@,
                perms[i]@,
            ),
{
    unsafe {
        core::intrinsics::volatile_set_memory(addr as *mut u8, 0, size);
    }
}

#[inline(always)]
#[verifier(external_body)]
pub fn mem_copy(
    src_addr: usize,
    dst_addr: usize,
    size: usize,
    Tracked(src_perm): Tracked<&SnpPointsToRaw>,
    Tracked(dst_perm): Tracked<&mut SnpPointsToRaw>,
)
    requires
        src_perm@.wf_not_null((src_addr as int, size as nat)),
        old(dst_perm)@.wf_not_null((dst_addr as int, size as nat)),
    ensures
        spec_mem_copy(src_perm@.bytes(), dst_perm@, old(dst_perm)@),
{
    unsafe {
        core::ptr::copy(src_addr as *mut u8, dst_addr as *mut u8, size);
    }
}

#[inline(always)]
#[verifier(external_body)]
pub fn mem_copy_to_pages(
    src_addr: usize,
    dst_addr: usize,
    size: usize,
    Tracked(src_perm): Tracked<&SnpPointsToRaw>,
    Tracked(dst_perm): Tracked<&mut Map<int, SnpPointsToRaw>>,
)
    requires
        forall|i|
            (dst_addr as int).to_page() <= i < ((dst_addr + size) as int).to_page() ==> old(
                dst_perm,
            ).contains_key(i),
        forall|i|
            old(dst_perm).contains_key(i) ==> old(dst_perm)[i]@.wf_not_null(
                ((i as int).to_addr(), PAGE_SIZE as nat),
            ),
        src_perm@.wf_not_null((src_addr as int, size as nat)),
        (dst_addr as int) % (PAGE_SIZE as int) == 0,
    ensures
        dst_perm.dom() =~~= old(dst_perm).dom(),
        forall|i: int| #[trigger]
            dst_perm.contains_key(i) ==> spec_mem_copy_page(
                i,
                dst_addr as int,
                size as nat,
                src_perm@.bytes(),
                dst_perm[i]@,
                old(dst_perm)[i]@,
            ),
{
    unsafe {
        core::ptr::copy(src_addr as *mut u8, dst_addr as *mut u8, size);
    }
}

} // verus!

================
File: ./source/verismo/src/ptr/ptr_s.rs
================

use super::*;

verus! {

pub open spec fn inv_snp_value<T: IsConstant + WellFormed>(snp: SwSnpMemAttr, val: T) -> bool {
    &&& val.wf()
    &&& !snp.is_confidential_to(1) ==> val.is_constant_to(1)
    &&& !snp.is_confidential_to(2) ==> val.is_constant_to(2)
    &&& !snp.is_confidential_to(3) ==> val.is_constant_to(3)
    &&& !snp.is_confidential_to(4) ==> val.is_constant_to(4)
}

impl<V: IsConstant + WellFormed + SpecSize> SnpPPtr<V> {
    #[verifier(external_body)]
    pub broadcast proof fn axiom_id_equal(&self, other: Self)
        ensures
            (self.id() == other.id()) == (*self === other),
    {
    }
}

impl<V: IsConstant + WellFormed + SpecSize> SnpMemAttrTrait for SnpPointsToData<V> {
    open spec fn snp(&self) -> SwSnpMemAttr {
        self.snp.sw
    }

    open spec fn hw_snp(&self) -> HwSnpMemAttr {
        self.snp.hw
    }
}

impl<V: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> SnpPointsToData<V> {
    pub open spec fn spec_write_field_rel<
        F: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>,
    >(self, prev: Self, offset: nat, val: Option<F>) -> bool {
        &&& if let Some(v) = val {
            &&& field_set(prev.get_value(), self.get_value(), offset, v)
            &&& self.value().is_Some()
        } else {
            self.value().is_None()
        }
        &&& self.ptr == prev.ptr
        &&& self.snp() === prev.snp()
        &&& self.wf()
    }
}

impl<V: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> VTypeCast<
    SnpPointsToBytes,
> for SnpPointsToData<V> {
    open spec fn vspec_cast_to(self) -> SnpPointsToBytes {
        SnpPointsToBytes {
            pptr: self.ptr,
            snp_bytes: self.value().get_Some_0().vspec_cast_to(),
            snp: self.snp,
        }
    }
}

impl<V: IsConstant + WellFormed + SpecSize> SnpPointsToData<V> {
    pub open spec fn sw_eq(self, rhs: Self) -> bool {
        &&& self.snp() === rhs.snp()
        &&& self.value() === rhs.value()
        &&& self.id() === rhs.id()
    }

    pub open spec fn only_val_updated(self, rhs: Self) -> bool {
        &&& self.snp() === rhs.snp()
        &&& self.id() === rhs.id()
        &&& self.value().is_Some()
        &&& self.wf()
    }

    pub open spec fn spec_write_rel(self, prev: Self, val: Option<V>) -> bool {
        &&& if let Some(v) = val {
            &&& self.value().get_Some_0() === v
            &&& self.value().is_Some()
        } else {
            self.value().is_None()
        }
        &&& self.ptr == prev.ptr
        &&& self.snp() === prev.snp()
        &&& self.hw_snp().hvupdate_rel(prev.hw_snp())
    }

    pub open spec fn spec_read_rel(self, val: V) -> bool {
        &&& self.value().is_Some() && self.hw_snp().is_vmpl0_private() ==> {
            self.value().get_Some_0() === val
        }
        &&& inv_snp_value(self.snp(), val)
        &&& self.snp.valid_access(self.id(), spec_size::<V>(), crate::arch::rmp::perm_s::Perm::Read)
    }
}

impl<V: IsConstant + WellFormed + SpecSize> IsConstant for SnpPointsToData<V> {
    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        self.value.is_Some() && self.value.get_Some_0().is_constant_to(vmpl)
    }

    open spec fn is_constant(&self) -> bool {
        self.value.is_Some() && self.value.get_Some_0().is_constant()
    }
}

impl<V: IsConstant + WellFormed + VTypeCast<SecSeqByte> + SpecSize> SnpPointsTo<V> {
    #[verifier(external_body)]
    pub proof fn trusted_into_raw(tracked self) -> (tracked points_to_raw: SnpPointsToRaw)
        requires
            self@.value.is_Some(),
            self@.wf(),
        ensures
            points_to_raw@ === self@.vspec_cast_to(),
            self@ === points_to_raw@.vspec_cast_to(),
            points_to_raw@.wf(),
    {
        unimplemented!{}
    }

    pub proof fn tracked_into_raw(tracked self) -> (tracked points_to_raw: SnpPointsToRaw)
        requires
            self@.value.is_Some(),
            self@.wf(),
        ensures
            points_to_raw@ === self@.vspec_cast_to(),
            self@ === points_to_raw@.vspec_cast_to(),
            points_to_raw@.wf(),
            self@.get_value().is_constant() ==> points_to_raw@.bytes().is_constant(),
            self@.get_value().is_constant_to(1) ==> points_to_raw@.bytes().is_constant_to(1),
            self@.get_value().is_constant_to(1) ==> points_to_raw@.bytes().is_constant_to(1),
            self@.get_value().is_constant_to(1) ==> points_to_raw@.bytes().is_constant_to(1),
            self@.get_value().is_constant_to(1) ==> points_to_raw@.bytes().is_constant_to(1),
    {
        proof_into_is_constant::<V, SecSeqByte>(self@.get_value());
        self.trusted_into_raw()
    }
}

} // verus!

================
File: ./source/verismo/src/ptr/ptr_e.rs
================

use super::*;
use crate::global::IsConsole;

verus! {

impl<V: IsConstant + WellFormed + SpecSize> SnpPPtr<V> {
    #[inline(always)]
    #[verifier(external_body)]
    pub const fn from_usize(u: usize_t) -> (p: Self)
        requires
            u.wf(),
        ensures
            p.uptr === u,
            p.wf(),
    {
        SnpPPtr { uptr: u, dummy: Ghost::assume_new() }
    }

    pub const fn nullptr() -> (p: Self)
        ensures
            p.uptr == INVALID_ADDR,
            p.uptr.is_constant(),
            p.wf(),
            p.is_null(),
    {
        Self::from_usize(INVALID_ADDR)
    }

    #[inline(always)]
    pub const fn to_usize(&self) -> (u: usize_t)
        ensures
            u as int == self.id(),
            u === self.uptr,
    {
        self.uptr
    }

    #[inline(always)]
    pub fn as_u64(&self) -> (u: u64_t)
        ensures
            u as int == self.id(),
            u === self.uptr.vspec_cast_to(),
    {
        self.uptr as u64
    }

    pub fn to<V2: IsConstant + WellFormed + SpecSize>(&self) -> (ret: SnpPPtr<V2>)
        ensures
            ret.id() == self.id(),
    {
        SnpPPtr::from_usize(self.to_usize())
    }

    pub fn check_valid(&self) -> (ret: bool)
        requires
            self.is_constant(),
        ensures
            ret === self.not_null(),
    {
        self.uptr.check_valid_addr(size_of::<V>())
    }
}

} // verus!
verus! {

/// SNP safe pointer usage
impl<V: IsConstant + WellFormed + SpecSize> SnpPPtr<V> {
    #[inline(always)]
    pub fn take(&self, Tracked(perm): Tracked<&mut SnpPointsTo<V>>) -> (v: V)
        requires
            old(perm)@.ptr_not_null_wf(*self),
            old(perm)@.value.is_Some(),
        ensures
            perm@.spec_write_rel(old(perm)@, None),
            old(perm)@.ensures_read(v),
            perm@.wf(),
            v.wf(),
    {
        let v = self._take(Tracked(perm));
        proof {
            HwSnpMemAttr::proof_hvupdate_rel_propograte(
                perm@.hw_snp(),
                old(perm)@.hw_snp(),
                old(perm)@.snp(),
            );
        }
        v
    }

    #[inline(always)]
    pub fn put(&self, Tracked(perm): Tracked<&mut SnpPointsTo<V>>, in_v: V)
        requires
            old(perm)@.ptr_not_null_wf(*self),
            old(perm)@.value().is_None(),
            old(perm)@.wf_value(in_v),
            inv_snp_value(old(perm)@.snp(), in_v),
        ensures
            perm@.spec_write_rel(old(perm)@, Some(in_v)),
            perm@.snp.wf(),
            perm@.wf(),
    {
        self._put(Tracked(perm), in_v);
        proof {
            HwSnpMemAttr::proof_hvupdate_rel_propograte(
                perm@.hw_snp(),
                old(perm)@.hw_snp(),
                old(perm)@.snp(),
            );
        }
    }

    /// Swaps the `in_v: V` passed in as an argument with the value in memory.
    #[inline(always)]
    pub fn replace(&self, Tracked(perm): Tracked<&mut SnpPointsTo<V>>, in_v: V) -> (out_v: V)
        requires
            old(perm)@.ptr_not_null_wf(*self),
            old(perm)@.is_assigned(),
            old(perm)@.wf_value(in_v),
        ensures
            perm@.spec_write_rel(old(perm)@, Some(in_v)),
            old(perm)@.ensures_read(out_v),
            perm@.wf(),
    {
        let ret = self._replace(Tracked(perm), in_v);
        proof {
            HwSnpMemAttr::proof_hvupdate_rel_propograte(
                perm@.hw_snp(),
                old(perm)@.hw_snp(),
                old(perm)@.snp(),
            );
        }
        ret
    }

    #[inline(always)]
    pub fn borrow<'a>(&self, Tracked(perm): Tracked<&'a SnpPointsTo<V>>) -> (v: &'a V)
        requires
            perm@.wf_not_null_at(self.id()) || perm@.is_wf_pte(self.id()),
            perm@.value.is_Some(),
            perm@.snp().is_vmpl0_private(),
        ensures
            perm@.ensures_read(*v),
            v.wf(),
    {
        self._borrow(Tracked(perm))
    }
}

impl<F: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte> + Clone> SnpPPtr<F> {
    pub fn copy_with<T2: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(
        &self,
        Tracked(perm): Tracked<SnpPointsTo<T2>>,
    ) -> (ret: (F, Tracked<SnpPointsTo<T2>>))
        requires
            perm@.wf_not_null_at(perm@.id()),
            perm@.value.is_Some(),
            inside_range(self.range_id(), perm@.range_id()),
            spec_size::<T2>() > 0,
            spec_size::<F>() > 0,
            self.is_constant(),
        ensures
            perm@.snp().ensures_read(
                Some(field_at(perm@.get_value(), (self.id() - perm@.id()) as nat)),
                ret.0,
            ),
            ret.1@@ === perm@,
    {
        proof {
            reveal(field_at);
        }
        let ghost offset = self.id() - perm@.id();
        let ghost val = perm@.get_value();
        let tracked perm = perm.tracked_into_raw();
        let tracked (left, right) = perm.trusted_split(offset as nat);
        let tracked (fieldp, right) = right.trusted_split(spec_size::<F>());
        assert(fieldp@.bytes() =~~= perm@.bytes().subrange(
            offset as int,
            offset as int + spec_size::<F>(),
        ));
        let tracked fieldp = fieldp.tracked_into();
        let ret = self._copy(Tracked(&fieldp));
        let tracked ret_perm = left.trusted_join(fieldp.tracked_into_raw().trusted_join(right));
        assert(ret_perm@.bytes() =~~= perm@.bytes());
        assert(val.vspec_cast_to() === perm@.bytes());
        assert(val === perm@.bytes().vspec_cast_to());
        (ret, Tracked(ret_perm.tracked_into()))
    }
}

impl<F: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>> SnpPPtr<F> {
    #[inline(always)]
    pub fn replace_with<T2: IsConstant + WellFormed + SpecSize + VTypeCast<SecSeqByte>>(
        &self,
        in_v: F,
        Tracked(perm): Tracked<SnpPointsTo<T2>>,
    ) -> (ret: (F, Tracked<SnpPointsTo<T2>>))
        requires
            in_v.wf(),
            (!perm@.snp().is_vm_confidential() ==> in_v.is_constant()),
            (perm)@.wf_not_null_at((perm)@.id()),
            (perm)@.value.is_Some(),
            inside_range(self.range_id(), (perm)@.range_id()),
            spec_size::<T2>() > 0,
            spec_size::<F>() > 0,
            self.is_constant(),
        ensures
            ret.1@@.spec_write_field_rel((perm)@, (self.id() - ret.1@@.id()) as nat, Some(in_v)),
            ret.1@@.wf(),
    {
        proof {
            reveal(field_set);
        }
        let ghost offset = self.id() - perm@.id();
        let ghost val = perm@.get_value();
        let tracked perm = perm.tracked_into_raw();
        let ghost old_perm = perm;
        let tracked (left, right) = perm.trusted_split(offset as nat);
        let tracked (fieldp, right) = right.trusted_split(spec_size::<F>());
        assert(fieldp@.bytes() =~~= perm@.bytes().subrange(
            offset as int,
            offset as int + spec_size::<F>(),
        ));
        let tracked mut fieldp = fieldp.tracked_into();
        let ret = self.replace(Tracked(&mut fieldp), in_v);
        let tracked ret_perm = left.trusted_join(fieldp.tracked_into_raw().trusted_join(right));
        proof {
            proof_cast_from_seq_unique(fieldp@.get_value());
            assert(left@.bytes() =~~= old_perm@.bytes().take(offset as int));
            assert(right@.bytes() =~~= old_perm@.bytes().skip(
                offset as int + spec_size::<F>() as int,
            ));
            assert(ret_perm@.bytes() =~~= left@.bytes() + in_v.vspec_cast_to() + right@.bytes());
        }
        (ret, Tracked(ret_perm.tracked_into()))
    }
}

} // verus!
verus! {

impl<T: IsConstant + WellFormed + SpecSize, const N: usize_t> SnpPPtr<Array<T, N>> {
    #[inline(always)]
    pub fn index(&self, i: usize) -> (ret: SnpPPtr<T>)
        requires
            i < N,
            0 < spec_size::<T>() < 0x10000,
            i < 0x10000,
            self.wf(),
            self.not_null(),
        ensures
            ret.id() == self.id() + i * spec_size::<T>(),
            inside_range(ret.range_id(), self.range_id()),
            self.is_constant() ==> ret.is_constant(),
    {
        let unit = size_of::<T>();
        let ghost totalsize = spec_size::<Array<T, N>>();
        proof {
            assert(self.id().spec_valid_addr_with(totalsize));
            Array::<T, N>::reveal_N(N as nat);
            assert(totalsize == N * spec_size::<T>());
            assert(unit == spec_size::<T>());
            assert(i * unit <= N * unit - unit) by (nonlinear_arith)
                requires
                    0 <= i < N,
                    0 < unit,
            ;
            assert(0 <= (i * unit) < MAXU64) by (nonlinear_arith)
                requires
                    0 <= i < 0x10000,
                    0 < unit < 0x10000,
            ;
        }
        let offset: usize = i * unit;
        SnpPPtr::from_usize(self.to_usize() + offset)
    }
}

} // verus!

================
File: ./source/verismo/src/arch/reg/mod.rs
================

use verismo_macro::*;

use crate::tspec::*;

verus! {

#[is_variant]
pub enum RegName {
    // register fields
    Rflags,
    Rax,
    Rsp,
    Cs,
    Ds,
    Ss,
    Es,
    Gs,
    Cpl,
    Cr0,
    Cr1,
    Cr2,
    Cr3,
    Cr4,
    XCr0,
    IdtrBaseLimit,
    GdtrBaseLimit,
    MSR(u32),
}

#[derive(SpecIntEnum)]
#[is_variant]
pub enum RflagBit {
    CF = 0,  // Carry flag
    R1 = 1,
    PF = 2,
    R2 = 3,
    AF = 4,
    R3 = 5,
    ZF = 6,
    SF = 7,
    TF = 8,  // Trap flag
    IF = 9,  // Interrupt enable flag
    DF = 10,
    ID = 21,  // Able to use CPUID
}

} // verus!
crate::macro_const! {
    pub const MSR_GHCB_BASE: u32 = 0xc0010130u32;
    pub const MSR_GS_BASE: u32 = 0xc0000101u32;
    pub const MSR_EFER_BASE: u32 = 0xc0000080;
    pub const MSR_SEV_STATUS: u32 = 0xc0010131u32;
}

impl RegName {
    verus! {

pub open spec fn spec_is_shared(&self) -> bool {
    match *self {
        RegName::MSR(regval) if regval == MSR_GHCB_BASE => true,
        _ => false,
    }
}

} // verus!
}

pub type RegValType = u64;

pub type RegDB = FMap<RegName, RegValType>;

verus! {

impl RegDB {
    pub open spec fn reg_inv(&self) -> bool {
        &&& self[RegName::Cpl] == 0
    }
}

} // verus!

================
File: ./source/verismo/src/arch/crypto/mod.rs
================

use crate::tspec::*;

pub mod encdec;

verus! {

// crypto_mask is unknown and a perfect secret;
// key is the secret an attacker would like to know
// but would be reused for multiple plaintext.
// plaintext from victim is also a secret input.
// plaintext from attacker is attacker-controller inputs.
//
// Without crypto_mask, encryption does not hold noninterference property when considering the secret plaintext.
//
// With crypto_mask, we can recover the noninterference property by assuming decrypt will return a value related to crypto_mask.
pub struct CryptoMask(pub int);

pub struct SymKey<T> {
    pub key: T,
}

pub struct Encrypted<K, T> {
    pub data: T,
    pub key: K,
    pub crypto_mask: T,
}

} // verus!

================
File: ./source/verismo/src/arch/crypto/encdec.rs
================

use super::*;
use crate::tspec::*;

verus! {

impl CryptoMask {
    pub spec fn get_mask<T>(&self) -> T;
}

} // verus!
verus! {

pub trait SpecEncrypt<T> {
    // crypto_mask is an unknown value to any entity.
    // crypto_mask ensures that the security of crypto is not broken.
    spec fn encrypt(&self, plain: T, crypto_mask: T) -> Encrypted<Self, T> where
        Self: core::marker::Sized,
    ;

    // It is used to allow spec_decrypt return value not related to cipher.
    spec fn decrypt(&self, cipher: Encrypted<Self, T>) -> T where Self: core::marker::Sized;
}

impl<K, T> SpecEncrypt<T> for SymKey<K> {
    open spec fn encrypt(&self, plain: T, crypto_mask: T) -> Encrypted<Self, T> {
        Encrypted { data: plain, key: *self, crypto_mask: crypto_mask }
    }

    open spec fn decrypt(&self, cipher: Encrypted<Self, T>) -> T {
        if cipher.key === *self {
            cipher.data
        } else {
            cipher.crypto_mask
        }
    }
}

pub trait SpecSignature<T> {
    spec fn spec_sign(&self) -> T where Self: core::marker::Sized;

    spec fn spec_verify(&self) -> bool where Self: core::marker::Sized;
}

} // verus!

================
File: ./source/verismo/src/arch/tlb/tlb_s.rs
================

use super::*;

verus! {

impl TLB {
    /// Load an entry
    #[verifier(inline)]
    pub open spec fn to_mem_map(&self, memid: MemID) -> MemMap<GuestVir, GuestPhy> {
        MemMap { db: self.spec_db()[memid] }
    }

    #[verifier(inline)]
    pub open spec fn load(&self, idx: TLBIdx, entry: SpecGuestPTEntry) -> Self {
        self.spec_set_db(self.spec_db().insert(idx.0, self.db[idx.0].insert(idx.1, entry)))
    }

    /// Remove an entry
    #[verifier(inline)]
    pub open spec fn invlpg(&self, idx: TLBIdx) -> Self {
        self.spec_set_db(self.spec_db().insert(idx.0, self.db[idx.0].remove(idx.1)))
    }

    /// Remove all entries for memid
    #[verifier(inline)]
    pub open spec fn flush_memid(&self, memid: MemID) -> Self {
        self.spec_set_db(self.spec_db().insert(memid, Map::empty()))
    }
}

} // verus!

================
File: ./source/verismo/src/arch/tlb/def_s.rs
================

use verismo_macro::*;

use crate::arch::addr_s::*;
use crate::arch::entities::MemID;
use crate::arch::pgtable::*;
use crate::tspec::*;

verus! {

/// Define TLB as a structural struct;
/// Use the hidden private int to represent the instance
/// but will never use the hidden int directly
#[derive(SpecSetter, SpecGetter)]
pub struct TLB {
    pub db: FMap<MemID, Map<GVN, SpecGuestPTEntry>>,
}

pub struct TLBIdx(pub MemID, pub GVN);

} // verus!

================
File: ./source/verismo/src/arch/tlb/mod.rs
================

use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::arch::pgtable::*;
use crate::tspec::*;

mod def_s;
pub use def_s::*;
mod tlb_p;
mod tlb_s;
mod tlb_u;

================
File: ./source/verismo/src/arch/tlb/tlb_p.rs
================

use super::*;
use crate::arch::attack::*;

verus! {

impl TLB {
    pub proof fn lemma_model1_inv_encrypted_priv_mem(&self, other: &Self, memid: MemID)
        requires
            self.model1_eq(other, memid),
            other.inv_encrypted_priv_mem(memid),
        ensures
            self.inv_encrypted_priv_mem(memid),
    {
        reveal(MemMap::inv_encrypted_priv_mem);
        let memmap = self.to_mem_map(memid);
        assert forall|gvn: GVN|
            gvn.is_valid() && memtype(
                memid,
                memmap.translate(gvn).get_Some_0(),
            ).need_c_bit() implies #[trigger] memmap.is_encrypted_or_none(gvn) by {
            if memmap.translate(gvn).is_Some() {
                assert(other.to_mem_map(memid).is_encrypted_or_none(gvn));
            }
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/tlb/tlb_u.rs
================

use super::*;
use crate::arch::attack::*;

verus! {

impl Model1Eq for TLB {
    open spec fn model1_eq(&self, other: &Self, memid: MemID) -> bool {
        self.db.spec_index(memid).submap_of(other.db.spec_index(memid))
    }
}

impl Model2Eq for TLB {
    open spec fn model2_eq(&self, other: &Self) -> bool {
        forall|memid|
            (#[trigger] self.db.spec_index(memid)).submap_of(#[trigger] other.db.spec_index(memid))
    }
}

} // verus!
verus! {

impl TLB {
    pub open spec fn inv_encrypted_priv_mem(&self, memid: MemID) -> bool {
        let memmap = self.to_mem_map(memid);
        memmap.inv_encrypted_priv_mem(memid)
    }
}

} // verus!

================
File: ./source/verismo/src/arch/attack/mod.rs
================

use crate::arch::entities::*;
use crate::tspec::*;

verus! {

pub trait Model1Eq {
    // self state is derived from other state when hypervisor and other VMPLs may actively change the state.
    spec fn model1_eq(&self, other: &Self, memid: MemID) -> bool;
}

pub trait Model2Eq {
    // self state is derived from other state when hypervisor may actively change the state.
    spec fn model2_eq(&self, other: &Self) -> bool;
}

pub spec fn spec_attack() -> bool;

} // verus!

================
File: ./source/verismo/src/arch/x64/def_s.rs
================

use verismo_macro::*;

use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::arch::mem::MemDB;
use crate::arch::memop::MemOp;
use crate::arch::reg::*;
use crate::tspec::*;

verus! {

#[is_variant]
pub enum Archx64Op {
    MemOp(MemOp<GuestVir>, CPU),
    RegWrite(CpuMemID, RegName, RegValType),
    RegRead(CpuMemID, RegName),
    VMGExit(CpuMemID),
    LoopHalt(CpuMemID),
}

#[is_variant]
pub enum Archx64Ret {
    None,
    ReadRet(ByteStream),
    RegValue(RegValType),
}

pub spec fn current_cpu() -> CPU;

#[derive(SpecGetter, SpecSetter)]
pub struct Archx64 {
    pub memdb: MemDB,
    pub regdb: Map<CpuMemID, RegDB>,
    pub entities: Map<MemID, Map<CPU, bool>>,
}

#[is_variant]
pub enum AECode {
    Mc,
    Intr,
    Nmi,
    Smi,
    Init,
    VIntr,
    Pause,
    Hlt,
    Npf,
    Vmmcall,
    VMGExit,
    Busy,
    Others,
}

#[is_variant]
pub enum NAECode {
    Npf,
    Vmmcall,
    Halt,
    NotValidated(Archx64Op),
    // TODO(ziqiao): Model more exits
    Others,
}

#[is_variant]
pub enum ExceptionCode {
    PFault(Archx64Op),
    GP(Archx64Op),
}

} // verus!

================
File: ./source/verismo/src/arch/x64/x64_s.rs
================

use super::def_s::Archx64;
use super::*;

verus! {

impl Archx64Op {
    pub open spec fn is_valid(&self) -> bool {
        match self {
            Archx64Op::MemOp(memop, _) => memop.is_valid(),
            _ => true,
        }
    }

    pub open spec fn start_cpu_with_vmsa(&self) -> Option<CPU> {
        match self {
            Archx64Op::MemOp(memop, _) => {
                if let MemOp::RmpOp(RmpOp::RmpAdjust(_, params)) = memop {
                    if params.vmsa {
                        Option::Some(memop.to_page().as_int() as nat)
                    } else {
                        Option::None
                    }
                } else {
                    Option::None
                }
            },
            _ => { Option::None },
        }
    }

    pub open spec fn cpu_memid(&self) -> CpuMemID {
        match *self {
            Archx64Op::MemOp(memop, cpu) => CpuMemID(cpu, memop.to_memid()),
            Archx64Op::RegWrite(id, reg_name, _) => id,
            Archx64Op::RegRead(id, _) => id,
            Archx64Op::VMGExit(id) => id,
            Archx64Op::LoopHalt(id) => id,
        }
    }

    pub open spec fn cpu(&self) -> CPU {
        self.cpu_memid().cpu()
    }

    pub open spec fn to_memid(&self) -> MemID {
        self.cpu_memid().memid()
    }

    pub open spec fn memop(&self) -> MemOp<GuestVir> {
        self.get_MemOp_0()
    }
}

impl Archx64 {
    #[verifier(external_body)]
    pub broadcast proof fn axiom_reg_dom(&self, cpumemid: CpuMemID)
        ensures
            self.spec_regdb().dom().contains(cpumemid),
    {
    }

    #[verifier(external_body)]
    pub broadcast proof fn axiom_entities_dom(&self, memid: MemID)
        ensures
            self.spec_entities().dom().contains(memid),
    {
    }

    pub open spec fn spec_cpu(&self) -> CPU {
        current_cpu()
    }

    pub open spec fn is_run(&self, cpu_memid: CpuMemID) -> bool {
        let memid = cpu_memid.memid();
        let cpu = cpu_memid.cpu();
        self.spec_entities()[memid].dom().contains(cpu) && self.spec_entities()[memid][cpu]
    }

    pub open spec fn is_stop(&self, cpu_memid: CpuMemID) -> bool {
        let memid = cpu_memid.memid();
        let cpu = cpu_memid.cpu();
        self.spec_entities()[memid].dom().contains(cpu) && !self.spec_entities()[memid][cpu]
    }

    pub open spec fn has_stopped(&self, memid: MemID) -> bool {
        exists|cpu|
            self.spec_entities()[memid].dom().contains(cpu)
                && !#[trigger] self.spec_entities()[memid][cpu]
    }

    pub open spec fn no_concurrent_cpu(&self, cpu_memid: CpuMemID) -> bool {
        let memid = cpu_memid.memid();
        let cpus = self.spec_entities()[memid].dom();
        cpus =~~= (set![cpu_memid.cpu()])
    }

    pub open spec fn stop_cpu(&self, memid: MemID, cpu: CPU) -> Self {
        let memid_entries = self.spec_entities()[memid].insert(cpu, false);
        self.spec_set_entities(self.spec_entities().insert(memid, memid_entries))
    }

    pub open spec fn start_cpu(&self, memid: MemID, cpu: CPU) -> Self {
        let memid_entries = self.spec_entities()[memid].insert(cpu, true);
        if !self.is_stop(CpuMemID(cpu, memid)) {
            self.spec_set_entities(self.spec_entities().insert(memid, memid_entries))
        } else {
            *self
        }
    }

    /// TODO
    pub open spec fn spec_hv_update(&self, op: Archx64Op) -> Self {
        self.stop_cpu(op.to_memid(), op.cpu())
    }

    /// TODO
    pub open spec fn spec_vc_handle(&self, op: Archx64Op, err: NAECode) -> Self {
        self.stop_cpu(op.to_memid(), op.cpu())
    }

    /// TODO
    pub open spec fn spec_exception_handle(&self, op: Archx64Op, err: ExceptionCode) -> Self {
        self.stop_cpu(op.to_memid(), op.cpu())
    }

    /// TODO
    pub open spec fn spec_vmexit_handle(&self, op: Archx64Op, err: AECode) -> Archx64 {
        match err {
            AECode::VMGExit => { *self },
            _ => { self.stop_cpu(op.to_memid(), op.cpu()) },
        }
    }

    #[verifier(inline)]
    pub open spec fn handle_mem_err_fn(err: MemError<MemOp<GuestVir>>) -> (
        bool,
        spec_fn(Self, Archx64Op) -> Self,
    ) {
        match err {
            MemError::NoRam(memop) => (
                true,
                |arch: Self, op: Archx64Op| arch.spec_vmexit_handle(op, AECode::Npf),
            ),
            MemError::NotValidated(memop) => (
                true,
                |arch: Self, op: Archx64Op|
                    arch.spec_vc_handle(
                        op,
                        NAECode::NotValidated(Archx64Op::MemOp(memop, op.cpu())),
                    ),
            ),
            MemError::NestedPF(memop) => (
                true,
                |arch: Self, op: Archx64Op| arch.spec_vmexit_handle(op, AECode::Npf),
            ),
            MemError::PageFault(memop) => (
                true,
                |arch: Self, op: Archx64Op|
                    arch.spec_exception_handle(
                        op,
                        ExceptionCode::PFault(Archx64Op::MemOp(memop, op.cpu())),
                    ),
            ),
            MemError::RmpOp(fault, memop) => if memop.is_RmpOp() {
                match fault {
                    RmpFault::Unsupported => {
                        (
                            true,
                            |arch: Self, op: Archx64Op|
                                arch.spec_exception_handle(
                                    op,
                                    ExceptionCode::GP(Archx64Op::MemOp(memop, op.cpu())),
                                ),
                        )
                    },
                    RmpFault::Size => {
                        (
                            false,
                            |arch: Self, op: Archx64Op|
                                arch.op_write_reg(
                                    op.cpu_memid(),
                                    RegName::Rax,
                                    RMP_FAIL_SIZEMISMATCH,
                                ),
                        )
                    },
                    RmpFault::Input => {
                        (
                            false,
                            |arch: Self, op: Archx64Op|
                                arch.op_write_reg(op.cpu_memid(), RegName::Rax, RMP_FAIL_INPUT),
                        )
                    },
                    RmpFault::Perm => {
                        (
                            false,
                            |arch: Self, op: Archx64Op|
                                arch.op_write_reg(
                                    op.cpu_memid(),
                                    RegName::Rax,
                                    RMP_FAIL_PERMISSION,
                                ),
                        )
                    },
                    RmpFault::DoubleVal => {
                        (
                            false,
                            |arch: Self, op: Archx64Op|
                                {
                                    let rflags = arch.spec_regdb()[op.cpu_memid()][RegName::Rflags];
                                    let update = bits_p::spec_bit_set(
                                        rflags as u64,
                                        RflagBit::CF.as_int() as u64,
                                    );
                                    arch.op_write_reg(op.cpu_memid(), RegName::Rflags, update)
                                },
                        )
                    },
                }
            } else {
                // unreacheable
                (true, |arch: Self, op: Archx64Op| arch.stop_cpu(op.to_memid(), op.cpu()))
            },
            MemError::Others(memop) => (
                true,
                |arch: Self, op: Archx64Op|
                    arch.spec_exception_handle(
                        op,
                        ExceptionCode::GP(Archx64Op::MemOp(memop, op.cpu())),
                    ),
            ),
        }
    }

    /// Update the VM status
    /// * The update will trigger corresponding traps handler
    /// * When err is VMExit related, it calls the spec_vmexit_handle.
    pub open spec fn handle_mem_err(&self, op: Archx64Op, err: MemError<MemOp<GuestVir>>) -> Self {
        let cpu_memid = op.cpu_memid();
        Self::handle_mem_err_fn(err).1(*self, op)
    }

    pub open spec fn op(&self, op: Archx64Op) -> Archx64 {
        if !self.is_run(op.cpu_memid()) {
            *self
        } else {
            match op {
                Archx64Op::MemOp(memop, _) => {
                    let init = if let MemOp::RmpOp(rmpop) = memop {
                        self.op_write_reg(op.cpu_memid(), RegName::Rax, 0).op_write_reg(
                            op.cpu_memid(),
                            RegName::Rflags,
                            0,
                        )
                    } else {
                        *self
                    };
                    match init.spec_memdb().op(memop) {
                        ResultWithErr::Ok(newmem) => {
                            let new = init.spec_set_memdb(newmem);
                            let cpu = op.start_cpu_with_vmsa();
                            if cpu.is_Some() {
                                new.start_cpu(op.to_memid(), cpu.get_Some_0())
                            } else {
                                new
                            }
                        },
                        ResultWithErr::Error(newmem, err) => {
                            init.spec_set_memdb(newmem).handle_mem_err(op, err)
                        },
                    }
                },
                Archx64Op::RegWrite(_, name, val) => {
                    if op.cpu() === self.spec_cpu() {
                        self.op_write_reg(op.cpu_memid(), name, val)
                    } else {
                        *self
                    }
                },
                Archx64Op::RegRead(_, _) => { *self },
                Archx64Op::LoopHalt(_) => { self.stop_cpu(op.to_memid(), op.cpu()) },
                Archx64Op::VMGExit(_) => {
                    if op.cpu() === self.spec_cpu() {
                        self.spec_vmexit_handle(op, AECode::VMGExit)
                    } else {
                        *self
                    }
                },
            }
        }
    }

    /// TODO: link Cr3 with memdb::l0_entry
    pub open spec fn op_write_reg(
        &self,
        memid: CpuMemID,
        name: RegName,
        val: RegValType,
    ) -> Archx64 {
        self.spec_set_regdb(
            self.spec_regdb().insert(memid, self.spec_regdb()[memid].insert(name, val)),
        )
    }

    pub open spec fn validation_ok(&self, cpumemid: CpuMemID) -> bool {
        &&& self.spec_regdb()[cpumemid][RegName::Rax] == 0
        &&& !spec_has_bit_set(self.spec_regdb()[cpumemid][RegName::Rflags], 0)
    }

    pub open spec fn rmpadjust_ok(&self, cpumemid: CpuMemID) -> bool {
        &&& self.spec_regdb()[cpumemid][RegName::Rax] == 0
    }

    pub open spec fn ret(&self, op: Archx64Op) -> Archx64Ret {
        match op {
            Archx64Op::MemOp(memop, _) => {
                match memop {
                    MemOp::Read(_, _) => {
                        if self.op(op).is_run(op.cpu_memid()) {
                            Archx64Ret::ReadRet(self.spec_memdb().ret(memop))
                        } else {
                            Archx64Ret::ReadRet(ByteStream::empty())
                        }
                    },
                    _ => { Archx64Ret::None },
                }
            },
            Archx64Op::RegRead(memid, name) => {
                Archx64Ret::RegValue(self.spec_regdb()[memid].spec_index(name))
            },
            _ => { Archx64Ret::None },
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/x64/mod.rs
================

use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::arch::errors::*;
use crate::arch::mem::MemDB;
use crate::arch::memop::MemOp;
use crate::arch::reg::*;
use crate::arch::rmp::*;
use crate::tspec::*;
use crate::*;

mod def_s;
pub use def_s::*;

mod x64_p;
mod x64_s;
mod x64_u;

================
File: ./source/verismo/src/arch/x64/x64_p.rs
================

use super::*;

verus! {

impl Archx64 {
    pub proof fn proof_op_inv_reg(&self, memid: MemID, arch_op: Archx64Op)
        requires
            self.inv(memid),
            arch_op.to_memid().is_sm(memid) ==> arch_op.op_requires(memid, self),
        ensures
            self.op(arch_op).inv_regdb_any_cpu(memid),
    {
        let new = self.op(arch_op);
        assert forall|cpu| new.inv_regdb(cpu, memid) by {
            assert(self.inv_regdb(cpu, memid));
            assert(new.spec_regdb()[CpuMemID(cpu, memid)][RegName::Cr3]
                === self.spec_regdb()[CpuMemID(cpu, memid)][RegName::Cr3]);
            assert(new.spec_regdb()[CpuMemID(cpu, memid)][RegName::Cpl]
                === self.spec_regdb()[CpuMemID(cpu, memid)][RegName::Cpl]);
        }
    }

    pub proof fn proof_op_inv(&self, memid: MemID, arch_op: Archx64Op)
        requires
            self.inv(memid),
            memid.to_vmpl().is_VMPL0(),
            arch_op.is_valid(),
            arch_op.to_memid().is_sm(memid) ==> arch_op.op_requires(memid, self),
        ensures
            self.op(arch_op).inv(memid),
    {
        let new = self.op(arch_op);
        if arch_op.is_MemOp() {
            self.spec_memdb().proof_op_inv(memid, arch_op.memop());
        }
        self.proof_op_inv_reg(memid, arch_op);
        assert(new.spec_memdb().inv(memid));
        assert(new.inv_regdb_any_cpu(memid));
        assert(new.spec_entities()[memid].dom().contains(new.spec_cpu()));
    }

    pub proof fn proof_run_indicate_memop_is_ok(&self, memid: MemID, arch_op: Archx64Op)
        requires
            self.inv(memid),
            arch_op.is_MemOp(),
            arch_op.memop().is_Read() || arch_op.memop().is_Write(),
            self.op(arch_op).is_run(arch_op.cpu_memid()),
        ensures
            self.spec_memdb().op(arch_op.memop()).is_Ok() || self.spec_memdb().op(
                arch_op.memop(),
            ).get_Error_1().is_RmpOp(),
            self.spec_memdb().to_mem_map(arch_op.to_memid()).translate(
                arch_op.memop().to_mem().to_page(),
            ).is_Some(),
    {
        if !self.spec_memdb().to_mem_map(arch_op.to_memid()).translate(
            arch_op.memop().to_mem().to_page(),
        ).is_Some() {
            self.spec_memdb().lemma_op_error(arch_op.memop());
            assert(!self.op(arch_op).is_run(arch_op.cpu_memid()));
        }
    }

    pub proof fn lemma_invalid_gmap_error(&self, memid: MemID, arch_op: Archx64Op)
        requires
            self.inv(memid),
            arch_op.is_MemOp(),
            arch_op.memop().use_gmap(),
            self.spec_memdb().to_mem_map(arch_op.to_memid()).translate(
                arch_op.memop().to_page(),
            ).is_None(),
        ensures
            !self.op(arch_op).is_run(arch_op.cpu_memid()) || self.op(arch_op).spec_memdb()
                === self.spec_memdb() || arch_op.memop().is_RmpOp() && self.op(
                arch_op,
            ).spec_regdb()[arch_op.cpu_memid()][RegName::Rax] != 0,
    {
        self.spec_memdb().lemma_op_error(arch_op.memop());
    }

    pub proof fn proof_invalid_gmap_error(&self, memid: MemID, arch_op: Archx64Op)
        requires
            self.inv(memid),
            arch_op.is_MemOp(),
            arch_op.memop().use_gmap(),
            memid.to_vmpl().is_VMPL0(),
            arch_op.to_memid().is_sm(memid),
            self.spec_memdb().to_mem_map_ok(arch_op.to_memid()).translate(
                arch_op.memop().to_page(),
            ).is_None(),
        ensures
            !self.op(arch_op).is_run(arch_op.cpu_memid()) || self.op(arch_op).spec_memdb()
                === self.spec_memdb() || arch_op.memop().is_RmpOp() && self.op(
                arch_op,
            ).spec_regdb()[arch_op.cpu_memid()][RegName::Rax] != 0,
    {
        self.spec_memdb().lemma_mem_map_to_mem_map_ok(memid, arch_op.memop().to_page());
        self.lemma_invalid_gmap_error(memid, arch_op);
    }
}

} // verus!

================
File: ./source/verismo/src/arch/x64/x64_u.rs
================

use super::*;
use crate::arch::attack::*;
use crate::arch::pgtable::SpecPageTableEntry;
verus! {

impl Model1Eq for Archx64 {
    open spec fn model1_eq(&self, other: &Self, memid: MemID) -> bool {
        &&& self.spec_memdb().model1_eq(&other.spec_memdb(), memid)
        &&& equal(self.spec_regdb(), other.spec_regdb())
        &&& equal(self.spec_entities(), other.spec_entities())
        &&& equal(self.spec_cpu(), other.spec_cpu())
    }
}

} // verus!
verus! {

impl Archx64 {
    pub open spec fn inv_regdb(&self, cpu: CPU, memid: MemID) -> bool {
        &&& self.spec_regdb()[CpuMemID(cpu, memid)].reg_inv()
        &&& self.spec_memdb().spec_g_page_table(memid).l0_entry(memid)
            === SpecPageTableEntry::new_val(
            (self.spec_regdb()[CpuMemID(cpu, memid)]).spec_index(RegName::Cr3) as int,
        )
    }

    pub open spec fn inv_regdb_any_cpu(&self, memid: MemID) -> bool {
        forall|cpu| (#[trigger] self.inv_regdb(cpu, memid))
    }

    pub open spec fn inv(&self, memid: MemID) -> bool {
        &&& self.spec_memdb().inv(memid)
        &&& self.inv_regdb_any_cpu(memid)
        &&& self.spec_entities()[memid].dom().contains(self.spec_cpu())
    }
}

impl Archx64Op {
    pub open spec fn reg_write_requires(regname: &RegName, val: &RegValType) -> bool {
        match regname {
            RegName::Cpl => { false },
            RegName::Cr3 => { false },
            _ => { true },
        }
    }

    pub open spec fn op_requires(&self, sm_memid: MemID, arch: &Archx64) -> bool {
        self.to_memid().is_sm(sm_memid) ==> match self {
            Archx64Op::MemOp(memop, _) => { arch.spec_memdb().vop_requires(*memop) },
            Archx64Op::RegWrite(_, reg_name, reg_val) => {
                Self::reg_write_requires(reg_name, reg_val)
            },
            Archx64Op::RegRead(_, _) => { true },
            Archx64Op::VMGExit(_) => { true },
            Archx64Op::LoopHalt(_) => { true },
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/pgtable/def_e.rs
================

use super::*;
use crate::tspec_e::*;

verus! {

#[derive(ExecStruct, NotPrimitive, VTypeCastSec, VTypeCast, SpecSize, WellFormed, IsConstant)]
#[repr(C, align(1))]
pub struct PageTableEntry<T> {
    pub value: crate::tspec_e::u64_s,
    pub dummy: Ghost<T>,
}

} // verus!
verus! {

impl<T> PageTableEntry<T> {
    pub open spec fn view(&self) -> SpecPageTableEntry<T> {
        SpecPageTableEntry { value: self.value.vspec_cast_to(), dummy: self.dummy }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/pgtable/def.rs
================

use verismo_macro::*;

use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::tspec::*;
use crate::{macro_def, BIT64};

verus! {

#[verifier::reject_recursive_types(T)]
#[verifier::reject_recursive_types(PT)]
pub ghost struct MemMap<T, PT> {
    pub db: Map<SpecPage<T>, SpecPageTableEntry<PT>>,
}

pub type SysMap = MemMap<GuestPhy, SysPhy>;

pub type GuestMap = MemMap<GuestVir, GuestPhy>;

macro_def! {MAX_PT_LEVEL: PTLevel::L0}

#[derive(Eq, PartialEq, Structural, SpecIntEnum)]
#[is_variant]
pub enum PteFlag {
    P = 0,  // Present
    W = 1,  // Write
    S = 2,  // Allow both user/supervisor
    PWT = 3,  // Writethrough
    PCD = 4,  // Cache disable
    A = 5,  // Accessed
    D = 6,  // Dirty
    C = 51,  // Encryption
    NX = 63,  // No-execute
}

#[derive(SpecGetter)]
pub ghost struct SpecPageTableEntry<T> {
    pub value: int,
    pub dummy: Ghost<T>,
}

pub struct PTEAccessParam {
    pub memid: MemID,
    pub gvn: GVN,
    pub lvl: PTLevel,
}

pub type SpecGuestPTEntry = SpecPageTableEntry<GuestPhy>;

pub type SpecSysPTEntry = SpecPageTableEntry<SysPhy>;

pub type GuestPTEntry = PageTableEntry<GuestPhy>;

pub type SysPTEntry = PageTableEntry<SysPhy>;

} // verus!
crate::macro_const! {
    #[macro_export]
    pub const PT_ENTRY_SIZE: u64 = 8u64;
    #[macro_export]
    pub const L3_OFFSET: u64 = 39u64;
    #[macro_export]
    pub const L2_OFFSET: u64 = 30u64;
    #[macro_export]
    pub const L1_OFFSET: u64 = 21u64;
    #[macro_export]
    pub const L0_OFFSET: u64 = 12u64;
    #[macro_export]
    pub const PT_ENTRY_NUM_BIT: u64 = 9u64;
}

crate::macro_const! {
    #[macro_export]
    pub const L3_PGSIZE: u64 = BIT64!(39);
    #[macro_export]
    pub const L2_PGSIZE: u64 = BIT64!(30);
    #[macro_export]
    pub const L1_PGSIZE: u64 = BIT64!(21);
    #[macro_export]
    pub const L0_PGSIZE: u64 = BIT64!(12);
    #[macro_export]

    pub const PT_ENTRY_NUM: u64 = BIT64!(9);
}

crate::macro_const! {
    #[macro_export]
    pub const PT_ENTRY_IDX_MASK: u64 = 0x1ff;
}

verus! {

pub spec fn spec_page_frame_bits() -> u64;

#[inline]
#[verifier(external_body)]
pub fn page_frame_bits() -> (ret: u64)
    ensures
        ret == spec_page_frame_bits(),
        48 <= spec_page_frame_bits() < 52,
{
    51
}

} // verus!

================
File: ./source/verismo/src/arch/pgtable/entry_p.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::tspec::*;
use crate::*;

verus! {

pub proof fn lemma_pt_entry_count()
    ensures
        PT_ENTRY_NUM!() == 512,
{
    assert(PT_ENTRY_NUM!() == 512) by (bit_vector);
}

impl PTLevel {
    #[verifier(bit_vector)]
    pub proof fn lemma_size_offset()
        ensures
            0x1000 == BIT64!(12u64),
            0x200000 == BIT64!(21u64),
            0x40000000 == BIT64!(30u64),
            0x8000000000u64 == BIT64!(39u64),
    {
    }

    pub proof fn proof_size_offset(lvl: Self, size: u64, offset: u64)
        requires
            size as int == lvl.spec_pgsize(),
            offset as int == lvl.spec_offset(),
        ensures
            size == (1u64 << offset),
    {
        bit_shl64_pow2_auto();
        assert(size == (1u64 << offset));
    }

    pub proof fn proof_table_index_range<T: AddrType>(&self, vaddr: SpecAddr<T>)
        ensures
            0 <= self.spec_table_index(vaddr) < PT_ENTRY_NUM!(),
    {
        bits_p::bit_shl64_auto();
        proof_div_pos_neg_rel(vaddr.value(), self.spec_pgsize());
        proof_mod_range(vaddr.value() / self.spec_pgsize(), PT_ENTRY_NUM!() as int)
    }

    pub proof fn proof_table_index<T: AddrType>(vaddr: u64, lvl: PTLevel)
        ensures
            0 <= lvl.spec_table_index(GVA::new(vaddr as int)) < PT_ENTRY_NUM!(),
            ((vaddr >> (lvl.spec_offset() as u64)) & PT_ENTRY_IDX_MASK!()) as int
                == lvl.spec_table_index(GVA::new(vaddr as int)),
    {
        Self::lemma_table_index::<T>(vaddr, lvl);
        lemma_pt_entry_count();
    }

    pub proof fn lemma_table_index<T: AddrType>(val: u64, lvl: PTLevel) -> (ret: int)
        ensures
            ret == lvl.spec_table_index(SpecAddr::<T>::new(val as int)),
            (val >> (lvl.spec_offset() as u64)) == (val / (lvl.spec_pgsize() as u64)),
            (val >> (lvl.spec_offset() as u64)) & PT_ENTRY_IDX_MASK!() == (val
                / lvl.spec_pgsize() as u64) % PT_ENTRY_NUM!(),
            ((val >> (lvl.spec_offset() as u64)) & PT_ENTRY_IDX_MASK!()) as int == ret,
    {
        let ret = lvl.spec_table_index(SpecAddr::<T>::new(val as int));
        let t1 = val >> (lvl.spec_offset() as u64);
        bit_shl64_pow2_auto();
        proof_bit_u64_and_rel_mod(t1, PT_ENTRY_NUM!());
        assert(t1 & PT_ENTRY_IDX_MASK!() == t1 % PT_ENTRY_NUM!());
        bit_rsh64_div_rel(val, 12);
        bit_rsh64_div_rel(val, 21);
        bit_rsh64_div_rel(val, 30);
        bit_rsh64_div_rel(val, 39);
        PTLevel::proof_size_offset(lvl, (lvl.spec_pgsize() as u64), (lvl.spec_offset() as u64));
        lemma_pt_entry_count();
        lemma_bits64!();
        bit_shl64_pow2_auto();
        assert(PT_ENTRY_NUM!() == PT_ENTRY_NUM!() as int);
        assert(ret == (val as int / lvl.spec_pgsize()) % (PT_ENTRY_NUM!() as int));
        assert((val as int / lvl.spec_pgsize()) == (val / lvl.spec_pgsize() as u64))
            by (nonlinear_arith)
            requires
                0 < lvl.spec_pgsize() < MAXU64!(),
        ;
        assert(ret == (val / lvl.spec_pgsize() as u64) % PT_ENTRY_NUM!());
        ret
    }

    proof fn test_spec_next_lvl()
        ensures
            PTLevel::L2.parent_lvl().get_Some_0() == PTLevel::L3,
    {
    }
}

impl<T: AddrType> SpecPageTableEntry<T> {
    pub proof fn lemma_each_table_is_one_page(&self, idx: nat)
        requires
            idx < PT_ENTRY_NUM!(),
        ensures
            self.addr_for_idx(idx).to_page() === self.spec_ppn(),
    {
        assert(PT_ENTRY_NUM!() == 512) by {
            bit_shl64_pow2_auto();
        };
        assert(PAGE_SIZE!() === PT_ENTRY_NUM!() * PT_ENTRY_SIZE!());
    }

    proof fn test_flags(entry: Self)
        requires
            entry.spec_value() == 0xff,
        ensures
            !entry.contains_flag(PteFlag::C),
    {
        reveal_with_fuel(spec_nat_pow2, 64);
        // Fixme: z3-4.11.2 need below assert but z3-4.10.1 does not need it
        bit_shl64_auto();
        assert(spec_int_pow2(PteFlag::C.as_int()) == BIT64!(51u64));
        //assert(entry.contains_flag(PteFlag::C) == true);
        //assert(entry.spec_value()/spec_int_pow2(PteFlag::C.as_int()) % 2 == 1);
    }
}

} // verus!

================
File: ./source/verismo/src/arch/pgtable/memmap_p.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::tspec::*;

verus! {

impl<VT: AddrType, PT: AddrType> MemMap<VT, PT> {
    pub proof fn lemma_is_one_to_one_map_two_diff_va(
        &self,
        vpage1: SpecPage<VT>,
        vpage2: SpecPage<VT>,
    )
        requires
            self.is_one_to_one_map(),
            vpage1 !== vpage2,
            self.translate(vpage1).is_Some(),
            self.translate(vpage2).is_Some(),
        ensures
            self.translate(vpage1).get_Some_0() !== self.translate(vpage2).get_Some_0(),
    {
        reveal(MemMap::is_one_to_one_map);
    }

    pub proof fn lemma_one_to_one_map_two_disjoint_vmem(
        &self,
        vmem1: SpecMem<VT>,
        vmem2: SpecMem<VT>,
    )
        requires
            self.is_one_to_one_map(),
            vmem1.disjoint(vmem2),
        ensures
            self.translate_addr_seq(vmem1).disjoint(self.translate_addr_seq(vmem2)),
    {
        let smem1 = self.translate_addr_seq(vmem1);
        let smem2 = self.translate_addr_seq(vmem2);
        if self.translate(vmem1.to_page()).is_None() || self.translate(vmem2.to_page()).is_None() {
            assert(smem1.len() == 0 || smem2.len() == 0);
            assert(smem1.disjoint(smem2));
        } else {
            reveal(MemMap::is_one_to_one_map);
            if smem1.to_page() === smem2.to_page() {
                assert(vmem1.to_page() === vmem2.to_page());
                assert(smem1.disjoint(smem2));
            } else {
                assert(smem1.disjoint(smem2)) by {
                    reveal(MemMap::is_one_to_one_map);
                }
            }
        }
    }

    pub proof fn lemma_two_disjoint_same_page_vmem(&self, vmem1: SpecMem<VT>, vmem2: SpecMem<VT>)
        requires
            vmem1.disjoint(vmem2),
            vmem1.to_page() === vmem2.to_page(),
        ensures
            self.translate_addr_seq(vmem1).disjoint(self.translate_addr_seq(vmem2)),
    {
    }

    pub proof fn lemma_identity_map_is_one_to_one(&self)
        requires
            self.is_identity_map(),
        ensures
            self.is_one_to_one_map(),
    {
        reveal(MemMap::is_identity_map);
        reveal(MemMap::is_one_to_one_map);
        assert forall|vpage: SpecPage<VT>| (#[trigger] self.translate(vpage)).is_Some() implies (
        self.reverse(self.translate(vpage).get_Some_0()).is_Some() && self.reverse(
            self.translate(vpage).get_Some_0(),
        ).get_Some_0() =~= vpage) by {
            assert(self.translate(vpage).get_Some_0().as_int() === vpage.as_int());
            assert(self.reverse(self.translate(vpage).get_Some_0()).is_Some());
            let p = self.reverse(self.translate(vpage).get_Some_0()).get_Some_0();
            assert(self.translate(p).get_Some_0().value() =~= p.value());
        }
        assert forall|ppage: SpecPage<PT>| (#[trigger] self.reverse(ppage)).is_Some() implies (
        self.translate(self.reverse(ppage).get_Some_0()).is_Some() && self.translate(
            self.reverse(ppage).get_Some_0(),
        ).get_Some_0() === ppage) by {}
    }

    pub proof fn lemma_valid_translate(&self, page: SpecPage<VT>)
        requires
            page.is_valid(),
            self.is_valid(),
            self.translate(page).is_Some(),
        ensures
            self.translate(page).get_Some_0().is_valid(),
    {
    }
}

} // verus!

================
File: ./source/verismo/src/arch/pgtable/entry_s.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::tspec::*;
use crate::*;

verus! {

impl PTLevel {
    pub open spec fn spec_pgsize(&self) -> int {
        let val = match *self {
            PTLevel::L3 => L3_PGSIZE!(),
            PTLevel::L2 => L2_PGSIZE!(),
            PTLevel::L1 => L1_PGSIZE!(),
            PTLevel::L0 => L0_PGSIZE!(),
        };
        val as int
    }

    #[verifier(inline)]
    pub open spec fn parent_lvl(&self) -> Option<PTLevel> {
        match *self {
            PTLevel::L3 => Option::None,
            PTLevel::L2 => Option::Some(PTLevel::L3),
            PTLevel::L1 => Option::Some(PTLevel::L2),
            PTLevel::L0 => Option::Some(PTLevel::L1),
        }
    }

    #[verifier(inline)]
    pub open spec fn child_lvl(&self) -> Option<PTLevel> {
        match *self {
            PTLevel::L3 => Option::Some(PTLevel::L2),
            PTLevel::L2 => Option::Some(PTLevel::L1),
            PTLevel::L1 => Option::Some(PTLevel::L0),
            PTLevel::L0 => Option::None,
        }
    }

    pub open spec fn spec_offset(&self) -> int {
        (39 - (self.as_int() * AsInt!(PT_ENTRY_NUM_BIT!())))
    }

    pub open spec fn spec_table_index<T: AddrType>(&self, vaddr: SpecAddr<T>) -> int {
        (vaddr.value() / self.spec_pgsize()) % (PT_ENTRY_NUM!() as int)
    }
}

} // verus!
verus! {

impl<T: AddrType> SpecPageTableEntry<T> {
    pub open spec fn new(value: int, dummy: Ghost<T>) -> Self {
        SpecPageTableEntry { value: value, dummy: dummy }
    }

    pub open spec fn new_val(value: int) -> Self {
        SpecPageTableEntry { value: value, dummy: spec_unused() }
    }

    pub open spec fn contains_flag(&self, flag: PteFlag) -> bool {
        (self.spec_value() / spec_int_pow2(flag.as_int())) % 2 == 1
    }

    /// Question: spec_int_pow2(52) will cause mod memdb always
    /// verified without checking the actual logic;
    pub open spec fn spec_ppn(&self) -> SpecPage<T> {
        let bits = spec_page_frame_bits();
        let addr = self.spec_value() % (BIT64!(bits) as int);
        SpecAddr::new2(addr, self.dummy).to_page()
    }

    pub open spec fn is_encrypted(&self) -> bool {
        self.contains_flag(PteFlag::C)
    }

    pub open spec fn is_present(&self) -> bool {
        self.contains_flag(PteFlag::P)
    }

    pub open spec fn is_writable(&self) -> bool {
        self.contains_flag(PteFlag::W)
    }

    pub open spec fn spec_addr(&self) -> SpecAddr<T> {
        self.spec_ppn().to_addr()
    }

    pub open spec fn spec_translate_page<VT: AddrType>(&self, v: SpecPage<VT>) -> Option::<
        SpecPage<T>,
    > {
        if self.is_present() {
            Option::Some(self.spec_ppn())
        } else {
            Option::None
        }
    }

    pub open spec fn spec_translate<VT: AddrType>(&self, v: SpecAddr<VT>) -> Option::<SpecAddr<T>> {
        if self.is_present() {
            Option::Some(self.spec_addr() + v.to_offset())
        } else {
            Option::None
        }
    }

    pub open spec fn addr_for_idx(&self, idx: nat) -> SpecAddr<T>
        recommends
            idx < PT_ENTRY_NUM!(),
    {
        let offset = (idx * PT_ENTRY_SIZE!()) as int;
        self.spec_addr() + offset
    }
}

} // verus!

================
File: ./source/verismo/src/arch/pgtable/mod.rs
================

mod def;
mod def_e;
pub use def::*;
pub use def_e::*;

mod entry_p;
mod entry_s;
mod memmap_p;
mod memmap_s;

================
File: ./source/verismo/src/arch/pgtable/memmap_s.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::tspec::*;

verus! {

impl<VT: AddrType, PT: AddrType> MemMap<VT, PT> {
    #[verifier(inline)]
    pub open spec fn spec_index(&self, vpage: SpecPage<VT>) -> Option<SpecPageTableEntry<PT>> {
        if self.db.dom().contains(vpage) {
            Option::Some(self.db.spec_index(vpage))
        } else {
            Option::None
        }
    }

    pub open spec fn is_valid(&self) -> bool {
        forall|page: SpecPage<VT>| #[trigger]
            page.is_valid() && self.translate(page).is_Some() ==> self.translate(
                page,
            ).get_Some_0().is_valid()
    }

    #[verifier(external_body)]
    pub broadcast proof fn axiom_is_valid(&self)
        ensures
            self.is_valid(),
    {
    }

    pub open spec fn is_encrypted(&self, vpage: SpecPage<VT>) -> Option<bool> {
        let entry = self.spec_index(vpage);
        if entry.is_Some() {
            Option::Some(entry.get_Some_0().is_encrypted())
        } else {
            Option::None
        }
    }

    pub open spec fn is_encrypted_and_some(&self, vpage: SpecPage<VT>) -> bool {
        self.translate(vpage).is_Some() && self.is_encrypted(vpage).get_Some_0()
    }

    pub open spec fn is_encrypted_or_none(&self, vpage: SpecPage<VT>) -> bool {
        self.translate(vpage).is_None() || self.is_encrypted(vpage).get_Some_0()
    }

    /// Simplified translation
    pub open spec fn translate(&self, vpage: SpecPage<VT>) -> Option<SpecPage<PT>> {
        let entry = self.spec_index(vpage);
        if entry.is_Some() {
            entry.get_Some_0().spec_translate_page(vpage)
        } else {
            Option::None
        }
    }

    /// Simplified translation
    pub open spec fn translate_addr(&self, addr: SpecAddr<VT>) -> Option<SpecAddr<PT>> {
        let page = addr.to_page();
        let ppage = self.translate(page);
        match ppage {
            Option::None => { Option::None },
            Option::Some(p) => { Option::Some(p.to_addr() + addr.to_offset()) },
        }
    }

    pub open spec fn translate_addr_seq(&self, addrs: SpecMem<VT>) -> SpecMem<PT> {
        if self.translate(addrs.to_page()).is_None() {
            SpecMem::from_range(SpecAddr::null(), 0)
        } else {
            addrs.convert(self.translate(addrs.to_page()).get_Some_0())
        }
    }

    pub open spec fn reverse(&self, page: SpecPage<PT>) -> Option<SpecPage<VT>> {
        if exists|gvn|
            (#[trigger] self.translate(gvn)).is_Some() && (self.translate(gvn).get_Some_0()
                =~= page) {
            let ret = choose|gvn|
                (#[trigger] self.translate(gvn)).is_Some() && (self.translate(gvn).get_Some_0()
                    =~= page);
            Option::Some(ret)
        } else {
            Option::None
        }
    }

    pub open spec fn reverse_trans_addr(&self, addr: SpecAddr<PT>) -> Option<SpecAddr<VT>> {
        let page = addr.to_page();
        let ppage = self.reverse(page);
        match ppage {
            Option::None => { Option::None },
            Option::Some(p) => { Option::Some(p.to_addr() + addr.to_offset()) },
        }
    }

    #[verifier(opaque)]
    /// Only used when proving model corrretness.
    /// Not used as SM's invariant.
    pub open spec fn is_one_to_one_map(&self) -> bool {
        &&& (forall|vpage: SpecPage<VT>|
            ((#[trigger] self.translate(vpage)).is_Some()) ==> (self.reverse(
                self.translate(vpage).get_Some_0(),
            ).is_Some() && self.reverse(self.translate(vpage).get_Some_0()).get_Some_0() =~= vpage))
        &&& (forall|ppage: SpecPage<PT>|
            ((#[trigger] self.reverse(ppage)).is_Some()) ==> (self.translate(
                self.reverse(ppage).get_Some_0(),
            ).is_Some() && self.translate(self.reverse(ppage).get_Some_0()).get_Some_0() =~= ppage))
    }

    #[verifier(opaque)]
    pub open spec fn is_identity_map(&self) -> bool {
        &&& (forall|vpage: SpecPage<VT>|
            ((#[trigger] self.translate(vpage)).is_Some()) ==> self.translate(
                vpage,
            ).get_Some_0().as_int() === vpage.as_int())
    }
}

impl MemMap<GuestVir, GuestPhy> {
    #[verifier(opaque)]
    pub open spec fn inv_encrypted_priv_mem(&self, memid: MemID) -> bool {
        &&& forall|gvn: GVN|
            gvn.is_valid() && self.need_c_bit(memid, gvn) ==> #[trigger] self.is_encrypted_or_none(
                gvn,
            )
    }

    pub open spec fn need_c_bit(&self, memid: MemID, gvn: GVN) -> bool {
        ||| memtype(
            memid,
            self.translate(gvn).get_Some_0(),
        ).need_c_bit()
        //||| rmp.has_gpn_memid(gvn, memid)

    }
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/access_s.rs
================

use super::*;
use crate::arch::rmp::perm_s::*;
use crate::*;

verus! {

impl RmpEntry {
    pub open spec fn new(val: HiddenRmpEntryForPSP) -> Self {
        arbitrary::<RmpEntry>().spec_set_val(val)
    }

    #[verifier(opaque)]
    //memid, enc, gpmem, perm
    pub open spec fn check_access(
        &self,
        memid: MemID,
        enc: bool,
        gpmem: GPMem,
        perm: Perm,
    ) -> ResultOrErr<Self, MemError<()>> {
        if enc && !self@.check_gpn(gpmem.to_page()) {
            ResultOrErr::Error(MemError::NestedPF(()))
        } else {
            self.check_access_no_addr_check(memid, enc, perm)
        }
    }

    pub open spec fn check_access_no_addr_check(
        &self,
        memid: MemID,
        enc: bool,
        perm: Perm,
    ) -> ResultOrErr<Self, MemError<()>> {
        if enc {
            if !self@.check_guest_reverse_mut_size_no_gpn(memid.to_asid(), PageSize::Size4k) {
                ResultOrErr::Error(MemError::NestedPF(()))
            } else if !self@.check_validated() {
                ResultOrErr::Error(MemError::NotValidated(()))
            } else if !self@.check_vmpl(memid.to_vmpl(), perm) {
                ResultOrErr::Error(MemError::NestedPF(()))
            } else {
                ResultOrErr::Ok(*self)
            }
        } else {
            if !self@.check_hypervisor_owned() {
                ResultOrErr::Error(MemError::NestedPF(()))
            } else {
                ResultOrErr::Ok(*self)
            }
        }
    }

    pub open spec fn trans(&self, op: RmpOp<SysPhy>) -> ResultWithErr<
        RmpEntry,
        MemError<RmpOp<SysPhy>>,
    > {
        let ret = match op {
            RmpOp::RmpUpdate(_, newentry) => { self.rmpupdate(newentry) },
            RmpOp::RmpAdjust(
                PageID { page, memid },
                RmpAdjustParam { gpn, psize, vmsa, vmpl, perms },
            ) => { self.rmpadjust(memid, vmpl, psize, gpn, vmsa, perms) },
            RmpOp::Pvalidate(PageID { page, memid }, PvalidateParam { gpn, psize, val }) => {
                self.pvalidate(memid, psize, gpn, val)
            },
        };
        let err = ret.to_err().with_param(op);
        ret.replace_err(err)
    }

    /// rmpupdate: only accept new asid gpa size assign immu;
    /// PSP will reset other fields automatically
    pub open spec fn _rmpupdate(&self, entry: RmpEntry) -> ResultWithErr<RmpEntry, MemError<()>> {
        let new = entry.view();
        if new.spec_size().is_Size2m() {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Perm, ()))
        } else if !new.spec_assigned() && (new.spec_immutable() || new.spec_asid()
            !== ASID_FOR_HV!()) {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Perm, ()))
        } else if self@.spec_immutable() {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Perm, ()))
        } else if !self@.spec_assigned() {
            let hidden = HiddenRmpEntryForPSP {
                immutable: false,
                assigned: false,
                validated: false,
                vmsa: false,
                asid: 0,
                gpn: GPN::new(0),
                size: PageSize::Size4k,
                perms: rmp_perm_init(),
            };
            ResultWithErr::Ok(self.spec_set_val(hidden))
        } else {
            let reset = new.spec_asid() != self@.spec_asid() || new.spec_gpn() != self@.spec_gpn()
                || new.spec_size() != self@.spec_size() || new.spec_assigned()
                != self@.spec_assigned() || new.spec_asid() == 0;
            let hidden = HiddenRmpEntryForPSP {
                immutable: new.spec_immutable(),
                assigned: new.spec_assigned(),
                validated: if reset {
                    false
                } else {
                    self@.validated
                },
                vmsa: if reset {
                    false
                } else {
                    self@.vmsa
                },
                asid: new.spec_asid(),
                gpn: new.spec_gpn(),
                size: new.spec_size(),
                perms: if reset {
                    rmp_perm_init()
                } else {
                    self@.perms
                },
            };
            ResultWithErr::Ok(self.spec_set_val(hidden))
        }
    }

    pub open spec fn rmpupdate(&self, entry: RmpEntry) -> ResultWithErr<RmpEntry, MemError<()>> {
        let new = if !entry.view().spec_assigned() || (entry.view().spec_asid()
            !== ASID_FOR_HV!()) {
            let newhidden = entry.view();
            let hidden = HiddenRmpEntryForPSP {
                immutable: newhidden.spec_immutable(),
                assigned: newhidden.spec_assigned(),
                validated: false,
                vmsa: false,
                asid: newhidden.spec_asid(),
                gpn: newhidden.spec_gpn(),
                size: newhidden.spec_size(),
                perms: rmp_perm_init(),
            };
            self.spec_set_val(hidden)
        } else {
            *self
        };
        if self@.spec_immutable() {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Perm, ()))
        } else {
            ResultWithErr::Ok(new)
        }
    }

    /// rmpadjust: only adjust vmsa and perm
    /// final step of the actual instruction;
    /// condition check happens in memory model
    pub open spec fn rmpadjust(
        &self,
        memid: MemID,
        vmpl: VMPL,
        psize: PageSize,
        gpn: GPN,
        vmsa: bool,
        perms: PagePerm,
    ) -> ResultWithErr<RmpEntry, MemError<()>>
        recommends
            memid.to_asid() == self.view().spec_asid(),
            memid.is_Guest(),
    {
        if vmpl.as_int() <= memid.to_vmpl().as_int() {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Perm, ()))
        } else if self.view().fault_rmp_update(memid.to_asid(), gpn, psize) {
            ResultWithErr::Error(*self, MemError::NestedPF(()))
        } else if !self.view().spec_validated() {
            ResultWithErr::Error(*self, MemError::NotValidated(()))
        } else if self.view().spec_size() == PageSize::Size4k && psize == PageSize::Size2m {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Size, ()))
        } else {
            let cur_perm = self.view().spec_perm(memid.to_vmpl());
            if !perms.subset_of(cur_perm) {
                ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Perm, ()))
            } else {
                let hidden = self.view().spec_set_perm(vmpl, perms).spec_set_vmsa(vmsa);
                ResultWithErr::Ok(self.spec_set_val(hidden))
            }
        }
    }

    /// pvalidate: change validate state
    /// final step of the actual instruction;
    /// input validation without rmp entry happens in MemDB::op_pvalidate
    pub open spec fn pvalidate(
        &self,
        memid: MemID,
        psize: PageSize,
        gpn: GPN,
        val: bool,
    ) -> ResultWithErr<RmpEntry, MemError<()>> {
        if !memid.to_vmpl().is_VMPL0() {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Perm, ()))
        } else if self.view().fault_rmp_update(memid.to_asid(), gpn, psize) {
            ResultWithErr::Error(*self, MemError::NestedPF(()))
        } else if self.view().spec_size() == PageSize::Size4k && psize == PageSize::Size2m {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Size, ()))
        } else if self.view().spec_validated() === val {
            ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::DoubleVal, ()))
        } else {
            let hidden = self.view().spec_set_validated(val);
            ResultWithErr::Ok(self.spec_set_val(hidden))
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/db_p.rs
================

use super::perm_s::*;
use super::*;

verus! {

pub proof fn rmp_proof_check_access_rmp_has_gpn_memid(
    rmp: &RmpMap,
    memid: MemID,
    enc: bool,
    gpmem: GPMem,
    perm: Perm,
    spn: SPN,
)
    requires
        rmp_check_access(rmp, memid, enc, gpmem, perm, spn).is_Ok(),
        enc,
    ensures
        rmp_has_gpn_memid(rmp, gpmem.to_page(), memid),
{
    reveal(RmpEntry::check_access);
}

pub proof fn rmp_lemma_model_eq_inv(rmp: &RmpMap, other: &RmpMap, memid: MemID)
    requires
        rmp_model_eq(rmp, other),
        rmp_inv(other),
    ensures
        rmp_inv(rmp),
        rmp_inv_memid_int(other, memid) ==> rmp_inv_memid_int(rmp, memid),
        rmp_inv_sw(other, memid) ==> rmp_inv_sw(rmp, memid),
{
    if rmp_inv_memid_int(other, memid) {
        assert forall|spn: SPN, vmpl: VMPL|
            {
                &&& rmp.dom().contains(spn)
                &&& memtype(memid, rmp[spn].view().spec_gpn()).is_sm_int()
                &&& vmpl.as_int() > memid.to_vmpl().as_int()
                &&& rmp[spn].view().spec_asid() === memid.to_asid()
            } implies !#[trigger] rmp[spn].view().check_vmpl(vmpl, Perm::Write) by {
            assert(!vmpl.is_VMPL0());
            rmp_lemma_hv_update_restrict(&other, *rmp, MemID::Hv);
            assert(*rmp === rmp_hv_update(other, *rmp, MemID::Hv));
            if rmp[spn] !== other[spn] {
                assert(rmp[spn].view().spec_perms() === rmp_perm_init());
                assert(rmp[spn].view().spec_perms().index(vmpl) === PagePerm::empty());
                assert(!rmp[spn].view().spec_perms().index(vmpl).contains(Perm::Write));
            } else {
                assert(!other[spn].view().check_vmpl(vmpl, Perm::Write));
            }
        }
    }
}

#[verifier(external_body)]
pub broadcast proof fn rmp_contains_all(rmp: &RmpMap, spn: SPN)
    ensures
        rmp.dom().contains(spn),
{
}

pub proof fn rmp_proof_op_dom_inv(rmp: &RmpMap, op: RmpOp<SysPhy>)
    ensures
        rmp_op(rmp, op).to_result().dom() === rmp.dom(),
{
    assert(rmp_op(rmp, op).to_result().dom() =~~= (rmp.dom()));
}

pub proof fn rmp_proof_op_inv(rmp: &RmpMap, op: RmpOp<SysPhy>)
    requires
        rmp_inv(rmp),
    ensures
        rmp_inv(&rmp_op(rmp, op).to_result()),
{
    reveal(rmp_inv);
    let spn = op.to_page_memid().page;
    let new = rmp_op(rmp, op).to_result();
    rmp_proof_op_dom_inv(rmp, op);
    if rmp.dom().contains(spn) {
        assert forall|i: SPN| new.dom().contains(i) implies #[trigger] new[i].inv() by {
            assert(rmp[i].inv());
            RmpEntry::lemma_trans_inv(rmp[spn], op);
        }
    }
}

pub proof fn rmp_proof_inv_sw(rmp: &RmpMap, op: RmpOp<SysPhy>, memid: MemID)
    requires
        rmp_inv_sw(rmp, memid),
        op.to_page_memid().memid.is_sm(memid) ==> op.sp_op_requires(
            rmp,
        ),
//rmp.sw_requires(op, memid),

    ensures
        rmp_inv_sw(&rmp_op(rmp, op).to_result(), memid),
{
    match op {
        RmpOp::Pvalidate(_, _) => {
            rmp_lemma_pvalidate_sw_inv(rmp, op, memid);
        },
        RmpOp::RmpAdjust(_, _) => {},
        RmpOp::RmpUpdate(_, _) => {},
    }
}

/// TODO: function body check has been running for 2 seconds
pub proof fn rmp_proof_inv_memid_int(rmp: &RmpMap, op: RmpOp<SysPhy>, memid: MemID)
    requires
        rmp_inv_memid_int(rmp, memid),
        rmp_inv(rmp),
        op.to_page_memid().memid.is_sm(memid) ==> op.sp_op_requires(rmp),
    ensures
        rmp_inv_memid_int(&rmp_op(rmp, op).to_result(), memid),
{
    reveal(rmp_inv);
    let new = rmp_op(rmp, op).to_result();
    rmp_proof_op_dom_inv(rmp, op);
    assert(new.dom() === rmp.dom());
    assert forall|spn: SPN, vmpl: VMPL|
        {
            &&& new.dom().contains(spn)
            &&& memtype(memid, new[spn].view().spec_gpn()).is_sm_int()
            &&& vmpl.as_int() > memid.to_vmpl().as_int()
            &&& new[spn].view().spec_asid() === memid.to_asid()
        } implies !#[trigger] new[spn].view().check_vmpl(vmpl, Perm::Write) by {
        assert(rmp[spn].inv());
        let new_perms = new[spn].view().spec_perms();
        let perms = rmp[spn].view().spec_perms();
        let new_vmpl_perm = new_perms[vmpl];
        let vmpl_perm = perms[vmpl];
        match op {
            RmpOp::Pvalidate(_, _) => {
                assert(new[spn].view().spec_gpn() === rmp[spn].view().spec_gpn());
                assert(new_perms === perms);
                assert(memtype(memid, rmp[spn].view().spec_gpn()).is_sm_int());
                assert(!vmpl_perm.contains(Perm::Write));
                assert(!new[spn].view().check_vmpl(vmpl, Perm::Write));
            },
            RmpOp::RmpAdjust(page_id, param) => {
                let update_spn = page_id.page;
                assert(new[spn].view().spec_gpn() === rmp[spn].view().spec_gpn());
                assert(memtype(memid, rmp[spn].view().spec_gpn()).is_sm_int());
                assert(!vmpl_perm.contains(Perm::Write));
                assert(!new[spn].view().check_vmpl(vmpl, Perm::Write));
            },
            RmpOp::RmpUpdate(_, _) => {
                if new[spn] != rmp[spn] {
                    assert(vmpl.as_int() > memid.to_vmpl().as_int());
                    assert(new_vmpl_perm === PagePerm::empty());
                    assert(!new_vmpl_perm.contains(Perm::Write));
                } else {
                    assert(!rmp[spn].view().check_vmpl(vmpl, Perm::Write));
                }
                assert(!new[spn].view().check_vmpl(vmpl, Perm::Write));
            },
        }
    }
}

pub proof fn rmp_lemma_pvalidate_sw_inv(rmp: &RmpMap, op: RmpOp<SysPhy>, memid: MemID)
    requires
        op.is_Pvalidate(),
        (op.to_page_memid().memid.to_vmpl().is_VMPL0() && (memid.to_asid()
            === op.to_page_memid().memid.to_asid())) ==> {
            !op.get_Pvalidate_1().val || !rmp_has_gpn_memid(rmp, op.get_Pvalidate_1().gpn, memid)
        },
        rmp_inv_sw(rmp, memid),
    ensures
        rmp_inv_sw(&rmp_op(rmp, op).to_result(), memid),
{
    let is_error = rmp_op(rmp, op).is_Error();
    let new = rmp_op(rmp, op).to_result();
    let gpn = op.get_Pvalidate_1().gpn;
    let val = op.get_Pvalidate_1().val;
    let op_memid = op.get_Pvalidate_0().memid;
    let op_spn = op.get_Pvalidate_0().page;
    assert forall|spn: SPN|
        {
            &&& new.dom().contains(spn)
            &&& (#[trigger] new[spn]).view().spec_validated()
            &&& (#[trigger] new[spn]).view().spec_asid() === memid.to_asid()
        } implies (rmp_reverse(&new, memid, rmp[spn].view().spec_gpn()) === spn) by {
        assert(rmp.dom().contains(spn));
        if op_memid.to_vmpl().is_VMPL0() && memid.to_asid() === op_memid.to_asid() {
            if !val {
                assert(rmp[spn].view().spec_validated());
                assert(rmp[spn] === new[spn]);
                assert(rmp_reverse(rmp, memid, rmp[spn].view().spec_gpn()) === spn)
            } else {
                assert(!rmp_has_gpn_memid(rmp, gpn, memid));
                if rmp[spn] !== new[spn] {
                    assert(new[spn].view().spec_gpn() === gpn) by {
                        reveal(RmpEntry::check_access);
                    }
                    assert forall|spn_test: SPN|
                        {
                            &&& spn_test !== spn
                            &&& #[trigger] rmp.dom().contains(spn_test)
                            &&& new[spn_test].view().spec_asid() === memid.to_asid()
                        } implies ((#[trigger] new[spn_test]).view().spec_gpn() !== gpn)
                        || !new[spn_test].view().spec_validated() by {
                        assert(new[spn_test] === rmp[spn_test]);
                        assert(!rmp_has_gpn_memid(rmp, gpn, memid));
                    }
                }
            }
        } else {
            if !op_memid.to_vmpl().is_VMPL0() {
                assert(is_error);
                assert(new[spn] === rmp[spn]);
            }
            if memid.to_asid() !== op_memid.to_asid() {
                if op_spn === spn {
                    assert(is_error);
                }
                assert(new[spn] === rmp[spn]);
            }
            assert(rmp_reverse(rmp, memid, rmp[spn].view().spec_gpn()) === spn);
            assert forall|spn_test: SPN|
                {
                    &&& #[trigger] rmp.dom().contains(spn_test)
                    &&& new[spn_test].view().spec_asid() === memid.to_asid()
                    &&& ((#[trigger] new[spn_test]).view().spec_gpn()
                        === rmp[spn].view().spec_gpn())
                    &&& new[spn_test].view().spec_validated()
                } implies spn_test === spn by {
                assert(new[spn_test] === rmp[spn_test]);
                assert(rmp_inv_sw(rmp, memid));
                assert((rmp_reverse(rmp, memid, rmp[spn].view().spec_gpn()) === spn_test));
            }
        }
    }
}

pub proof fn rmp_lemma_hv_update_inv(rmp: &RmpMap, newrmp: RmpMap, hv_id: MemID)
    requires
        rmp_inv(rmp),
    ensures
        rmp_inv(&rmp_hv_update(rmp, newrmp, hv_id)),
{
    reveal(rmp_inv);
    assert forall|i: SPN| rmp.dom().contains(i) implies #[trigger] rmp[i].inv() by {
        let spn_id = PageID { page: i, memid: hv_id };
        RmpEntry::lemma_trans_inv(rmp[i], RmpOp::RmpUpdate(spn_id, newrmp[i]));
    }
}

pub proof fn rmp_lemma_hv_update_restrict(rmp: &RmpMap, newrmp: RmpMap, hv_id: MemID)
    requires
        rmp_inv(rmp),
    ensures
        forall|i|
            (rmp.dom().contains(i) && (#[trigger] rmp_hv_update(rmp, newrmp, hv_id)[i] !== rmp[i]))
                ==> (!rmp_hv_update(rmp, newrmp, hv_id)[i]@.spec_validated() && rmp_hv_update(
                rmp,
                newrmp,
                hv_id,
            )[i]@.spec_perms() === rmp_perm_init()),
{
    reveal(rmp_inv);
}

pub proof fn rmp_lemma_hv_update_restrict_at(
    rmp: &RmpMap,
    hv_id: MemID,
    newrmp: RmpMap,
    memid: MemID,
    enc: bool,
    gpmem: GPMem,
    perm: Perm,
    spn: SPN,
)
    requires
        rmp_inv(rmp),
        memid.is_Guest(),
        enc,
    ensures
        (!rmp_check_access(&rmp_hv_update(rmp, newrmp, hv_id), memid, enc, gpmem, perm, spn).is_Ok()
            || (rmp_check_access(&rmp_hv_update(rmp, newrmp, hv_id), memid, enc, gpmem, perm, spn)
            === rmp_check_access(rmp, memid, enc, gpmem, perm, spn))),
{
    reveal(RmpEntry::check_access);
    rmp_lemma_hv_update_inv(rmp, newrmp, hv_id);
    reveal(rmp_inv);
    let rmp2 = rmp_hv_update(rmp, newrmp, hv_id);
    if !rmp2.dom().contains(spn) || !rmp2[spn]@.spec_validated() {
        assert(!rmp_check_access(&rmp2, memid, enc, gpmem, perm, spn).is_Ok()) by {
            reveal(RmpEntry::check_access);
        }
    } else {
        assert(rmp2[spn] === rmp[spn]);
    }
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/def_s.rs
================

use verismo_macro::*;

use super::perm_s::{PagePerm, Perm, RmpPerm};
use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::arch::errors::*;
use crate::tspec::*;
verus! {

#[derive(SpecGetter, SpecSetter)]
pub ghost struct HiddenRmpEntryForPSP {
    pub immutable: bool,
    pub assigned: bool,
    pub validated: bool,
    pub vmsa: bool,
    pub asid: ASID,
    pub gpn: GPN,
    pub size: PageSize,
    pub perms: RmpPerm,
}

#[derive(SpecGetter, SpecSetter)]
pub ghost struct RmpEntry {
    pub val: HiddenRmpEntryForPSP,
}

impl RmpEntry {
    pub proof fn proof_eq(self, r: Self)
        ensures
            (self@ === r@) == (self === r),
    {
    }
}

pub ghost struct RmpAdjustParam {
    pub gpn: GPN,
    pub psize: PageSize,
    pub vmsa: bool,
    pub vmpl: VMPL,
    pub perms: PagePerm,
}

pub ghost struct PvalidateParam {
    pub gpn: GPN,
    pub psize: PageSize,
    pub val: bool,
}

pub type RmpUpdateParam = RmpEntry;

pub type RmpMap = Map<SPN, RmpEntry>;

#[is_variant]
pub ghost enum RmpOp<AddrT> {
    RmpAdjust(PageID<AddrT>, RmpAdjustParam),
    Pvalidate(PageID<AddrT>, PvalidateParam),
    RmpUpdate(PageID<AddrT>, RmpUpdateParam),
}

crate::macro_const_int! {
    #[macro_export]
    pub const RMP_FAIL_INPUT: u64 = 1;
    #[macro_export]
    pub const RMP_FAIL_PERMISSION: u64 = 2;
    #[macro_export]
    pub const RMP_FAIL_INUSE: u64 = 3;
    #[macro_export]
    pub const RMP_FAIL_OVERLAP: u64 = 4;
    #[macro_export]
    pub const RMP_FAIL_SIZEMISMATCH: u64 = 6;
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/db_u.rs
================

use super::perm_s::*;
use super::*;

verus! {

#[verifier(opaque)]
pub open spec fn rmp_inv(rmp: &RmpMap) -> bool {
    &&& (forall|i: SPN| rmp.dom().contains(i) ==> #[trigger] rmp[i].inv())
}

pub open spec fn rmp_model_eq(rmp: &RmpMap, other: &RmpMap) -> bool {
    &&& (rmp_inv(other) ==> rmp_inv(rmp))
    &&& (forall|memid|
        (rmp_inv_sw(other, memid) ==> rmp_inv_sw(rmp, memid)))  // tODO: prove this

    &&& rmp.dom() === other.dom()
    &&& (forall|spn: SPN|
        rmp[spn].view().spec_validated() ==> #[trigger] rmp[spn] === #[trigger] other[spn])
    &&& (*rmp === rmp_hv_update(other, *rmp, MemID::Hv))
}

pub open spec fn rmp_reverse(rmp: &RmpMap, memid: MemID, gpn: GPN) -> SPN {
    choose|spn|
        {
            &&& (#[trigger] rmp[spn]).view().spec_gpn() === gpn
            &&& rmp.dom().contains(spn)
            &&& rmp[spn].view().spec_validated()
            &&& rmp[spn].view().spec_asid() === memid.to_asid()
        }
}

#[verifier(inline)]
pub open spec fn rmp_reverse_mem(rmp: &RmpMap, memid: MemID, gpmem: GPMem) -> SPMem {
    let spn = rmp_reverse(rmp, memid, gpmem.to_page());
    gpmem.convert(spn)
}

/*
    #[verifier(inline)]
    pub open spec fn rmp_reverse_addr(rmp: &RmpMap, memid: MemID, gpa: GPA) -> SPA {
        let spn = rmp_reverse(rmp, memid, gpa.to_page());
        gpa.convert(spn)
    }*/

pub open spec fn rmp_has_gpn(rmp: &RmpMap, gpn: GPN) -> bool {
    exists|spn|
        {
            &&& (#[trigger] rmp[spn]).view().spec_gpn() === gpn
            &&& #[trigger] rmp.dom().contains(spn)
            &&& rmp[spn].view().spec_validated()
        }
}

pub open spec fn rmp_has_gpn_memid(rmp: &RmpMap, gpn: GPN, memid: MemID) -> bool {
    exists|spn|
        {
            &&& (#[trigger] rmp[spn]).view().spec_gpn() === gpn
            &&& rmp.dom().contains(spn)
            &&& rmp[spn].view().spec_validated()
            &&& rmp[spn].view().spec_asid() === memid.to_asid()
        }
}

// To guarantee integrity, sm should validate GPA once.
// Checking double pvalidate error in rflag is not enough.
// To dynamically change share/private, verismo must track which gpa
// is validated and invalidated
pub open spec fn rmp_inv_sw(rmp: &RmpMap, memid: MemID) -> bool {
    forall|spn: SPN|
        {
            &&& #[trigger] rmp.dom().contains(spn)
            &&& (#[trigger] rmp[spn]).view().spec_validated()
            &&& rmp[spn].view().spec_asid() === memid.to_asid()
        } ==> (rmp_reverse(rmp, memid, rmp[spn].view().spec_gpn()) === spn)
}

// If memtype is sm_init, the gpn is only assigned to memid,
// If it is validated, hypervisor or other VMPLs cannot write to it.
pub open spec fn rmp_inv_memid_int(rmp: &RmpMap, memid: MemID) -> bool {
    forall|spn: SPN, vmpl: VMPL|
        {
            &&& rmp.dom().contains(spn)
            &&& memtype(memid, rmp[spn].view().spec_gpn()).is_sm_int()
            &&& vmpl.as_int() > memid.to_vmpl().as_int()
            &&& rmp[spn].view().spec_asid() === memid.to_asid()
        } ==> !#[trigger] rmp[spn].view().check_vmpl(vmpl, Perm::Write)
}

pub open spec fn rmp_hv_update(rmp: &RmpMap, newrmp: RmpMap, memid: MemID) -> RmpMap {
    RmpMap::new(
        |spn: SPN| rmp.dom().contains(spn),
        |spn: SPN| { rmp.spec_index(spn).rmpupdate(newrmp.spec_index(spn)).to_result() },
    )
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/perm_s.rs
================

use crate::arch::entities::VMPL;
use crate::tspec::*;

verus! {

#[is_variant]
pub enum Perm {
    Read,
    Write,
    ExeU,
    ExeS,
    Shadow,
}

pub type PagePerm = Set<Perm>;

impl IntValue for PagePerm {
    open spec fn as_int(&self) -> int {
        let v1: int = if self.contains(Perm::Read) {
            1
        } else {
            0
        };
        let v2: int = if self.contains(Perm::Write) {
            2
        } else {
            0
        };
        let v3: int = if self.contains(Perm::ExeU) {
            4
        } else {
            0
        };
        let v4: int = if self.contains(Perm::ExeS) {
            8
        } else {
            0
        };
        let v5: int = if self.contains(Perm::Shadow) {
            16
        } else {
            0
        };
        v1 + v2 + v3 + v4 + v5
    }

    open spec fn from_int(val: int) -> Set<Perm> {
        let ret = Set::empty();
        let ret = if val % 2 == 1 {
            ret.insert(Perm::Read)
        } else {
            ret
        };
        let ret = if (val / 2) % 2 == 1 {
            ret.insert(Perm::Write)
        } else {
            ret
        };
        let ret = if (val / 4) % 2 == 1 {
            ret.insert(Perm::ExeU)
        } else {
            ret
        };
        let ret = if (val / 8) % 2 == 1 {
            ret.insert(Perm::ExeS)
        } else {
            ret
        };
        let ret = if (val / 16) % 2 == 1 {
            ret.insert(Perm::Shadow)
        } else {
            ret
        };
        ret
    }
}

} // verus!
//#[derive(SpecGetter)]
pub type RmpPerm = Map<VMPL, PagePerm>;

verus! {

/// VMPL0 gets full permission by default, other VMPLs have none.
#[verifier(inline)]
pub open spec fn rmp_perm_init() -> RmpPerm {
    Map::new(
        |vmpl: VMPL| true,
        |vmpl: VMPL|
            if vmpl.is_VMPL0() {
                PagePerm::full()
            } else {
                PagePerm::empty()
            },
    )
}

#[verifier(inline)]
pub open spec fn rmp_perm_is_init(p: RmpPerm) -> bool {
    &&& p[VMPL::VMPL0] === PagePerm::full()
    &&& p[VMPL::VMPL1] === PagePerm::empty()
    &&& p[VMPL::VMPL2] === PagePerm::empty()
    &&& p[VMPL::VMPL3] === PagePerm::empty()
}

#[verifier(inline)]
pub open spec fn rmp_perm_is_valid(p: RmpPerm) -> bool {
    &&& p.index(VMPL::VMPL0) === PagePerm::full()
    &&& p.dom() === Set::full()
}

#[verifier(external_body)]
pub broadcast proof fn rmp_perm_track_dom(p: RmpPerm, vmpl: VMPL)
    ensures
        p.dom().contains(vmpl),
{
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/rmpop_u.rs
================

use super::db_u::*;
use super::perm_s::Perm;
use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::tspec::*;

verus! {

impl<AddrT: AddrType> RmpOp<AddrT> {
    pub open spec fn to_page_memid(&self) -> PageID<AddrT> {
        match *self {
            RmpOp::RmpAdjust(page_id, _) => page_id,
            RmpOp::Pvalidate(page_id, _) => page_id,
            RmpOp::RmpUpdate(page_id, _) => page_id,
        }
    }
}

impl RmpOp<GuestVir> {
    pub open spec fn set_gpn(&self, page: SpecPage<GuestPhy>) -> RmpOp<GuestPhy> {
        match *self {
            RmpOp::RmpAdjust(page_id, RmpAdjustParam { gpn, psize, vmsa, vmpl, perms }) => {
                RmpOp::RmpAdjust(
                    PageID { memid: page_id.memid, page },
                    RmpAdjustParam { gpn: page, psize, vmsa, vmpl, perms },
                )
            },
            RmpOp::Pvalidate(page_id, PvalidateParam { gpn, val, psize }) => {
                RmpOp::Pvalidate(
                    PageID { memid: page_id.memid, page },
                    PvalidateParam { gpn: page, val, psize },
                )
            },
            RmpOp::RmpUpdate(page_id, param) => {
                RmpOp::RmpUpdate(PageID { memid: page_id.memid, page }, param)
            },
        }
    }
}

impl RmpOp<GuestPhy> {
    pub open spec fn gp_op_requires(&self, rmp: &RmpMap) -> bool {
        let sp_op = self.set_spn(spec_unused());
        &&& sp_op.sp_op_requires(rmp)
    }

    pub open spec fn inv(&self) -> bool {
        self.is_Pvalidate() ==> self.get_Pvalidate_1().gpn === self.to_page_memid().page
    }

    pub open spec fn set_spn(&self, page: SpecPage<SysPhy>) -> RmpOp<SysPhy> {
        match *self {
            RmpOp::RmpAdjust(page_id, param) => {
                RmpOp::RmpAdjust(PageID { memid: page_id.memid, page }, param)
            },
            RmpOp::Pvalidate(page_id, param) => {
                RmpOp::Pvalidate(PageID { memid: page_id.memid, page }, param)
            },
            RmpOp::RmpUpdate(page_id, param) => {
                RmpOp::RmpUpdate(PageID { memid: page_id.memid, page }, param)
            },
        }
    }

    pub open spec fn get_gpn(&self) -> GPN {
        match *self {
            RmpOp::RmpAdjust(page_id, RmpAdjustParam { gpn, psize, vmsa, vmpl, perms }) => { gpn },
            RmpOp::Pvalidate(page_id, PvalidateParam { gpn, val, psize }) => { gpn },
            RmpOp::RmpUpdate(page_id, param) => { page_id.page },
        }
    }
}

impl RmpOp<SysPhy> {
    pub open spec fn op_requires_stateless(&self) -> bool {
        match *self {
            RmpOp::RmpAdjust(
                PageID { page, memid },
                RmpAdjustParam { gpn, psize, vmsa, vmpl, perms },
            ) => {
                //&&& !memtype(memid, gpn).need_c_bit()
                &&& {
                    ||| !perms.contains(Perm::Write)
                    ||| !memtype(memid, gpn).is_sm_int()
                }
            },
            RmpOp::Pvalidate(PageID { page, memid }, PvalidateParam { gpn, psize, val }) => {
                true
                //&&& !memtype(memid, gpn).need_c_bit()

            },
            RmpOp::RmpUpdate(page_id, param) => { true },
        }
    }

    pub open spec fn sp_op_requires(&self, rmp: &RmpMap) -> bool {
        &&& self.op_requires_stateless()
        &&& match *self {
            RmpOp::Pvalidate(PageID { page, memid }, PvalidateParam { gpn, psize, val }) => {
                !rmp_has_gpn_memid(rmp, gpn, memid) || !memid.to_vmpl().is_VMPL0()
            },
            _ => { true },
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/entry_s.rs
================

use super::perm_s::{PagePerm, RmpPerm, *};
use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;

verus! {

impl HiddenRmpEntryForPSP {
    pub open spec fn inv_hvupdate_rel(self, preventry: Self) -> bool {
        &&& self.is_valid()
        &&& !preventry.validated
            ==> !self.validated
        //&&& self.validated ==> (self.perms === preventry.perms && self.asid === preventry.asid && preventry.validated && self.vmsa == preventry.vmsa && self.size == preventry.size)
        //&&& (self !== preventry)  ==> ((self.perms === super::perm_s::rmp_perm_init()))

        &&& (self !== preventry) ==> {
            &&& self.perms[VMPL::VMPL0] =~~= PagePerm::full()
            &&& self.perms[VMPL::VMPL0] =~~= preventry.perms[VMPL::VMPL0]
            &&& self.perms[VMPL::VMPL1].subset_of(preventry.perms[VMPL::VMPL1])
            &&& self.perms[VMPL::VMPL2].subset_of(preventry.perms[VMPL::VMPL2])
            &&& self.perms[VMPL::VMPL3].subset_of(preventry.perms[VMPL::VMPL3])
            &&& (self.perms[VMPL::VMPL1] === PagePerm::empty() || self.perms[VMPL::VMPL1]
                === preventry.perms[VMPL::VMPL1])
            &&& (self.perms[VMPL::VMPL2] === PagePerm::empty() || self.perms[VMPL::VMPL2]
                === preventry.perms[VMPL::VMPL2])
            &&& (self.perms[VMPL::VMPL3] === PagePerm::empty() || self.perms[VMPL::VMPL3]
                === preventry.perms[VMPL::VMPL3])
        }
    }

    #[verifier(inline)]
    pub open spec fn spec_set_perm(&self, vmpl: VMPL, perm: PagePerm) -> Self {
        self.spec_set_perms(self.spec_perms().insert(vmpl, perm))
    }

    #[verifier(inline)]
    pub open spec fn spec_perm(&self, vmpl: VMPL) -> PagePerm {
        self.spec_perms().index(vmpl)
    }

    // is private to vmpl-x and vmpl-0
    pub open spec fn is_vmpl_private(&self, vmpl: int) -> bool {
        &&& self.spec_validated()
        &&& vmpl != VMPL::VMPL1.as_int() ==> !self.spec_perms()[VMPL::VMPL1].contains(Perm::Write)
        &&& vmpl != VMPL::VMPL2.as_int() ==> !self.spec_perms()[VMPL::VMPL2].contains(Perm::Write)
        &&& vmpl != VMPL::VMPL3.as_int() ==> !self.spec_perms()[VMPL::VMPL3].contains(Perm::Write)
    }

    pub open spec fn is_confidential_to(&self, vmpl: int) -> bool
        recommends
            0 < vmpl <= 4,
    {
        if vmpl >= 4 {
            self.spec_validated()
        } else {
            &&& self.spec_validated()
            &&& self.spec_perms()[VMPL::spec_from_int(vmpl).get_Some_0()] =~= PagePerm::empty()
        }
    }

    pub open spec fn is_init_for_asid(&self, asid: int) -> bool {
        // Do not allow asid reuse;
        self.spec_asid() != asid ||
        // Before VM's operatioin
        (self.spec_asid() == asid && rmp_perm_is_init(self.perms))
    }

    pub open spec fn is_valid(&self) -> bool {
        &&& (self.spec_asid() != 0 || !self.spec_assigned())
        &&& (!self.spec_validated() || (self.spec_assigned() && self.spec_asid() != 0))
        &&& self.perms[VMPL::VMPL0]
            === Set::full()
        //&&& !self.validated ==> self.perms === super::perm_s::rmp_perm_init() Not Hold

    }

    pub open spec fn fault_rmp_update(&self, asid: ASID, gpn: GPN, size: PageSize) -> bool {
        ||| self.spec_immutable()
        ||| !self.spec_assigned()
        ||| self.spec_gpn() !== gpn
        ||| self.spec_asid() !== asid
        ||| (self.spec_size() == PageSize::Size2m && size == PageSize::Size4k)
        ||| !self.is_valid()
    }

    pub open spec fn okay_with_pvalidate(
        &self,
        asid: ASID,
        gpn: GPN,
        size: PageSize,
        validated: bool,
    ) -> bool {
        &&& self.spec_immutable() == false
        &&& self.spec_assigned() == true
        &&& self.spec_asid() == asid
        &&& self.spec_gpn() === gpn
        &&& self.spec_size() == size
        &&& self.spec_validated() == (!validated)
    }

    pub open spec fn check_hypervisor_owned(&self) -> bool {
        !self.spec_assigned()
    }

    pub open spec fn check_guest_reverse_mut_size_no_gpn(
        &self,
        asid: ASID,
        size: PageSize,
    ) -> bool {
        &&& self.spec_immutable() == false
        &&& self.spec_assigned() == true
        &&& self.spec_asid() == asid
        &&& self.spec_size() == size
    }

    pub open spec fn check_gpn(&self, gpn: GPN) -> bool {
        self.spec_gpn() === gpn
    }

    pub open spec fn check_guest_reverse_mut_size(
        &self,
        asid: ASID,
        gpn: GPN,
        size: PageSize,
    ) -> bool {
        &&& self.check_gpn(gpn)
        &&& self.check_guest_reverse_mut_size_no_gpn(asid, size)
    }

    pub open spec fn check_validated(&self) -> bool {
        self.spec_validated()
    }

    #[verifier(inline)]
    pub open spec fn check_vmpl(&self, vmpl: VMPL, p: super::perm_s::Perm) -> bool {
        self.spec_perm(vmpl).contains(p)
    }
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/mod.rs
================

use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::arch::errors::*;
use crate::tspec::*;

mod access_p;
mod access_u;
mod db_p;
mod db_s;
mod db_u;
mod def_s;
pub use db_p::*;
pub use db_s::*;
pub use db_u::*;
pub use def_s::*;
pub use perm_s::*;
mod access_s;
mod entry_s;
pub mod perm_s;
mod rmpop_u;

================
File: ./source/verismo/src/arch/rmp/db_s.rs
================

use super::perm_s::*;
use super::*;

verus! {

pub open spec fn rmp_op(rmp: &RmpMap, op: RmpOp<SysPhy>) -> ResultWithErr<
    RmpMap,
    MemError<RmpOp<SysPhy>>,
> {
    let spn = op.to_page_memid().page;
    if !rmp.dom().contains(spn) {
        ResultWithErr::Error(*rmp, MemError::NestedPF(op))
    } else {
        let newentry = rmp.index(spn).trans(op);
        let new = rmp.insert(spn, newentry.to_result());
        newentry.with_ret(new)
    }
}

pub open spec fn rmp_check_access(
    rmp: &RmpMap,
    memid: MemID,
    enc: bool,
    gpmem: GPMem,
    perm: Perm,
    spn: SPN,
) -> ResultOrErr<RmpEntry, MemError<()>> {
    let rmpentry: RmpEntry = rmp.index(spn);
    if !rmp.dom().contains(spn) {
        ResultOrErr::Error(MemError::NestedPF(()))
    } else {
        rmpentry.check_access(memid, enc, gpmem, perm)
    }
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/access_p.rs
================

use super::*;

verus! {

impl RmpEntry {
    pub proof fn lemma_trans_inv(entry: RmpEntry, op: RmpOp<SysPhy>)
        requires
            entry.inv(),
        ensures
            entry.trans(op).to_result().inv(),
    {
        match op {
            RmpOp::RmpUpdate(_, newentry) => {
                assert(entry.rmpupdate(newentry).to_result().inv());
            },
            RmpOp::RmpAdjust(
                PageID { page, memid },
                RmpAdjustParam { gpn, psize, vmsa, vmpl, perms },
            ) => {
                assert(entry.rmpadjust(memid, vmpl, psize, gpn, vmsa, perms).to_result().inv());
            },
            RmpOp::Pvalidate(PageID { page, memid }, PvalidateParam { gpn, psize, val }) => {
                assert(entry.pvalidate(memid, psize, gpn, val).to_result().inv());
            },
        }
    }

    pub proof fn lemma_hvtrans_inv(entry: RmpEntry, op: RmpOp<SysPhy>) -> (next: RmpEntry)
        requires
            entry.inv(),
            op.is_RmpUpdate(),
        ensures
            next === entry.trans(op).to_result(),
            next.inv(),
            next@.inv_hvupdate_rel(entry@),
    {
        let next = entry.trans(op).to_result();
        if (next !== entry) {
            assert(next@.perms =~~= super::perm_s::rmp_perm_init());
            assert(next@.perms[VMPL::VMPL0] =~~= super::perm_s::PagePerm::full());
            assert(next@.perms[VMPL::VMPL0] =~~= entry@.perms[VMPL::VMPL0]);
            assert(next@.perms[VMPL::VMPL1].subset_of(entry@.perms[VMPL::VMPL1]));
        }
        next
    }

    pub proof fn lemma_hvtrans_inv_induct(
        entry: RmpEntry,
        prev_entry: RmpEntry,
        op: RmpOp<SysPhy>,
    ) -> (next: RmpEntry)
        requires
            entry.inv(),
            entry@.inv_hvupdate_rel(prev_entry@),
            op.is_RmpUpdate(),
        ensures
            next === entry.trans(op).to_result(),
            next.inv(),
            next@.inv_hvupdate_rel(prev_entry@),
    {
        entry.trans(op).to_result()
    }
}

} // verus!

================
File: ./source/verismo/src/arch/rmp/access_u.rs
================

use super::*;
use crate::arch::rmp::perm_s::*;

verus! {

impl RmpEntry {
    pub open spec fn view(&self) -> HiddenRmpEntryForPSP {
        self.spec_val()
    }
}

impl RmpEntry {
    pub open spec fn inv(&self) -> bool {
        self@.is_valid()
    }
}

} // verus!

================
File: ./source/verismo/src/arch/addr_s/page.rs
================

use super::*;
use crate::tspec::*;
use crate::*;

#[macro_export]
/// Ensure dummy holder does not take effect when comparing
macro_rules! define_dummy_holder_axiom {
    () => {
        verus!{
        #[verifier(external_body)]
        pub broadcast proof fn axiom_equal(left: Self, right: Self)
        ensures
            (left.value() == right.value()) == #[trigger](left =~= right),
            (left.value() == right.value()) == (left === right),
        {}

        #[verifier(external_body)]
        pub broadcast proof fn axiom_addr_type_dummy_holder(&self)
        ensures
            self.dummy === arbitrary(),
        {}
    }
    };
}

verus! {

impl<T> SpecAddr<T> {
    define_dummy_holder_axiom!{}
}

impl<T> SpecPage<T> {
    define_dummy_holder_axiom!{}
}

} // verus!
verus! {

impl<T> SpecAddr<T> {
    #[verifier::inline]
    pub open spec fn value(&self) -> int {
        self.as_int()
    }

    pub open spec fn to_u64(&self) -> u64 {
        self.value as u64
    }

    pub open spec fn is_valid(&self) -> bool {
        &&& 0 <= self.value()
            < VM_MEM_SIZE!()
        //&&& self.value() % BLOCK_SIZE!() == 0

    }

    pub open spec fn is_valid_end(&self) -> bool {
        &&& 0 <= self.value()
            <= VM_MEM_SIZE!()
        //&&& self.value() % BLOCK_SIZE!() == 0

    }

    pub open spec fn is_valid_with(&self, size: nat) -> bool {
        (*self + size as int).is_valid_end() && self.value() >= 0
    }

    pub open spec fn is_aligned(&self, align: int) -> bool {
        &&& (self.value() % align == 0)
    }

    pub open spec fn to_page(&self) -> SpecPage<T> {
        SpecPage::new(self.value() / PAGE_SIZE!())
    }

    pub open spec fn to_up_page(&self) -> SpecPage<T> {
        (*self + (PAGE_SIZE!() - 1)).to_page()
    }

    pub open spec fn align_up_by(&self, align: int) -> (ret: Self) {
        Self::new(spec_align_up(self.value(), align as int))
    }

    pub open spec fn align_by(&self, align: int) -> (ret: Self) {
        Self::new(spec_align_down(self.value(), align as int))
    }

    pub open spec fn to_offset(&self) -> int {
        self.value() % (PAGE_SIZE as int)
    }

    pub open spec fn convert<T2>(&self, page: SpecPage<T2>) -> SpecAddr<T2> {
        page.to_addr() + self.to_offset() as int
    }

    pub open spec fn new2(val: int, dummy: Ghost<T>) -> Self {
        SpecAddr { value: spec_cast_integer::<_, nat>(val), dummy: dummy }
    }

    pub open spec fn new(val: int) -> Self {
        Self::new2(val, spec_unused())
    }

    pub open spec fn new_u64(val: u64) -> Self {
        Self::new(spec_cast_integer::<_, int>(val))
    }

    pub open spec fn null() -> Self {
        Self::new(0)
    }

    pub open spec fn is_null(&self) -> bool {
        self.value() == 0
    }

    pub open spec fn to_mem(&self, n: nat) -> SpecMem<T> {
        SpecMem::from_range(*self, n)
    }

    #[verifier(inline)]
    pub open spec fn to_set(&self, n: nat) -> Set<int> {
        range_to_set(self.as_int(), n)
    }

    #[verifier(inline)]
    pub open spec fn to_set_with(&self, end: SpecAddr<T>) -> Set<int> {
        self.to_set((end.value() - self.value()) as nat)
    }

    // Replace to_set with lemma_to_set to include basic proofs.
    pub proof fn lemma_to_set(&self, n: nat) -> (ret: Set<int>)
        ensures
            ret.len() == n,
            ret.finite(),
            ret === self.to_set(n),
            n == 0 ==> ret.is_empty(),
    {
        let ret = lemma_to_set(self.as_int(), n);
        lemma_ret_is_empty(ret);
        ret
    }
}

} // verus!
verus! {

impl<T> SpecPage<T> {
    pub open spec fn value(&self) -> int {
        self.as_int()
    }

    pub open spec fn to_u64(&self) -> u64 {
        self.value as u64
    }

    pub open spec fn is_valid(&self) -> bool {
        0 <= self.value() < VM_PAGE_NUM!()
    }

    pub open spec fn is_valid_end(&self) -> bool {
        0 <= self.value() <= VM_PAGE_NUM!()
    }

    pub open spec fn is_valid_with(&self, size: nat) -> bool {
        self.value() >= 0 && self.value() + size <= VM_PAGE_NUM!()
    }

    pub open spec fn is_valid_with_int(&self, size: int) -> bool {
        self.value() >= 0 && self.value() + size <= VM_PAGE_NUM!()
    }

    pub open spec fn new(val: int) -> Self {
        Self::new2(val, spec_unused())
    }

    pub open spec fn new2(val: int, dummy: Ghost<T>) -> Self {
        SpecPage { value: val as nat, dummy: dummy }
    }

    pub open spec fn null() -> Self {
        Self::new(0)
    }

    pub open spec fn is_null(&self) -> bool {
        self.value() == 0
    }

    pub open spec fn to_addr(&self) -> SpecAddr<T> {
        let page = *self;
        SpecAddr::new(self.value() * PAGE_SIZE!())
    }

    pub open spec fn len_with_end(&self, end: SpecPage<T>) -> nat {
        let gva1 = self.to_addr();
        let gva2 = end.to_addr();
        let len = (gva2.value() - gva1.value());
        if len > 0 {
            len as nat
        } else {
            0
        }
    }

    #[verifier(inline)]
    pub open spec fn to_set(&self, n: nat) -> Set<int> {
        range_to_set(self.as_int(), n)
    }

    #[verifier(inline)]
    pub open spec fn to_set_with(&self, end: SpecPage<T>) -> Set<int> {
        self.to_set((end.value() - self.value()) as nat)
    }

    // Replace to_set with lemma_to_set to include basic proofs.
    pub proof fn lemma_to_set(&self, n: nat) -> (ret: Set<int>)
        ensures
            ret.len() == n,
            ret.finite(),
            ret === self.to_set(n),
            n == 0 <==> ret.is_empty(),
    {
        let ret = lemma_to_set(self.as_int(), n);
        lemma_ret_is_empty(ret);
        ret
    }

    pub open spec fn to_mem(&self) -> SpecMem<T> {
        let page = *self;
        page.to_addr().to_mem(PAGE_SIZE!() as nat)
    }

    pub open spec fn valid_as_size(&self, psize: PageSize) -> bool {
        match psize {
            PageSize::Size4k => { true },
            PageSize::Size2m => { self.value() % (PAGE_2M_SIZE!() / PAGE_SIZE!()) == 0 },
        }
    }
}

#[verifier(inline)]  // inline due to unsupported for bit-vector
pub open spec fn spec_page_align_down(addr: int) -> int {
    //addr - addr%PAGE_SIZE is unsupported
    addr / (PAGE_SIZE!() as int) * (PAGE_SIZE!() as int)
}

impl<T> IntValue for SpecPage<T> {
    open spec fn as_int(&self) -> int {
        self.value as int
    }

    open spec fn from_int(val: int) -> Self {
        Self::new(val)
    }
}

impl<T> IntValue for SpecAddr<T> {
    open spec fn as_int(&self) -> int {
        self.value as int
    }

    open spec fn from_int(val: int) -> Self {
        Self::new(val)
    }
}

impl<T> IntOrd for SpecPage<T> {
    #[verifier(inline)]
    open spec fn ord_int(&self) -> int {
        self.as_int()
    }
}

impl<T> IntOrd for SpecAddr<T> {
    #[verifier(inline)]
    open spec fn ord_int(&self) -> int {
        self.as_int()
    }
}

impl<T> SpecMem<T> {
    #[verifier(external_body)]
    pub broadcast proof fn axiom_inv(&self)
        ensures
            (self.offset() + self.len()) <= PAGE_SIZE!(),
            self.offset() < PAGE_SIZE!(),
    {
    }

    pub proof fn proof_same_page(&self)
        ensures
            self.last().to_page() == self.to_page(),
            self.first().to_page() == self.to_page(),
    {
        let first_value = self.first().value();
        assert(first_value == self.to_page().to_addr().value() + self.offset());
        assert((self.to_page().to_addr().value() + self.offset()) / PAGE_SIZE!()
            === self.to_page().value());
        if self.len() > 0 {
            let last_value = self.last().value();
            assert(last_value == self.to_page().to_addr().value() + self.offset() + self.len() - 1);
            assert(self.offset() + self.len() <= PAGE_SIZE!());
            assert((self.to_page().as_int() * PAGE_SIZE!() + self.offset() + self.len() - 1)
                / PAGE_SIZE!() == self.to_page().as_int());
        } else {
            assert(self.last() == self.first());
        }
    }

    #[verifier(inline)]
    pub open spec fn inv(&self) -> bool {
        self.offset() + self.len() <= PAGE_SIZE!()
    }

    pub open spec fn from_range(addr: SpecAddr<T>, size: nat) -> Self
        recommends
            (addr + size as int - 1int).to_page() === addr.to_page(),
    {
        SpecMem { first: addr, size: size }
    }

    pub open spec fn convert<T2>(&self, pn: SpecPage<T2>) -> SpecMem<T2> {
        SpecMem { first: pn.to_addr() + self.first().to_offset(), size: self.size }
    }

    pub open spec fn is_aligned(&self, align: int) -> bool {
        self.offset() as int % align == 0
    }

    #[verifier(inline)]
    pub open spec fn contains(&self, addr: SpecAddr<T>) -> bool {
        self.to_set().contains(addr.as_int())
    }

    pub open spec fn is_valid(&self) -> bool {
        &&& self.len() > 0
        &&& self.to_page().is_valid()
    }

    pub open spec fn disjoint(&self, rhs: Self) -> bool {
        range_disjoint(self.first().as_int(), self.len(), rhs.first().as_int(), rhs.len())
    }

    pub proof fn lemma_disjoint(&self, rhs: Self) -> (ret: bool)
        ensures
            ret == self.disjoint(rhs),
            ret == self.to_set().disjoint(rhs.to_set()),
    {
        lemma_range_set_disjoint(self.first().as_int(), self.len(), rhs.first().as_int(), rhs.len())
    }

    #[verifier(inline)]
    pub open spec fn spec_index(&self, i: int) -> SpecAddr<T> {
        self.first() + i
    }

    #[verifier(inline)]
    pub open spec fn first(&self) -> SpecAddr<T> {
        self.first
    }

    pub open spec fn last(&self) -> SpecAddr<T> {
        if self.len() > 0 {
            self.first() + (self.len() - 1) as int
        } else {
            self.first()
        }
    }

    #[verifier(inline)]
    pub open spec fn end(&self) -> SpecAddr<T> {
        self.first() + (self.len() as int)
    }

    pub open spec fn offset(&self) -> nat {
        self.first().to_offset() as nat
    }

    pub open spec fn len(&self) -> nat {
        self.size
    }

    #[verifier(inline)]
    pub open spec fn to_set(&self) -> Set<int> {
        self.first().to_set(self.len())
    }

    pub proof fn lemma_to_set(&self) -> (ret: Set<int>)
        ensures
            ret.len() == self.len(),
            ret.finite(),
            ret === self.to_set(),
            ret === self.first().to_set(self.len()),
    {
        self.first().lemma_to_set(self.len())
    }

    pub open spec fn to_page(&self) -> SpecPage<T> {
        self.first().to_page()
    }

    pub proof fn lemma_aligned_mem_eq_or_disjoint(mem1: SpecMem<T>, mem2: SpecMem<T>, align: int)
        requires
            align == 8,
            mem1.is_aligned(align),
            mem2.is_aligned(align),
            mem1.len() == align,
            mem2.len() == align,
        ensures
            mem1 === mem2 || mem1.disjoint(mem2),
    {
        if !(mem1 =~= mem2) {
            assert(!(mem1.first() =~= mem2.first()));
            assert forall|i| 0 <= i < mem1.len() implies !#[trigger] mem2.contains(mem1[i]) by {
                assert forall|j| 0 <= j < mem2.len() implies !(mem2[j] =~= mem1[i]) by {
                    assert(mem2.first() != mem1.first());
                    assert(mem2[j] == mem2.first() + j);
                    assert(mem1[j] == mem1.first() + j);
                }
            }
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/addr_s/def_s.rs
================

use super::*;
use crate::tspec_e::*;

verus! {

global size_of usize == 8;

#[derive(Copy, Clone, VTypeCast, ExecStruct, NotPrimitive, VTypeCastSec, SpecSize, WellFormed, IsConstant)]
pub struct GuestVir;

#[derive(Copy, Clone, VTypeCast, ExecStruct, NotPrimitive, VTypeCastSec, SpecSize, WellFormed, IsConstant)]
pub struct GuestPhy;

#[derive(Copy, Clone, VTypeCast, ExecStruct, NotPrimitive, VTypeCastSec, SpecSize, WellFormed, IsConstant)]
pub struct SysPhy;

pub trait AddrType {

}

impl AddrType for GuestVir {

}

impl AddrType for GuestPhy {

}

impl AddrType for SysPhy {

}

pub type SizeType = u64;

/// 4K Page
#[verifier::ext_equal]
pub struct SpecPage<T> {
    pub value: nat,
    pub dummy: Ghost<T>,
}

// T: AddrType
#[verifier::ext_equal]
pub struct SpecAddr<T> {
    pub value: nat,
    pub dummy: Ghost<T>,
}

// T: AddrType
pub struct SpecMem<T> {
    pub first: SpecAddr<T>,
    pub size: nat,
}

pub type GVN = SpecPage<GuestVir>;

pub type GPN = SpecPage<GuestPhy>;

pub type SPN = SpecPage<SysPhy>;

pub type GVA = SpecAddr<GuestVir>;

pub type GPA = SpecAddr<GuestPhy>;

pub type SPA = SpecAddr<SysPhy>;

pub type GVMem = SpecMem<GuestVir>;

pub type GPMem = SpecMem<GuestPhy>;

pub type SPMem = SpecMem<SysPhy>;

// VM constants
// Need to publish those constant if it is used in verification;
// otherwise, the root module will not understand those constant in spec.
//#[allow(unused_variables)]
crate::macro_const_int! {
    #[macro_export]
    pub const PAGE_SHIFT: usize = 12usize;
    #[macro_export]
    pub const PAGE_SIZE: usize = 0x1000usize;
    #[macro_export]
    pub const PAGE_2M_SIZE: usize = 0x200000usize;
    #[macro_export]
    pub const VM_MEM_SIZE: usize = 0x10_0000_0000_0000usize;
    #[macro_export]
    pub const VM_PAGE_NUM: usize = 0x100_0000_0000usize;
    #[macro_export]
    pub const BLOCK_SIZE: usize = 1usize;
}

#[is_variant]
#[derive(Copy, Clone, PartialEq, Eq, SpecIntEnum)]
pub enum PageSize {
    Size4k = 0,
    Size2m = 1,
}

verus! {
impl PageSize {
    pub open spec fn size(&self) -> int
    {
        match self {
            PageSize::Size4k => {
                PAGE_SIZE!()
            },
            PageSize::Size2m => {
                PAGE_2M_SIZE!()
            }
        }
    }
}
}

} // verus!

================
File: ./source/verismo/src/arch/addr_s/mod.rs
================

#[macro_use]
mod def_s;
mod page;

pub use def_s::*;

use crate::tspec::*;

================
File: ./source/verismo/src/arch/ramdb/ram_s.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::arch::crypto::encdec::SpecEncrypt;
use crate::arch::crypto::*;
use crate::arch::entities::*;
use crate::tspec::*;
use crate::*;

crate::macro_const_int! {
    pub const MEM_UNIT_SIZE: u64 = 16u64;
}

verus! {

pub open spec fn idx2(spa_val: int) -> int {
    spa_val / (MEM_UNIT_SIZE as int)
}

pub open spec fn idx(spa: SPA) -> int {
    idx2(spa.as_int())
}

pub open spec fn memrange_contains_block(range: SpecMem<SysPhy>, i: int) -> bool {
    &&& idx(range.first()) <= i
    &&& idx(range.last()) >= i
}

impl RamDB {
    pub open spec fn len(&self) -> nat {
        self.data.len()
    }

    #[verifier(opaque)]
    pub open spec fn inv(&self) -> bool {
        forall|spa: SPA|
            (0 <= spa.as_int() < self.spec_data().len()) ==> (
            #[trigger] self.data[spa.as_int()]).key.key.1 == idx(spa)
    }

    /// The encryption of data is done with a 128-bit key in a mode which
    /// utilizes an additional physical address-based tweak to protect
    /// against cipher-text block moveattacks
    pub open spec fn write<T: VTypeCast<Seq<u8>>>(&self, asid: ASID, spa: SPMem, data: T) -> Self {
        let bytes = stream_from_data(data).subrange(0, spec_size::<T>() as int);
        self.write_raw(asid, spa, bytes)
    }

    pub open spec fn to_write(
        &self,
        i: SPA,
        asid: ASID,
        spmem: SPMem,
        bytes: Seq<Byte>,
    ) -> EncryptedByte {
        let crypto_mask: Byte = self.crypto_mask[self.spec_write_count()].get_mask();
        if spmem.contains(i) {
            let k = i.as_int() - spmem[0].as_int();
            SymKey { key: (asid, idx(i)) }.encrypt(bytes[k], crypto_mask)
        } else if memrange_contains_block(spmem, idx(i)) {
            SymKey { key: (asid, idx(i)) }.encrypt(
                SymKey { key: (asid, idx(i)) }.decrypt(self.data[i.as_int()]),
                crypto_mask,
            )
        } else {
            self.data[i.as_int()]
        }
    }

    pub open spec fn write_raw(&self, asid: ASID, spmem: SPMem, bytes: Seq<Byte>) -> Self {
        let mut new = *self;
        let bytes = Stream::new(
            self.spec_data().len(),
            |i: int| self.to_write(SPA::new(i), asid, spmem, bytes),
        );
        self.spec_set_data(bytes).spec_set_write_count(self.spec_write_count() + 1)
    }

    pub open spec fn read_one_byte(&self, asid: ASID, spa: SPA) -> Byte {
        SymKey { key: (asid, idx(spa)) }.decrypt(self.data[spa.as_int()])
    }

    pub open spec fn read_byte_at(&self, asid: ASID, spmem: SPMem, i: int) -> Byte {
        if 0 <= i < spmem.len() {
            self.read_one_byte(asid, spmem[i])
        } else {
            0
        }
    }

    /// Only the VM itself can decrypt the data at the exact spa;
    #[verifier(inline)]
    pub open spec fn read_bytes_by_asid(&self, asid: ASID, spmem: SPMem) -> ByteStream {
        Stream::new(spmem.len(), |i: int| self.read_one_byte(asid, spmem[i]))
    }

    //#[verifier(opaque)]
    pub open spec fn read_by_asid<T: VTypeCast<Seq<u8>>>(&self, asid: ASID, spmem: SPMem) -> T {
        let bytes = self.read_bytes_by_asid(asid, spmem);
        stream_to_data(bytes)
    }

    pub open spec fn disjoint_write_read_requires(
        &self,
        asid: ASID,
        spa: SPMem,
        rspa: SPMem,
    ) -> bool {
        &&& rspa.disjoint(spa)
    }

    /// Only the VM itself can decrypt the data at the exact spa;
    #[verifier(opaque)]
    pub open spec fn read_all_by_asid(&self, asid: ASID) -> ByteStream {
        Stream::new(
            self.data.len(),
            |i: int|
                if !SPA::new(0).to_page().is_null() {
                    SymKey { key: (asid, idx2(i)) }.decrypt(self.data[i])
                } else {
                    0
                },
        )
    }

    pub open spec fn hv_update(&self, newram: Self, memid: MemID) -> Self {
        newram
    }

    pub open spec fn hv_view(&self, memid: MemID) -> ByteStream {
        self.read_all_by_asid(memid.to_asid())
    }
}

} // verus!

================
File: ./source/verismo/src/arch/ramdb/def.rs
================

use verismo_macro::*;

use crate::arch::addr_s::*;
use crate::arch::crypto::{CryptoMask, Encrypted, SymKey};
use crate::arch::entities::*;
use crate::tspec::*;

pub type MemKey = SymKey<(ASID, int)>;

pub type EncryptedByte = Encrypted<MemKey, Byte>;

verus! {

#[derive(SpecGetter, SpecSetter)]
pub struct RamDB {
    pub data: Seq<EncryptedByte>,
    pub write_count: int,
    pub crypto_mask: Seq<CryptoMask>,
}

} // verus!

================
File: ./source/verismo/src/arch/ramdb/ramdb.rs-todo
================

use super::memid::ASID;
use super::*;
use crate::arch::addr::SPA;
use crate::crypto::*;
use crate::pervasive::map::Map;
use crate::pervasive::option::*;
use crate::pervasive::seq::Seq;
use crate::pervasive::set::Set;
use crate::tspec::size_s::*;
use crate::tspec::Byte;
use crate::*;

pub type MemKey = SymKey<(ASID, int)>;

crate::macro_const_int! {
    pub const MEM_UNIT_SIZE: u64 = 16u64;
}

type EncryptedByte = Encrypted<MemKey, Byte>;

#[derive(SpecGetter, SpecSetter)]
pub struct RamDB {
    pub data: Stream<EncryptedByte>,
    pub dom: Set<SPA>,
    pub write_count: int,
    pub crypto_mask: Stream<CryptoMask>,
}

verus! {
    pub open spec fn idx(spa: SPA) -> int{
        idx2(spa.value())
    }

    pub open spec fn idx2(spa_val: int) -> int{
        spa_val / MEM_UNIT_SIZE!()
    }

    impl SPMem {
        pub open spec fn contains_block_idx(&self, i: int) -> bool
        {
            &&& idx(self[0]) <= i
            &&& idx(self[self.len() -1]) >= i
        }
    }

impl RamDB {
    pub open spec fn dom(&self) -> Set<SPA> {
        self.dom
    }

    pub closed spec fn inv(&self) -> bool {
        forall |spa: SPA|  (#[trigger] self.data[spa.value()]).key.key.1 == idx(spa)
    }

    pub proof fn lemma_write_inv(&self, asid: ASID, spmem: SPMem, bytes: Seq<Byte>)
    requires
        self.inv()
    ensures
        self.write_raw(asid, spmem, bytes).inv(),
        self.write_raw(asid, spmem, bytes).dom() === self.dom(),
    {
        reveal(Self::write_raw);
        let new = self.write_raw(asid, spmem, bytes);
        assert forall |spa: SPA|
            (#[trigger] new.data[spa.value()]).key.key.1 ==  idx(spa)
        by {
            self.lemma_write_raw(asid, spmem, bytes);
            assert(new.data[spa.value()] === self.to_write(spa, asid, spmem, bytes));
        };
    }

    /// The encryption of data is done with a 128-bit key in a mode which
    /// utilizes an additional physical address-based tweak to protect
    /// against cipher-text block moveattacks
    pub open spec fn write<T>(&self, asid: ASID, spa: SPMem, data: T) -> Self
    {
        let bytes = ByteStream::from_data(data).spec_take(0, spec_size::<T>());
        self.write_raw(asid, spa, bytes)
    }

    pub open spec fn to_write(&self, i: SPA, asid: ASID, spmem: SPMem, bytes: Seq<Byte>) -> EncryptedByte
    {
        let crypto_mask: Byte = self.crypto_mask[self.spec_write_count()].get_mask();
        if spmem.contains(i)
        {
            let k = i.value() - spmem[0].value();
            SymKey{key: (asid, idx(i))}.encrypt(
                bytes[k],
                crypto_mask)
        } else if spmem.contains_block_idx(idx(i))  {
            SymKey{key: (asid, idx(i))}.encrypt(
                SymKey{key: (asid, idx(i))}.decrypt(self.data[i.value()]),
                crypto_mask
            )
        } else {
            self.data[i.value()]
        }
    }

    pub open spec fn write_raw(&self, asid: ASID, spmem: SPMem, bytes: Seq<Byte>) -> Self
    {
        let bytes =
        Stream::new(
            |i: int|
            self.to_write(SPA::new(i), asid, spmem, bytes)
        );
        self.spec_set_data(bytes).spec_set_write_count(self.spec_write_count() + 1)
    }

    pub proof fn lemma_write_raw(&self, asid: ASID, spmem: SPMem, bytes: Seq<Byte>)
    ensures
        forall|spa: SPA| self.write_raw(asid, spmem, bytes).spec_data()[spa.value()] === #[trigger] self.to_write(spa, asid, spmem, bytes)
    {
        reveal(Self::write_raw);
    }

    pub open spec fn read_one_byte(&self, asid: ASID, spa: SPA) -> Byte {
        SymKey{key: (asid, idx(spa))}.decrypt(self.data[spa.value()])
    }

    pub open spec fn read_byte_at(&self, asid: ASID, spmem: SPMem, i: int) -> Byte {
        if 0 <= i < spmem.len() {
            self.read_one_byte(asid, spmem[i])
        } else {
            0
        }
    }

    /// Only the VM itself can decrypt the data at the exact spa;
    #[verifier(inline)]
    pub open spec fn read_bytes_by_asid(&self, asid: ASID, spmem: SPMem) -> ByteStream {
        Stream::new(
            |i: int| self.read_byte_at(asid, spmem, i)
        )
    }

    //#[verifier(opaque)]
    pub open spec fn read_by_asid<T>(&self, asid: ASID, spmem: SPMem) -> T {
        let bytes = self.read_bytes_by_asid(asid, spmem);

       ByteStream::to_data(bytes)
    }

    pub proof fn lemma_write_change_byte(&self, asid: ASID, spmem: SPMem, data: ByteStream, rspa: SPA)
    requires
        spmem.is_valid(),
        spmem.contains(rspa),
    ensures
        self.write_raw(asid, spmem, data.to_seq(spmem.len())).read_one_byte(asid, rspa) === data[rspa.value() - spmem.first().value()]
    {
        reveal(Self::write_raw);
        let bytes = data.to_seq(spmem.len());
        let crypto_mask: Byte = self.crypto_mask[self.spec_write_count()].get_mask();
        let new = self.write_raw(asid, spmem, bytes);
        let read_byte = new.read_one_byte(asid, rspa);
        let k = rspa.value() - spmem.first().value();
        assert(0 <= k < spmem.len());
        assert(new.data[rspa.value()] ===
        SymKey{key: (asid, idx(spmem[k]))}.encrypt(
            bytes[k],
            crypto_mask)
        );
        assert(read_byte === SymKey{key: (asid, idx(spmem[k]))}.decrypt(new.data[spmem[k].value()]));
        assert(bytes[k] === read_byte);
    }

    pub proof fn lemma_write_unchange_byte(&self, asid: ASID, spmem: SPMem, data: ByteStream, rspa: SPA)
    requires
        spmem.is_valid(),
        !spmem.contains(rspa),
    ensures
        self.write_raw(asid, spmem, data.to_seq(spmem.len())).read_one_byte(asid, rspa) === self.read_one_byte(asid, rspa)
    {
        reveal(Self::write_raw);
    }

    pub proof fn lemma_write_unchange_byte_any_enc(&self, asid: ASID, spmem: SPMem, data: ByteStream, rasid: ASID, rspa: SPA)
    requires
        spmem.is_valid(),
        idx(spmem.first()) > idx(rspa) || idx(spmem.last()) < idx(rspa),
    ensures
        self.write_raw(asid, spmem, data.to_seq(spmem.len())).read_one_byte(rasid, rspa) === self.read_one_byte(rasid, rspa)
    {
        reveal(Self::write_raw);
    }

    //#[verifier(external_body)]
    pub proof fn proof_read_write(&self, asid: ASID, spmem: SPMem, data: ByteStream)
    requires
        spmem.is_valid(),
    ensures
        self.write_raw(asid, spmem, data.to_seq(spmem.len())).read_bytes_by_asid(asid, spmem).to_seq(spmem.len()) === data.to_seq(spmem.len()),
    {
        reveal(Self::read_bytes_by_asid);
        reveal(Self::write_raw);
        let bytes = data.to_seq(spmem.len());
        let new = self.write_raw(asid, spmem, bytes);
        let read_bytes = new.read_bytes_by_asid(asid, spmem);
        let crypto_mask: Byte = self.crypto_mask[self.spec_write_count()].get_mask();
        assert(
            read_bytes === Stream::new(
                |i: int| new.read_byte_at(asid, spmem, i)
            )
        );
        assert(
            new.data === Stream::new(
                |i: int| self.to_write(SPA::new(i), asid, spmem, bytes)
            )
        );
        assert(bytes.len() === spmem.len());
        assert forall | k|
            0 <= k < bytes.len()
        implies
            (bytes[k] === read_bytes[k])
        by {
            let i = spmem[k].value();
            assert(k == i - spmem[0].value());
            assert(spmem[k].to_page() === spmem.to_page());
            assert(0 <= k < spmem.len());
            assert(spmem.contains(spmem[k]));
            self.lemma_write_change_byte(asid, spmem, data, spmem[k]);
        }
        assert(bytes.ext_equal(read_bytes.to_seq(spmem.len())));
    }

    pub open spec fn spa_is_encrypted(&self, spa: SPA, asid: ASID) -> bool
    {
        true
        /*
        self.data[spa.value()].key === SymKey{
            key: (asid, idx(spa))
        }*/
    }

    pub open spec fn disjoint_write_read_requires(&self, asid: ASID, spa: SPMem, rspa: SPMem) -> bool
    {
        &&& rspa.disjoint(spa)
        /*&&& (forall |i| 0 <= i < spa.len() ==>
                self.spa_is_encrypted(#[trigger] spa[i], asid))*/
    }

    //#[verifier(external_body)]
    pub proof fn proof_read_write_no_change(&self, asid: ASID, spmem: SPMem, data: ByteStream, rspa: SPMem)
    requires
        self.inv(),
        spmem.is_valid(),
        //self.disjoint_write_read_requires(asid, spmem, rspa),
        rspa.disjoint(spmem)
    ensures
        self.write_raw(asid, spmem, data.to_seq(spmem.len())).read_bytes_by_asid(asid, rspa) === self.read_bytes_by_asid(asid, rspa),
    {
        let new = self.write_raw(asid, spmem, data.to_seq(spmem.len()));
        reveal(Self::read_bytes_by_asid);
        let bytes = Stream::new(
            |i: int| self.read_byte_at(asid, rspa, i)
        );
        let new_bytes = Stream::new(
            |i: int| new.read_byte_at(asid, rspa, i)
        );
        assert forall |s: int|
            0 <= s < rspa.len()
        implies
            bytes[s] === new_bytes[s]
        by {
            self.lemma_write_unchange_byte(asid, spmem, data, rspa[s]);
        }
        assert(new_bytes === bytes) by {
            assert(new_bytes.ext_equal(bytes));
        }
    }


    /// Only the VM itself can decrypt the data at the exact spa;
    #[verifier(opaque)]
    pub open spec fn read_all_by_asid(&self, asid: ASID) -> ByteStream {
        Stream::new(
            |i: int| if !SPA::new(0).to_page().is_null() {
                    SymKey{key: (asid, idx2(i))}.decrypt(self.data[i])
                } else {
                    0
                }
        )
    }

    pub open spec fn hv_update(&self, newram: Self, memid: MemID) -> Self {
        newram
    }

    pub open spec fn hv_view(&self, memid: MemID) -> ByteStream {
        self.read_all_by_asid(memid.to_asid())
    }
}
}

================
File: ./source/verismo/src/arch/ramdb/mod.rs
================

pub mod def;
pub use def::*;

mod ram_p;
pub mod ram_s;

================
File: ./source/verismo/src/arch/ramdb/ram_p.rs
================

use super::ram_s::*;
use super::*;
use crate::arch::addr_s::*;
use crate::arch::crypto::encdec::SpecEncrypt;
use crate::arch::crypto::SymKey;
use crate::arch::entities::ASID;
use crate::tspec::*;
use crate::*;

verus! {

impl RamDB {
    #[verifier(external_body)]
    pub broadcast proof fn axiom_ram_len1(&self)
        ensures
            #[trigger] self.data.len() == VM_MEM_SIZE!(),
    {
    }

    #[verifier(external_body)]
    pub broadcast proof fn axiom_ram_len2(&self, spa: SPA)
        ensures
            #[trigger] spa.as_int() < #[trigger] self.data.len(),
    {
    }

    pub proof fn lemma_write_inv(&self, asid: ASID, spmem: SPMem, bytes: Seq<Byte>)
        requires
            self.inv(),
        ensures
            self.write_raw(asid, spmem, bytes).inv(),
    {
        reveal(RamDB::inv);
        let new = self.write_raw(asid, spmem, bytes);
        assert forall|spa: SPA| (0 <= spa.as_int() < self.spec_data().len()) implies (
        #[trigger] new.data[spa.as_int()]).key.key.1 == idx(spa) by {
            assert(0 <= spa.as_int() < self.spec_data().len());
            self.lemma_write_raw(spa, asid, spmem, bytes);
            assert(new.data[spa.as_int()] === self.to_write(spa, asid, spmem, bytes));
            assert((self.data[spa.as_int()]).key.key.1 == idx(spa));
        };
    }

    pub proof fn lemma_write_raw(&self, spa: SPA, asid: ASID, spmem: SPMem, bytes: Seq<Byte>)
        requires
            0 <= spa.as_int() < self.spec_data().len(),
        ensures
            self.write_raw(asid, spmem, bytes).spec_data()[spa.as_int()] === self.to_write(
                spa,
                asid,
                spmem,
                bytes,
            ),
    {
        reveal(RamDB::write_raw);
    }

    pub proof fn lemma_write_change_byte(
        &self,
        asid: ASID,
        spmem: SPMem,
        bytes: Seq<Byte>,
        rspa: SPA,
    )
        requires
            spmem.last().as_int() < self.len(),
            spmem.is_valid(),
            spmem.contains(rspa),
            bytes.len() == spmem.len(),
        ensures
            self.write_raw(asid, spmem, bytes).read_one_byte(asid, rspa) === bytes[rspa.as_int()
                - spmem.first().as_int()],
    {
        let crypto_mask: Byte = self.crypto_mask[self.spec_write_count()].get_mask();
        let new = self.write_raw(asid, spmem, bytes);
        let read_byte = new.read_one_byte(asid, rspa);
        let k = rspa.as_int() - spmem.first().as_int();
        assert(0 <= k < spmem.len());
        assert(0 <= rspa.as_int() < self.len());
        self.lemma_write_raw(rspa, asid, spmem, bytes);
        assert(new.data[rspa.as_int()] === SymKey { key: (asid, idx(spmem[k])) }.encrypt(
            bytes[k],
            crypto_mask,
        ));
        assert(read_byte === SymKey { key: (asid, idx(spmem[k])) }.decrypt(
            new.data[spmem[k].as_int()],
        ));
        assert(bytes[k] === read_byte);
    }

    pub proof fn lemma_write_unchange_byte(
        &self,
        asid: ASID,
        spmem: SPMem,
        bytes: ByteStream,
        rspa: SPA,
    )
        requires
            spmem.is_valid(),
            !spmem.contains(rspa),
            spmem.last().as_int() < self.len(),
            bytes.len() == spmem.len(),
            rspa.as_int() < self.len(),
        ensures
            self.write_raw(asid, spmem, bytes).read_one_byte(asid, rspa) === self.read_one_byte(
                asid,
                rspa,
            ),
    {
        reveal(RamDB::write_raw);
        let new = self.write_raw(asid, spmem, bytes);
        if !memrange_contains_block(spmem, idx(rspa)) {
            assert(self.to_write(rspa, asid, spmem, bytes) === self.spec_data()[rspa.as_int()]);
            assert(self.to_write(SPA::new(rspa.as_int()), asid, spmem, bytes) === self.write_raw(
                asid,
                spmem,
                bytes,
            ).spec_data()[rspa.as_int()]);
        }
    }

    pub proof fn lemma_write_unchange_byte_any_enc(
        &self,
        asid: ASID,
        spmem: SPMem,
        bytes: ByteStream,
        rasid: ASID,
        rspa: SPA,
    )
        requires
            spmem.is_valid(),
            spmem.last().as_int() < self.len(),
            rspa.as_int() < self.len(),
            !memrange_contains_block(spmem, idx(rspa)),
            bytes.len() == spmem.len(),
        ensures
            self.write_raw(asid, spmem, bytes).read_one_byte(rasid, rspa) === self.read_one_byte(
                rasid,
                rspa,
            ),
    {
        reveal(RamDB::write_raw);
        let new = self.write_raw(asid, spmem, bytes);
        assert(self.to_write(rspa, asid, spmem, bytes) == self.data[rspa.as_int()]);
        assert(self.to_write(rspa, asid, spmem, bytes) == new.data[rspa.as_int()]);
    }

    pub proof fn proof_read_write(&self, asid: ASID, spmem: SPMem, bytes: ByteStream)
        requires
            spmem.is_valid(),
            spmem.last().as_int() < self.len(),
            bytes.len() == spmem.len(),
        ensures
            self.write_raw(asid, spmem, bytes).read_bytes_by_asid(asid, spmem) === bytes,
    {
        reveal(RamDB::read_bytes_by_asid);
        reveal(RamDB::write_raw);
        let new = self.write_raw(asid, spmem, bytes);
        let read_bytes = new.read_bytes_by_asid(asid, spmem);
        let crypto_mask: Byte = self.crypto_mask[self.spec_write_count()].get_mask();
        assert(new.data === Stream::new(
            self.data.len(),
            |i: int| self.to_write(SPA::new(i), asid, spmem, bytes),
        ));
        assert(bytes.len() === spmem.len());
        assert forall|k| 0 <= k < bytes.len() implies (bytes[k] === read_bytes[k]) by {
            let i = spmem[k].as_int();
            assert(k == i - spmem[0].as_int());
            assert(spmem[k].to_page() === spmem.to_page());
            assert(0 <= k < spmem.len());
            assert(spmem.contains(spmem[k]));
            self.lemma_write_change_byte(asid, spmem, bytes, spmem[k]);
        }
        assert(bytes =~~= (read_bytes));
    }

    //#[verifier(external_body)]
    pub proof fn proof_read_write_no_change(
        &self,
        asid: ASID,
        spmem: SPMem,
        data: ByteStream,
        rspmem: SPMem,
    )
        requires
            self.inv(),
            spmem.is_valid(),
            spmem.last().as_int() < self.len(),
            rspmem.is_valid(),
            rspmem.last().as_int() < self.len(),
            data.len() == spmem.len(),
            rspmem.disjoint(spmem),
        ensures
            self.write_raw(asid, spmem, data).read_bytes_by_asid(asid, rspmem)
                === self.read_bytes_by_asid(asid, rspmem),
    {
        reveal(RamDB::inv);
        let new = self.write_raw(asid, spmem, data);
        reveal(RamDB::read_bytes_by_asid);
        let bytes = Stream::new(rspmem.len(), |i: int| self.read_one_byte(asid, rspmem[i]));
        let new_bytes = Stream::new(rspmem.len(), |i: int| new.read_one_byte(asid, rspmem[i]));
        assert forall|s: int| 0 <= s < rspmem.len() implies bytes[s] === new_bytes[s] by {
            self.lemma_write_unchange_byte(asid, spmem, data, rspmem[s]);
        }
        assert(new_bytes === bytes) by {
            assert(new_bytes =~~= (bytes));
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/mod.rs
================

#![cfg_attr(rustfmt, rustfmt_skip)]
pub mod addr_s;
pub mod entities;
pub mod attack;
pub mod crypto;
pub mod rmp;
pub mod memop;
pub mod errors;
#[macro_use]
pub mod reg;
pub mod ramdb;
pub mod pgtable;
pub mod tlb;
pub mod vram;
pub mod ptram;
pub mod mem;
pub mod x64;
// pub mod ptr;
// pub mod addr_e;

// pub mod addr_s;
// pub mod attack;
// pub mod crypto;
// pub mod entities;
// pub mod errors;
// pub mod mem;
// #[macro_use]
// pub mod pgtable;
// pub mod memop;
// pub mod ptram;
// pub mod ramdb;
// pub mod reg;
// pub mod rmp;
// pub mod tlb;
// pub mod vram;
// pub mod x64;
//pub mod addr_e;

================
File: ./source/verismo/src/arch/memop/gvmemop.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;

verus! {

impl MemOp<GuestVir> {
    pub open spec fn translate_gpn(&self, gpmem: GPMem, enc: bool) -> MemOp<GuestPhy> {
        match *self {
            MemOp::Read(addr_id, _) => {
                MemOp::Read(AddrMemID { memid: addr_id.memid, range: gpmem }, enc)
            },
            MemOp::Write(addr_id, _, data) => {
                MemOp::Write(AddrID { memid: addr_id.memid, addr: gpmem.first() }, enc, data)
            },
            MemOp::RmpOp(rmpop) => { MemOp::RmpOp(rmpop.set_gpn(gpmem.to_page())) },
            MemOp::InvlPage(gvn_id) => {
                MemOp::InvlPage(AddrMemID { memid: gvn_id.memid, range: gpmem })
            },
            MemOp::FlushAll(memid) => { MemOp::FlushAll(memid) },
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/memop/memop.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;

verus! {

impl<AddrT: AddrType> MemOp<AddrT> {
    #[verifier(inline)]
    pub open spec fn is_PValidate(&self) -> bool {
        self.is_RmpOp() && self.get_RmpOp_0().is_Pvalidate()
    }

    pub open spec fn to_addr_memid(&self) -> AddrMemID<AddrT> {
        match *self {
            MemOp::Read(addr_id, _) => { addr_id },
            MemOp::Write(addr_id, _, bytes) => {
                AddrMemID { range: addr_id.addr.to_mem(bytes.len()), memid: addr_id.memid }
            },
            MemOp::RmpOp(rmpop) => {
                let PageID { memid, page } = rmpop.to_page_memid();
                AddrMemID { range: page.to_mem(), memid }
            },
            MemOp::InvlPage(addr_id) => { addr_id },
            MemOp::FlushAll(memid) => {
                AddrMemID { range: SpecMem::from_range(SpecAddr::null(), 0), memid }
            },
        }
    }

    #[verifier(inline)]
    pub open spec fn to_memid(&self) -> MemID {
        self.to_addr_memid().memid
    }

    #[verifier(inline)]
    pub open spec fn to_mem(&self) -> SpecMem<AddrT> {
        self.to_addr_memid().range
    }

    #[verifier(inline)]
    pub open spec fn to_page(&self) -> SpecPage<AddrT> {
        self.to_addr_memid().range.to_page()
    }

    #[verifier(inline)]
    pub open spec fn use_gmap(&self) -> bool {
        self.is_Read() || self.is_Write() || self.is_RmpOp()
    }

    pub open spec fn is_valid(&self) -> bool {
        self.to_mem().is_valid()
    }
}

} // verus!

================
File: ./source/verismo/src/arch/memop/gpmemop.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::arch::entities::*;

verus! {

impl MemOp<GuestPhy> {
    pub open spec fn translate_spn(&self, spmem: SpecMem<SysPhy>) -> MemOp<SysPhy> {
        match *self {
            MemOp::Read(addr_id, enc) => {
                MemOp::Read(AddrMemID { memid: addr_id.memid, range: spmem }, enc)
            },
            MemOp::Write(addr_id, enc, data) => {
                MemOp::Write(AddrID { memid: addr_id.memid, addr: spmem.first() }, enc, data)
            },
            MemOp::RmpOp(rmpop) => { MemOp::RmpOp(rmpop.set_spn(spmem.to_page())) },
            MemOp::InvlPage(gvn_id) => {
                MemOp::InvlPage(AddrMemID { memid: gvn_id.memid, range: spmem })
            },
            MemOp::FlushAll(memid) => { MemOp::FlushAll(memid) },
        }
    }

    #[verifier(inline)]
    pub open spec fn to_enc(&self) -> bool {
        match *self {
            MemOp::Read(_, enc) => { enc },
            MemOp::Write(_, enc, _) => { enc },
            _ => { false },
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/memop/mod.rs
================

mod gpmemop;
mod gvmemop;
mod memop;

use verismo_macro::*;

use crate::arch::entities::*;
use crate::arch::rmp::RmpOp;
use crate::tspec::*;

verus! {

#[is_variant]
/// Memory Operation for GVA, GPA, and SPA.
/// When AddrT = GuestVir, encrypt: bool is not unused and so just set it to false by default
pub enum MemOp<AddrT> {
    Read(AddrMemID<AddrT>, bool),  // addr, encrypt
    Write(AddrID<AddrT>, bool, ByteStream),  // addr, encrypt, content to write,
    InvlPage(AddrMemID<AddrT>),
    FlushAll(MemID),
    RmpOp(RmpOp<AddrT>),
}

} // verus!

================
File: ./source/verismo/src/arch/vram/def.rs
================

use verismo_macro::*;

use crate::arch::addr_s::SPN;
use crate::arch::ramdb::RamDB;
use crate::arch::rmp::RmpEntry;
use crate::tspec::*;

verus! {

/// SEV-SNP only prevents software-based integrity attack.
/// Active DRAM corruption is not resolved.
///
/// There is no actually integrity check in hardware (integrity_check = false).
/// The integrity is achieved by restricting the HV's access to private memory;
/// We may prove that the when the attacker cannot physically replace memory,
/// the RMP enforcement with integrity_check = false achieves the equivalent result with integrity_check = true.
#[derive(SpecSetter, SpecGetter)]
pub struct VRamDB {
    pub sram: RamDB,
    pub rmp: Map<SPN, RmpEntry>,
}

} // verus!

================
File: ./source/verismo/src/arch/vram/vram_p.rs
================

use super::*;
use crate::arch::ramdb::ram_s::*;
use crate::arch::rmp::perm_s::Perm;

verus! {

impl VRamDB {
    pub proof fn proof_rmp_check_access_rmp_has_gpn_memid(
        &self,
        op: MemOp<GuestPhy>,
        sysmap: SysMap,
    )
        requires
            op.is_Read() || op.is_Write(),
            self.op(sysmap, op).is_Ok(),
            op.to_enc(),
        ensures
            rmp_has_gpn_memid(&self.rmp, op.to_mem().to_page(), op.to_memid()),
    {
        reveal(VRamDB::op);
        let spn = sysmap.translate(op.to_mem().to_page()).get_Some_0();
        if op.is_Write() {
            rmp_proof_check_access_rmp_has_gpn_memid(
                &self.rmp,
                op.to_memid(),
                op.to_enc(),
                op.to_mem(),
                Perm::Write,
                spn,
            );
        }
        if op.is_Read() {
            rmp_proof_check_access_rmp_has_gpn_memid(
                &self.rmp,
                op.to_memid(),
                op.to_enc(),
                op.to_mem(),
                Perm::Read,
                spn,
            );
        }
    }

    pub proof fn lemma_read_no_change(&self, op: MemOp<GuestPhy>, sysmap: SysMap)
        requires
            op.is_Read(),
            op.is_valid(),
        ensures
            &self.op(sysmap, op).to_result() === self,
    {
        reveal(VRamDB::op);
    }

    pub proof fn lemma_error_no_change(&self, op: MemOp<GuestPhy>, sysmap: SysMap)
        requires
            self.op(sysmap, op).is_Error(),
            op.is_valid(),
        ensures
            &self.op(sysmap, op).to_result() === self,
    {
        reveal(VRamDB::op);
    }

    pub proof fn lemma_constant_gpn_to_spn_when_enc(&self, op: MemOp<GuestPhy>, sysmap: SysMap)
        requires
            self.inv(),
            self.inv_sw(op.to_memid()),
            op.is_valid(),
            op.is_Read() || op.is_Write(),
            self.op(sysmap, op).is_Ok() || self.op(sysmap, op).get_Error_1().is_RmpOp(),
            op.is_Read() ==> op.get_Read_1(),
            op.is_Write() ==> op.get_Write_1(),
        ensures
            sysmap.translate(op.to_mem().to_page()).is_Some(),
            rmp_has_gpn_memid(&self.spec_rmp(), op.to_mem().to_page(), op.to_memid()),
            rmp_reverse(&self.spec_rmp(), op.to_memid(), op.to_mem().to_page())
                === sysmap.translate(op.to_mem().to_page()).get_Some_0(),
    {
        reveal(VRamDB::op);
        reveal(RmpEntry::check_access);
        reveal(rmp_inv);
        let spn = sysmap.translate(op.to_mem().to_page());
        assert(spn.is_Some()) by {
            if spn.is_None() {
                if op.is_Read() {
                    assert(self.op(sysmap, op).is_Error());
                }
                if op.is_Write() {
                    assert(self.op(sysmap, op).is_Error());
                }
            }
        }
        let spn = spn.get_Some_0();
        assert(self.spec_rmp()[spn].view().spec_validated());
        assert(rmp_reverse(&self.spec_rmp(), op.to_memid(), self.spec_rmp()[spn].view().spec_gpn())
            === spn) by {
            assert(self.inv_sw(op.to_memid()));
        }
    }

    pub proof fn proof_read_enc_byte_to_bytes(&self, memid: MemID, gpmem: GPMem, i: int)
        requires
            0 <= i < gpmem.len(),
            gpmem.is_valid(),
        ensures
            self.get_enc_bytes_ok(AddrMemID { memid, range: gpmem }).is_Some()
                === self.get_enc_byte_ok(memid, gpmem[i]).is_Some(),
            self.get_enc_bytes_ok(AddrMemID { memid, range: gpmem }).is_Some()
                ==> self.get_enc_bytes_ok(AddrMemID { memid, range: gpmem }).get_Some_0()[i]
                === self.get_enc_byte_ok(memid, gpmem[i]).get_Some_0(),
    {
        gpmem.proof_same_page();
    }

    /// TODO: function body check has been running for 2 seconds
    pub proof fn lemma_write_enc_bytes_effect_same_read(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpmem: GPMem,
    )
        requires
            self.inv(),
            memop.is_Write(),
            memop.is_valid(),
            gpmem.is_valid(),
            gpmem.len() > 0,
            self.inv_sw(memid),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            gpmem === memop.to_mem(),
            memop.to_memid().to_asid() === memid.to_asid() || !memop.get_Write_1(),
        ensures
            (other.get_enc_bytes_ok(AddrMemID { memid, range: gpmem }).is_Some()
                && memop.get_Write_1()) ==> (other.get_enc_bytes_ok(
                AddrMemID { memid, range: gpmem },
            ).get_Some_0() === memop.get_Write_2()),
            (other.get_enc_bytes_ok(AddrMemID { memid, range: gpmem }).is_Some()
                && !memop.get_Write_1()) ==> {
                other.get_enc_bytes_ok(AddrMemID { memid, range: gpmem }) === self.get_enc_bytes_ok(
                    AddrMemID { memid, range: gpmem },
                )
            },
            self.rmp.dom() === self.op(sysmap, memop).to_result().rmp.dom(),
    {
        let gpmem_id = AddrMemID { memid, range: gpmem };
        let read1 = self.get_enc_bytes_ok(gpmem_id);
        let read2 = other.get_enc_bytes_ok(gpmem_id);
        let w_enc = memop.get_Write_1();
        //self.lemma_inv_dom(other, sysmap, memop);
        reveal(VRamDB::op);
        assert(read2.is_Some() === read1.is_Some());
        if gpmem === memop.to_mem() && read2.is_Some() && w_enc {
            assert forall|i| 0 <= i < gpmem.len() implies read2.get_Some_0()[i]
                === #[trigger] memop.get_Write_2()[i] by {
                self.lemma_write_effect_in_range(other, sysmap, memop, memid, gpmem[i]);
                other.proof_read_enc_byte_to_bytes(memid, gpmem, i);
            }
            assert(memop.get_Write_2() =~~= (read2.get_Some_0()));
        }
        if gpmem === memop.to_mem() && read2.is_Some() && !w_enc {
            assert forall|i| 0 <= i < gpmem.len() implies read2.get_Some_0()[i]
                === #[trigger] read1.get_Some_0()[i] by {
                if 0 <= i < gpmem.len() {
                    self.lemma_write_effect_in_range(other, sysmap, memop, memid, gpmem[i]);
                    other.proof_read_enc_byte_to_bytes(memid, gpmem, i);
                }
            }
            assert(read1.get_Some_0() =~~= (read2.get_Some_0()));
        }
    }

    pub proof fn lemma_write_enc_bytes_effect_disjoint_read(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpmem: GPMem,
    )
        requires
            self.inv(),
            memid.is_vmpl0(),
            memop.is_valid(),
            memop.is_Write(),
            gpmem.is_valid(),
            gpmem.len() > 0,
            self.inv_sw(memid),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            gpmem.disjoint(
                memop.to_mem(),
            ),
    //memop.to_memid().to_asid() === memid.to_asid() || !memop.get_Write_1(),

        ensures
            other.get_enc_bytes_ok(AddrMemID { memid, range: gpmem }) === self.get_enc_bytes_ok(
                AddrMemID { memid, range: gpmem },
            ),
            self.rmp.dom() === self.op(sysmap, memop).to_result().rmp.dom(),
    {
        let gpmem_id = AddrMemID { memid, range: gpmem };
        let read1 = self.get_enc_bytes_ok(gpmem_id);
        let read2 = other.get_enc_bytes_ok(gpmem_id);
        let w_enc = memop.get_Write_1();
        //self.lemma_inv_dom(other, sysmap, memop);
        reveal(VRamDB::op);
        assert(read2.is_Some() === read1.is_Some());
        if read2.is_Some() {
            assert forall|i| 0 <= i < gpmem.len() implies read2.get_Some_0()[i]
                === read1.get_Some_0()[i] by {
                if 0 <= i < gpmem.len() {
                    other.proof_read_enc_byte_to_bytes(memid, gpmem, i);
                    self.proof_read_enc_byte_to_bytes(memid, gpmem, i);
                    if memop.to_memid().to_asid() === memid.to_asid() || !memop.get_Write_1() {
                        self.lemma_write_effect_out_range(other, sysmap, memop, memid, gpmem[i]);
                    } else {
                        self.lemma_write_byte_other_vm(other, sysmap, memop, memid, gpmem[i]);
                    }
                }
            }
            assert(read1.get_Some_0() =~~= (read2.get_Some_0()));
        }
    }

    pub proof fn lemma_write_bytes_effect_by_other_vm_or_shared(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpmem: GPMem,
    )
        requires
            self.inv(),
            memid.is_vmpl0(),
            memop.is_valid(),
            memop.is_Write(),
            gpmem.is_valid(),
            gpmem.len() > 0,
            self.inv_sw(memid),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            (memop.to_memid().to_asid() !== memid.to_asid() && memop.get_Write_1())
                || !memop.get_Write_1(),
        ensures
            other.get_enc_bytes_ok(AddrMemID { memid, range: gpmem }) === self.get_enc_bytes_ok(
                AddrMemID { memid, range: gpmem },
            ),
            self.rmp.dom() === self.op(sysmap, memop).to_result().rmp.dom(),
    {
        let gpmem_id = AddrMemID { memid, range: gpmem };
        let read1 = self.get_enc_bytes_ok(gpmem_id);
        let read2 = other.get_enc_bytes_ok(gpmem_id);
        let w_enc = memop.get_Write_1();
        //self.lemma_inv_dom(other, sysmap, memop);
        reveal(VRamDB::op);
        assert(read2.is_Some() === read1.is_Some());
        if read2.is_Some() {
            assert forall|i| 0 <= i < gpmem.len() implies read2.get_Some_0()[i]
                === read1.get_Some_0()[i] by {
                if 0 <= i < gpmem.len() {
                    other.proof_read_enc_byte_to_bytes(memid, gpmem, i);
                    self.proof_read_enc_byte_to_bytes(memid, gpmem, i);
                    if w_enc {
                        self.lemma_write_byte_other_vm(other, sysmap, memop, memid, gpmem[i]);
                    } else {
                        self.lemma_write_shared_effect(other, sysmap, memop, memid, gpmem[i]);
                    }
                }
            }
            assert(read1.get_Some_0() =~~= (read2.get_Some_0()));
        }
    }

    pub proof fn lemma_write_sm_int_Rinv(
        &self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            self.inv(),
            self.inv_sw(memid),
            self.inv_memid_int(memid),
            gpa.is_valid(),
            memop.is_valid(),
            memop.is_Write(),
            memid.is_vmpl0(),
            !memop.to_memid().is_sm(memid),
            memtype(memid, gpa.to_page()).is_sm_int(),
        ensures
            self.op(sysmap, memop).to_result().get_enc_byte_ok(memid, gpa).is_Some() ==> self.op(
                sysmap,
                memop,
            ).to_result().get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa),
    {
        let new = self.op(sysmap, memop).to_result();
        if self.op(sysmap, memop).is_Ok() {
            if memop.to_mem().contains(gpa) {
                self.lemma_write_sm_int_ok(memid, memop, sysmap);
                self.lemma_write_byte_othervm_or_shared(&new, sysmap, memop, memid, gpa);
            } else {
                self.lemma_write_effect_out_range(&new, sysmap, memop, memid, gpa);
            }
        }
        self.lemma_op_err_Ginv(sysmap, memop);
    }

    pub proof fn lemma_write_byte_othervm_or_shared(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            self.inv(),
            gpa.is_valid(),
            memop.is_valid(),
            memop.is_Write(),
            memid.is_vmpl0(),
            self.inv_sw(memid),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            (memop.to_memid().to_asid() !== memid.to_asid() && memop.get_Write_1())
                || !memop.get_Write_1(),
        ensures
            other.get_enc_byte_ok(memid, gpa).is_Some() ==> other.get_enc_byte_ok(memid, gpa)
                === self.get_enc_byte_ok(memid, gpa),
    {
        let w_enc = memop.get_Write_1();
        if w_enc {
            self.lemma_write_byte_other_vm(other, sysmap, memop, memid, gpa);
        } else {
            self.lemma_write_shared_effect(other, sysmap, memop, memid, gpa);
        }
    }

    pub proof fn lemma_write_byte_other_vm(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            memid.is_vmpl0(),
            self.inv(),
            self.inv_sw(memid),
            memop.is_valid(),
            memop.is_Write(),
            gpa.is_valid(),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            (memop.to_memid().to_asid() !== memid.to_asid()),
        ensures
            other.get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa),
    {
        reveal(VRamDB::op);
        assert(other.get_enc_byte_ok(memid, gpa).is_Some() === self.get_enc_byte_ok(
            memid,
            gpa,
        ).is_Some());
        let w_asid = if memop.get_Write_1() {
            memop.to_memid().to_asid()
        } else {
            ASID_FOR_HV!()
        };
        if self.get_enc_byte_ok(memid, gpa).is_Some() {
            let rspn = rmp_reverse(&self.spec_rmp(), memid, gpa.to_page());
            let rspa = gpa.convert(rspn);
            assert(self.spec_rmp()[rspn].inv()) by {
                reveal(rmp_inv);
            }
            assert(self.spec_rmp()[rspn].view().spec_asid() === memid.to_asid());
            assert(self.spec_rmp()[rspn].view().spec_validated());
            let wspmem = sysmap.translate_addr_seq(memop.to_mem());
            assert(w_asid !== memid.to_asid());
            assert(wspmem.to_page() !== rspn) by {
                reveal(RmpEntry::check_access);
            }
            assert(!memrange_contains_block(wspmem, idx(rspa))) by {
                let wspn = wspmem.to_page();
                let base_addr = wspn.to_addr().as_int();
                assert(base_addr <= wspmem.first().as_int() < base_addr + PAGE_SIZE!());
                assert(base_addr <= wspmem.last().as_int() < base_addr + PAGE_SIZE!());
                // The following proofs are unstable. Keep those assertions to help solver.
                assert(!(rspn =~= wspn));
                assert(base_addr == wspn.as_int() * PAGE_SIZE!());
                let rbase_addr = rspn.to_addr().as_int();
                assert(rbase_addr == rspn.as_int() * PAGE_SIZE!());
                assert(rbase_addr <= rspa.as_int() < rbase_addr + PAGE_SIZE!());
                assert((rbase_addr <= base_addr - PAGE_SIZE!()) || (rbase_addr >= base_addr
                    + PAGE_SIZE!()));
            }
            self.spec_sram().lemma_write_unchange_byte_any_enc(
                w_asid,
                wspmem,
                memop.get_Write_2(),
                memid.to_asid(),
                rspa,
            );
        }
    }

    pub proof fn lemma_write_effect_change_other_vm_enc(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
        rsysmap: SysMap,
        renc: bool,
    )
        requires
            self.inv(),
            self.inv_sw(memid),
            gpa.is_valid(),
            memop.is_valid(),
            memop.is_Write(),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            (memop.to_memid().to_asid() !== memid.to_asid() && memop.get_Write_1()),
        ensures
    //other.read_bytes(AddrMemID{range: gpa, memid}, renc, rsysmap) === self.read_bytes(AddrMemID{range: gpa, memid}, renc, rsysmap),

            other.read_bytes(
                AddrMemID { range: SpecMem::from_range(gpa, 1), memid },
                renc,
                rsysmap,
            ).is_Ok() ==> (other.get_byte(memid, gpa, renc, rsysmap) === self.get_byte(
                memid,
                gpa,
                renc,
                rsysmap,
            )),
            other.get_enc_byte_ok(memid, gpa).is_Some() ==> other.get_enc_byte_ok(memid, gpa)
                === self.get_enc_byte_ok(memid, gpa),
    {
        reveal(VRamDB::op);
        reveal(VRamDB::op_write);
        reveal(RmpEntry::check_access);
        reveal(rmp_inv);
        let rmp = self.spec_rmp();
        assert(rmp === other.spec_rmp());
        let rspn = rmp_reverse(&rmp, memid, gpa.to_page());
        let rspa = gpa.convert(rspn);
        let AddrMemID { range, memid: w_memid } = memop.to_addr_memid();
        let data = memop.get_Write_2();
        let w_enc = memop.get_Write_1();
        let wspmem = sysmap.translate_addr_seq(range);
        let rspa_by_sysmap = rsysmap.translate_addr(gpa);
        assert(rmp === other.spec_rmp());
        assert(rmp[wspmem.to_page()].view().spec_asid() === memop.to_memid().to_asid());
        assert(rmp[wspmem.to_page()].view().spec_validated());
        assert(rmp[wspmem.to_page()].view().spec_assigned());
        assert(rmp[wspmem.to_page()].inv());
        if other.read_bytes(
            AddrMemID { range: SpecMem::from_range(gpa, 1), memid },
            renc,
            rsysmap,
        ).is_Ok() {
            assume(other.get_byte(memid, gpa, renc, rsysmap) === self.get_byte(
                memid,
                gpa,
                renc,
                rsysmap,
            ));
        }
        assume(other.get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa));
    }

    pub proof fn lemma_write_shared_effect(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            self.inv(),
            self.inv_sw(memid),
            gpa.is_valid(),
            memid.is_vmpl0(),
            memop.is_valid(),
            memop.is_Write(),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            !memop.get_Write_1(),
        ensures
    //other.get_enc_byte_ok(memid, gpa).is_Some() ==>

            other.get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa),
            (memop.to_mem().contains(gpa) && !other.get_enc_byte_ok(memid, gpa).is_Some()) ==> (
            other.get_byte(memop.to_memid(), gpa, memop.get_Write_1(), sysmap).get_Some_0()
                === memop.get_Write_2()[gpa.value() - memop.to_mem().first().value()]),
    {
        if let MemOp::Write(gpa_id, w_enc, bytes) = memop {
            let gpmem_id = memop.to_addr_memid();
            if gpmem_id.range.contains(gpa) {
                self.lemma_write_effect_in_range(other, sysmap, memop, memid, gpa);
                if other.get_enc_byte_ok(memid, gpa).is_Some() {
                    assert(other.get_enc_byte_ok(memid, gpa).get_Some_0() === self.get_enc_byte_ok(
                        memid,
                        gpa,
                    ).get_Some_0());
                } else {
                    assert(other.get_byte(memid, gpa, false, sysmap).get_Some_0()
                        === memop.get_Write_2()[gpa.value() - memop.to_mem().first().value()]);
                }
            } else {
                self.lemma_write_effect_out_range(other, sysmap, memop, memid, gpa);
            }
        }
    }

    pub proof fn lemma_write_effect_in_range(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            self.inv(),
            self.inv_sw(memid),
            gpa.is_valid(),
            memop.is_valid(),
            memop.is_Write(),
            memop.to_mem().contains(gpa),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            memop.to_memid().to_asid() === memid.to_asid() || !memop.get_Write_1(),
        ensures
            other.get_byte(memid, gpa, false, sysmap).is_Some(),
            (other.get_enc_byte_ok(memid, gpa).is_Some() && memop.get_Write_1()) ==> (
            other.get_enc_byte_ok(memid, gpa).get_Some_0() === memop.get_Write_2()[gpa.value()
                - memop.to_mem().first().value()]),
            !(other.get_enc_byte_ok(memid, gpa).is_Some() && memop.get_Write_1()) ==> (
            other.get_byte(memop.to_memid(), gpa, memop.get_Write_1(), sysmap).get_Some_0()
                === memop.get_Write_2()[gpa.value() - memop.to_mem().first().value()]),
            (!other.get_enc_byte_ok(memid, gpa).is_Some() && !memop.get_Write_1()) ==> {
                other.get_byte(memid, gpa, false, sysmap).get_Some_0()
                    === memop.get_Write_2()[gpa.value() - memop.to_mem().first().value()]
            },
            !memop.get_Write_1() ==> {
                other.get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa)
            },
    {
        reveal(VRamDB::op);
        reveal(VRamDB::op_write);
        let rmp = self.spec_rmp();
        assert(rmp === other.spec_rmp());
        let rspn = rmp_reverse(&rmp, memid, gpa.to_page());
        let rspa = gpa.convert(rspn);
        assert(rspn =~= rspa.to_page());
        let AddrMemID { range, memid: w_memid } = memop.to_addr_memid();
        let data = memop.get_Write_2();
        let w_enc = memop.get_Write_1();
        let wspmem = sysmap.translate_addr_seq(range);
        assert(gpa.to_page() === range.to_page());
        reveal(RmpEntry::check_access);
        reveal(rmp_inv);
        assert(rmp[wspmem.to_page()].inv());
        if other.get_enc_byte_ok(memid, gpa).is_Some() && w_enc {
            assert(rmp[rspn].inv());
            assert(w_memid.to_asid() === memid.to_asid());
            assert(wspmem.contains(rspa)) by {
                assert(wspmem === rmp_reverse_mem(&rmp, w_memid, range));
                assert(range.to_page() === gpa.to_page());
                assert(wspmem.to_page() === rspa.to_page());
            }
            assert(self.get_enc_byte_ok(memid, gpa).is_Some());
            self.spec_sram().lemma_write_change_byte(memid.to_asid(), wspmem, data, rspa);
        } else {
            let w_asid = if w_enc {
                w_memid.to_asid()
            } else {
                ASID_FOR_HV!()
            };
            let rspa_by_sysmap = sysmap.translate_addr(gpa);
            assert(rspa_by_sysmap.is_Some());
            let rspa_by_sysmap = rspa_by_sysmap.get_Some_0();
            let read_byte_sysmap = other.get_byte(w_memid, gpa, w_enc, sysmap);
            assert(read_byte_sysmap.is_Some());
            if !w_enc {
                let read_byte = other.get_byte(memid, gpa, false, sysmap);
                assert(read_byte.is_Some());
                if other.get_enc_byte_ok(memid, gpa).is_Some() {
                    assert(rmp[rspn].inv());
                    assert(!rmp[wspmem.to_page()].view().spec_assigned());
                    assert(rmp[rspn].view().spec_validated());
                    assert(wspmem.to_page() !== rspn);
                    //self.spec_sram().lemma_write_unchange_byte_any_enc(w_asid, wspmem, data, memid.to_asid(), rspa);
                    //assert(other.get_enc_byte_ok(memid, gpa).get_Some_0() === self.get_enc_byte_ok(memid, gpa).get_Some_0());
                }
            }
            if wspmem.to_page() !== rspn {
                assert(0 <= rspa.as_int() - rspn.as_int() * (PAGE_SIZE as int) < (
                PAGE_SIZE as int));
                self.spec_sram().lemma_write_unchange_byte_any_enc(
                    w_asid,
                    wspmem,
                    data,
                    memid.to_asid(),
                    rspa,
                );
            }
            assert(other.spec_sram() === self.spec_sram().write_raw(w_asid, wspmem, data));
            self.spec_sram().lemma_write_change_byte(w_asid, wspmem, data, rspa_by_sysmap);
            assert(other.spec_sram().read_one_byte(w_asid, rspa_by_sysmap)
                === data[rspa_by_sysmap.value() - wspmem.first().value()]);
            assert(read_byte_sysmap.get_Some_0() === data[rspa_by_sysmap.value()
                - wspmem.first().value()]);
        }
    }

    pub proof fn lemma_write_effect_out_range(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            self.inv(),
            self.inv_sw(memid),
            memid.is_vmpl0(),
            gpa.is_valid(),
            memop.is_valid(),
            memop.is_Write(),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            !memop.to_mem().contains(gpa),
        ensures
            (other.get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa)),
    {
        if memop.to_memid().to_asid() === memid.to_asid() || !memop.get_Write_1() {
            self.lemma_write_effect_out_range_same_vm(other, sysmap, memop, memid, gpa);
        } else {
            self.lemma_write_byte_other_vm(other, sysmap, memop, memid, gpa);
        }
    }

    pub proof fn lemma_write_effect_out_range_same_vm(
        &self,
        other: &Self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            self.inv(),
            self.inv_sw(memid),
            memop.is_valid(),
            memop.is_Write(),
            self.op(sysmap, memop).is_Ok(),
            other === &self.op(sysmap, memop).to_result(),
            !memop.to_mem().contains(gpa),
            memop.to_memid().to_asid() === memid.to_asid() || !memop.get_Write_1(),
        ensures
            (other.get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa)),
    {
        reveal(VRamDB::op);
        reveal(VRamDB::op_write);
        let rmp = self.spec_rmp();
        assert(rmp === other.spec_rmp());
        let w_enc = memop.get_Write_1();
        let w_gpmem = memop.to_mem();
        let w_memid = memop.to_memid();
        let data = memop.get_Write_2();
        let wspmem = sysmap.translate_addr_seq(w_gpmem);
        wspmem.proof_same_page();
        if self.get_enc_byte_ok(memid, gpa).is_Some() {
            assert(rmp_has_gpn_memid(&rmp, gpa.to_page(), memid));
            let rspn = rmp_reverse(&rmp, memid, gpa.to_page());
            let rspa = gpa.convert(rspn);
            assert(rspa.to_page().value() == rspn.value());
            assert(rspa.to_page() =~= rspn);
            reveal(RmpEntry::check_access);
            assert(rmp[rspn].view().spec_validated());
            if w_enc {
                assert(rmp[wspmem.to_page()].view().spec_gpn() === w_gpmem.to_page());
                assert(rmp[rspa.to_page()].view().spec_gpn() === gpa.to_page());
                assert(rmp[wspmem.to_page()].view().spec_asid() === w_memid.to_asid());
                assert(rmp[rspa.to_page()].view().spec_asid() === memid.to_asid());
                assert(wspmem === rmp_reverse_mem(&rmp, w_memid, w_gpmem));
                if wspmem.to_page() === rspa.to_page() {
                    assert(!wspmem.contains(rspa));
                    assert(memid.to_asid() === w_memid.to_asid());
                    self.spec_sram().lemma_write_unchange_byte(memid.to_asid(), wspmem, data, rspa);
                } else {
                    self.spec_sram().lemma_write_unchange_byte_any_enc(
                        w_memid.to_asid(),
                        wspmem,
                        data,
                        memid.to_asid(),
                        rspa,
                    );
                }
                assert(other.get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa));
            } else {
                assert(!rmp[wspmem.to_page()].view().spec_validated()) by {
                    assert(!rmp[wspmem.to_page()].view().spec_assigned());
                    assert(rmp[wspmem.to_page()].inv()) by {
                        reveal(rmp_inv);
                    }
                }
                assert(wspmem.to_page() !== rspa.to_page());
                self.spec_sram().lemma_write_unchange_byte_any_enc(
                    w_memid.to_asid(),
                    wspmem,
                    data,
                    memid.to_asid(),
                    rspa,
                );
                assume(other.get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa));
            }
        } else {
            assert(self.get_enc_byte_ok(memid, gpa).is_None());
        }
    }

    pub proof fn proof_op_inv(&self, sysmap: SysMap, memop: MemOp<GuestPhy>)
        requires
            self.inv(),
            memop.is_valid(),
            sysmap.is_valid(),
        ensures
            self.op(sysmap, memop).to_result().inv(),
    {
        reveal(VRamDB::inv);
        reveal(VRamDB::op);
        reveal(VRamDB::op_read);
        reveal(VRamDB::op_write);
        reveal(VRamDB::rmp_op);
        //reveal(RamDB::write_raw);
        let new = self.op(sysmap, memop).to_result();
        match memop {
            MemOp::RmpOp(rmpop) => {
                let spn = sysmap.translate(rmpop.get_gpn());
                if spn.is_Some() {
                    rmp_proof_op_dom_inv(&self.spec_rmp(), rmpop.set_spn(spn.get_Some_0()));
                    rmp_proof_op_inv(&self.spec_rmp(), rmpop.set_spn(spn.get_Some_0()));
                }
            },
            MemOp::Write(gpa_id, enc, data) => {
                let use_asid = if enc {
                    gpa_id.memid.to_asid()
                } else {
                    ASID_FOR_HV!()
                };
                if self.op(sysmap, memop).is_Ok() {
                    let spa = sysmap.translate_addr_seq(memop.to_mem());
                    self.spec_sram().lemma_write_inv(use_asid, spa, data);
                    assert(new.inv());
                }
            },
            _ => {},
        }
        let new = self.op(sysmap, memop).to_result();
    }

    pub proof fn proof_op_inv_sw(&self, sysmap: SysMap, memop: MemOp<GuestPhy>, memid: MemID)
        requires
            self.inv(),
            self.inv_sw(memid),
            self.inv_memid_int(memid),
            memop.is_valid(),
            memop.to_memid().is_sm(memid) ==> self.gpmemop_requires(memop, sysmap),
        ensures
            self.op(sysmap, memop).to_result().inv(),
            self.op(sysmap, memop).to_result().inv_sw(memid),
            self.op(sysmap, memop).to_result().inv_memid_int(memid),
    {
        reveal(VRamDB::inv);
        reveal(VRamDB::op);
        reveal(VRamDB::op_read);
        reveal(VRamDB::op_write);
        reveal(VRamDB::rmp_op);
        //reveal(RamDB::write_raw);
        self.proof_op_inv(sysmap, memop);
        match memop {
            MemOp::RmpOp(rmpop) => {
                let spn = sysmap.translate(rmpop.get_gpn());
                if spn.is_Some() {
                    rmp_proof_inv_sw(&self.spec_rmp(), rmpop.set_spn(spn.get_Some_0()), memid);
                    rmp_proof_inv_memid_int(
                        &self.spec_rmp(),
                        rmpop.set_spn(spn.get_Some_0()),
                        memid,
                    );
                }
            },
            _ => {},
        }
    }

    pub proof fn lemma_op_err_Ginv(&self, sysmap: SysMap, memop: MemOp<GuestPhy>)
        requires
            memop.is_valid(),
        ensures
            !self.op(sysmap, memop).is_Ok() ==> self.op(sysmap, memop).to_result() === *self,
            self.op(sysmap, memop).to_result() !== *self ==> self.op(sysmap, memop).is_Ok(),
    {
        reveal(VRamDB::op);
        reveal(VRamDB::op_read);
        reveal(VRamDB::op_write);
        reveal(VRamDB::rmp_op);
    }

    pub proof fn lemma_write_read_consistant(
        &self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        rgpa_id: GPAMemID,
        enc: bool,
    )
        requires
            memop.is_valid(),
            memop.is_Write(),
            memop.to_mem() === rgpa_id.range,  // same gpa + memid
            memop.to_memid() === rgpa_id.memid,  // same gpa + memid
            memop.get_Write_1() === enc,  // same enc
            self.op(sysmap, memop).is_Ok(),
        ensures
            memop.get_Write_2() === self.op(sysmap, memop).to_result().get_bytes(
                rgpa_id,
                enc,
                sysmap,
            ),
    {
        reveal(VRamDB::op);
        reveal(VRamDB::op_write);
        let data = memop.get_Write_2();
        let new_vram = self.op(sysmap, memop).to_result();
        let gpmem = memop.to_mem();
        let memid = memop.to_memid();
        let spa = sysmap.translate_addr_seq(gpmem);
        assert(spa.len() === gpmem.len());
        let use_asid = if enc {
            memid.to_asid()
        } else {
            ASID_FOR_HV!()
        };
        self.spec_sram().proof_read_write(use_asid, spa, data);
    }

    pub proof fn lemma_read_op_enc_bytes_ok(&self, sysmap: SysMap, gpa_id: GPAMemID, enc: bool)
        requires
            enc,
            gpa_id.range.is_valid(),
            self.inv_sw(gpa_id.memid),
            self.op(sysmap, MemOp::Read(gpa_id, enc)).is_Ok(),
        ensures
            self.get_enc_bytes_ok(gpa_id).is_Some(),
            self.get_enc_bytes_ok(gpa_id).get_Some_0() === self.read_bytes(
                gpa_id,
                enc,
                sysmap,
            ).to_result(),
            sysmap.translate(gpa_id.range.to_page()).is_Some(),
            sysmap.translate(gpa_id.range.to_page()).get_Some_0() === rmp_reverse(
                &self.spec_rmp(),
                gpa_id.memid,
                gpa_id.range.to_page(),
            ),
    {
        reveal(VRamDB::op);
        self.lemma_read_enc_byte_ok(sysmap, gpa_id, enc);
    }

    pub proof fn lemma_read_enc_byte_ok(&self, sysmap: SysMap, gpa_id: GPAMemID, enc: bool)
        requires
            enc,
            gpa_id.range.is_valid(),
            self.inv_sw(gpa_id.memid),
            self.read_bytes(gpa_id, enc, sysmap).is_Ok(),
        ensures
            self.get_enc_bytes_ok(gpa_id).is_Some(),
            self.get_enc_bytes_ok(gpa_id).get_Some_0() === self.read_bytes(
                gpa_id,
                enc,
                sysmap,
            ).to_result(),
            sysmap.translate(gpa_id.range.to_page()).is_Some(),
            sysmap.translate(gpa_id.range.to_page()).get_Some_0() =~= rmp_reverse(
                &self.spec_rmp(),
                gpa_id.memid,
                gpa_id.range.to_page(),
            ),
    {
        let rmp = self.spec_rmp();
        let AddrMemID { range: gpmem, memid } = gpa_id;
        let gpn = gpmem.to_page();
        let spmem = rmp_reverse_mem(&rmp, memid, gpmem);
        assert(sysmap.translate(gpn).is_Some());
        assert(sysmap.translate_addr_seq(gpmem) === spmem) by {
            assert(self.inv_sw(gpa_id.memid));
            reveal(RmpEntry::check_access);
        }
        assert(rmp_has_gpn_memid(&rmp, gpn, memid)) by {
            reveal(RmpEntry::check_access);
        }
    }

    pub proof fn lemma_read_diff_sysmap(
        &self,
        sysmap1: SysMap,
        sysmap2: SysMap,
        gpa_id: GPAMemID,
        enc: bool,
    )
        requires
            enc,
            gpa_id.range.is_valid(),
            rmp_inv_sw(&self.rmp, gpa_id.memid),
        ensures
            (self.read_bytes(gpa_id, enc, sysmap1) === self.read_bytes(gpa_id, enc, sysmap2))
                || self.read_bytes(gpa_id, enc, sysmap1).is_Error() || self.read_bytes(
                gpa_id,
                enc,
                sysmap2,
            ).is_Error(),
    {
        if self.read_bytes(gpa_id, enc, sysmap1).is_Ok() {
            self.lemma_read_enc_byte_ok(sysmap1, gpa_id, enc);
        }
        if self.read_bytes(gpa_id, enc, sysmap2).is_Ok() {
            self.lemma_read_enc_byte_ok(sysmap2, gpa_id, enc);
        }
    }

    pub proof fn lemma_read_enc_ok_valid_model_eq(&self, other: &Self, gpa_id: GPAMemID)
        requires
            rmp_inv_sw(&other.rmp, gpa_id.memid),
            gpa_id.range.is_valid(),
            other.inv(),
            self.model1_eq(other, gpa_id.memid) || self.model2_eq(other),
        ensures
            other.get_enc_bytes_ok(gpa_id).is_None() ==> self.get_enc_bytes_ok(gpa_id).is_None(),
    {
        let gpa = gpa_id.range;
        let memid = gpa_id.memid;
        assert(rmp_inv_sw(&self.rmp, gpa_id.memid)) by {
            rmp_lemma_model_eq_inv(&self.rmp, &other.rmp, gpa_id.memid);
        }
        if other.get_enc_bytes_ok(gpa_id).is_None() {
            assert(!rmp_has_gpn_memid(&other.spec_rmp(), gpa.to_page(), memid));
            assert(!rmp_has_gpn_memid(&self.spec_rmp(), gpa.to_page(), memid)) by {
                assert(self.spec_rmp().dom() === other.spec_rmp().dom());
                assert forall|spn: SPN|
                    self.spec_rmp()[spn].view().spec_validated()
                        && self.spec_rmp()[spn].view().spec_asid() === memid.to_asid()
                        && self.spec_rmp().dom().contains(
                        spn,
                    ) implies #[trigger] self.spec_rmp()[spn].view().spec_gpn()
                    !== gpa.to_page() by {
                    assert(other.spec_rmp()[spn] === self.spec_rmp()[spn]);
                    assert(other.spec_rmp()[spn].view().spec_gpn() !== gpa.to_page()) by {
                        assert(!rmp_has_gpn_memid(&other.spec_rmp(), gpa.to_page(), memid));
                    }
                }
            }
        }
    }

    pub proof fn lemma_write_sm_int_ok(&self, memid: MemID, memop: MemOp<GuestPhy>, sysmap: SysMap)
        requires
            self.inv_sw(memid),
            self.inv(),
            self.inv_memid_int(memid),
            memtype(memid, memop.to_addr_memid().range.to_page()).is_sm_int(),
            self.op(sysmap, memop).is_Ok(),
            memop.is_valid(),
            memop.is_Write(),
            memid.is_vmpl0(),
        ensures
            memop.to_memid().is_sm(memid) || memop.to_memid().to_asid() !== memid.to_asid()
                || !memop.get_Write_1(),
    {
        reveal(RmpEntry::check_access);
        reveal(VRamDB::op);
        let rmp = self.spec_rmp();
        if memop.get_Write_1() {
            let gpn = memop.to_page();
            let op_memid = memop.to_memid();
            let spn = sysmap.translate(gpn);
            assert(spn.is_Some());
            let spn = spn.get_Some_0();
            assert(rmp[spn].view().check_vmpl(op_memid.to_vmpl(), Perm::Write));
            assert(rmp[spn].view().spec_gpn() === gpn);
            assert(rmp[spn].view().spec_asid() === op_memid.to_asid());
            assert(rmp[spn].inv()) by {
                reveal(rmp_inv);
            }
            if memop.to_memid().to_asid() === memid.to_asid() {
                assert(rmp_reverse(&rmp, memid, gpn) === spn);
                assert(memop.to_memid().is_sm(memid));
            }
        }
    }

    pub proof fn lemma_write_enc_must_has_gpn_in_rmp(
        &self,
        memid: MemID,
        memop: MemOp<GuestPhy>,
        sysmap: SysMap,
    )
        requires
            rmp_inv_sw(&self.rmp, memid),
            self.inv(),
            self.op(sysmap, memop).is_Ok(),
            memop.is_valid(),
            memop.is_Write(),
            memop.get_Write_1(),
        ensures
            self.get_enc_bytes_ok(memop.to_addr_memid()).is_Some(),
    {
        reveal(RmpEntry::check_access);
        reveal(VRamDB::op);
    }

    pub proof fn lemma_read_enc_ok_model1_eq(&self, other: &Self, gpa_id: GPAMemID)
        requires
            rmp_inv_sw(&other.rmp, gpa_id.memid),
            other.inv(),
            gpa_id.range.is_valid(),
            rmp_inv_memid_int(&other.rmp, gpa_id.memid),
            memtype(gpa_id.memid, gpa_id.range.to_page()).is_sm_int(),
            self.model1_eq(other, gpa_id.memid),
        ensures
            self.get_enc_bytes_ok(gpa_id).is_Some() ==> self.get_enc_bytes_ok(gpa_id)
                === other.get_enc_bytes_ok(gpa_id),
            other.get_enc_bytes_ok(gpa_id).is_None() ==> self.get_enc_bytes_ok(gpa_id).is_None(),
    {
        reveal(RmpEntry::check_access);
        self.lemma_read_enc_ok_valid_model_eq(other, gpa_id);
        let gpa = gpa_id.range;
        let memid = gpa_id.memid;
        let vmpl = memid.to_vmpl();
        assert(rmp_inv_sw(&self.rmp, gpa_id.memid)) by {
            rmp_lemma_model_eq_inv(&self.rmp, &other.rmp, memid);
        }
        assert(rmp_inv_memid_int(&self.rmp, memid)) by {
            rmp_lemma_model_eq_inv(&self.rmp, &other.rmp, memid);
        }
        let spn1 = rmp_reverse(&self.rmp, memid, gpa.to_page());
        let spn2 = rmp_reverse(&other.rmp, memid, gpa.to_page());
        let spmem1 = rmp_reverse_mem(&self.rmp, memid, gpa);
        let spmem2 = rmp_reverse_mem(&other.rmp, memid, gpa);
        let read_bytes1 = self.get_enc_bytes_ok(gpa_id);
        let read_bytes2 = other.get_enc_bytes_ok(gpa_id);
        if self.get_enc_bytes_ok(gpa_id).is_Some() {
            assert(self.rmp[spn1].view().spec_validated());
            assert(other.rmp[spn2].view().spec_validated());
            assert(self.rmp[spn1].view() === other.rmp[spn1].view());
            assert(other.rmp[spn1].view().spec_gpn() === gpa.to_page());
            assert(rmp_reverse(&other.rmp, memid, gpa.to_page()) === spn1);
            assert(spn1 === spn2 && spmem1 === spmem2);
            assert(read_bytes1 === read_bytes2) by {
                let bytes1 = self.spec_sram().read_bytes_by_asid(memid.to_asid(), spmem1);
                let bytes2 = other.spec_sram().read_bytes_by_asid(memid.to_asid(), spmem2);
                assert(bytes1 === bytes2) by {
                    assert forall|i: int| 0 <= i < spmem1.len() implies bytes1[i] === bytes2[i] by {
                        let spa = spmem1[i];
                        assert(spa.to_page() =~= spn1);
                        let rmpentry1 = self.rmp[spa.to_page()].view();
                        let rmpentry2 = other.rmp[spa.to_page()].view();
                        assert(rmpentry1.spec_validated() && rmpentry2.spec_validated());
                        assert(rmpentry1 === rmpentry2);
                        //assert(vmpl >= VMPL::VMPL0 ||  !other.rmp[spa.to_page()].view().check_vmpl(VMPL::VMPL0, Perm::Write));
                        assert(vmpl >= VMPL::VMPL1 || !other.rmp[spa.to_page()].view().check_vmpl(
                            VMPL::VMPL1,
                            Perm::Write,
                        )) by {
                            rmp_inv_memid_int(&other.rmp, gpa_id.memid);
                        }
                        assert(vmpl >= VMPL::VMPL2 || !other.rmp[spa.to_page()].view().check_vmpl(
                            VMPL::VMPL2,
                            Perm::Write,
                        ));
                        assert(vmpl >= VMPL::VMPL3 || !other.rmp[spa.to_page()].view().check_vmpl(
                            VMPL::VMPL3,
                            Perm::Write,
                        ));
                        assert(self.spec_sram().spec_data()[spa.value()]
                            === other.spec_sram().spec_data()[spa.value()]) by {
                            assert(self.model1_eq(other, memid));
                        }
                        assert(bytes1[i] === bytes2[i]);
                    }
                    assert(bytes1 =~~= (bytes2));
                }
            }
        }
    }

    pub proof fn lemma_read_enc_ok_model2_eq(&self, other: &Self, gpa_id: GPAMemID)
        requires
            rmp_inv_sw(&other.rmp, gpa_id.memid),
            other.inv(),
            gpa_id.range.is_valid(),
            self.model2_eq(other),
        ensures
            self.get_enc_bytes_ok(gpa_id).is_Some() ==> self.get_enc_bytes_ok(gpa_id)
                === other.get_enc_bytes_ok(gpa_id),
            other.get_enc_bytes_ok(gpa_id).is_None() ==> self.get_enc_bytes_ok(gpa_id).is_None(),
    {
        reveal(RmpEntry::check_access);
        self.lemma_read_enc_ok_valid_model_eq(other, gpa_id);
        let gpa = gpa_id.range;
        let memid = gpa_id.memid;
        assert(rmp_inv_sw(&self.rmp, gpa_id.memid)) by {
            rmp_lemma_model_eq_inv(&self.rmp, &other.rmp, memid);
        }
        let spn1 = rmp_reverse(&self.rmp, memid, gpa.to_page());
        let spn2 = rmp_reverse(&other.rmp, memid, gpa.to_page());
        let spmem1 = rmp_reverse_mem(&self.rmp, memid, gpa);
        let spmem2 = rmp_reverse_mem(&other.rmp, memid, gpa);
        let read_bytes1 = self.get_enc_bytes_ok(gpa_id);
        let read_bytes2 = other.get_enc_bytes_ok(gpa_id);
        if self.get_enc_bytes_ok(gpa_id).is_Some() {
            assert(self.rmp[spn1].view().spec_validated());
            assert(other.rmp[spn2].view().spec_validated());
            assert(self.rmp[spn1].view() === other.rmp[spn1].view());
            assert(other.rmp[spn1].view().spec_gpn() === gpa.to_page());
            assert(rmp_reverse(&other.rmp, memid, gpa.to_page()) === spn1);
            assert(spn1 === spn2 && spmem1 === spmem2);
            assert(read_bytes1 === read_bytes2) by {
                let bytes1 = self.spec_sram().read_bytes_by_asid(memid.to_asid(), spmem1);
                let bytes2 = other.spec_sram().read_bytes_by_asid(memid.to_asid(), spmem2);
                assert(bytes1 === bytes2) by {
                    assert forall|i: int| 0 <= i < spmem1.len() implies bytes1[i] === bytes2[i] by {
                        let spa = spmem1[i];
                        assert(spa.to_page() =~= spn1);
                        let rmpentry1 = self.rmp[spa.to_page()].view();
                        let rmpentry2 = other.rmp[spa.to_page()].view();
                        assert(rmpentry1.spec_validated() && rmpentry2.spec_validated());
                        assert(rmpentry1 === rmpentry2);
                        assert(self.spec_sram().spec_data()[spa.value()]
                            === other.spec_sram().spec_data()[spa.value()]) by {
                            assert(self.model2_eq(other));
                        }
                        assert(bytes1[i] === bytes2[i]);
                    }
                    assert(bytes1 =~~= (bytes2));
                }
            }
        }
    }

    pub proof fn lemma_model_eq_inv(self, other: &Self, memid: MemID)
        requires
            self.model1_eq(other, memid),
            other.inv(),
        ensures
            self.inv(),
            other.inv_sw(memid) ==> self.inv_sw(memid),
            other.inv_memid_int(memid) ==> self.inv_memid_int(memid),
    {
        reveal(VRamDB::inv);
        rmp_lemma_model_eq_inv(&self.spec_rmp(), &other.spec_rmp(), memid);
        assert(self.spec_sram().inv());
    }

    pub proof fn lemma_possible_ops_mod_enc_bytes(
        self,
        anysysmap: SysMap,
        op: MemOp<GuestPhy>,
        gpa: GPA,
        memid: MemID,
    )
        requires
            gpa.is_valid(),
            self.get_enc_byte_ok(memid, gpa) !== self.op(anysysmap, op).to_result().get_enc_byte_ok(
                memid,
                gpa,
            ),
            memid.to_vmpl().is_VMPL0(),
            op.to_memid().is_sm(memid),
        ensures
            op.is_Write() || (op.is_RmpOp()),
    {
        reveal(VRamDB::op);
    }
}

} // verus!

================
File: ./source/verismo/src/arch/vram/vram_rmp_p.rs
================

use super::*;

verus! {

impl VRamDB {
    pub proof fn lemma_rmpop_enc_byte_Ginv(
        &self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            self.inv(),
            memop.is_RmpOp(),
            self.inv_sw(memid),
            memop.get_RmpOp_0().inv(),
            //self.op(sysmap, memop).is_Ok(),
            memop.to_memid().is_sm(memid) ==> self.gpmemop_requires(memop, sysmap),
        ensures
            (self.op(sysmap, memop).to_result().get_enc_byte_ok(memid, gpa).is_Some()
                && self.get_enc_byte_ok(memid, gpa).is_Some()) ==> self.op(
                sysmap,
                memop,
            ).to_result().get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa),
            (self.op(sysmap, memop).to_result().get_enc_byte_ok(memid, gpa).is_Some()
                && !self.get_enc_byte_ok(memid, gpa).is_Some()) ==> (memop.to_page()
                === gpa.to_page() && memop.is_RmpOp() && memop.get_RmpOp_0().is_Pvalidate()
                && memop.get_RmpOp_0().get_Pvalidate_1().val),
    {
        reveal(VRamDB::op);
        let other = &self.op(sysmap, memop).to_result();
        assert(self.spec_sram() === other.spec_sram());
        let gpn = gpa.to_page();
        let rmp = self.spec_rmp();
        let rmpop = memop.get_RmpOp_0();
        let new_rmp = other.spec_rmp();
        let spn = rmp_reverse(&rmp, memid, gpn);
        let new_spn = rmp_reverse(&new_rmp, memid, gpn);
        if other.get_enc_byte_ok(memid, gpa).is_Some() && self.get_enc_byte_ok(
            memid,
            gpa,
        ).is_Some() {
            assert(rmp_has_gpn_memid(&rmp, gpn, memid));
            assert(rmp_has_gpn_memid(&new_rmp, gpn, memid));
            assert(rmp[spn].view().spec_validated());
            assert(new_rmp[spn].view().spec_validated());
            assert(rmp[spn].view().spec_gpn() === new_rmp[new_spn].view().spec_gpn());
            assert(spn === new_spn);
        } else if other.get_enc_byte_ok(memid, gpa).is_Some() {
            assert(rmpop.is_Pvalidate());
            assert(rmpop.get_Pvalidate_1().val);
            assert(rmp_has_gpn_memid(&new_rmp, gpn, memid));
            assert(new_rmp[new_spn].view().spec_validated());
            assert(new_rmp[new_spn].view().spec_gpn() === gpn);
            assert(memop.to_page() === gpn);
        }
    }

    /// Rmpop.Rinv: when other memid execute RmpOp
    pub proof fn lemma_rmpop_enc_byte_vm_Rinv(
        &self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    ) -> (ok: bool)
        requires
            self.inv(),
            memop.is_RmpOp(),
            self.inv_sw(memid),
            memid.is_vmpl0(),
            memop.is_valid(),
            memop.to_memid().is_sm(memid) ==> self.gpmemop_requires(memop, sysmap),
            memop.to_memid() != memid,
        ensures
            ok == (self.op(sysmap, memop).is_Ok() && self.op(
                sysmap,
                memop,
            ).to_result().get_enc_byte_ok(memid, gpa).is_Some()),
            ok ==> self.op(sysmap, memop).to_result().get_enc_byte_ok(memid, gpa)
                === self.get_enc_byte_ok(memid, gpa),
    {
        reveal(VRamDB::op);
        let new = &self.op(sysmap, memop).to_result();
        assert(self.spec_sram() === new.spec_sram());
        let gpn = gpa.to_page();
        let rmp = self.spec_rmp();
        let new_rmp = new.spec_rmp();
        let ok = self.op(sysmap, memop).is_Ok() && new.get_enc_byte_ok(memid, gpa).is_Some();
        if ok {
            let spn = rmp_reverse(&rmp, memid, gpn);
            let new_spn = rmp_reverse(&new_rmp, memid, gpn);
            assert(rmp_has_gpn_memid(&new_rmp, gpn, memid));
            assert(new_rmp[spn].view().spec_validated());
            assert(rmp[spn].view().spec_validated());
            assert(rmp_has_gpn_memid(&rmp, gpn, memid));
            assert(self.get_enc_byte_ok(memid, gpa).is_Some());
            assert(rmp[spn].view().spec_gpn() === new_rmp[new_spn].view().spec_gpn());
            assert(spn === new_spn);
        }
        ok
    }

    pub proof fn lemma_rmpop_effect_unchange(
        &self,
        sysmap: SysMap,
        memop: MemOp<GuestPhy>,
        memid: MemID,
        gpa: GPA,
    )
        requires
            self.inv(),
            self.inv_sw(memid),
            self.inv_memid_int(memid),
            memop.is_RmpOp(),
            memop.get_RmpOp_0().inv(),
            self.op(sysmap, memop).is_Ok(),
            memop.is_valid(),
            memop.to_page() !== gpa.to_page(),
            memid.is_vmpl0(),
            memop.to_memid().is_sm(memid) ==> self.gpmemop_requires(
                memop,
                sysmap,
            ),
    //memop.to_memid().to_asid() === memid.to_asid() || !memop.get_Write_1(),

        ensures
            self.op(sysmap, memop).to_result().get_enc_byte_ok(memid, gpa).is_Some() ==> (self.op(
                sysmap,
                memop,
            ).to_result().get_enc_byte_ok(memid, gpa) === self.get_enc_byte_ok(memid, gpa)),
    {
        let new = &self.op(sysmap, memop).to_result();
        reveal(VRamDB::op);
        assert(self.spec_sram() === new.spec_sram());
        assert(new.inv_sw(memid)) by {
            self.proof_op_inv_sw(sysmap, memop, memid);
        }
        let encbyte = self.get_enc_byte_ok(memid, gpa);
        let new_encbyte = new.get_enc_byte_ok(memid, gpa);
        let gpn = gpa.to_page();
        let op_gpn = memop.to_page();
        let spn = rmp_reverse(&self.spec_rmp(), memid, gpn);
        let new_spn = rmp_reverse(&new.spec_rmp(), memid, gpn);
        let op_spn = rmp_reverse(&self.spec_rmp(), memid, op_gpn);
        if !rmp_has_gpn_memid(&self.spec_rmp(), gpn, memid) {
            assert(encbyte.is_None());
            if rmp_has_gpn_memid(&new.spec_rmp(), gpn, memid) {
                assert(new.spec_rmp()[new_spn].view().spec_validated());
                assert(new.spec_rmp()[new_spn].view().spec_asid() === memid.to_asid());
                assert(memop.is_PValidate());
                assert(self.spec_rmp()[new_spn].view().spec_asid() === memid.to_asid());
                assert(!self.spec_rmp()[new_spn].view().spec_validated());
                assert(new.spec_rmp()[new_spn].view().spec_gpn() === op_gpn) by {
                    reveal(RmpEntry::check_access);
                }
                assert(new.spec_rmp()[new_spn].view().spec_gpn() === gpn);
            }
            assert(!rmp_has_gpn_memid(&new.spec_rmp(), gpn, memid))
        }
        if new_encbyte.is_Some() {
            assert(rmp_has_gpn_memid(&new.spec_rmp(), gpn, memid));
            assert(rmp_has_gpn_memid(&self.spec_rmp(), gpn, memid));
            assert(new.spec_rmp()[new_spn].view().spec_gpn() === gpn);
            assert(self.spec_rmp()[spn].view().spec_gpn() === gpn);
            assert(rmp_reverse(&new.spec_rmp(), memid, gpn) === new_spn);
            assert(rmp_reverse(&self.spec_rmp(), memid, gpn) === new_spn);
            assert(new_spn === spn);
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/vram/mod.rs
================

use crate::arch::addr_s::*;
use crate::arch::attack::*;
use crate::arch::entities::*;
use crate::arch::errors::*;
use crate::arch::memop::MemOp;
use crate::arch::pgtable::*;
use crate::arch::rmp::perm_s::Perm;
use crate::arch::rmp::*;
use crate::tspec::*;
use crate::*;

pub mod def;
mod vram_rmp_p;
pub use def::*;

mod vram_p;
pub mod vram_s;

================
File: ./source/verismo/src/arch/vram/vram_s.rs
================

use super::*;
use crate::arch::rmp::perm_s::Perm;

verus! {

impl Model1Eq for VRamDB {
    open spec fn model1_eq(&self, other: &Self, memid: MemID) -> bool {
        let rmp = other.spec_rmp();
        let vmpl = memid.to_vmpl();
        &&& (forall|spa: SPA|
            (rmp.dom().contains(#[trigger] spa.to_page())
                && rmp[spa.to_page()].view().spec_validated() && (vmpl >= VMPL::VMPL0
                || !rmp[spa.to_page()].view().check_vmpl(VMPL::VMPL0, Perm::Write)) && (vmpl
                >= VMPL::VMPL1 || !rmp[spa.to_page()].view().check_vmpl(VMPL::VMPL1, Perm::Write))
                && (vmpl >= VMPL::VMPL2 || !rmp[spa.to_page()].view().check_vmpl(
                VMPL::VMPL2,
                Perm::Write,
            )) && (vmpl >= VMPL::VMPL3 || !rmp[spa.to_page()].view().check_vmpl(
                VMPL::VMPL3,
                Perm::Write,
            ))) ==> (#[trigger] self.spec_sram().spec_data()[spa.value()]
                === #[trigger] other.spec_sram().spec_data()[spa.value()]))
        &&& rmp_model_eq(&self.spec_rmp(), &other.spec_rmp())
        &&& (other.spec_sram().inv() ==> self.spec_sram().inv())
    }
}

impl Model2Eq for VRamDB {
    open spec fn model2_eq(&self, other: &Self) -> bool {
        &&& (forall|spa: SPA|
            (self.spec_rmp()[spa.to_page()].view().spec_validated()
                && other.spec_rmp()[spa.to_page()].view().spec_validated()) ==> (
            self.spec_sram().spec_data()[#[trigger] spa.value()]
                === other.spec_sram().spec_data()[spa.value()]))
        &&& rmp_model_eq(&self.spec_rmp(), &other.spec_rmp())
        &&& (other.spec_sram().inv() ==> self.spec_sram().inv())
    }
}

impl VRamDB {
    // write_requires_nosysma
    pub open spec fn pte_write_requires_nosysmap(
        &self,
        gpmem_id: GPAMemID,
        enc: bool,
        data: ByteStream,
    ) -> bool {
        let gpmem = gpmem_id.range;
        let memid = gpmem_id.memid;
        let old_pte: Option<GuestPTEntry> = self.get_enc_data_ok(gpmem_id);
        let new_pte = stream_to_data::<GuestPTEntry>(data);
        let target_gpn = new_pte@.spec_ppn();
        let ptesize = spec_size::<GuestPTEntry>() as int;
        // If it is the PTE and the target gpn need c bit
        let is_last_entry = new_pte@.is_present() || memtype(
            memid,
            gpmem.to_page(),
        ).get_PTE_0().is_L0();
        let need_c_bit = (memtype(memid, target_gpn).need_c_bit() && is_last_entry);
        &&& if old_pte.is_Some() && gpmem.len() > 0 {
            let old_pte = old_pte.get_Some_0();
            &&& enc
            &&& gpmem_id.range.is_aligned(ptesize)
            &&& memtype(memid, gpmem.to_page()).is_PTE()
            &&& gpmem_id.range.len() == ptesize
            &&& target_gpn === old_pte@.spec_ppn()
            &&& need_c_bit ==> new_pte@.is_encrypted()
        } else {
            true
        }
    }

    pub open spec fn gpwrite_requires(
        &self,
        memid: MemID,
        range: GPMem,
        enc: bool,
        data: ByteStream,
    ) -> bool {
        let memty = memtype(memid, range.to_page());
        if memty.need_c_bit() {
            &&& enc
            &&& if memty.is_PTE() {
                self.pte_write_requires_nosysmap(AddrMemID { range, memid }, true, data)
            } else {
                true
            }
        } else {
            true
        }
    }

    /// Restrict VMPL0's bahavior.
    /// For memory read/write,
    ///     if the memory range is private for VMPL0, enc = true
    /// For RMP change, restrict the pvalidate and rmpadjust ops.
    pub open spec fn gpmemop_requires(&self, op: MemOp<GuestPhy>, sysmap: SysMap) -> bool {
        let AddrMemID { range: addr, memid } = op.to_addr_memid();
        match op {
            MemOp::Read(AddrMemID { range: addr, memid }, enc) => {
                if memtype(memid, addr.to_page()).need_c_bit() {
                    enc
                } else {
                    true
                }
            },
            MemOp::Write(gpmem_id, enc, data) => { self.gpwrite_requires(memid, addr, enc, data) },
            MemOp::RmpOp(rmpop) => { rmpop.gp_op_requires(&self.spec_rmp()) },
            _ => { true },
        }
    }

    pub open spec fn inv(&self) -> bool {
        &&& self.spec_sram().inv()
        &&& rmp_inv(&self.spec_rmp())
    }

    pub open spec fn inv_sw(&self, memid: MemID) -> bool {
        rmp_inv_sw(&self.spec_rmp(), memid)
    }

    pub open spec fn inv_memid_int(&self, memid: MemID) -> bool {
        rmp_inv_memid_int(&self.spec_rmp(), memid)
    }

    pub open spec fn write_read_requires_inner(
        &self,
        memid: MemID,
        enc: bool,
        gpa: GPMem,
        rgpa: GPMem,
        sysmap: SysMap,
    ) -> bool {
        let use_asid = if enc {
            memid.to_asid()
        } else {
            ASID_FOR_HV!()
        };
        let spa = sysmap.translate_addr_seq(gpa);
        let rspa = sysmap.translate_addr_seq(rgpa);
        let rmpcheck_w = rmp_check_access(
            &self.spec_rmp(),
            memid,
            enc,
            gpa,
            Perm::Write,
            spa.to_page(),
        );
        let rmpcheck_r = rmp_check_access(
            &self.spec_rmp(),
            memid,
            enc,
            gpa,
            Perm::Read,
            rspa.to_page(),
        );
        ||| rmpcheck_w.is_Error()
        ||| rmpcheck_r.is_Error()
        ||| self.spec_sram().disjoint_write_read_requires(use_asid, spa, rspa)
    }

    pub open spec fn get<T: VTypeCast<Seq<u8>>>(
        &self,
        gpmem_id: GPAMemID,
        enc: bool,
        sysmap: SysMap,
    ) -> T {
        stream_to_data(self.get_bytes(gpmem_id, enc, sysmap))
    }

    pub open spec fn get_enc_byte_ok(&self, memid: MemID, gpa: GPA) -> Option<Byte> {
        let spn = rmp_reverse(&self.spec_rmp(), memid, gpa.to_page());
        let spa = gpa.convert(spn);
        let ret = self.sram.read_one_byte(memid.to_asid(), spa);
        if rmp_has_gpn_memid(&self.spec_rmp(), gpa.to_page(), memid) {
            Option::Some(ret)
        } else {
            Option::None
        }
    }

    pub open spec fn get_enc_bytes_ok(&self, gpmem_id: GPAMemID) -> Option<ByteStream> {
        let gpmem = gpmem_id.range;
        let spa = rmp_reverse_mem(&self.spec_rmp(), gpmem_id.memid, gpmem);
        let ret = self.sram.read_bytes_by_asid(gpmem_id.memid.to_asid(), spa);
        if rmp_has_gpn_memid(&self.spec_rmp(), gpmem.to_page(), gpmem_id.memid) {
            Option::Some(ret)
        } else {
            Option::None
        }
    }

    pub open spec fn get_enc_data_ok<T: VTypeCast<Seq<u8>>>(&self, gpmem_id: GPAMemID) -> Option<
        T,
    > {
        let bytes = self.get_enc_bytes_ok(gpmem_id);
        if bytes.is_Some() {
            Option::Some(stream_to_data(bytes.get_Some_0()))
        } else {
            Option::None
        }
    }

    pub open spec fn get_byte(&self, memid: MemID, gpa: GPA, enc: bool, sysmap: SysMap) -> Option<
        Byte,
    > {
        let spa = sysmap.translate_addr(gpa);
        let use_asid = if enc {
            memid.to_asid()
        } else {
            ASID_FOR_HV!()
        };
        if spa.is_Some() {
            Option::Some(self.sram.read_one_byte(use_asid, spa.get_Some_0()))
        } else {
            Option::None
        }
    }

    pub open spec fn get_bytes(&self, gpmem_id: GPAMemID, enc: bool, sysmap: SysMap) -> ByteStream {
        let spmem = sysmap.translate_addr_seq(gpmem_id.range);
        let use_asid = if enc {
            gpmem_id.memid.to_asid()
        } else {
            ASID_FOR_HV!()
        };
        self.sram.read_bytes_by_asid(use_asid, spmem)
    }

    /// Only the VM itself can decrypt the data at the exact spa;
    pub open spec fn read_bytes(
        &self,
        gpmem_id: GPAMemID,
        enc: bool,
        sysmap: SysMap,
    ) -> ResultOrErr<ByteStream, MemError<()>> {
        recommends(gpmem_id.memid.is_Guest());
        let op = ();
        let gpa = gpmem_id.range;
        let memid = gpmem_id.memid;
        let rmp = self.rmp;
        let asid = memid.to_asid();
        let spn = sysmap.translate(gpa.to_page());
        let use_asid = if enc {
            asid
        } else {
            ASID_FOR_HV!()
        };
        if spn.is_Some() {
            let rmpcheck = rmp_check_access(&rmp, memid, enc, gpa, Perm::Read, spn.get_Some_0());
            if rmpcheck.is_Ok() {
                let data = self.get_bytes(gpmem_id, enc, sysmap);
                ResultOrErr::Ok(data)
            } else {
                ResultOrErr::Error(MemError::NestedPF(op))
            }
        } else {
            ResultOrErr::Error(MemError::NestedPF(op))
        }
    }

    pub open spec fn spec_ret_bytes(&self, memop: MemOp<GuestPhy>, sysmap: SysMap) -> ByteStream {
        match memop {
            MemOp::Read(gpmem_id, enc) => match self.read_bytes(gpmem_id, enc, sysmap) {
                ResultOrErr::Ok(_) => self.get_bytes(gpmem_id, enc, sysmap),
                ResultOrErr::Error(err) => ByteStream::empty(),
            },
            _ => ByteStream::empty(),
        }
    }

    pub open spec fn op_read(&self, gpmem_id: GPAMemID, enc: bool, sysmap: SysMap) -> ResultWithErr<
        Self,
        MemError<()>,
    > {
        recommends(gpmem_id.memid.is_Guest());
        match self.read_bytes(gpmem_id, enc, sysmap) {
            ResultOrErr::Ok(_) => ResultWithErr::Ok(*self),
            ResultOrErr::Error(err) => ResultWithErr::Error(*self, err),
        }
    }

    pub open spec fn op_write(
        &self,
        gpa_id: AddrID<GuestPhy>,
        enc: bool,
        data: ByteStream,
        sysmap: SysMap,
    ) -> ResultWithErr<Self, MemError<()>>
        recommends
            gpa_id.memid.is_Guest(),
    {
        let gpa = gpa_id.addr;
        let gpmem = gpa.to_mem(data.len());
        let memid = gpa_id.memid;
        let rmp = self.rmp;
        let asid = memid.to_asid();
        let spn = sysmap.translate(gpa.to_page());
        let spa_seq = sysmap.translate_addr_seq(gpmem);
        let use_asid = if enc {
            asid
        } else {
            ASID_FOR_HV!()
        };
        if spa_seq.is_valid() {
            let spn = spn.get_Some_0();
            let rmpcheck = rmp_check_access(&rmp, memid, enc, gpmem, Perm::Write, spn);
            if rmpcheck.is_Ok() {
                let new = self.spec_set_sram(self.spec_sram().write_raw(use_asid, spa_seq, data));
                ResultWithErr::Ok(new)
            } else {
                ResultWithErr::Error(*self, rmpcheck.get_Error_0().with_param(()))
            }
        } else {
            ResultWithErr::Error(*self, MemError::NestedPF(()))
        }
    }

    pub open spec fn rmp_op(&self, sysmap: SysMap, rmpop: RmpOp<GuestPhy>) -> ResultWithErr<
        Self,
        MemError<RmpOp<GuestPhy>>,
    > {
        let spn = sysmap.translate(rmpop.get_gpn());
        if spn.is_Some() {
            let spn = spn.get_Some_0();
            let spa_rmpop = rmpop.set_spn(spn);
            match rmp_op(&self.spec_rmp(), spa_rmpop) {
                ResultWithErr::Ok(newrmp) => ResultWithErr::Ok(self.spec_set_rmp(newrmp)),
                ResultWithErr::Error(_, err) => ResultWithErr::Error(*self, err.with_param(rmpop)),
            }
        } else {
            ResultWithErr::Error(*self, MemError::NestedPF(rmpop))
        }
    }

    #[verifier(opaque)]
    pub open spec fn op(&self, sysmap: SysMap, memop: MemOp<GuestPhy>) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestPhy>>,
    > {
        match memop {
            MemOp::RmpOp(rmpop) => {
                let ret = self.rmp_op(sysmap, rmpop);
                ret.replace_err(ret.to_err().with_param(memop))
            },
            MemOp::Read(gpmem_id, enc) => {
                let ret = self.op_read(gpmem_id, enc, sysmap);
                ret.replace_err(ret.to_err().with_param(memop))
            },
            MemOp::Write(gpa_id, enc, data) => {
                let ret = self.op_write(gpa_id, enc, data, sysmap);
                ret.replace_err(ret.to_err().with_param(memop))
            },
            _ => { ResultWithErr::Ok(*self) },
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/ptram/def_s.rs
================

use verismo_macro::*;

use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::arch::pgtable::*;
use crate::arch::vram::*;
use crate::tspec::*;

verus! {

#[derive(SpecGetter, SpecSetter)]
pub struct GuestPTRam {
    pub ram: VRamDB,
    pub l0_entry: Map<MemID, SpecGuestPTEntry>,
}

pub struct PTEAccessParam {
    pub memid: MemID,
    pub gvn: GVN,
    pub lvl: PTLevel,
}

} // verus!

================
File: ./source/verismo/src/arch/ptram/ptram_p.rs
================

use super::*;
use crate::arch::attack::*;
use crate::arch::memop::MemOp;
use crate::arch::rmp::{RmpEntry, RmpMap, *};
use crate::arch::vram::VRamDB;

verus! {

impl GuestPTRam {
    /// Prove the correctness of our model
    /// Prove the PTRam contains all page table data,
    /// ensuring PTRam does not need to query DataRam.
    pub proof fn proof_pte_addr_must_in_ptram(
        &self,
        sysmap: SysMap,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
        map_gpa: GPMem,
    )
        requires
            self.inv(memid),
            map_gpa.is_valid(),
            gvn.is_valid(),
            self.spec_ram().inv_sw(memid),
            self.valid_access(memid, map_gpa, sysmap),
            self.map_entry_gpa(sysmap, memid, gvn, lvl).is_Some(),
            map_gpa === self.map_entry_gpa(sysmap, memid, gvn, lvl).get_Some_0(),
        ensures
            sysmap.translate(map_gpa.to_page()).is_Some(),
            self.spec_ram().rmp.dom().contains(sysmap.translate(map_gpa.to_page()).get_Some_0()),
    {
        reveal(GuestPTRam::inv_dom_ok);
        self.lemma_map_entry_gpa_any_sysmap(memid, gvn, lvl, sysmap);
        self.lemma_map_entry_gpa_is_pte_type(memid, gvn, lvl);
        assert(memtype(memid, map_gpa.to_page()).is_PTE());
    }

    /*pub proof fn proof_mem_map_le_mem_map_ok(&self, sysmap: SysMap, memid: MemID, gvn: GVN, pt_rmp: RmpMap)
    requires
        self.inv(memid),
    ensures
        self.to_mem_map(sysmap, memid).db.le(self.to_mem_map_ok(memid).db)
    {
        assert forall |gvn|
            self.to_mem_map(sysmap, memid).db[gvn].is_Some()
        implies
            #[trigger] self.to_mem_map(sysmap, memid)[gvn] === self.to_mem_map_ok(memid)[gvn]
        by {
            self.lemma_map_entry_any_sysmap(memid, gvn, PTLevel::L0, sysmap);
        }
    }*/
    /// Prove the correctness of our model
    /// Prove the guest mapping is the identity mapping;
    pub proof fn proof_identity_mapping(
        &self,
        sysmap: SysMap,
        memid: MemID,
        gvn: GVN,
        pt_rmp: RmpMap,
    )
        requires
            self.inv(memid),
            gvn.is_valid(),
        ensures
            self.to_mem_map(sysmap, memid).is_identity_map(),
    {
        let memmap = self.to_mem_map(sysmap, memid);
        assert forall|gvn: GVN|
            gvn.is_valid() && (#[trigger] memmap.translate(gvn)).is_Some() implies memmap.translate(
            gvn,
        ).get_Some_0().as_int() == gvn.as_int() by {
            assert(self.map_entry(sysmap, memid, gvn, PTLevel::L0).is_Some());
            let ppn = self.map_entry(sysmap, memid, gvn, PTLevel::L0).get_Some_0().spec_ppn();
            assert(ppn == memmap.translate(gvn).get_Some_0());
            reveal(GuestPTRam::inv_content_ok);
            reveal(GuestPTRam::inv_for_identity_map_ok);
            self.lemma_map_entry_any_sysmap(memid, gvn, PTLevel::L0, sysmap);
        }
        reveal(MemMap::<_, _>::is_identity_map);
    }

    #[verifier(external_body)]
    pub proof fn lemma_map_entry_gpa_valid(
        &self,
        sysmap: SysMap,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
    )
        requires
            gvn.is_valid(),
            self.map_entry_gpa(sysmap, memid, gvn, lvl).is_Some(),
        ensures
            self.map_entry_gpa(sysmap, memid, gvn, lvl).get_Some_0().to_page().is_valid(),
    {
    }

    #[verifier(external_body)]
    pub proof fn lemma_map_entry_gpa_ok_valid(&self, memid: MemID, gvn: GVN, lvl: PTLevel)
        requires
            gvn.is_valid(),
            self.map_entry_gpa_ok(memid, gvn, lvl).is_Some(),
        ensures
            self.map_entry_gpa_ok(memid, gvn, lvl).get_Some_0().to_page().is_valid(),
    {
    }

    pub proof fn lemma_map_entry_gpa_is_pte_type(&self, memid: MemID, gvn: GVN, lvl: PTLevel)
        requires
            self.inv(memid),
            gvn.is_valid(),
            self.map_entry_gpa_ok(memid, gvn, lvl).is_Some(),
        ensures
            memtype(memid, self.map_entry_gpa_ok(memid, gvn, lvl).get_Some_0().to_page()).is_pt(
                lvl,
            ),
            self.map_entry_gpa_ok(memid, gvn, lvl).get_Some_0().to_page().is_valid(),
    {
        self.lemma_map_entry_gpa_ok_valid(memid, gvn, lvl);
        let l0_entry = self.l0_entry(memid);
        let idx = lvl.spec_table_index(gvn.to_addr()) as nat;
        let pte_gpa_ok = self.map_entry_gpa_ok(memid, gvn, lvl);
        assert(0 <= idx < PT_ENTRY_NUM!()) by {
            lvl.proof_table_index_range(gvn.to_addr());
        }
        assert(memtype(memid, pte_gpa_ok.get_Some_0().to_page()).is_pt(lvl)) by {
            reveal(GuestPTRam::inv_content_ok);
            reveal_with_fuel(GuestPTRam::pgtb_walk_addrs_recursive_ok, 1);
            match lvl.parent_lvl() {
                Option::Some(next_lvl) => {
                    let prev_pte = self.map_entry_ok(memid, gvn, next_lvl);
                    assert(prev_pte.is_Some());
                    let prev_pte = prev_pte.get_Some_0();
                    prev_pte.lemma_each_table_is_one_page(idx);
                    assert(pte_gpa_ok.get_Some_0().to_page() === prev_pte.spec_ppn());
                    assert(self.inv_content_gpa_ok(memid, gvn));
                },
                Option::None => {
                    l0_entry.lemma_each_table_is_one_page(idx);
                    assert(pte_gpa_ok.get_Some_0().to_page() === l0_entry.spec_ppn());
                },
            }
        }
    }

    /// Prove inv when PTE update meets requirements
    pub proof fn proof_memop_inv(
        old_pt: &Self,
        new_pt: &Self,
        sysmap: SysMap,
        memid: MemID,
        memop: MemOp<GuestPhy>,
    )
        requires
            old_pt.inv(memid),
            memid.is_vmpl0(),
            memop.is_valid(),
            new_pt === &old_pt.spec_set_ram(old_pt.ram.op(sysmap, memop).to_result()),
            //old_pt.spec_ram().op(sysmap, memop).is_Ok(),
            memop.to_memid().is_sm(memid) ==> old_pt.spec_ram().gpmemop_requires(
                memop,
                sysmap,
            ),
    //memop.is_Write() ==> Self::write_pt_requires(&old_pt.spec_ram(), memop.to_addr_memid(), memop.get_Write_1(), memop.get_Write_2(), sysmap)

        ensures
            new_pt.inv(memid),
    {
        reveal(VRamDB::op);
        old_pt.spec_ram().proof_op_inv_sw(sysmap, memop, memid);
        assert(new_pt.spec_ram().inv_sw(memid));
        assert(new_pt.spec_ram().inv_memid_int(memid));
        match memop {
            MemOp::Read(gpmem_id, enc) => {
                if old_pt.ram.op(sysmap, memop).is_Ok() {
                    Self::lemma_safe_read(memid, old_pt, new_pt, gpmem_id, enc, sysmap);
                }
            },
            MemOp::Write(gpa_id, enc, data) => {
                if old_pt.ram.op(sysmap, memop).is_Ok() {
                    Self::lemma_safe_write(memid, old_pt, new_pt, gpa_id, enc, data, sysmap);
                    //assume(new_pt.inv(memid));
                }
            },
            MemOp::InvlPage(gpa_id) => {},
            MemOp::FlushAll(_) => {},
            MemOp::RmpOp(rmpop) => {
                assume(new_pt.inv(memid));
            },
        }
    }

    proof fn lemma_safe_read(
        memid: MemID,
        old_pt: &Self,
        new_pt: &Self,
        gpmem_id: GPAMemID,
        enc: bool,
        sysmap: SysMap,
    )
        requires
            old_pt.inv(memid),
            gpmem_id.range.is_valid(),
            new_pt === &old_pt.spec_set_ram(
                old_pt.spec_ram().op_read(gpmem_id, enc, sysmap).to_result(),
            ),
        ensures
            new_pt.inv_dom_ok(memid),
            new_pt.inv_content_ok(memid),
    {
        reveal(GuestPTRam::inv_dom_ok);
        reveal(VRamDB::op);
        let ram = old_pt.spec_ram();
        ram.proof_op_inv(sysmap, MemOp::Read(gpmem_id, enc));
    }

    proof fn lemma_write_pte_inv_ppn_enc(
        old_pt: &Self,
        new_pt: &Self,
        sysmap: SysMap,
        memid: MemID,
        memop: MemOp<GuestPhy>,
        gvn: GVN,
        lvl: PTLevel,
    )
        requires
            old_pt.inv(memid),
            memid.is_vmpl0(),
            //old_pt.spec_ram().inv_enc(memid),
            gvn.is_valid(),
            memop.is_valid(),
            memop.is_Write(),
            memop.to_memid().is_sm(memid) ==> old_pt.spec_ram().gpmemop_requires(memop, sysmap),
            //Self::write_pt_requires(&old_pt.spec_ram(), memop.to_addr_memid(), memop.get_Write_1(), memop.get_Write_2(), sysmap),
            //memid === memop.to_addr_memid().memid,
            new_pt.l0_entry(memid) === old_pt.l0_entry(memid),
            new_pt === &old_pt.spec_set_ram(old_pt.spec_ram().op(sysmap, memop).to_result()),
            new_pt.map_entry_ok(memid, gvn, lvl).is_Some(),
            old_pt.spec_ram().op(sysmap, memop).is_Ok(),
        ensures
            old_pt.map_entry_exe_ok(memid, gvn, lvl).is_Some(),
            (old_pt.need_c_bit(memid, gvn) && lvl.is_L0()) ==> new_pt.map_entry_ok(
                memid,
                gvn,
                lvl,
            ).get_Some_0().is_encrypted(),
            new_pt.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn() === old_pt.map_entry_ok(
                memid,
                gvn,
                lvl,
            ).get_Some_0().spec_ppn(),
    {
        Self::lemma_write_pte_inv_ppn(old_pt, new_pt, sysmap, memid, memop, gvn, lvl);
        let wgpmem = memop.to_mem();
        let write_pte: GuestPTEntry = stream_to_data(memop.get_Write_2());
        let old_pte_gpa = old_pt.map_entry_gpa_ok(memid, gvn, lvl);
        let old_gpn = old_pt.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn();
        assert(memtype(memid, old_pte_gpa.get_Some_0().to_page()).is_pt(lvl)) by {
            old_pt.lemma_map_entry_gpa_is_pte_type(memid, gvn, lvl);
        }
        let old_pte = old_pt.map_entry_exe_ok(memid, gvn, lvl);
        if old_pte_gpa.get_Some_0() === wgpmem && memop.to_memid().is_sm(memid)
            && memop.to_memid().to_asid() == memid.to_asid() {
            assert(old_pte.is_Some());
            assert(old_pte === old_pt.spec_ram().get_enc_data_ok::<GuestPTEntry>(
                AddrMemID { range: wgpmem, memid },
            ));
            assert(old_pt.spec_ram().op(sysmap, memop).is_Ok());
            assert(old_pte.is_Some());
            assert(write_pte.view().spec_ppn() === old_pte.get_Some_0().view().spec_ppn());
            if old_pt.need_c_bit(memid, gvn) && lvl.is_L0() {
                assert(write_pte.view().is_encrypted());
            }
        }
        if old_pt.need_c_bit(memid, gvn) && lvl.is_L0() {
            reveal(GuestPTRam::inv_content_ok);
            reveal(GuestPTRam::inv_encrypted_priv_mem_ok);
            assert(old_pt.map_entry_ok(memid, gvn, PTLevel::L0).get_Some_0()
                === old_pte.get_Some_0().view());
            assert(old_pte.get_Some_0().view().is_encrypted());
        }
    }

    /*proof fn lemma_write_pte_inv_ppn(old_pt: &Self, new_pt: &Self, sysmap: SysMap, memid: MemID, memop: MemOp<GuestPhy>, gvn: GVN, lvl: PTLevel)
    requires
        old_pt.inv(memid),
        memid.is_vmpl0(),
        gvn.is_valid(),
        memop.is_valid(),
        memop.is_Write(),
        memop.to_memid().is_sm(memid) ==> old_pt.spec_ram().gpmemop_requires(memop, sysmap),
        new_pt.l0_entry(memid) === old_pt.l0_entry(memid),
        new_pt === &old_pt.spec_set_ram(old_pt.spec_ram().op(sysmap, memop).to_result()),
        new_pt.map_entry_exe_ok(memid, gvn, lvl).is_Some(),
        old_pt.spec_ram().op(sysmap, memop).is_Ok(),
    ensures
        old_pt.map_entry_exe_ok(memid, gvn, lvl).is_Some(),
        ({
            ||| old_pt.map_entry_gpa_ok(memid, gvn, lvl).get_Some_0() !== memop.to_mem()
            ||| !memop.to_memid().is_sm(memid)
            ||| memop.to_memid().to_asid() !== memid.to_asid()
        }) ==> {
            new_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0() === old_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0()
        },
        ({
            &&& old_pt.map_entry_gpa_ok(memid, gvn, lvl).get_Some_0() === memop.to_mem()
            &&& memop.to_memid().is_sm(memid)
            &&& memop.to_memid().to_asid() == memid.to_asid()
        }) ==> {
            new_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0() === stream_to_data(memop.get_Write_2()) &&
            new_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0().view().spec_ppn() === old_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0().view().spec_ppn()
        }
    decreases
        lvl.as_int(),
    {
        let pte = new_pt.map_entry_exe_ok(memid, gvn, lvl);
        let old_pte = old_pt.map_entry_exe_ok(memid, gvn, lvl);

        let pte_gpa = new_pt.map_entry_gpa_ok(memid, gvn, lvl);
        let old_pte_gpa = old_pt.map_entry_gpa_ok(memid, gvn, lvl);

        match lvl.parent_lvl() {
            Option::Some(next_lvl) => {
                Self::lemma_write_pte_inv_ppn(old_pt, new_pt, sysmap, memid, memop, gvn, next_lvl);
                assert(pte_gpa === old_pte_gpa);
                assert(pte_gpa.is_Some());
            }
            _ => {
                assert(pte_gpa === old_pte_gpa) by {
                    reveal_with_fuel(GuestPTRam::pgtb_walk_addrs_recursive_ok, 1);
                }
            }
        }
        assert(pte_gpa === old_pte_gpa);
        assert(pte.is_Some());
        if let MemOp::Write(_, enc, data) = memop {
            let write_pte: GuestPTEntry = stream_to_data(data);
            let gpmem_id = memop.to_addr_memid();
            let AddrMemID {range: wgpmem, memid: op_memid} = gpmem_id;
            assert(old_pt.spec_ram().inv()) by {
                reveal(GuestPTRam::inv_dom_ok);
            }
            let pte_gpa = pte_gpa.get_Some_0();
            assert(old_pt.map_entry_gpa_ok(memid, gvn, lvl).is_Some());
            let old_pte_gpa = old_pte_gpa.get_Some_0();
            assert(memtype(memid, old_pte_gpa.to_page()).is_PTE()) by {
                old_pt.lemma_map_entry_gpa_is_pte_type(memid, gvn, lvl);
            }
            assert(old_pte_gpa.to_page().is_valid()) by {
                old_pt.lemma_map_entry_gpa_ok_valid(memid, gvn, lvl);
            }
            reveal_with_fuel(GuestPTRam::pgtb_walk_addrs_recursive_ok, 1);
            if gpmem_id.memtype().is_PTE() {
                old_pt.spec_ram().lemma_write_sm_int_ok(memid, memop, sysmap);
                if !memop.to_memid().is_sm(memid) || !enc {
                    old_pt.spec_ram().lemma_write_bytes_effect_by_other_vm_or_shared(&new_pt.spec_ram(), sysmap, memop, memid, old_pte_gpa);
                    assert(old_pt.spec_ram().get_enc_bytes_ok(AddrMemID{memid, range:old_pte_gpa}) ===
                        new_pt.spec_ram().get_enc_bytes_ok(AddrMemID{memid, range:old_pte_gpa}));
                    assert(pte === old_pte);
                } else {
                    assert(old_pt.spec_ram().gpmemop_requires(memop, sysmap));
                    assert(old_pt.spec_ram().pte_write_requires_nosysmap(gpmem_id, true, data));
                    if (old_pte.is_Some()) {
                        assert(rmp_has_gpn_memid(&old_pt.spec_ram().rmp, old_pte_gpa.to_page(), memid));
                        if(!rmp_has_gpn_memid(&old_pt.spec_ram().rmp, wgpmem.to_page(), op_memid)) {
                            assert(wgpmem.to_page() !== old_pte_gpa.to_page());
                        }
                    } else {
                        assert(!rmp_has_gpn_memid(&old_pt.spec_ram().rmp, old_pte_gpa.to_page(), memid));
                        old_pt.spec_ram().proof_rmp_check_access_rmp_has_gpn_memid(memop, sysmap);
                        assert(wgpmem.to_page() !== old_pte_gpa.to_page());
                        assert(wgpmem.disjoint(old_pte_gpa));
                    }
                    assert(spec_size::<GuestPTEntry>() == PT_ENTRY_SIZE!());
                    assert(wgpmem.len() == 0 || (wgpmem.len() == PT_ENTRY_SIZE!() as int && wgpmem.is_aligned(PT_ENTRY_SIZE!() as int)) || wgpmem.disjoint(old_pte_gpa));
                    if (wgpmem.len() == PT_ENTRY_SIZE!() as int && wgpmem.is_aligned(PT_ENTRY_SIZE!() as int)) {
                        GPMem::lemma_aligned_mem_eq_or_disjoint(old_pte_gpa,
                            wgpmem, PT_ENTRY_SIZE!() as int);
                    }
                    if old_pte_gpa === wgpmem {
                        assert(enc) by {
                            reveal(RmpEntry::check_access);
                        }
                        old_pt.spec_ram().lemma_write_enc_bytes_effect_same_read(&new_pt.spec_ram(), sysmap, memop, memid, old_pte_gpa);
                        assert(op_memid.to_asid() === memid.to_asid());
                        assert(pte.get_Some_0() === write_pte);
                        assert(op_memid.is_sm(memid));
                        let old_value: Option<GuestPTEntry> = old_pt.spec_ram().get_enc_data_ok(memop.to_addr_memid());
                        assert(old_value.is_Some()) by {
                            old_pt.spec_ram().lemma_write_enc_must_has_gpn_in_rmp(memid, memop, sysmap);
                        }
                        assert(write_pte.view().spec_ppn() === old_pte.get_Some_0().view().spec_ppn());
                    } else {
                        old_pt.spec_ram().lemma_write_enc_bytes_effect_disjoint_read(&new_pt.spec_ram(), sysmap, memop, memid, old_pte_gpa);
                        assert(pte === old_pte);
                    }
                }
            } else {
                assert(memtype(memid, old_pte_gpa.to_page()).is_PTE());
                old_pte_gpa.lemma_disjoint(wgpmem);
                old_pt.spec_ram().lemma_write_enc_bytes_effect_disjoint_read(&new_pt.spec_ram(), sysmap, memop, memid, old_pte_gpa);
                assert(old_pt.spec_ram().get_enc_bytes_ok(AddrMemID{memid, range:old_pte_gpa}) ===
                        new_pt.spec_ram().get_enc_bytes_ok(AddrMemID{memid, range:old_pte_gpa}));
                assert(pte === old_pte);
            }
        }
    }*/
    //#[verifier(external_body)]
    proof fn lemma_safe_write(
        memid: MemID,
        old_pt: &Self,
        new_pt: &Self,
        gpa_id: AddrID<GuestPhy>,
        enc: bool,
        data: ByteStream,
        sysmap: SysMap,
    )
        requires
            old_pt.inv(memid),
            memid.is_vmpl0(),
            gpa_id.addr.is_valid(),
            old_pt.ram.op_write(gpa_id, enc, data, sysmap).is_Ok(),
            //sysmap.is_one_to_one_map(),
            new_pt === &old_pt.spec_set_ram(
                old_pt.spec_ram().op(sysmap, MemOp::Write(gpa_id, enc, data)).to_result(),
            ),
            //Self::write_pt_requires(&old_pt.spec_ram(), gpa_id, enc, data, sysmap),
            gpa_id.memid.is_sm(memid) ==> old_pt.spec_ram().gpmemop_requires(
                MemOp::Write(gpa_id, enc, data),
                sysmap,
            ),
        ensures
            new_pt.inv(memid),
    {
        reveal(GuestPTRam::inv_dom_ok);
        reveal(GuestPTRam::inv_content_ok);
        reveal(VRamDB::op);
        reveal(VRamDB::op_write);
        let rmp = old_pt.spec_ram().spec_rmp();
        let op_memid = gpa_id.memid;
        let memop = MemOp::Write(gpa_id, enc, data);
        assert(rmp === new_pt.spec_ram().spec_rmp());
        assert(new_pt.ram.rmp.dom() === old_pt.ram.rmp.dom());
        assert(new_pt.inv_dom_ok(memid)) by {
            old_pt.spec_ram().proof_op_inv(sysmap, memop);
            assert(new_pt.spec_ram().inv());
        }
        assert(new_pt.inv_for_identity_map_ok(memid)) by {
            reveal(GuestPTRam::inv_for_identity_map_ok);
            assert forall|gvn: GVN|
                gvn.is_valid() && new_pt.map_entry_ok(
                    memid,
                    gvn,
                    MAX_PT_LEVEL!(),
                ).is_Some() implies (#[trigger] new_pt.map_entry_ok(
                memid,
                gvn,
                MAX_PT_LEVEL!(),
            )).get_Some_0().spec_ppn().value() === gvn.value() by {
                Self::lemma_write_pte_inv_ppn_enc(
                    old_pt,
                    new_pt,
                    sysmap,
                    memid,
                    memop,
                    gvn,
                    MAX_PT_LEVEL!(),
                );
                assert(old_pt.map_entry_ok(
                    memid,
                    gvn,
                    MAX_PT_LEVEL!(),
                ).get_Some_0().spec_ppn().value() === gvn.value());
            }
        }
        assert(new_pt.inv_encrypted_priv_mem_ok(memid)) by {
            reveal(GuestPTRam::inv_encrypted_priv_mem_ok);
            assert forall|gvn: GVN|
                gvn.is_valid() && (new_pt.need_c_bit(memid, gvn) && new_pt.map_entry_ok(
                    memid,
                    gvn,
                    MAX_PT_LEVEL!(),
                ).is_Some()) implies #[trigger] new_pt.map_entry_ok(
                memid,
                gvn,
                MAX_PT_LEVEL!(),
            ).get_Some_0().is_encrypted() by {
                let pte_gpa = new_pt.map_entry_gpa_ok(memid, gvn, MAX_PT_LEVEL!()).get_Some_0();
                Self::lemma_write_pte_inv_ppn_enc(
                    old_pt,
                    new_pt,
                    sysmap,
                    memid,
                    memop,
                    gvn,
                    MAX_PT_LEVEL!(),
                );
                assert(old_pt.map_entry_ok(memid, gvn, MAX_PT_LEVEL!()).get_Some_0().is_encrypted())
            }
        }
        assert forall|gvn: GVN| gvn.is_valid() implies #[trigger] new_pt.inv_content_gpa_ok(
            memid,
            gvn,
        ) by {
            assert forall|lvl: PTLevel|
                !lvl.is_L0() && (#[trigger] new_pt.map_entry_ok(
                    memid,
                    gvn,
                    lvl,
                )).is_Some() implies memtype(
                memid,
                new_pt.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn(),
            ).is_pt(lvl.child_lvl().get_Some_0()) by {
                Self::lemma_write_pte_inv_ppn_enc(old_pt, new_pt, sysmap, memid, memop, gvn, lvl);
                assert(old_pt.map_entry_ok(memid, gvn, lvl).is_Some());
                assert(old_pt.inv_content_gpa_ok(memid, gvn));
                assert(memtype(
                    memid,
                    old_pt.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn(),
                ).is_pt(lvl.child_lvl().get_Some_0()));
                assert(old_pt.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn()
                    === new_pt.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn());
            }
        }
        assert(new_pt.spec_ram().inv_sw(memid));
    }

    pub proof fn lemma_pgtb_walk_addrs_recursive_any_sysmap(
        &self,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
        sysmap: SysMap,
    )
        requires
            self.pgtb_walk_addrs_recursive(sysmap, memid, gvn, lvl).is_Some(),
            self.spec_ram().inv_sw(memid),
            gvn.is_valid(),
        ensures
            self.pgtb_walk_addrs_recursive(sysmap, memid, gvn, lvl)
                === self.pgtb_walk_addrs_recursive_ok(memid, gvn, lvl),
        decreases lvl.as_int(),
    {
        reveal_with_fuel(GuestPTRam::pgtb_walk_addrs_recursive, 1);
        let rmp = self.spec_ram().spec_rmp();
        let vram = self.spec_ram();
        let sram = self.spec_ram().spec_sram();
        match lvl.parent_lvl() {
            Option::None => {
                assert(self.pgtb_walk_addrs_recursive(sysmap, memid, gvn, lvl)
                    === self.pgtb_walk_addrs_recursive_ok(memid, gvn, lvl));
            },
            Option::Some(next_lvl) => {
                self.lemma_pgtb_walk_addrs_recursive_any_sysmap(memid, gvn, next_lvl, sysmap);
                let next_pte_gpmem = self.pgtb_walk_addrs_recursive(sysmap, memid, gvn, next_lvl);
                assert(next_pte_gpmem.is_Some());
                let next_pte_gpmem = next_pte_gpmem.get_Some_0();
                self.lemma_map_entry_gpa_valid(sysmap, memid, gvn, next_lvl);
                vram.lemma_read_enc_byte_ok(
                    sysmap,
                    AddrMemID { range: next_pte_gpmem, memid },
                    true,
                );
            },
        }
    }

    pub proof fn lemma_map_entry_gpa_model1_eq(
        &self,
        other: &Self,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
    )
        requires
            other.inv(memid),
            gvn.is_valid(),
            rmp_inv_sw(&other.spec_ram().spec_rmp(), memid),
            rmp_inv_memid_int(&other.spec_ram().spec_rmp(), memid),
            self.model1_eq(other, memid),
        ensures
            self.map_entry_gpa_ok(memid, gvn, lvl).is_Some() ==> self.map_entry_gpa_ok(
                memid,
                gvn,
                lvl,
            ) === other.map_entry_gpa_ok(memid, gvn, lvl),
            other.map_entry_gpa_ok(memid, gvn, lvl).is_None() ==> self.map_entry_gpa_ok(
                memid,
                gvn,
                lvl,
            ).is_None(),
        decreases lvl.as_int(),
    {
        reveal_with_fuel(GuestPTRam::pgtb_walk_addrs_recursive_ok, 1);
        match lvl.parent_lvl() {
            Option::None => {
                assert(self.pgtb_walk_addrs_recursive_ok(memid, gvn, lvl)
                    === other.pgtb_walk_addrs_recursive_ok(memid, gvn, lvl));
            },
            Option::Some(next_lvl) => {
                self.lemma_map_entry_gpa_model1_eq(other, memid, gvn, next_lvl);
                let next_pte_gpmem = self.pgtb_walk_addrs_recursive_ok(memid, gvn, next_lvl);
                if next_pte_gpmem.is_Some() {
                    let next_pte_gpmem = next_pte_gpmem.get_Some_0();
                    assert(memtype(memid, next_pte_gpmem.to_page()).is_sm_int()) by {
                        other.lemma_map_entry_gpa_is_pte_type(memid, gvn, next_lvl);
                    }
                    self.lemma_map_entry_gpa_ok_valid(memid, gvn, next_lvl);
                    self.spec_ram().lemma_read_enc_ok_model1_eq(
                        &other.spec_ram(),
                        AddrMemID { range: next_pte_gpmem, memid },
                    );
                } else {
                    //assert(self.pgtb_walk_addrs_recursive_ok(memid, gvn, lvl) === other.pgtb_walk_addrs_recursive_ok(memid, gvn, lvl));
                }
            },
        }
    }

    pub proof fn lemma_map_entry_model1_eq(
        &self,
        other: &Self,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
    )
        requires
            other.inv(memid),
            gvn.is_valid(),
            rmp_inv_sw(&other.spec_ram().spec_rmp(), memid),
            rmp_inv_memid_int(&other.spec_ram().spec_rmp(), memid),
            self.model1_eq(other, memid),
        ensures
            self.map_entry_ok(memid, gvn, lvl).is_Some() ==> self.map_entry_ok(memid, gvn, lvl)
                === other.map_entry_ok(memid, gvn, lvl),
            other.map_entry_ok(memid, gvn, lvl).is_None() ==> self.map_entry_ok(
                memid,
                gvn,
                lvl,
            ).is_None(),
    {
        self.lemma_map_entry_gpa_model1_eq(other, memid, gvn, lvl);
        let pte_gpa = self.map_entry_gpa_ok(memid, gvn, lvl);
        let pte_gpa2 = other.map_entry_gpa_ok(memid, gvn, lvl);
        if self.map_entry_ok(memid, gvn, lvl).is_Some() {
            self.lemma_map_entry_gpa_ok_valid(memid, gvn, lvl);
            assert(pte_gpa.is_Some());
            assert(pte_gpa === pte_gpa2);
            let pte_gpa = pte_gpa.get_Some_0();
            let pte_gpa2 = pte_gpa2.get_Some_0();
            assert(memtype(memid, pte_gpa2.to_page()).is_PTE()) by {
                other.lemma_map_entry_gpa_is_pte_type(memid, gvn, lvl);
            }
            self.spec_ram().lemma_read_enc_ok_model1_eq(
                &other.spec_ram(),
                AddrMemID { range: pte_gpa, memid },
            );
        }
    }

    pub proof fn lemma_map_entry_gpa_any_sysmap(
        &self,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
        sysmap: SysMap,
    )
        requires
            gvn.is_valid(),
            self.map_entry_gpa(sysmap, memid, gvn, lvl).is_Some(),
            self.spec_ram().inv_sw(memid),
        ensures
            self.map_entry_gpa(sysmap, memid, gvn, lvl) === self.map_entry_gpa_ok(memid, gvn, lvl),
        decreases lvl.as_int(),
    {
        self.lemma_pgtb_walk_addrs_recursive_any_sysmap(memid, gvn, lvl, sysmap);
    }

    pub proof fn lemma_map_entry_any_sysmap(
        &self,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
        sysmap: SysMap,
    )
        requires
            gvn.is_valid(),
            self.map_entry(sysmap, memid, gvn, lvl).is_Some(),
            self.spec_ram().inv_sw(memid),
        ensures
            self.map_entry(sysmap, memid, gvn, lvl) === self.map_entry_ok(memid, gvn, lvl),
        decreases lvl.as_int(),
    {
        self.lemma_pgtb_walk_addrs_recursive_any_sysmap(memid, gvn, lvl, sysmap);
        let pte_gpa = self.map_entry_gpa(sysmap, memid, gvn, lvl);
        let pte_gpa_ok = self.map_entry_gpa_ok(memid, gvn, lvl);
        assert(pte_gpa.is_Some());
        assert(pte_gpa_ok === pte_gpa);
        let pte_gpa = pte_gpa.get_Some_0();
        self.lemma_map_entry_gpa_ok_valid(memid, gvn, lvl);
        self.spec_ram().lemma_read_enc_byte_ok(sysmap, AddrMemID { range: pte_gpa, memid }, true);
    }
}

} // verus!

================
File: ./source/verismo/src/arch/ptram/ptram_u.rs
================

use super::*;
use crate::arch::attack::*;

verus! {

impl Model1Eq for GuestPTRam {
    open spec fn model1_eq(&self, other: &Self, memid: MemID) -> bool {
        self.spec_ram().model1_eq(&other.spec_ram(), memid) && equal(
            self.l0_entry(memid),
            other.l0_entry(memid),
        )
    }
}

impl Model2Eq for GuestPTRam {
    open spec fn model2_eq(&self, other: &Self) -> bool {
        self.spec_ram().model2_eq(&other.spec_ram()) && equal(self.l0_entry, other.l0_entry)
    }
}

impl GuestPTRam {
    // Returns the GPA of a page table entry for translating a page gvn.
    // To succeed, the sysmap must be consistant with rmp table
    pub open spec fn pgtb_walk_addrs_recursive_ok(
        &self,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
    ) -> Option<GPMem> {
        decreases(lvl.as_int());
        let vram = self.spec_ram();
        let rmp = self.spec_ram().spec_rmp();
        let sram = self.spec_ram().spec_sram();
        let l0_entry = self.l0_entry(memid);
        let next_opt = lvl.parent_lvl();
        let idx = AsNat!(lvl.spec_table_index(gvn.to_addr()));
        if next_opt.is_None() {
            Option::Some(GPMem::from_range(l0_entry.addr_for_idx(idx), AsNat!(PT_ENTRY_SIZE!())))
        } else {
            let next_lvl = next_opt.get_Some_0();
            if next_lvl.as_int() < lvl.as_int() {
                let next_pte_addrs = self.pgtb_walk_addrs_recursive_ok(memid, gvn, next_lvl);
                if next_pte_addrs.is_Some() {
                    let next_pte_gpmem = next_pte_addrs.get_Some_0();
                    let next_pte = vram.get_enc_data_ok(AddrMemID { range: next_pte_gpmem, memid });
                    if next_pte.is_Some() {
                        let next_pte: GuestPTEntry = next_pte.get_Some_0();
                        Option::Some(
                            GPMem::from_range(
                                next_pte.view().addr_for_idx(idx),
                                PT_ENTRY_SIZE!() as nat,
                            ),
                        )
                    } else {
                        Option::None
                    }
                } else {
                    Option::None
                }
            } else {
                // unreachable
                Option::None
            }
        }
    }

    pub open spec fn valid_access(&self, memid: MemID, gpa: GPMem, sysmap: SysMap) -> bool {
        self.spec_ram().read_bytes(AddrMemID { range: gpa, memid }, true, sysmap).is_Ok()
    }

    pub open spec fn l0_entry(&self, memid: MemID) -> SpecGuestPTEntry {
        self.l0_entry[memid]
    }

    pub open spec fn inv(&self, memid: MemID) -> bool {
        &&& self.inv_content_ok(memid)
        &&& self.inv_dom_ok(memid)
        &&& self.spec_ram().inv_sw(memid)
        &&& self.spec_ram().inv_memid_int(memid)
        &&& self.spec_ram().inv()
    }

    ///
    /// * If a gpn is PTE type and there is a spa mapped by the gpn, the db should include the ram data at that spa.
    /// * If a gpn (mapping to spa) is not PTE type, db does not contains spa
    pub open spec fn inv_dom_ok(&self, memid: MemID) -> bool {
        //let rmp = self.spec_ram().spec_rmp();
        /*&&& (forall |spn|
                self.ram.dom().contains(spn) === (memtype(memid, (#[trigger]rmp[spn]).view().spec_gpn()).is_PTE() && rmp.dom().contains(spn)))*/
        self.spec_ram().inv()
    }

    pub open spec fn inv_content_gpa_ok(&self, memid: MemID, gvn: GVN) -> bool {
        forall|lvl: PTLevel|
            (!lvl.is_L0() && (#[trigger] self.map_entry_ok(memid, gvn, lvl)).is_Some()) ==> memtype(
                memid,
                self.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn(),
            ).is_pt(lvl.child_lvl().get_Some_0())
    }

    #[verifier(opaque)]
    pub open spec fn inv_content_ok(&self, memid: MemID) -> bool {
        &&& (forall|gvn: GVN| gvn.is_valid() ==> #[trigger] self.inv_content_gpa_ok(memid, gvn))
        &&& self.inv_for_identity_map_ok(memid)
        &&& self.inv_encrypted_priv_mem_ok(memid)
        &&& memtype(memid, self.l0_entry(memid).spec_ppn()).is_pt(PTLevel::L3)
        &&& memid.is_Guest()
    }

    #[verifier(opaque)]
    pub open spec fn inv_for_identity_map_ok(&self, memid: MemID) -> bool {
        &&& (forall|gvn: GVN|
            (gvn.is_valid() && (#[trigger] self.map_entry_ok(memid, gvn, PTLevel::L0)).is_Some())
                ==> self.map_entry_ok(memid, gvn, PTLevel::L0).get_Some_0().spec_ppn().value()
                === gvn.value())
    }

    pub open spec fn need_c_bit(&self, memid: MemID, gvn: GVN) -> bool {
        let rmp = self.spec_ram().spec_rmp();
        let entry = self.map_entry_ok(memid, gvn, PTLevel::L0).get_Some_0();
        memtype(
            memid,
            entry.spec_ppn(),
        ).need_c_bit()
        //||rmp.has_gpn_memid(entry.spec_ppn(), memid))

    }

    /// Any PTE mapping to private GPN should be marked as encrypted.
    #[verifier(opaque)]
    pub open spec fn inv_encrypted_priv_mem_ok(&self, memid: MemID) -> bool {
        &&& (forall|gvn: GVN|
            (gvn.is_valid() && self.need_c_bit(memid, gvn) && self.map_entry_ok(
                memid,
                gvn,
                PTLevel::L0,
            ).is_Some()) ==> #[trigger] self.map_entry_ok(
                memid,
                gvn,
                PTLevel::L0,
            ).get_Some_0().is_encrypted())
    }

    pub open spec fn map_entry_gpa_ok(&self, memid: MemID, gvn: GVN, lvl: PTLevel) -> Option<
        GPMem,
    > {
        self.pgtb_walk_addrs_recursive_ok(memid, gvn, lvl)
    }

    pub open spec fn map_entry_ok(&self, memid: MemID, gvn: GVN, lvl: PTLevel) -> Option<
        SpecGuestPTEntry,
    > {
        let ret = self.map_entry_exe_ok(memid, gvn, lvl);
        match ret {
            Option::Some(exe_ret) => { Option::Some(exe_ret.view()) },
            _ => Option::None,
        }
    }

    pub open spec fn map_entry_exe_ok(&self, memid: MemID, gvn: GVN, lvl: PTLevel) -> Option<
        GuestPTEntry,
    > {
        let pte_gpa = self.map_entry_gpa_ok(memid, gvn, lvl);
        if pte_gpa.is_Some() {
            let pte_gpa = pte_gpa.get_Some_0();
            let entry = self.spec_ram().get_enc_data_ok::<GuestPTEntry>(
                AddrMemID { range: pte_gpa, memid },
            );
            if entry.is_Some() {
                Option::Some(entry.get_Some_0())
            } else {
                Option::None
            }
        } else {
            Option::None
        }
    }

    pub open spec fn to_mem_map_ok(&self, memid: MemID) -> MemMap<GuestVir, GuestPhy> {
        let map = Map::new(
            |gvn: GVN| gvn.is_valid() && self.map_entry_ok(memid, gvn, PTLevel::L0).is_Some(),
            |gvn: GVN| self.map_entry_ok(memid, gvn, PTLevel::L0).get_Some_0(),
        );
        MemMap { db: map }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/ptram/mod.rs
================

use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::arch::pgtable::*;
use crate::tspec::*;
use crate::*;

mod def_s;
pub use def_s::*;
mod ptram_p;
mod ptram_p2;
mod ptram_s;
mod ptram_u;

================
File: ./source/verismo/src/arch/ptram/ptram_s.rs
================

use super::*;
verus! {

impl GuestPTRam {
    #[verifier(opaque)]
    // Returns the GPA of a page table entry for translating a page gvn.
    pub open spec fn pgtb_walk_addrs_recursive(
        &self,
        sysmap: SysMap,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
    ) -> Option<GPMem>
        decreases lvl.as_int(),
    {
        let l0_entry = self.l0_entry(memid);
        let next_opt = lvl.parent_lvl();
        let idx = lvl.spec_table_index(gvn.to_addr()) as nat;
        if next_opt.is_None() {
            Option::Some(GPMem::from_range(l0_entry.addr_for_idx(idx), PT_ENTRY_SIZE!() as nat))
        } else {
            let next_lvl = next_opt.get_Some_0();
            if next_lvl.as_int() < lvl.as_int() {
                let next_pte_addrs = self.pgtb_walk_addrs_recursive(sysmap, memid, gvn, next_lvl);
                if next_pte_addrs.is_Some() {
                    let next_pte_gpa = next_pte_addrs.get_Some_0();
                    let next_pte = self.hw_read_pte(memid, sysmap, next_pte_gpa);
                    if next_pte.is_Some() && self.valid_access(memid, next_pte_gpa, sysmap) {
                        Option::Some(
                            GPMem::from_range(
                                next_pte.get_Some_0().addr_for_idx(idx),
                                PT_ENTRY_SIZE!() as nat,
                            ),
                        )
                    } else {
                        Option::None
                    }
                } else {
                    Option::None
                }
            } else {  // unreachable
                Option::None
            }
        }
    }

    /// Hardware read the data from spa without checking RMP.
    pub open spec fn hw_read_pte(&self, memid: MemID, sysmap: SysMap, gpa: GPMem) -> Option<
        SpecGuestPTEntry,
    > {
        let val = self.ram.get::<GuestPTEntry>(AddrMemID { range: gpa, memid }, true, sysmap);
        Option::Some(
            val@,
        )
        //Option::Some(ByteStream::to_data(self.spec_ram().spec_ret_bytes(MemOp::Read(AddrMemID{range: gpa, memid}, true), sysmap)))

    }

    pub open spec fn map_entry_gpa(
        &self,
        sysmap: SysMap,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
    ) -> Option<GPMem> {
        self.pgtb_walk_addrs_recursive(sysmap, memid, gvn, lvl)
    }

    pub open spec fn map_entry(
        &self,
        sysmap: SysMap,
        memid: MemID,
        gvn: GVN,
        lvl: PTLevel,
    ) -> Option<SpecGuestPTEntry> {
        let pte_gpa = self.map_entry_gpa(sysmap, memid, gvn, lvl);
        if pte_gpa.is_Some() && self.valid_access(memid, pte_gpa.get_Some_0(), sysmap) {
            self.hw_read_pte(memid, sysmap, pte_gpa.get_Some_0())
        } else {
            Option::None
        }
    }

    #[verifier(opaque)]
    pub open spec fn valid_translate(&self, sysmap: SysMap, memid: MemID, gvn: GVN) -> bool {
        &&& self.map_entry(sysmap, memid, gvn, PTLevel::L0).is_Some()
        &&& (forall|lvl|
            self.valid_access(
                memid,
                (#[trigger] self.map_entry_gpa(sysmap, memid, gvn, lvl)).get_Some_0(),
                sysmap,
            ))
    }

    // pt_rmp: is a RMP table with only spa whose gpa is of PTE type.
    pub open spec fn to_mem_map(&self, sysmap: SysMap, memid: MemID) -> MemMap<GuestVir, GuestPhy> {
        let map = Map::new(
            |gvn: GVN| gvn.is_valid() && self.map_entry(sysmap, memid, gvn, PTLevel::L0).is_Some(),
            |gvn: GVN| self.map_entry(sysmap, memid, gvn, PTLevel::L0).get_Some_0(),
        );
        MemMap { db: map }
    }

    pub open spec fn gpn_is_encrypted(&self, sysmap: SysMap, gvn: GVN, memid: MemID) -> bool {
        self.map_entry(sysmap, memid, gvn, PTLevel::L0).get_Some_0().is_encrypted()
    }
}

} // verus!

================
File: ./source/verismo/src/arch/ptram/ptram_p2.rs
================

use super::*;
use crate::arch::attack::*;
use crate::arch::memop::MemOp;
use crate::arch::rmp::{RmpEntry, RmpMap, *};
use crate::arch::vram::VRamDB;

verus! {

impl GuestPTRam {
    pub proof fn lemma_write_pte_inv_ppn(
        old_pt: &Self,
        new_pt: &Self,
        sysmap: SysMap,
        memid: MemID,
        memop: MemOp<GuestPhy>,
        gvn: GVN,
        lvl: PTLevel,
    )
        requires
            old_pt.inv(memid),
            memid.is_vmpl0(),
            gvn.is_valid(),
            memop.is_valid(),
            memop.is_Write(),
            memop.to_memid().is_sm(memid) ==> old_pt.spec_ram().gpmemop_requires(memop, sysmap),
            new_pt.l0_entry(memid) === old_pt.l0_entry(memid),
            new_pt === &old_pt.spec_set_ram(old_pt.spec_ram().op(sysmap, memop).to_result()),
            new_pt.map_entry_exe_ok(memid, gvn, lvl).is_Some(),
            old_pt.spec_ram().op(sysmap, memop).is_Ok(),
        ensures
            old_pt.map_entry_exe_ok(memid, gvn, lvl).is_Some(),
            ({
                ||| old_pt.map_entry_gpa_ok(memid, gvn, lvl).get_Some_0() !== memop.to_mem()
                ||| !memop.to_memid().is_sm(memid)
                ||| memop.to_memid().to_asid() !== memid.to_asid()
            }) ==> {
                new_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0() === old_pt.map_entry_exe_ok(
                    memid,
                    gvn,
                    lvl,
                ).get_Some_0()
            },
            ({
                &&& old_pt.map_entry_gpa_ok(memid, gvn, lvl).get_Some_0() === memop.to_mem()
                &&& memop.to_memid().is_sm(memid)
                &&& memop.to_memid().to_asid() == memid.to_asid()
            }) ==> {
                new_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0() === stream_to_data(
                    memop.get_Write_2(),
                ) && new_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0().view().spec_ppn()
                    === old_pt.map_entry_exe_ok(memid, gvn, lvl).get_Some_0().view().spec_ppn()
            },
        decreases lvl.as_int(),
    {
        let pte = new_pt.map_entry_exe_ok(memid, gvn, lvl);
        let old_pte = old_pt.map_entry_exe_ok(memid, gvn, lvl);
        let pte_gpa = new_pt.map_entry_gpa_ok(memid, gvn, lvl);
        let old_pte_gpa = old_pt.map_entry_gpa_ok(memid, gvn, lvl);
        match lvl.parent_lvl() {
            Option::Some(next_lvl) => {
                Self::lemma_write_pte_inv_ppn(old_pt, new_pt, sysmap, memid, memop, gvn, next_lvl);
                assert(pte_gpa === old_pte_gpa);
                assert(pte_gpa.is_Some());
            },
            _ => {
                assert(pte_gpa === old_pte_gpa) by {
                    reveal_with_fuel(GuestPTRam::pgtb_walk_addrs_recursive_ok, 1);
                }
            },
        }
        assert(pte_gpa === old_pte_gpa);
        assert(pte.is_Some());
        if let MemOp::Write(_, enc, data) = memop {
            let write_pte: GuestPTEntry = stream_to_data(data);
            let gpmem_id = memop.to_addr_memid();
            let AddrMemID { range: wgpmem, memid: op_memid } = gpmem_id;
            assert(old_pt.spec_ram().inv()) by {
                reveal(GuestPTRam::inv_dom_ok);
            }
            let pte_gpa = pte_gpa.get_Some_0();
            assert(old_pt.map_entry_gpa_ok(memid, gvn, lvl).is_Some());
            let old_pte_gpa = old_pte_gpa.get_Some_0();
            assert(memtype(memid, old_pte_gpa.to_page()).is_PTE()) by {
                old_pt.lemma_map_entry_gpa_is_pte_type(memid, gvn, lvl);
            }
            assert(old_pte_gpa.to_page().is_valid()) by {
                old_pt.lemma_map_entry_gpa_ok_valid(memid, gvn, lvl);
            }
            reveal_with_fuel(GuestPTRam::pgtb_walk_addrs_recursive_ok, 1);
            if gpmem_id.memtype().is_PTE() {
                old_pt.spec_ram().lemma_write_sm_int_ok(memid, memop, sysmap);
                if !memop.to_memid().is_sm(memid) || !enc {
                    old_pt.spec_ram().lemma_write_bytes_effect_by_other_vm_or_shared(
                        &new_pt.spec_ram(),
                        sysmap,
                        memop,
                        memid,
                        old_pte_gpa,
                    );
                    assert(old_pt.spec_ram().get_enc_bytes_ok(
                        AddrMemID { memid, range: old_pte_gpa },
                    ) === new_pt.spec_ram().get_enc_bytes_ok(
                        AddrMemID { memid, range: old_pte_gpa },
                    ));
                    assert(pte === old_pte);
                } else {
                    assert(old_pt.spec_ram().gpmemop_requires(memop, sysmap));
                    assert(old_pt.spec_ram().pte_write_requires_nosysmap(gpmem_id, true, data));
                    if (old_pte.is_Some()) {
                        assert(rmp_has_gpn_memid(
                            &old_pt.spec_ram().rmp,
                            old_pte_gpa.to_page(),
                            memid,
                        ));
                        if (!rmp_has_gpn_memid(
                            &old_pt.spec_ram().rmp,
                            wgpmem.to_page(),
                            op_memid,
                        )) {
                            assert(wgpmem.to_page() !== old_pte_gpa.to_page());
                        }
                    } else {
                        assert(!rmp_has_gpn_memid(
                            &old_pt.spec_ram().rmp,
                            old_pte_gpa.to_page(),
                            memid,
                        ));
                        old_pt.spec_ram().proof_rmp_check_access_rmp_has_gpn_memid(memop, sysmap);
                        assert(wgpmem.to_page() !== old_pte_gpa.to_page());
                        assert(wgpmem.disjoint(old_pte_gpa));
                    }
                    assert(spec_size::<GuestPTEntry>() == PT_ENTRY_SIZE!());
                    assert(wgpmem.len() == 0 || (wgpmem.len() == PT_ENTRY_SIZE!() as int
                        && wgpmem.is_aligned(PT_ENTRY_SIZE!() as int)) || wgpmem.disjoint(
                        old_pte_gpa,
                    ));
                    if (wgpmem.len() == PT_ENTRY_SIZE!() as int && wgpmem.is_aligned(
                        PT_ENTRY_SIZE!() as int,
                    )) {
                        GPMem::lemma_aligned_mem_eq_or_disjoint(
                            old_pte_gpa,
                            wgpmem,
                            PT_ENTRY_SIZE!() as int,
                        );
                    }
                    if old_pte_gpa === wgpmem {
                        assert(enc);
                        old_pt.spec_ram().lemma_write_enc_bytes_effect_same_read(
                            &new_pt.spec_ram(),
                            sysmap,
                            memop,
                            memid,
                            old_pte_gpa,
                        );
                        assert(op_memid.to_asid() === memid.to_asid());
                        assert(pte.get_Some_0() === write_pte);
                        assert(op_memid.is_sm(memid));
                        let old_value: Option<GuestPTEntry> = old_pt.spec_ram().get_enc_data_ok(
                            memop.to_addr_memid(),
                        );
                        assert(old_value.is_Some()) by {
                            old_pt.spec_ram().lemma_write_enc_must_has_gpn_in_rmp(
                                memid,
                                memop,
                                sysmap,
                            );
                        }
                        assert(write_pte.view().spec_ppn()
                            === old_pte.get_Some_0().view().spec_ppn());
                    } else {
                        old_pt.spec_ram().lemma_write_enc_bytes_effect_disjoint_read(
                            &new_pt.spec_ram(),
                            sysmap,
                            memop,
                            memid,
                            old_pte_gpa,
                        );
                        assert(pte === old_pte);
                    }
                }
            } else {
                assert(memtype(memid, old_pte_gpa.to_page()).is_PTE());
                old_pte_gpa.lemma_disjoint(wgpmem);
                old_pt.spec_ram().lemma_write_enc_bytes_effect_disjoint_read(
                    &new_pt.spec_ram(),
                    sysmap,
                    memop,
                    memid,
                    old_pte_gpa,
                );
                assert(old_pt.spec_ram().get_enc_bytes_ok(AddrMemID { memid, range: old_pte_gpa })
                    === new_pt.spec_ram().get_enc_bytes_ok(
                    AddrMemID { memid, range: old_pte_gpa },
                ));
                assert(pte === old_pte);
            }
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/errors/mod.rs
================

use super::*;
use crate::tspec::*;

verus! {

#[is_variant]
pub enum MemError<Param> {
    Others(Param),  // vaddr, memid
    NoRam(Param),  // vaddr, memid
    NotValidated(Param),  // Failed validation check
    NestedPF(Param),
    PageFault(Param),
    RmpOp(RmpFault, Param),
}

#[is_variant]
pub enum RmpFault {
    Unsupported,
    Size,
    Input,
    Perm,
    DoubleVal,
}

impl<Param> MemError<Param> {
    verus! {
        pub open spec fn trigger_trap(&self) -> bool {
            match *self {
                MemError::RmpOp(fault, _) => {
                    fault === RmpFault::Unsupported
                },
                _ => {
                    true
                }
            }
        }
        pub open spec fn from_err<T>(err: MemError<T>, param: Param) -> Self
        {
            err.with_param(param)
        }

        pub open spec fn with_param<T>(&self, param: T) -> MemError<T>
        {
            match *self {
                MemError::Others(_) => MemError::Others(param),
                MemError::NoRam(_) => MemError::NoRam(param),
                MemError::NotValidated(_) => MemError::NotValidated(param),
                MemError::NestedPF(_) => MemError::NestedPF(param),
                MemError::PageFault(_) => MemError::PageFault(param),
                MemError::RmpOp(fault, _) => MemError::RmpOp(fault, param),
            }
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/entities/memtype.rs
================

use verismo_macro::*;

use super::*;
use crate::arch::addr_s::*;
use crate::tspec::*;

verus! {

#[derive(PartialEq, Eq, Structural, SpecIntEnum)]
#[is_variant]
pub enum PTLevel {
    L3 = 0,
    L2,
    L1,
    L0,
}

/// In Init stage, there is only PTE, SmPrivCode, SmBootData, and some invalidated
/// The Init process transition some invalidated pages to SmPrivData, SmVmplPage, and Others. In this stage, no data flow from private -> others;
/// In PostInit, rmp change is not allowed for any private mem
#[is_variant]
pub enum MemType {
    PTE(PTLevel),  // page table
    SmPrivData,  // heap + secret page
    SmPrivCode,  // code
    SmPrivStack,  // stack
    SmBootData,  // not hidden from Hv
    SmVmplPage,  // shared between other vmpl
    RichOSMem,  // Validated page used by other VMPL
    HvShared,  // Shared page with HV for communication
}

} // verus!
verus! {

impl MemType {
    #[verifier(inline)]
    pub open spec fn is_pt(&self, level: PTLevel) -> bool {
        self === &MemType::PTE(level)
    }

    // Is the data integrity important for SM?
    // Both Hv and VMPL > 0 will fails the SM or will not change content
    #[verifier(inline)]
    pub open spec fn is_sm_int(&self) -> bool {
        ||| self.is_SmPrivData()
        ||| self.is_SmBootData()
        ||| self.is_SmPrivCode()
        ||| self.is_SmPrivStack()
        ||| self.is_PTE()
    }

    // Is the data integrity important for the VM (for all VMPLs)?
    #[verifier(inline)]
    pub open spec fn is_vm_int(&self) -> bool {
        ||| self.is_sm_int()
        ||| self.is_SmVmplPage()
    }

    #[verifier(inline)]
    pub open spec fn need_c_bit(&self) -> bool {
        self.is_vm_int()
        // || self.is_sm_conf()

    }

    // This is a correctness requirement
    #[verifier(inline)]
    pub open spec fn need_c_bit_cleared(&self) -> bool {
        self.is_HvShared()
    }
}

} // verus!
verus! {

/// gpn -> memory type.
/// A software should strictly follows the memory layout defined by this fn.
pub spec fn memtype_inner(gpn: GPN) -> MemType;

pub open spec fn memtype(memid: MemID, gpn: GPN) -> MemType {
    memtype_inner(gpn)
}

} // verus!
#[macro_export]
macro_rules! ASID_FOR_HV {
    () => {
        spec_cast_integer::<u64, nat>(0u64)
    };
}

================
File: ./source/verismo/src/arch/entities/params.rs
================

use super::*;
use crate::arch::addr_s::*;
use crate::tspec::*;

verus! {

pub struct EncGPA {
    pub memid: MemID,
    pub gpa: SpecMem<GuestPhy>,
    pub enc: bool,
}

impl EncGPA {
    pub open spec fn gpa_id(&self) -> GPAMemID {
        AddrMemID { range: self.gpa, memid: self.memid }
    }
}

} // verus!
verus! {

pub struct AddrMemID<AddrT> {
    pub range: SpecMem<AddrT>,
    pub memid: MemID,
}

impl AddrMemID<GuestPhy> {
    pub open spec fn memtype(&self) -> MemType {
        memtype(self.memid, self.range.to_page())
    }
}

} // verus!
verus! {

pub type GPAMemID = AddrMemID<GuestPhy>;

pub type GVAMemID = AddrMemID<GuestVir>;

pub struct PageID<AddrT> {
    pub page: SpecPage<AddrT>,
    pub memid: MemID,
}

impl PageID<GuestPhy> {
    pub open spec fn memtype(&self) -> MemType {
        memtype(self.memid, self.page)
    }
}

pub struct AddrID<AddrT> {
    pub addr: SpecAddr<AddrT>,
    pub memid: MemID,
}

} // verus!

================
File: ./source/verismo/src/arch/entities/mod.rs
================

use verismo_macro::*;

use crate::tspec::*;
mod memid;
mod memtype;
mod params;

pub use memid::*;
pub use memtype::*;
pub use params::*;

verus! {

pub type ASID = nat;

pub type CPU = nat;

#[derive(PartialEq, Eq, SpecIntEnum, Copy, Clone)]
#[is_variant]
pub enum VMPL {
    VMPL0,
    VMPL1,
    VMPL2,
    VMPL3,
}

} // verus!

================
File: ./source/verismo/src/arch/entities/memid.rs
================

use super::*;
use crate::*;

verus! {

#[is_variant]
pub enum MemID {
    Guest(nat, VMPL),
    Hv,
}

pub struct CpuMemID(pub CPU, pub MemID);

verus! {
    impl CpuMemID {
        pub open spec fn cpu(&self) -> CPU {
            self.0
        }

        pub open spec fn memid(&self) -> MemID {
            self.1
        }
    }
}

verus! {
pub open spec fn current_vmpl() -> VMPL {
    VMPL::VMPL0
}

impl MemID {
    pub open spec fn is_sm(&self, sm_memid: MemID) -> bool {
        &&& self.to_vmpl().as_int() <= sm_memid.to_vmpl().as_int()
        &&& (self.to_asid() === sm_memid.to_asid())
    }

    pub open spec fn is_vmpl0(&self) -> bool {
        self.to_vmpl().is_VMPL0() && self.is_Guest()
    }

    pub open spec fn to_asid(&self) -> ASID {
        match *self {
            MemID::Guest(id_minus_one, _,) => {
                id_minus_one + 1
            }
            _ => {
                ASID_FOR_HV!()
            }
        }
    }

    pub open spec fn to_vmpl(&self) -> VMPL
    recommends
        self.is_Guest()
    {
        match *self {
            MemID::Guest(_, vmpl) => {
                vmpl
            }
            _ => {
                VMPL::VMPL0
            }
        }
    }
}
}

} // verus!

================
File: ./source/verismo/src/arch/mem/mem_u.rs
================

use super::*;
use crate::arch::attack::*;
verus! {

impl MemDB {
    pub open spec fn to_gpop_ok(&self, memop: MemOp<GuestVir>) -> MemOp<GuestPhy> {
        let gvmem = memop.to_mem();
        let op_memid = memop.to_memid();
        let guestmap = self.to_mem_map_ok(op_memid);
        let gpmem = gvmem.convert(guestmap.translate(gvmem.to_page()).get_Some_0());
        let enc = guestmap.is_encrypted(gvmem.to_page());
        memop.translate_gpn(gpmem, enc.get_Some_0())
    }

    pub open spec fn vop_requires(&self, memop: MemOp<GuestVir>) -> bool {
        let gvn = memop.to_page();
        let gp_memop = self.to_gpop_ok(memop);
        let gmap = self.to_mem_map_ok(memop.to_memid());
        let sysmap = self.spec_sysmap()[memop.to_memid()];
        if gmap.translate(gvn).is_Some() {
            self.spec_vram().gpmemop_requires(gp_memop, sysmap)
        } else {
            true
        }
    }

    // No sysmap dependency
    pub open spec fn inv(&self, memid: MemID) -> bool {
        &&& self.spec_g_page_table(memid).inv(memid)
        &&& self.to_mem_map_ok(memid).is_identity_map()
        &&& self.spec_tlb().inv_encrypted_priv_mem(memid)
        &&& self.spec_vram().inv()
        &&& self.spec_vram().inv_sw(memid)
        &&& memid.is_Guest()
    }

    pub open spec fn to_mem_map_ok(&self, memid: MemID) -> MemMap<GuestVir, GuestPhy> {
        MemMap {
            db: self.spec_g_page_table(memid).to_mem_map_ok(memid).db.union_prefer_right(
                self.spec_tlb().to_mem_map(memid).db,
            ),
        }
    }
}

} // verus!
verus! {

impl Model1Eq for MemDB {
    open spec fn model1_eq(&self, other: &Self, memid: MemID) -> bool {
        self.spec_vram().model1_eq(&other.spec_vram(), memid) && self.spec_tlb().model1_eq(
            &other.spec_tlb(),
            memid,
        ) && equal(self.l0_entry.spec_index(memid), other.l0_entry.spec_index(memid))
    }
}

impl Model2Eq for MemDB {
    open spec fn model2_eq(&self, other: &Self) -> bool {
        self.spec_vram().model2_eq(&other.spec_vram()) && self.spec_tlb().model2_eq(
            &other.spec_tlb(),
        ) && equal(self.l0_entry, other.l0_entry)
    }
}

} // verus!

================
File: ./source/verismo/src/arch/mem/mem_p.rs
================

use super::*;
use crate::arch::attack::*;

verus! {

impl MemOp<GuestVir> {
    proof fn lemma_vop_require_to_gop_require(&self, memid: MemID, memdb: &MemDB)
        requires
            memid.to_vmpl().is_VMPL0(),
            self.to_memid().is_sm(memid) ==> memdb.vop_requires(*self),
            memdb.inv(memid),
            memdb.spec_vram() !== memdb.op(*self).to_result().spec_vram(),
        ensures
            self.to_memid().is_sm(memid) ==> memdb.spec_vram().gpmemop_requires(
                memdb.to_gpop(*self),
                memdb.sysmap[self.to_memid()],
            ),
    {
        if self.to_memid().is_sm(memid) {
            assert(memdb.vop_requires(*self));
            if memdb.to_mem_map(self.to_memid()).translate(self.to_page()).is_Some() {
                memdb.lemma_mem_map_to_mem_map_ok(self.to_memid(), self.to_page());
                assert(memdb.to_mem_map_ok(self.to_memid()).translate(self.to_page()).is_Some())
            } else {
                assert(memdb.spec_vram() === memdb.op(*self).to_result().spec_vram());
            }
        }
    }
}

impl MemDB {
    pub proof fn lemma_rmp_inv(&self, new: &Self, memop: MemOp<GuestVir>, spn: SPN)
        requires
            self.to_spop(self.to_gpop(memop)).to_page() !== spn || !memop.is_RmpOp(),
            new === &self.op(memop).to_result(),
        ensures
            self.spec_vram().spec_rmp().dom().contains(spn)
                === new.spec_vram().spec_rmp().dom().contains(spn),
            self.spec_vram().spec_rmp().dom().contains(spn) ==> self.spec_vram().spec_rmp()[spn]
                === new.spec_vram().spec_rmp()[spn],
    {
        reveal(VRamDB::op);
        reveal(rmp_inv);
    }

    pub proof fn lemma_mem_map_to_mem_map_ok(&self, memid: MemID, gvn: GVN)
        requires
            self.inv(memid),
        ensures
            self.to_mem_map(memid)[gvn].is_Some() ==> self.to_mem_map(memid)[gvn]
                === self.to_mem_map_ok(memid)[gvn],
    {
        if self.to_mem_map(memid)[gvn].is_Some() {
            if self.to_mem_map(memid)[gvn] !== self.spec_tlb().to_mem_map(memid)[gvn] {
                self.spec_g_page_table(memid).lemma_map_entry_any_sysmap(
                    memid,
                    gvn,
                    PTLevel::L0,
                    self.sysmap[memid],
                );
            }
        }
    }

    pub proof fn lemma_guest_mem_map_equal_or_flushed(
        &self,
        new: &Self,
        memid: MemID,
        op: MemOp<GuestVir>,
        gvn: GVN,
    )
        requires
            self.spec_g_page_table(memid).inv(memid),
            new.spec_g_page_table(memid).inv(memid),
            self.to_mem_map_ok(memid).is_identity_map(),
            new === &self.op(op).to_result() || new.model1_eq(self, memid),
        ensures
            ({
                ||| new.to_mem_map_ok(memid)[gvn] === self.spec_g_page_table(memid).to_mem_map_ok(
                    memid,
                )[gvn]
                ||| new.to_mem_map_ok(memid)[gvn] === new.spec_g_page_table(memid).to_mem_map_ok(
                    memid,
                )[gvn]
                ||| new.to_mem_map_ok(memid)[gvn] === self.to_mem_map_ok(memid)[gvn]
                ||| new.to_mem_map_ok(memid)[gvn].is_None()
            }),
    {
        let old_pt_entry = self.spec_g_page_table(memid).to_mem_map(self.sysmap[memid], memid)[gvn];
        let old_pt_entry_ok = self.spec_g_page_table(memid).to_mem_map_ok(memid)[gvn];
        let old_tlb_entry = self.spec_tlb().to_mem_map(memid)[gvn];
        let old_entry = self.to_mem_map(memid)[gvn];
        let pt_entry = new.spec_g_page_table(memid).to_mem_map(new.sysmap[memid], memid)[gvn];
        let pt_entry_ok = new.spec_g_page_table(memid).to_mem_map_ok(memid)[gvn];
        let tlb_entry = new.spec_tlb().to_mem_map(memid)[gvn];
        let entry = new.to_mem_map(memid)[gvn];
        if old_tlb_entry.is_Some() {
            assert(old_entry == old_tlb_entry);
        } else {
            assert(old_entry === old_pt_entry);
            if old_entry.is_Some() {
                self.spec_g_page_table(memid).lemma_map_entry_any_sysmap(
                    memid,
                    gvn,
                    PTLevel::L0,
                    self.sysmap[memid],
                );
                assert(old_entry === old_pt_entry_ok);
            }
        }
        if tlb_entry.is_Some() {
            assert(entry == tlb_entry);
        } else {
            assert(entry === pt_entry);
            if entry.is_Some() {
                new.spec_g_page_table(memid).lemma_map_entry_any_sysmap(
                    memid,
                    gvn,
                    PTLevel::L0,
                    new.sysmap[memid],
                );
                assert(entry === pt_entry_ok);
            }
        }
        if (old_entry !== entry) && entry.is_Some() {
            if (entry == tlb_entry) {
                if tlb_entry != old_tlb_entry {
                    // load tlb entry from page table
                    assert(tlb_entry === old_entry);
                }
            } else {
                assert(entry === pt_entry_ok);
                if new.model1_eq(self, memid) {
                    new.spec_g_page_table(memid).lemma_map_entry_model1_eq(
                        &self.spec_g_page_table(memid),
                        memid,
                        gvn,
                        PTLevel::L0,
                    );
                    assert(pt_entry_ok.is_Some());
                    assert(pt_entry_ok === old_pt_entry_ok);
                }
                if pt_entry_ok != old_pt_entry_ok {
                    assert(new === &self.op(op).to_result());
                    assert(op.is_Write() || op.is_RmpOp()) by {
                        reveal(VRamDB::op);
                    }
                    if tlb_entry.is_Some() {
                        if (tlb_entry !== old_tlb_entry) {
                            assert(tlb_entry === old_pt_entry_ok);
                        }
                    } else {
                        assert(entry == pt_entry_ok);
                    }
                }
            }
        }
    }

    pub proof fn lemma_identity_map(&self, new: &Self, memid: MemID, op: MemOp<GuestVir>)
        requires
            self.spec_g_page_table(memid).inv(memid),
            self.to_mem_map_ok(memid).is_identity_map(),
            new.spec_g_page_table(memid).inv(memid),
            new === &self.op(op).to_result() || new.model1_eq(self, memid),
        ensures
            new.to_mem_map_ok(memid).is_identity_map(),
    {
        reveal(MemMap::is_identity_map);
        let new_mem_map = new.to_mem_map_ok(memid);
        assert forall|vpage: GVN|
            (#[trigger] new_mem_map.translate(vpage)).is_Some() implies new_mem_map.translate(
            vpage,
        ).get_Some_0().as_int() === vpage.as_int() by {
            let old_pt_entry_ok = self.spec_g_page_table(memid).map_entry_ok(
                memid,
                vpage,
                PTLevel::L0,
            );
            let old_entry = self.to_mem_map_ok(memid)[vpage];
            let pt_entry_ok = new.spec_g_page_table(memid).map_entry_ok(memid, vpage, PTLevel::L0);
            let entry = new_mem_map[vpage];
            self.lemma_guest_mem_map_equal_or_flushed(new, memid, op, vpage);
            assert(entry.is_Some());
            if entry === old_pt_entry_ok {
                assert(old_pt_entry_ok.get_Some_0().spec_ppn().as_int() === vpage.as_int()) by {
                    reveal(GuestPTRam::inv_content_ok);
                    reveal(GuestPTRam::inv_for_identity_map_ok);
                    assert(self.spec_g_page_table(memid).inv_for_identity_map_ok(memid));
                }
            } else if entry === pt_entry_ok {
                assert(pt_entry_ok.get_Some_0().spec_ppn().as_int() === vpage.as_int()) by {
                    reveal(GuestPTRam::inv_content_ok);
                    reveal(GuestPTRam::inv_for_identity_map_ok);
                    assert(new.spec_g_page_table(memid).inv_for_identity_map_ok(memid));
                }
            } else {
                assert(entry === old_entry);
                assert(self.to_mem_map_ok(memid).translate(vpage).get_Some_0().as_int()
                    === vpage.as_int());
            }
        }
    }

    pub proof fn lemma_encrypted(&self, memid: MemID, gvn: GVN)
        requires
            self.inv(memid),
            gvn.is_valid(),
            self.to_mem_map_ok(memid).need_c_bit(memid, gvn),
        ensures
            self.to_mem_map_ok(memid).is_encrypted_or_none(gvn),
            self.to_mem_map(memid).is_encrypted_or_none(gvn),
    {
        let sysmap = self.sysmap[memid];
        if self.to_mem_map_ok(memid).translate(gvn).is_Some() {
            assert(self.to_mem_map_ok(memid).is_encrypted(gvn).get_Some_0()) by {
                reveal(MemDB::inv);
                reveal(GuestPTRam::inv_content_ok);
                reveal(GuestPTRam::inv_encrypted_priv_mem_ok);
                assert(self.spec_g_page_table(memid).inv_encrypted_priv_mem_ok(memid));
                assert(self.spec_tlb().inv_encrypted_priv_mem(memid));
                if self.spec_tlb().to_mem_map(memid).spec_index(gvn).is_Some() {
                    assert(self.spec_tlb().to_mem_map(memid).is_encrypted_or_none(gvn)) by {
                        reveal(MemMap::inv_encrypted_priv_mem);
                    }
                } else {
                    assert(self.spec_g_page_table(memid).map_entry_ok(
                        memid,
                        gvn,
                        PTLevel::L0,
                    ).is_Some());
                    //self.spec_g_page_table(memid).lemma_map_entry_any_sysmap(memid, gvn, pgtable::PTLevel::L0, sysmap);
                    //assert(self.spec_g_page_table(memid).map_entry_ok(memid, gvn, pgtable::PTLevel::L0).is_Some());
                    assert(self.spec_g_page_table(memid).map_entry_ok(
                        memid,
                        gvn,
                        PTLevel::L0,
                    ).get_Some_0().is_encrypted());
                }
            }
        }
        assert(self.to_mem_map_ok(memid).is_encrypted_or_none(gvn));
        if self.to_mem_map(memid).translate(gvn).is_Some() {
            self.lemma_mem_map_to_mem_map_ok(memid, gvn);
        }
    }

    pub proof fn lemma_op_err_Ginv(&self, memop: MemOp<GuestVir>)
        requires
            memop.is_valid(),
        ensures
            !self.op(memop).is_Ok() ==> self.op(memop).to_result().spec_vram() === self.spec_vram(),
            self.op(memop).to_result().spec_vram() !== self.spec_vram() ==> self.op(memop).is_Ok(),
    {
        reveal(RmpEntry::check_access);
        if self.to_mem_map(memop.to_memid()).translate(memop.to_mem().to_page()).is_Some() {
            let gpmemop = self.to_gpop(memop);
            self.to_mem_map(memop.to_memid()).lemma_valid_translate(memop.to_mem().to_page());
            assert(gpmemop.is_valid());
            self.spec_vram().lemma_op_err_Ginv(self.spec_sysmap()[memop.to_memid()], gpmemop);
        }
    }

    // rmp update does not need to check sysmap;
    // only hv can do rmpupdate
    pub proof fn lemma_op_run(&self, memop: MemOp<GuestVir>)
        requires
            memop.use_gmap(),
            self.op(memop).is_Ok(),
        ensures
            self.to_mem_map(memop.to_memid()).translate(memop.to_page()).is_Some(),
            //self.to_mem_map_ok(memop.to_memid()).translate(memop.to_page()).is_Some(),
            !memop.is_RmpOp() ==> self.sysmap[memop.to_memid()].translate(
                self.to_gpop(memop).to_page(),
            ).is_Some(),
    {
        reveal(VRamDB::op);
        reveal(RmpEntry::check_access);
    }

    pub proof fn lemma_op_error(&self, memop: MemOp<GuestVir>)
        requires
            memop.use_gmap(),
            self.to_mem_map(memop.to_memid()).translate(memop.to_page()).is_None(),
        ensures
            !self.op(memop).is_Ok(),
            self.op(memop).get_Error_1().trigger_trap() || (memop.is_RmpOp() && self.op(
                memop,
            ).get_Error_1().is_RmpOp() && !self.op(
                memop,
            ).get_Error_1().get_RmpOp_0().is_DoubleVal()),
    {
        reveal(VRamDB::op);
        reveal(RmpEntry::check_access);
    }

    //#[verifier(external_body)]
    proof fn lemma_op_read_or_write_tlb_inv(&self, new: &Self, memid: MemID, memop: MemOp<GuestVir>)
        requires
            self.inv(memid),
            memop.use_gmap(),
            new === &self.op(memop).to_result(),
        ensures
            new.spec_tlb().inv_encrypted_priv_mem(memid),
    {
        let guestmap = self.to_mem_map(memid);
        let new_guestmap = self.to_mem_map(memid);
        let guestmap_tlb = self.spec_tlb().to_mem_map(memid);
        let new_guestmap_tlb = new.spec_tlb().to_mem_map(memid);
        assert(new.spec_tlb().inv_encrypted_priv_mem(memid)) by {
            reveal(GuestPTRam::inv_content_ok);
            reveal(MemMap::inv_encrypted_priv_mem);
            assert forall|gvn: GVN|
                gvn.is_valid() && new_guestmap_tlb.need_c_bit(
                    memid,
                    gvn,
                ) implies #[trigger] new_guestmap_tlb.is_encrypted_or_none(gvn) by {
                if new_guestmap_tlb.translate(gvn).is_Some() {
                    if new_guestmap_tlb[gvn] === guestmap_tlb[gvn] {
                        assert(guestmap_tlb.is_encrypted_or_none(gvn));
                    } else {
                        assert(new_guestmap_tlb[gvn] === guestmap[gvn]);
                        assert(memtype(memid, guestmap.translate(gvn).get_Some_0()).need_c_bit());
                        self.lemma_mem_map_to_mem_map_ok(memid, gvn);
                        self.lemma_encrypted(memid, gvn);
                    }
                }
            }
        }
    }

    proof fn lemma_op_flush_tlb_inv(&self, new: &Self, memid: MemID, memop: MemOp<GuestVir>)
        requires
            self.inv(memid),
            memop.is_FlushAll() || memop.is_InvlPage(),
            new === &self.op(memop).to_result(),
        ensures
            new.spec_tlb().inv_encrypted_priv_mem(memid),
    {
        let op_memid = memop.to_addr_memid().memid;
        let guestmap = self.to_mem_map(memid);
        let new_guestmap = self.to_mem_map(memid);
        let guestmap_tlb = self.spec_tlb().to_mem_map(memid);
        let new_guestmap_tlb = new.spec_tlb().to_mem_map(memid);
        assert(new.spec_tlb().inv_encrypted_priv_mem(memid)) by {
            reveal(GuestPTRam::inv_content_ok);
            reveal(MemMap::inv_encrypted_priv_mem);
            assert forall|gvn: GVN|
                gvn.is_valid() && memtype(
                    memid,
                    new_guestmap_tlb.translate(gvn).get_Some_0(),
                ).need_c_bit() implies #[trigger] new_guestmap_tlb.is_encrypted_or_none(gvn) by {
                if new_guestmap_tlb.translate(gvn).is_Some() {
                    if op_memid === memid {
                        assert(new_guestmap_tlb[gvn] === guestmap_tlb[gvn]);
                        assert(guestmap_tlb.is_encrypted_or_none(gvn));
                    } else {
                        if let MemOp::FlushAll(op_memid) = memop {
                            if (self.spec_tlb() !== new.spec_tlb()) {
                                assert(op_memid !== memid);
                                //assert(self.spec_tlb().spec_db().dom().contains(memid));
                                //assert(new.spec_tlb().spec_db().dom().contains(memid));
                                assert(self.spec_tlb().spec_db()[memid]
                                    === new.spec_tlb().spec_db()[memid]);
                            }
                        } else if let MemOp::InvlPage(addr_id) = memop {
                            assert(addr_id.memid != memid);
                            assert(new_guestmap_tlb === guestmap_tlb) by {
                                assert(self.spec_tlb().spec_db()[memid]
                                    === new.spec_tlb().spec_db()[memid]);
                            }
                        }
                    }
                }
            }
        }
    }

    pub proof fn proof_model_eq1_inv(&self, oldmemdb: &Self, memid: MemID)
        requires
            oldmemdb.inv(memid),
            self.model1_eq(oldmemdb, memid),
        ensures
            self.inv(memid),
    {
        reveal(MemDB::inv);
        self.spec_vram().lemma_model_eq_inv(&oldmemdb.spec_vram(), memid);
        self.proof_model1_pgram_inv(oldmemdb, memid);
        self.spec_tlb().lemma_model1_inv_encrypted_priv_mem(&oldmemdb.spec_tlb(), memid);
        oldmemdb.lemma_identity_map(self, memid, spec_unused());
    }

    pub proof fn proof_op_inv(&self, memid: MemID, memop: MemOp<GuestVir>)
        requires
            self.inv(memid),
            memid.is_vmpl0(),
            memop.is_valid(),
            //self.op(memop).is_Ok() || self.spec_memdb().op(arch_op.get_MemOp_0()).get_Error_1().is_RmpOp(),
            memop.to_memid().is_sm(memid) ==> self.vop_requires(memop),
        ensures
            self.op(memop).to_result().inv(memid),
    {
        reveal(MemDB::inv);
        let new = self.op(memop).to_result();
        let op_memid = memop.to_memid();
        let sysmap = self.sysmap[op_memid];
        let old_g_pgtable = self.spec_g_page_table(memid);
        let new_g_pgtable = new.spec_g_page_table(memid);
        let gpa_memop = self.to_gpop(memop);
        if self.to_mem_map(op_memid).translate(memop.to_mem().to_page()).is_Some() {
            self.to_mem_map(op_memid).lemma_valid_translate(memop.to_mem().to_page());
            let gvn = memop.to_page();
            self.spec_vram().proof_op_inv(sysmap, gpa_memop);
            assert(self.spec_g_page_table(memid).spec_ram().inv()) by {
                reveal(GuestPTRam::inv_dom_ok);
            }
            assert(old_g_pgtable.inv(memid));
            if memop.use_gmap() {
                //assert(self.spec_vram().op(sysmap, gpa_memop).is_Ok());
                if new.spec_vram() !== self.spec_vram() {
                    memop.lemma_vop_require_to_gop_require(memid, self);
                    GuestPTRam::proof_memop_inv(
                        &old_g_pgtable,
                        &new_g_pgtable,
                        sysmap,
                        memid,
                        gpa_memop,
                    );
                    self.spec_vram().proof_op_inv_sw(sysmap, gpa_memop, memid);
                    assert(new_g_pgtable.inv(memid));
                    assert(new.spec_vram().inv());
                }
            } else {
                assert(new_g_pgtable === old_g_pgtable);
                assert(new.spec_vram() === self.spec_vram());
            }
        }
        match memop {
            MemOp::Read(_, _) => {
                self.lemma_op_read_or_write_tlb_inv(&new, memid, memop);
            },
            MemOp::Write(_, _, _) => {
                self.lemma_op_read_or_write_tlb_inv(&new, memid, memop);
            },
            MemOp::RmpOp(_) => {
                self.lemma_op_read_or_write_tlb_inv(&new, memid, memop);
            },
            MemOp::InvlPage(_) => {
                self.lemma_op_flush_tlb_inv(&new, memid, memop);
            },
            MemOp::FlushAll(_) => {
                self.lemma_op_flush_tlb_inv(&new, memid, memop);
            },
        }
        self.lemma_identity_map(&new, memid, memop);
    }

    pub proof fn lemma_rmpop_Ginv(&self, memop: MemOp<GuestVir>, memid: MemID, gpa: GPA)
        requires
            self.inv(memid),
            memid.is_vmpl0(),
            memop.is_RmpOp(),
            memop.to_memid().is_sm(memid) ==> self.vop_requires(memop),
        ensures
            (self.op(memop).to_result().spec_vram().get_enc_byte_ok(memid, gpa).is_Some()
                && self.spec_vram().get_enc_byte_ok(memid, gpa).is_Some()) ==> self.op(
                memop,
            ).to_result().spec_vram().get_enc_byte_ok(memid, gpa)
                === self.spec_vram().get_enc_byte_ok(memid, gpa),
            (self.op(memop).to_result().spec_vram().get_enc_byte_ok(memid, gpa).is_Some()
                && !self.spec_vram().get_enc_byte_ok(memid, gpa).is_Some()) ==> (memop.is_RmpOp()
                && memop.get_RmpOp_0().is_Pvalidate() && memop.get_RmpOp_0().get_Pvalidate_1().val
                && self.to_gpop(memop).to_page() === gpa.to_page()),
    {
        let op_memid = memop.to_memid();
        let op_sysmap = self.spec_sysmap()[op_memid];
        let op_gvn = memop.to_page();
        let gpmemop = self.to_gpop(memop);
        if memop.to_memid().is_sm(memid) && self.to_mem_map(op_memid).translate(op_gvn).is_Some() {
            self.lemma_mem_map_to_mem_map_ok(op_memid, op_gvn);
            assert(self.spec_vram().gpmemop_requires(gpmemop, op_sysmap));
            self.spec_vram().lemma_rmpop_enc_byte_Ginv(op_sysmap, gpmemop, memid, gpa);
        } else if !memop.to_memid().is_sm(memid) {
            self.spec_vram().lemma_rmpop_enc_byte_Ginv(op_sysmap, gpmemop, memid, gpa);
        } else {
            assert(memop.to_memid().is_sm(memid) && !self.to_mem_map(op_memid).translate(
                op_gvn,
            ).is_Some());
            self.lemma_mem_map_to_mem_map_ok(op_memid, op_gvn);
            assert(!self.to_mem_map(op_memid).translate(op_gvn).is_Some());
            assert(self === &self.op(memop).to_result());
        }
    }

    pub proof fn proof_op_read_Ginv(&self, memop: MemOp<GuestVir>)
        requires
            memop.is_Read(),
        ensures
            self.op(memop).to_result().spec_vram() === self.spec_vram(),
            self.op(memop).to_result().spec_l0_entry() === self.spec_l0_entry(),
            self.op(memop).is_Ok() || !self.op(memop).to_err().is_RmpOp(),
    {
        reveal(RmpEntry::check_access);
        reveal(VRamDB::op);
    }

    pub proof fn proof_op_read_map_Ginv(&self, memop: MemOp<GuestVir>, memid: MemID)
        requires
            memop.is_Read(),
        ensures
            self.op(memop).to_result().spec_g_page_table(memid) === self.spec_g_page_table(memid),
            self.op(memop).to_result().to_mem_map(memid) === self.to_mem_map(memid),
    {
        reveal(VRamDB::op);
        self.proof_op_read_Ginv(memop);
        let news = self.op(memop).to_result();
        let oldmap = self.to_mem_map(memid).db;
        let newmap = news.to_mem_map(memid).db;
        assert(oldmap === newmap) by {
            assert forall|k| #[trigger] newmap.dom().contains(k) implies newmap[k]
                === oldmap[k] by {}
            assert(oldmap =~~= (newmap));
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/mem/def_s.rs
================

use verismo_macro::*;

use crate::arch::entities::*;
use crate::arch::pgtable::{SpecGuestPTEntry, SysMap};
use crate::arch::rmp::RmpOp;
use crate::arch::tlb::TLB;
use crate::arch::vram::VRamDB;
use crate::tspec::*;

verus! {

#[derive(SpecGetter, SpecSetter)]
pub struct MemDB {
    pub vram: VRamDB,
    pub l0_entry: Map<MemID, SpecGuestPTEntry>,
    pub sysmap: Map<MemID, SysMap>,
    pub tlb: TLB,
}

} // verus!

================
File: ./source/verismo/src/arch/mem/mod.rs
================

use verismo_macro::*;

use crate::arch::addr_s::*;
use crate::arch::entities::*;
use crate::arch::errors::*;
use crate::arch::memop::MemOp;
use crate::arch::pgtable::{SpecGuestPTEntry, SysMap, *};
use crate::arch::ptram::GuestPTRam;
use crate::arch::rmp::*;
use crate::arch::tlb::*;
use crate::arch::vram::VRamDB;
use crate::tspec::*;

mod def_s;
pub use def_s::*;
mod mem_model1_p;
mod mem_p;
mod mem_s;
mod mem_u;

================
File: ./source/verismo/src/arch/mem/mem_s.rs
================

use super::*;

verus! {

impl MemDB {
    pub open spec fn spec_g_page_table(&self, memid: MemID) -> GuestPTRam {
        spec_unused::<GuestPTRam>().spec_set_ram(self.spec_vram()).spec_set_l0_entry(
            self.spec_l0_entry(),
        )
    }

    pub open spec fn to_mem_map(&self, memid: MemID) -> MemMap<GuestVir, GuestPhy> {
        MemMap {
            db: self.spec_g_page_table(memid).to_mem_map(
                self.spec_sysmap()[memid],
                memid,
            ).db.union_prefer_right(self.spec_tlb().to_mem_map(memid).db),
        }
    }

    pub open spec fn to_spop(&self, memop: MemOp<GuestPhy>) -> MemOp<SysPhy> {
        let AddrMemID { range: gva, memid: op_memid } = memop.to_addr_memid();
        let sysmap = self.sysmap[op_memid];
        let spa = sysmap.translate_addr_seq(gva);
        memop.translate_spn(spa)
    }

    pub open spec fn to_gpop(&self, memop: MemOp<GuestVir>) -> MemOp<GuestPhy> {
        let gvmem = memop.to_mem();
        let op_memid = memop.to_memid();
        let guestmap = self.to_mem_map(op_memid);
        let gpmem = gvmem.convert(guestmap.translate(gvmem.to_page()).get_Some_0());
        let enc = guestmap.is_encrypted(gvmem.to_page());
        memop.translate_gpn(gpmem, enc.get_Some_0())
    }

    pub open spec fn op(&self, memop: MemOp<GuestVir>) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestVir>>,
    > {
        let ret = match memop {
            MemOp::Read(_, _) => { self.op_by_gpn_memtype(memop) },
            MemOp::Write(_, _, _) => {
                //self.op_write(gva_memid, data)
                self.op_by_gpn_memtype(memop)
            },
            MemOp::RmpOp(rmpop) => {
                match rmpop {
                    RmpOp::RmpAdjust(_, _) => self.op_rmpadjust(memop),
                    RmpOp::Pvalidate(_, _) => self.op_pvalidate(memop),
                    RmpOp::RmpUpdate(_, _) => self.op_rmpupdate(memop),
                }
            },
            MemOp::InvlPage(gva_memid) => self.op_invlpg(gva_memid),
            MemOp::FlushAll(memid) => self.op_flush(memid),
        };
        ret.replace_err(ret.to_err().with_param(memop))
    }

    pub open spec fn op_invlpg(&self, gva_memid: GVAMemID) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestVir>>,
    > {
        let gva = gva_memid.range;
        let memid = gva_memid.memid;
        let tlb_idx = TLBIdx(memid, gva[0].to_page());
        ResultWithErr::Ok(self.spec_set_tlb(self.spec_tlb().invlpg(tlb_idx)))
    }

    pub open spec fn op_flush(&self, memid: MemID) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestVir>>,
    > {
        ResultWithErr::Ok(self.spec_set_tlb(self.spec_tlb().flush_memid(memid)))
    }

    pub open spec fn op_read(&self, gva_memid: GVAMemID) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestVir>>,
    > {
        self.op_by_gpn_memtype(MemOp::Read(gva_memid, false))
    }

    pub open spec fn ret(&self, memop: MemOp<GuestVir>) -> ByteStream {
        let gpa_memop = self.to_gpop(memop);
        let op_memid = memop.to_memid();
        let sysmap = self.spec_sysmap()[op_memid];
        if self.op(memop).is_Error() {
            ByteStream::empty()
        } else {
            self.spec_vram().spec_ret_bytes(gpa_memop, sysmap)
        }
    }

    /// Access checks:
    ///     1. guest page table check (present)
    ///     2. sys page table check (see VRamDB)
    ///     3. rmp check (see VRamDB)
    pub open spec fn op_by_gpn_memtype(&self, memop: MemOp<GuestVir>) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestVir>>,
    > {
        let memid = memop.to_memid();
        let gva = memop.to_mem();
        let sysmap = self.spec_sysmap()[memid];
        let guestmap = self.to_mem_map(memid);
        let valid_gpa = guestmap.translate(gva.to_page()).is_Some();
        let tlb_idx = TLBIdx(memid, gva.to_page());
        let gpa_memop = self.to_gpop(memop);
        if !valid_gpa {
            ResultWithErr::Error(*self, MemError::PageFault(memop))
        } else {
            let entry = guestmap[gva.to_page()];
            let tmp = self.spec_set_tlb(self.tlb.load(tlb_idx, entry.get_Some_0()));
            // Update data related RAM or RMP
            match self.spec_vram().op(sysmap, gpa_memop) {
                ResultWithErr::Ok(ret) => { ResultWithErr::Ok(tmp.spec_set_vram(ret)) },
                ResultWithErr::Error(ret, err) => {
                    ResultWithErr::Error(tmp.spec_set_vram(ret), err.with_param(memop))
                },
            }
        }
    }

    pub open spec fn op_pvalidate(&self, op: MemOp<GuestVir>) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestVir>>,
    > {
        if let MemOp::RmpOp(RmpOp::Pvalidate(gvn_memid, param)) = op {
            let memid = gvn_memid.memid;
            let gvn = gvn_memid.page;
            let psize = param.psize;
            if memid.is_Hv() || memid.to_vmpl() != VMPL::VMPL0 {
                ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Unsupported, op))
            } else if (psize == PageSize::Size2m) && !gvn.valid_as_size(psize) {
                ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Input, op))
            } else {
                self.op_by_gpn_memtype(op)
            }
        } else {
            ResultWithErr::Ok(*self)
        }
    }

    pub open spec fn op_rmpadjust(&self, op: MemOp<GuestVir>) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestVir>>,
    > {
        if let MemOp::RmpOp(RmpOp::RmpAdjust(gvn_memid, param)) = op {
            let memid = gvn_memid.memid;
            let gvn = gvn_memid.page;
            let psize = param.psize;
            let vmpl = param.vmpl;
            if memid.is_Hv() {
                ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Unsupported, op))
            } else if (psize == PageSize::Size2m) && !gvn.valid_as_size(psize) {
                ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Input, op))
            } else if memid.to_vmpl() >= vmpl {
                ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Perm, op))
            } else {
                self.op_by_gpn_memtype(op)
            }
        } else {
            ResultWithErr::Ok(*self)
        }
    }

    pub open spec fn op_rmpupdate(&self, op: MemOp<GuestVir>) -> ResultWithErr<
        Self,
        MemError<MemOp<GuestVir>>,
    > {
        if let MemOp::RmpOp(RmpOp::RmpUpdate(gvn_memid, param)) = op {
            let memid = gvn_memid.memid;
            let gvn = gvn_memid.page;
            if !memid.is_Hv() {
                ResultWithErr::Error(*self, MemError::RmpOp(RmpFault::Unsupported, op))
            } else {
                self.op_by_gpn_memtype(op)
            }
        } else {
            ResultWithErr::Ok(*self)
        }
    }
}

} // verus!

================
File: ./source/verismo/src/arch/mem/mem_model1_p.rs
================

use super::*;
use crate::arch::attack::*;

verus! {

impl MemDB {
    #[verifier(external_body)]
    pub proof fn proof_model1_content_integrity(
        &self,
        old_memdb: &MemDB,
        memid: MemID,
        memop: MemOp<GuestVir>,
    )
        requires
            memop.is_Read(),
            old_memdb.inv(memid),
            self.model1_eq(old_memdb, memid),
            self.op(memop).is_Ok(),
            self.to_mem_map(memid).is_encrypted_or_none(memop.to_page()),
        ensures
            self.ret(memop) === old_memdb.ret(memop),
    {
    }

    pub proof fn proof_model1_pgram_inv(&self, old_memdb: &MemDB, memid: MemID)
        requires
            old_memdb.inv(memid),
            self.model1_eq(old_memdb, memid),
        ensures
            self.spec_g_page_table(memid).inv(memid),
    {
        reveal(GuestPTRam::inv_dom_ok);
        reveal(GuestPTRam::inv_content_ok);
        self.spec_vram().lemma_model_eq_inv(&old_memdb.spec_vram(), memid);
        let new_pt = self.spec_g_page_table(memid);
        let old_pt = old_memdb.spec_g_page_table(memid);
        assert(new_pt.inv_dom_ok(memid));
        assert forall|gvn: GVN| gvn.is_valid() implies #[trigger] new_pt.inv_content_gpa_ok(
            memid,
            gvn,
        ) by {
            assert forall|lvl: PTLevel|
                (!lvl.is_L0() && (#[trigger] new_pt.map_entry_ok(
                    memid,
                    gvn,
                    lvl,
                )).is_Some()) implies memtype(
                memid,
                new_pt.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn(),
            ).is_pt(lvl.child_lvl().get_Some_0()) by {
                new_pt.lemma_map_entry_model1_eq(&old_pt, memid, gvn, lvl);
                assert(old_pt.map_entry_ok(memid, gvn, lvl).is_Some());
                assert(old_pt.inv_content_gpa_ok(memid, gvn));
                assert(memtype(
                    memid,
                    old_pt.map_entry_ok(memid, gvn, lvl).get_Some_0().spec_ppn(),
                ).is_pt(lvl.child_lvl().get_Some_0()));
            }
        }
        assert(new_pt.inv_for_identity_map_ok(memid)) by {
            reveal(GuestPTRam::inv_for_identity_map_ok);
            assert forall|gvn: GVN|
                gvn.is_valid() && (#[trigger] new_pt.map_entry_ok(
                    memid,
                    gvn,
                    PTLevel::L0,
                )).is_Some() implies new_pt.map_entry_ok(
                memid,
                gvn,
                PTLevel::L0,
            ).get_Some_0().spec_ppn().value() === gvn.value() by {
                new_pt.lemma_map_entry_model1_eq(&old_pt, memid, gvn, PTLevel::L0);
                assert(old_pt.map_entry_ok(memid, gvn, PTLevel::L0).get_Some_0().spec_ppn().value()
                    === gvn.value());
            }
        }
        assert(new_pt.inv_encrypted_priv_mem_ok(memid)) by {
            reveal(GuestPTRam::inv_encrypted_priv_mem_ok);
            assert forall|gvn: GVN|
                (gvn.is_valid() && new_pt.need_c_bit(memid, gvn) && new_pt.map_entry_ok(
                    memid,
                    gvn,
                    PTLevel::L0,
                ).is_Some()) implies #[trigger] new_pt.map_entry_ok(
                memid,
                gvn,
                PTLevel::L0,
            ).get_Some_0().is_encrypted() by {
                new_pt.lemma_map_entry_model1_eq(&old_pt, memid, gvn, PTLevel::L0);
            }
        }
    }
}

} // verus!

================
File: ./source/verismo/src/allocator/linkedlist.rs
================

use core::mem::size_of;

use super::*;
use crate::addr_e::AddrTrait;
use crate::debug::VPrintAtLevel;
use crate::linkedlist::{LinkedList, Node, SpecListItem};
use crate::ptr::*;

verus! {

pub const MIN_VERISMO_PADDR: usize = 0x10_000;

} // verus!
verismo_simple! {
    pub struct LinkedListAllocator {
        perms: Tracked<Map<nat, SnpPointsToRaw>>,
        // Use the last bytes of each mem range to store nodes
        // each entry stores the end addr and the next entry
        free_list: LinkedList<usize_t>,
    }

    pub ghost struct SpecLinkedListAllocator {
        pub perms: Map<nat, SnpPointsToRaw>,
        // Use the last bytes of each mem range to store nodes
        pub free_list: LinkedList<usize_t>,
    }
}

verus! {

impl SpecLinkedListAllocator {
    pub closed spec fn start(&self, i: nat) -> int {
        let SpecListItem { ptr: node_ptr, snp: node_snp, val } = self.free_list@[i as int];
        val as int
    }

    pub closed spec fn wf_perm(&self, i: nat) -> bool {
        let freelist = self.free_list@;
        let SpecListItem { ptr: node_ptr, snp: node_snp, val } = freelist[i as int];
        let node_size = spec_size::<Node<usize_t>>();
        let start = val as int;
        let end = node_ptr.id() + node_size;
        let perm = self.perms[i]@;
        &&& self.perms.contains_key(i)
        &&& (val as int) == perm.range().0
        &&& perm.size() + node_size + start == end
        &&& perm.wf_freemem((node_ptr.id() - perm.size(), perm.size()))
        &&& perm.snp() === SwSnpMemAttr::spec_default()
        &&& node_snp === SwSnpMemAttr::spec_default()
        &&& node_ptr.not_null()
        &&& val.is_constant()
    }

    pub closed spec fn len(&self) -> nat {
        self.free_list@.len()
    }

    pub closed spec fn wf(&self) -> bool {
        &&& forall|i: nat| i < self.len() ==> #[trigger] self.wf_perm(i)
    }

    pub closed spec fn inv(&self) -> bool {
        &&& self.free_list.inv()
        &&& self.wf()
    }

    proof fn lemma_use_perms_map_as_seq(&self)
        requires
            self.wf(),
        ensures
            forall|k: nat| k < self.len() ==> #[trigger] self.perms.contains_key(k),
    {
        assert forall|k: nat| k < self.len() implies #[trigger] self.perms.contains_key(k) by {
            assert(self.wf_perm(k));
        }
    }
}

} // verus!
verus! {

impl LinkedListAllocator {
    proof fn lemma_use_perms_map_as_seq(&self)
        requires
            self@.wf(),
        ensures
            forall|k: nat| k < self@.len() ==> #[trigger] self.perms@.contains_key(k),
    {
        self@.lemma_use_perms_map_as_seq();
        assert forall|k: nat| k < self@.len() implies #[trigger] self.perms@.contains_key(k) by {
            assert(self@.perms.contains_key(k));
        }
    }

    pub open spec fn invfn() -> spec_fn(Self) -> bool {
        |v: LinkedListAllocator| v@.inv()
    }

    pub closed spec fn view(&self) -> SpecLinkedListAllocator {
        SpecLinkedListAllocator { perms: self.perms@, free_list: self.free_list }
    }

    pub open spec fn spec_minsize() -> nat {
        spec_size::<Node<usize_t>>()
    }
}

} // verus!
verus! {

impl LinkedListAllocator {
    #[inline]
    pub fn minsize() -> (ret: usize)
        ensures
            ret == Self::spec_minsize(),
            ret.is_constant(),
    {
        size_of::<Node<usize_t>>()
    }

    pub const fn new() -> (ret: Self)
        ensures
            ret@.len() == 0,
            ret@.inv(),
    {
        LinkedListAllocator { free_list: LinkedList::new(), perms: Tracked(Map::tracked_empty()) }
    }

    pub fn find_prev_of_addr(&self, addr: usize_t) -> (ret: (SnpPPtr<Node<usize_t>>, Ghost<int>))
        requires
            self@.inv(),
            addr.is_constant(),
        ensures
            self@.free_list.contains_ptr_at(ret.0, ret.1@),
            forall|i| ret.1@ <= i < self@.len() ==> self@.free_list@[i].val < addr,
    {
        let mut node_ptr = self.free_list.head_ptr();
        let mut prev_ptr = SnpPPtr::nullptr();
        let ghost mut idx: int = self@.len() as int;
        while node_ptr.check_valid()
            invariant
                addr.is_constant(),
                self@.inv(),
                self.free_list.is_constant(),
                node_ptr.is_constant(),
                prev_ptr.is_constant(),
                0 <= idx <= self@.len(),
                (idx == 0) == (node_ptr.is_null()),
                self@.free_list.contains_ptr_at(prev_ptr, idx),
                idx > 0 ==> self@.free_list.contains_ptr_at(node_ptr, idx - 1),
                forall|i: int| idx <= i < self@.len() ==> self.free_list@[i].val < addr,
            ensures
                prev_ptr.is_constant(),
                0 <= idx <= self@.len(),
                forall|i| idx <= i < self@.len() ==> self.free_list@[i].val < addr,
                self@.free_list.contains_ptr_at(prev_ptr, idx),
        {
            let ghost i = idx - 1;
            let node = self.free_list.node_at(node_ptr.clone(), Ghost(i));
            let start = node.val;
            assert(self@.wf_perm(i as nat));
            if start >= addr {
                break ;
            }
            prev_ptr = node_ptr;
            node_ptr = node.next_ptr();
            proof {
                idx = idx - 1;
            }
        }
        (prev_ptr, Ghost(idx))
    }

    // TODO(ziqiao): search and insert to keep the list sorted.
    pub fn add_mem(
        &mut self,
        start: &mut usize,
        end: &mut usize,
        Tracked(perm): Tracked<SnpPointsToRaw>,
    )
        requires
            (*old(start)).spec_valid_addr_with(1),
            (*old(end)).spec_valid_addr_with(0),
            old(start).is_constant(),
            old(end).is_constant(),
            (*old(start)) < (*old(end)),
            old(self)@.inv(),
            perm@.wf_freemem(((*old(start) as int), (*old(end) - *old(start)) as nat)),
            *old(end) - *old(start) >= Self::spec_minsize(),
            perm@.snp() === SwSnpMemAttr::spec_default(),
        ensures
            self@.inv(),
            *start === *old(start),
            *end === *old(end),
    {
        let start = *start;
        let end = *end;
        let ghost old_self = *self;
        let ghost size = (end - start) as nat;
        let add_ptr = SnpPPtr::from_usize(end - size_of::<Node<usize_t>>());
        let node = Node { next: 0, val: start };
        let tracked (free_perm, add_perm) = perm.trusted_split(
            (size - Self::spec_minsize()) as nat,
        );
        let tracked mut add_perm = add_perm.trusted_into();
        add_ptr.replace(Tracked(&mut add_perm), node);
        let (prev, Ghost(idx)) = self.find_prev_of_addr(start);
        proof {
            self.lemma_use_perms_map_as_seq();
            assert(self.perms@ === self@.perms);
        }
        let ghost n = self@.len();
        self.free_list.insert(prev, add_ptr, Ghost(idx), Tracked(add_perm));
        proof {
            tracked_seq_insert(self.perms.borrow_mut(), idx as nat, free_perm, n);
            assert(self@.wf_perm(idx as nat));
            assert forall|i: nat| i < self@.len() implies #[trigger] self@.wf_perm(i) by {
                if i < idx {
                    assert(old_self@.wf_perm(i));
                    assert(self.free_list@[i as int] === old_self.free_list@[i as int]);
                    assert(self.perms@[i] === old_self.perms@[i]);
                    assert(self@.wf_perm(i));
                }
                if i > idx {
                    assert(old_self@.wf_perm((i - 1) as nat));
                    assert(self.free_list@[i as int] === old_self.free_list@[i - 1]);
                    assert(self.perms@[i] === old_self.perms@[(i - 1) as nat]);
                    assert(self@.wf_perm(i));
                }
            }
        }
    }

    pub fn remove_range_from(
        &mut self,
        node_ptr: SnpPPtr<Node<usize_t>>,
        start: usize_t,
        new_start: usize_t,
        Ghost(i): Ghost<int>,
    ) -> (perm: Tracked<SnpPointsToRaw>)
        requires
            node_ptr.is_constant(),
            start.is_constant(),
            new_start.is_constant(),
            0 <= i < old(self)@.len(),
            old(self)@.free_list@[i].ptr.id() == node_ptr.id(),
            node_ptr.not_null(),
            old(self)@.inv(),
            start == old(self)@.start(i as nat),
            (start as int) < (new_start as int) <= old(self)@.free_list@[i].ptr.id(),
        ensures
            self@.inv(),
            self@.free_list@ =~~= old(self)@.free_list@.update(
                i,
                SpecListItem { ptr: node_ptr, snp: old(self)@.free_list@[i].snp, val: new_start },
            ),
            perm@@.wf_freemem(perm@@.range()),
            perm@@.range() === range(start as int, new_start as int),
    {
        let ghost oldself = self@;
        let tracked mut perm;
        assert(oldself.wf_perm(i as nat));
        proof {
            assert forall|k: nat| k < self@.len() implies #[trigger] self@.perms.contains_key(
                k,
            ) by {
                assert(self@.wf_perm(k));
            }
        }
        self.free_list.update_node_at(Ghost(i), node_ptr, new_start);
        proof {
            perm = self.perms.borrow_mut().tracked_remove(i as nat);
            let ghost real_size = (new_start - start) as nat;
            assert(real_size <= perm@.size());
            let tracked (used, free_perm) = perm.trusted_split(real_size);
            self.perms.borrow_mut().tracked_insert(i as nat, free_perm);
            perm = used;
            assert(self@.wf_perm(i as nat)) by {
                assert(self@.free_list@[i as int].val === new_start);
                assert(self@.perms[i as nat] === free_perm);
            }
            assert forall|k: nat| k < self@.len() implies self@.wf_perm(k) by {
                assert(oldself.wf_perm(k));
                if i != k {
                    assert(self@.free_list@[k as int] === oldself.free_list@[k as int]);
                    assert(self@.perms[k as nat] === oldself.perms[k as nat]);
                }
            }
        }
        assert(self@.inv());
        Tracked(perm)
    }

    pub fn remove_mem(&mut self, prev_ptr: SnpPPtr<Node<usize_t>>, Ghost(i): Ghost<int>) -> (perm:
        Tracked<SnpPointsToRaw>)
        requires
            0 <= i < old(self)@.len(),
            prev_ptr.is_constant(),
            i != (old(self)@.len() - 1) ==> old(self)@.free_list@[i + 1].ptr.id() == prev_ptr.id(),
            i == (old(self)@.len() - 1) <==> prev_ptr.is_null(),
            old(self)@.inv(),
            old(self)@.free_list@[i].ptr.not_null(),
        ensures
            self@.inv(),
            self@.free_list@ =~~= old(self)@.free_list@.remove(i),
            perm@@.wf_freemem(perm@@.range()),
            perm@@.range() === range(
                old(self)@.free_list@[i].val as int,
                old(self)@.free_list@[i].ptr.id() + spec_size::<Node<usize_t>>(),
            ),
    {
        let ghost oldself = self@;
        assert(oldself.wf_perm(i as nat));
        proof {
            self.lemma_use_perms_map_as_seq();
            /*assert forall |k: nat| k < self@.len()
                implies #[trigger] self@.perms.contains_key(k)
                by {
                    assert(self@.wf_perm(k));
                }*/
        }
        let Tracked(nodep) = self.free_list.remove(prev_ptr, Ghost(i));
        let tracked mut perm;
        proof {
            perm = map_lib::tracked_seq_remove(self.perms.borrow_mut(), i as nat, self@.len() + 1);
            let tracked nodep = nodep.trusted_into_raw();
            perm = perm.trusted_join(nodep);
            assert forall|k: nat| k < self@.len() implies self@.wf_perm(k) by {
                assert(oldself.wf_perm(k));
                assert(oldself.wf_perm((k + 1) as nat));
                if k < i {
                    assert(self@.free_list@[k as int] === oldself.free_list@[k as int]);
                    assert(self@.perms[k as nat] === oldself.perms[k as nat]);
                } else {
                    assert(self@.free_list@[k as int] === oldself.free_list@[k as int + 1]);
                    assert(self@.perms[k] === oldself.perms[k + 1]);
                    assert(self@.wf_perm(k));
                }
            }
        }
        Tracked(perm)
    }

    pub fn _alloc_inner(&mut self, size: &mut usize_t, align: usize_t) -> (ret: Option<
        (usize, Tracked<SnpPointsToRaw>),
    >)
        requires
            spec_bit64_is_pow_of_2(align as int),
            *old(size) as int > 0,
            old(self)@.inv(),
            *old(size) >= Self::spec_minsize(),
            old(size).is_constant(),
            align.is_constant(),
        ensures
            self@.inv(),
            size.is_constant(),
            ret.is_Some() ==> (MAXU64 - align as int) > (ret.get_Some_0().0 as int),
            ret.is_Some() ==> valid_free_ptr(*size, ret.get_Some_0()),
            ret.is_Some() ==> *old(size) <= *size,
            ret.is_Some() ==> ret.get_Some_0().is_constant(),
            ret.is_Some() ==> inside_range(
                (spec_align_up(ret.get_Some_0().0 as int, align as int), *old(size) as nat),
                (ret.get_Some_0().0 as int, *size as nat),
            ),
    {
        let mut node_ptr = self.free_list.head_ptr();
        let mut prev_ptr = SnpPPtr::<Node<usize>>::nullptr();
        let tracked mut ret_perm: Option<SnpPointsToRaw> = Option::None;
        let mut ret: Option<usize> = Option::None;
        let ghost mut idx: int = 0;
        let expect_size = *size;
        while node_ptr.check_valid()
            invariant_except_break
                self@.inv(),
                self.free_list.is_constant(),
                node_ptr.is_constant(),
                align.is_constant(),
                spec_bit64_is_pow_of_2(align as int),
                expect_size > 0,
                expect_size === (*size),
                expect_size.is_constant(),
                prev_ptr.is_constant(),
                0 <= idx <= self@.len(),
                (idx == self@.len()) == (node_ptr.is_null()),
                (idx == 0) == (prev_ptr.is_null()),
                idx != 0 ==> prev_ptr.id() == self.free_list@[self.free_list.reverse_index(
                    idx - 1,
                )].ptr.id(),
                idx < self@.len() ==> node_ptr.id() == self.free_list@[self.free_list.reverse_index(
                    idx,
                )].ptr.id(),
                ret.is_Some() == ret_perm.is_Some(),
                !ret.is_Some(),
            ensures
                self@.inv(),
                self.free_list.is_constant(),
                ret.is_Some() ==> ret_perm.get_Some_0()@.wf_freemem(
                    (ret.get_Some_0() as int, *size as nat),
                ),
                ret.is_Some() ==> inside_range(
                    (spec_align_up(ret.get_Some_0() as int, align as int), expect_size as nat),
                    ret_perm.get_Some_0()@.range(),
                ),
                (!node_ptr.not_null()) ==> ret.is_None(),
                ret.is_Some() == ret_perm.is_Some(),
                ret.is_Some() ==> ret.get_Some_0().is_constant(),
                size.is_constant(),
                ret.is_Some() ==> (MAXU64) - align as int > ret.get_Some_0() as int,
        {
            let ghost i = self.free_list.reverse_index(idx);
            let ghost prev_i = self.free_list.reverse_index(idx - 1);
            let ghost oldself = self@;
            assert(idx < self@.len());
            let node = self.free_list.node_at(node_ptr.clone(), Ghost(i));
            proof {
                if idx == self@.len() - 1 {
                    assert(!node.next.spec_valid_addr_with(spec_size::<Node<usize_t>>()));
                } else {
                    assert(node.next.spec_valid_addr_with(spec_size::<Node<usize_t>>()));
                }
                assert(self@.wf_perm(i as nat));
                if prev_ptr.not_null() {
                    assert(self@.wf_perm(prev_i as nat));
                }
                assert(node_ptr.not_null());
            }
            let minsize = size_of::<Node<usize>>();
            assert(minsize == spec_size::<Node<usize_t>>());
            let start = node.val;
            let end = node_ptr.to_usize() + minsize;
            if (MAXU64 as usize) - align > start {
                let aligned_start: usize = align_up_by(start as u64, align as u64) as usize;
                if start >= MIN_VERISMO_PADDR && end > aligned_start && end - aligned_start
                    > expect_size {
                    let mut new_start = aligned_start + expect_size;
                    let tracked mut perm;
                    if end - new_start < minsize {
                        new_start = end;
                        let Tracked(p) = self.remove_mem(prev_ptr, Ghost(i));
                        proof {
                            perm = p;
                        }
                    } else {
                        let Tracked(p) = self.remove_range_from(
                            node_ptr,
                            start,
                            new_start,
                            Ghost(i),
                        );
                        proof {
                            perm = p;
                        }
                    }
                    assert(self.free_list.inv());
                    assert(self@.wf());
                    *size = new_start - start;
                    proof { ret_perm = Some(perm) }
                    ;
                    ret = Some(start);
                    break ;
                }
            }
            prev_ptr = node_ptr;
            node_ptr = node.next_ptr();
            proof {
                idx = idx + 1;
                assert(!ret.is_Some());
            }
        }
        if ret.is_some() {
            Some((ret.unwrap(), Tracked(ret_perm.tracked_unwrap())))
        } else {
            None
        }
    }

    pub fn alloc_inner(&mut self, size: usize_t, align: usize_t) -> (ret: Option<
        (usize, Tracked<SnpPointsToRaw>),
    >)
        requires
            spec_bit64_is_pow_of_2(align as int),
            (size) as int > 0,
            old(self)@.inv(),
            (size) >= Self::spec_minsize(),
            (size).is_constant(),
            align.is_constant(),
        ensures
            self@.inv(),
            self.wf(),
            ret.is_Some() ==> alloc_valid_ptr(size, ret.get_Some_0()),
            ret.is_Some() ==> ret.get_Some_0().is_constant(),
            ret.is_Some() ==> (spec_align_up(ret.get_Some_0().0 as int, align as int), size as nat)
                === (ret.get_Some_0().0 as int, size as nat),
    {
        let mut real_size = size;
        let ghost old_size = size as nat;
        if let Some((addr, perm)) = self._alloc_inner(&mut real_size, align) {
            let ghost prev_addr = addr;
            let addr: usize_t = align_up_by(addr as u64, align as u64) as usize;
            let Tracked(mut ret_perm) = perm;
            proof {
                if addr > prev_addr {
                    ret_perm = ret_perm.trusted_split((addr as int - prev_addr as int) as nat).1;
                }
                if ret_perm@.size() > old_size {
                    ret_perm = ret_perm.trusted_split(old_size as nat).0;
                } else {
                    assert(old_size == ret_perm@.size());
                }
            }
            assert(ret_perm@.wf_freemem((addr as int, size as nat)));
            mem_set_zeros(addr, size, Tracked(&mut ret_perm));
            assert(ret_perm@.wf_freemem((addr as int, size as nat)));
            assert(ret_perm@.wf_const_default((addr as int, size as nat)));
            Some((addr, Tracked(ret_perm)))
        } else {
            None
        }
    }

    // No dealloc
    pub fn dealloc_inner(
        &mut self,
        addr: usize_t,
        size: usize_t,
        Tracked(perm): Tracked<SnpPointsToRaw>,
    )
        requires
            perm@.wf_default((addr as int, size as nat)),
            size > 0,
            size.is_constant(),
            addr.is_constant(),
            size >= Self::spec_minsize(),
            old(self)@.inv(),
        ensures
            self@.inv(),
            self.wf(),
    {
        let mut start = addr;
        let mut end = addr + size;
        let tracked mut perm = perm;
        mem_set_zeros(addr, size, Tracked(&mut perm));
        self.add_mem(&mut start, &mut end, Tracked(perm));
    }
}

} // verus!
verus! {

impl LinkedListAllocator {
    pub fn remove_one_range(&mut self) -> (ret: Option<((usize, usize), Tracked<SnpPointsToRaw>)>)
        requires
            old(self)@.inv(),
        ensures
            self@.inv(),
            self.wf(),
            ret.is_None() ==> (*self) =~~= (*old(self)),
            ret.is_Some() == (old(self)@.len() > 0),
            ret.is_Some() ==> self@.len() <= (old(self)@.len() - 1),
            ret.is_Some() ==> ret.get_Some_0().1@@.wf_const_default(
                (ret.get_Some_0().0.0 as int, ret.get_Some_0().0.1 as nat),
            ),
    {
        let mut prev_ptr = SnpPPtr::<Node<usize_t>>::nullptr();
        let mut node_ptr = self.free_list.head_ptr();
        if !node_ptr.check_valid() {
            return None;
        }
        let ghost head_i = self.free_list.reverse_index(0);
        let node = self.free_list.node_at(node_ptr.clone(), Ghost(head_i));
        assert(self@.wf_perm(head_i as nat));
        let mut start = node.val;
        let node_addr = node_ptr.to_usize();
        let end: usize_t = size_of::<Node<usize_t>>() + node_addr;
        let size = end - start;
        //let zero_start = if start < MIN_VERISMO_PADDR {MIN_VERISMO_PADDR} else {start};
        // let zero_size = if end > zero_start {end - zero_start} else {0};
        let Tracked(mut p) = self.remove_mem(prev_ptr, Ghost(head_i));
        Some(((start, size), Tracked(p)))
    }
}

} // verus!

================
File: ./source/verismo/src/allocator/trusted.rs
================

use alloc::alloc::Layout;

use super::*;
use crate::global::*;
use crate::lock::VSpinLock;

#[verifier::external]
pub fn verismo_size(layout: &Layout) -> usize {
    if layout.size() < 16 {
        16
    } else {
        layout.size()
    }
}

#[verifier::external]
unsafe impl core::alloc::GlobalAlloc for VSpinLock<VeriSMoAllocator> {
    #[verifier::external]
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        let res = self.alloc_aligned(
            verismo_size(&layout),
            layout.align(),
            Tracked::assume_new(),
            Tracked::assume_new(),
        );
        if res.is_ok() {
            res.unwrap().0 as *mut u8
        } else {
            0 as *mut u8
        }
    }

    #[verifier::external]
    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        self.dealloc_(
            ptr as usize,
            verismo_size(&layout),
            Tracked::assume_new(),
            Tracked::assume_new(),
            Tracked::assume_new(),
        );
    }
}

================
File: ./source/verismo/src/allocator/locked.rs
================

use super::*;
use crate::debug::VPrintAtLevel;
use crate::registers::CoreIdPerm;

verus! {

impl VSpinLock<VeriSMoAllocator> {
    pub open spec fn lock_alloc_requires(&self, cpu: nat, alloc_lockperm: LockPermToRaw) -> bool {
        &&& self.lock_default_mem_requires(cpu, alloc_lockperm)
        &&& alloc_lockperm.invfn.value_invfn() === VeriSMoAllocator::invfn()
    }

    pub fn alloc_(
        &self,
        size: usize,
        align: usize,
        Tracked(alloc_lockperm): Tracked<LockPermRaw>,
        Tracked(coreid): Tracked<&CoreIdPerm>,
    ) -> (res: (Result<(usize, Tracked<SnpPointsToRaw>), ()>, Tracked<LockPermRaw>))
        requires
            self.is_constant(),
            size >= VeriSMoAllocator::spec_minsize(),
            self.lock_alloc_requires(coreid@.cpu, alloc_lockperm@),
            spec_bit64_is_pow_of_2(align as int),
        ensures
            self.lock_alloc_requires(coreid@.cpu, res.1@@),
            res.0.is_Ok() ==> talloc_valid_ptr(size, res.0.get_Ok_0()) && (
            res.0.get_Ok_0().0 as int) % (align as int) == 0,
    {
        let tracked alloc_lockperm = alloc_lockperm;
        (new_strlit("\n new")).leak_debug();
        let (ptr, Tracked(mut allocperm), Tracked(alloc_lockperm)) = self.acquire(
            Tracked(alloc_lockperm),
            Tracked(coreid),
        );
        (new_strlit(":")).leak_debug();
        let mut allocator = ptr.take(Tracked(&mut allocperm));
        let mut size = size;
        let result = allocator.alloc_inner(size, align);
        ptr.put(Tracked(&mut allocperm), allocator);
        self.release(Tracked(&mut alloc_lockperm), Tracked(allocperm), Tracked(coreid));
        if let Some((addr, perm)) = result {
            (new_strlit(": "), addr).leak_debug();
            (Ok((addr, perm)), Tracked(alloc_lockperm))
        } else {
            (Err(()), Tracked(alloc_lockperm))
        }
    }

    pub fn alloc_aligned(
        &self,
        size: usize,
        align: usize,
        Tracked(alloc_lockperm0): Tracked<&mut Map<int, LockPermRaw>>,
        Tracked(coreid): Tracked<&CoreIdPerm>,
    ) -> (res: Result<(usize, Tracked<SnpPointsToRaw>), ()>)
        requires
            self.is_constant(),
            size >= VeriSMoAllocator::spec_minsize(),
            old(alloc_lockperm0).contains_key(0),
            self.lock_alloc_requires(coreid@.cpu, old(alloc_lockperm0)[0]@),
            spec_bit64_is_pow_of_2(align as int),
        ensures
            alloc_lockperm0.contains_key(0),
            self.lock_alloc_requires(coreid@.cpu, alloc_lockperm0[0]@),
            res.is_Ok() ==> talloc_valid_ptr(size, res.get_Ok_0()) && (res.get_Ok_0().0 as int) % (
            align as int) == 0,
    {
        let tracked alloc_perm = alloc_lockperm0.tracked_remove(0);
        let (ret, Tracked(alloc_perm)) = self.alloc_(
            size,
            align,
            Tracked(alloc_perm),
            Tracked(coreid),
        );
        proof {
            alloc_lockperm0.tracked_insert(0, alloc_perm);
        }
        ret
    }

    pub fn dealloc_(
        &self,
        addr: usize,
        size: usize,
        Tracked(memperm): Tracked<SnpPointsToRaw>,
        Tracked(alloc_lockperm): Tracked<LockPermRaw>,
        Tracked(coreid): Tracked<&CoreIdPerm>,
    ) -> (newlockperm: Tracked<LockPermRaw>)
        requires
            self.is_constant(),
            memperm@.wf_default((addr as int, size as nat)),
            size > 0,
            size >= VeriSMoAllocator::spec_minsize(),
            self.lock_alloc_requires(coreid@.cpu, alloc_lockperm@),
        ensures
            self.lock_alloc_requires(coreid@.cpu, newlockperm@@),
    {
        let tracked mut memperm = memperm;
        let tracked mut alloc_lockperm = alloc_lockperm;
        let (ptr, Tracked(mut allocperm), Tracked(mut alloc_lockperm)) = self.acquire(
            Tracked(alloc_lockperm),
            Tracked(coreid),
        );
        let mut allocator = ptr.take(Tracked(&mut allocperm));
        assert(alloc_lockperm@.invfn.inv(allocator));
        assert(VeriSMoAllocator::invfn()(allocator));
        ((new_strlit("\ndealloc: "), addr), size).leak_debug();
        allocator.dealloc_inner(addr, size, Tracked(memperm));
        ptr.put(Tracked(&mut allocperm), allocator);
        self.release(Tracked(&mut alloc_lockperm), Tracked(allocperm), Tracked(coreid));
        Tracked(alloc_lockperm)
    }
}

} // verus!

================
File: ./source/verismo/src/allocator/bit_p.rs
================

use super::*;

verus! {

pub proof fn lemma_get_low_bits_via_bit_op(val: u64, align: u64) -> (ret: u64)
    requires
        spec_bit64_is_pow_of_2(align as int),
        (val & sub(align, 1) == 0),
    ensures
        ret == val & add(!val, 1),
        val != 0 ==> (ret >= align && spec_bit64_is_pow_of_2(ret as int)),
        val == 0 ==> ret == 0,
        val & sub(ret, 1) == 0,
        val & ret == ret,
{
    bit_shl64_pow2_auto();
    let ret = val & add(!val, 1);
    assert(val & ret == ret) by (bit_vector)
        requires
            ret == val & add(!val, 1),
    ;
    assert(val & sub(ret, 1) == 0u64) by (bit_vector)
        requires
            ret == val & add(!val, 1),
    ;
    if val == 0 {
        assert(val & add(!val, 1) == 0u64) by (bit_vector)
            requires
                val == 0u64,
        ;
        assert(ret == 0);
    } else {
        assert(spec_bit64_is_shl_by_bits(ret)) by (bit_vector)
            requires
                ret == val & add(!val, 1),
                0 < val,
        ;
        assert(ret >= align) by (bit_vector)
            requires
                val & sub(align, 1) == 0u64,
                ret == val & add(!val, 1),
                spec_bit64_is_pow_of_2(align as int),
                0 < val,
        ;
    }
    ret
}

#[verifier(external_body)]
pub proof fn proof_buddy(current_addr: u64, current_bucket: u64, current_size: u64) -> (ret: u64)
    requires
        current_bucket < 63,
        current_addr % current_size == 0,
        current_size == (1u64 << current_bucket),
    ensures
        ret == current_addr ^ current_size,
        ret == current_addr + current_size || ret + current_size == current_addr,
        current_addr as int % (current_size * 2) == 0 ==> ret == current_addr + current_size,
        current_addr as int % (current_size * 2) != 0 ==> ret + current_size == current_addr,
        ret % (1u64 << add(current_bucket, 1)) == 0,
        current_size * 2 == (1u64 << add(current_bucket, 1)),
        current_size > 0,
{
    current_addr ^ (1u64 << current_bucket)
}

} // verus!

================
File: ./source/verismo/src/allocator/mod.rs
================

mod bit_p;
mod buddy;
mod buddy_new;
mod linkedlist;
mod locked;
mod trusted;

pub use bit_p::*;
pub use buddy::BuddyAllocator;
pub use buddy_new::new_array_linked_list32;
pub use linkedlist::LinkedListAllocator;
use verismo_macro::*;

pub use self::trusted::*;
use crate::ptr::*;
use crate::tspec::map_lib::tracked_seq_insert;
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

// Choose different allocator type;
pub type VeriSMoAllocator = LinkedListAllocator;
use crate::lock::*;

verus! {

pub open spec fn alloc_valid_ptr(size: usize, ret: (usize, Tracked<SnpPointsToRaw>)) -> bool {
    &&& ret.1@@.wf_const_default((ret.0 as int, size as nat))
}

pub open spec fn talloc_valid_ptr(size: usize, ret: (usize, Tracked<SnpPointsToRaw>)) -> bool {
    &&& ret.1@@.wf_const_default((ret.0 as int, size as nat))
}

pub open spec fn valid_free_ptr(size: usize_t, ret: (usize_t, Tracked<SnpPointsToRaw>)) -> bool {
    &&& ret.1@@.wf_freemem((ret.0 as int, size as nat))
}

} // verus!

================
File: ./source/verismo/src/allocator/buddy.rs
================

use core::mem::size_of;

use super::*;
use crate::addr_e::AddrTrait;
use crate::arch::addr_s::GVA;
use crate::linkedlist::{LinkedList, Node, SpecListItem};
use crate::ptr::*;

crate::macro_const! {
    #[macro_export]
    pub const MIN_ADDR_ALIGN: usize = 8usize;
    #[macro_export]
    pub const ORDER: usize = 32usize;
    pub const ORDER_USIZE: usize = ORDER!() as usize;
    //ORDER as usize;
}

verismo_simple! {
pub struct BuddyAllocator {
    perms: Tracked<Map<(nat, nat), SnpPointsToRaw>>,
    // Use the last bytes of each mem range to store nodes
    free_lists: Array<LinkedList<()>, ORDER_USIZE>,
}

pub ghost struct SpecBuddyAllocator{
    pub perms: Map<(nat, nat), SnpPointsToRaw>,
    // Use the last bytes of each mem range to store nodes
    pub free_lists: Seq<LinkedList<()>>,
}
}

verus! {

impl BuddyAllocator {
    pub closed spec fn view(&self) -> SpecBuddyAllocator {
        SpecBuddyAllocator { perms: self.perms@, free_lists: self.free_lists@ }
    }
}

impl SpecBuddyAllocator {
    pub open spec fn valid_bucket(bucket: nat) -> bool {
        &&& bucket < ORDER!()
        &&& spec_bit64(spec_cast_integer::<_, u64>(bucket)) >= MIN_ADDR_ALIGN!()
    }

    pub open spec fn wf_bucket(&self, bucket: nat) -> bool {
        //&&& self.free_lists[bucket as int].is_Some()
        &&& self.free_lists[bucket as int].inv()
        &&& self.free_lists[bucket as int].is_constant()
    }

    pub open spec fn wf_perm(&self, bucket: nat, i: nat) -> bool {
        if (bucket < self.free_lists.len() && i < self.free_lists[bucket as int]@.len()) {
            let SpecListItem { ptr: node_ptr, snp: node_snp, val } =
                self.free_lists[bucket as int]@[i as int];
            let perm = self.perms[(bucket, i)]@;
            let size: nat = spec_bit64(bucket as u64) as nat;
            &&& self.perms.contains_key((bucket, i))
            &&& perm.wf_freemem((node_ptr.id() + MIN_ADDR_ALIGN!(), perm.size()))
            &&& node_snp === SwSnpMemAttr::spec_default()
            &&& node_ptr.not_null()
            &&& node_ptr.uptr.spec_valid_addr_with(size)
            &&& perm.size() + MIN_ADDR_ALIGN!() == size
        } else {
            !self.perms.contains_key((bucket, i))
        }
    }

    pub open spec fn wf_before_validate_write(&self) -> bool {
        &&& self.free_lists.len() == ORDER
        &&& forall|b: nat| b < self.free_lists.len() ==> self.wf_bucket(b)
        &&& forall|b: nat, i: nat| #[trigger] self.wf_perm(b, i)
    }

    pub open spec fn inv(&self) -> bool {
        &&& self.wf_before_validate_write()
    }

    pub open spec fn spec_pop_or_push_element(&self, old_self: Self, bucket: nat) -> bool {
        let list = self.free_lists[bucket as int];
        let old_list = old_self.free_lists[bucket as int];
        &&& list@ =~~= old_list@.drop_last()
        &&& old_list@ =~~= list@.push(old_list@.last())
        &&& self.free_lists =~~= old_self.free_lists.update(bucket as int, list)
        &&& self.perms =~~= old_self.perms.remove((bucket, list@.len()))
    }

    pub open spec fn spec_add_one_to_bucket(
        &self,
        old_self: &Self,
        bucket: nat,
        nodeptr: SnpPPtr<Node<()>>,
        node_perm: SnpPointsTo<Node<()>>,
        free_perm: SnpPointsToRaw,
    ) -> bool {
        let size = spec_bit64(bucket as u64) as nat;
        &&& self.wf_bucket(bucket as nat)
        &&& old_self.wf_bucket(bucket as nat)
        &&& Self::valid_bucket(bucket)
        &&& old_self.free_lists.len() == ORDER
        &&& self.free_lists.len() == ORDER
        &&& node_perm@.ptr_not_null_wf(nodeptr)
        &&& node_perm@.snp() === SwSnpMemAttr::spec_default()
        &&& nodeptr.uptr.spec_valid_addr_with(size)
        &&& free_perm@.size() + MIN_ADDR_ALIGN!() == size
        &&& free_perm@.wf_freemem((nodeptr.id() + MIN_ADDR_ALIGN!(), free_perm@.size()))
        &&& free_perm@.snp() === SwSnpMemAttr::spec_default()
    }

    pub open spec fn spec_add_one_to_bucket2(
        &self,
        old_self: &Self,
        bucket: nat,
        nodeptr: SnpPPtr<Node<()>>,
        node_perm: SnpPointsTo<Node<()>>,
        free_perm: SnpPointsToRaw,
    ) -> bool {
        let list = old_self.free_lists[bucket as int];
        let oldlen = list@.len();
        let newlist = self.free_lists[bucket as int];
        &&& self.perms =~~= old_self.perms.insert((bucket, oldlen), free_perm)
        &&& self.perms.remove((bucket, oldlen)) =~~= old_self.perms
        &&& self.free_lists =~~= old_self.free_lists.update(
            bucket as int,
            self.free_lists[bucket as int],
        )
        &&& newlist@ =~~= list@.push(
            SpecListItem {
                ptr: nodeptr,
                snp: node_perm@.snp(),
                val: node_perm@.value().get_Some_0().val,
            },
        )
    }

    pub proof fn proof_add_one_to_bucket(
        &self,
        old_self: &Self,
        bucket: nat,
        nodeptr: SnpPPtr<Node<()>>,
        node_perm: SnpPointsTo<Node<()>>,
        free_perm: SnpPointsToRaw,
    )
        requires
            self.spec_add_one_to_bucket(old_self, bucket, nodeptr, node_perm, free_perm),
            self.spec_add_one_to_bucket2(old_self, bucket, nodeptr, node_perm, free_perm),
        ensures
            old_self.inv() == self.inv(),
    {
        let list = old_self.free_lists[bucket as int];
        let oldlen = list@.len();
        if old_self.inv() {
            assert(self.inv()) by {
                //assert((*self).sec_label() === SecLevel::Low) by {
                //reveal_with_fuel(Array::<LinkedList<()>, ORDER_USIZE>::sec_label_from_array, 32);
                //}
                assert forall|b: nat| b < self.free_lists.len() implies self.wf_bucket(b) by {
                    assert(self.wf_bucket(bucket as nat));
                    /*if b != bucket {
                    assert(self.free_lists[b as int] === old_self.free_lists[b as int]);
                }*/
                    assert(old_self.wf_bucket(b));
                    //assert(self.free_lists[b as int].is_Some());
                    //assert(self.free_lists[b as int].wf());
                    //assert(self.free_lists[b as int].is_constant());
                }
                assert forall|b: nat, i: nat| #[trigger] self.wf_perm(b, i) by {
                    assert(old_self.wf_perm(b, i));
                    if b < ORDER && i < self.free_lists[b as int]@.len() {
                        assert(self.wf_perm(b, i));
                    } else {
                        assert(self.wf_perm(b, i));
                    }
                }
            }
        }
        if self.inv() {
            assert(old_self.inv()) by {
                assert forall|b: nat| b < old_self.free_lists.len() implies old_self.wf_bucket(
                    b,
                ) by {
                    assert(self.wf_bucket(b));
                }
                assert forall|b: nat, i: nat| #[trigger] old_self.wf_perm(b, i) by {
                    assert(self.wf_perm(b, i));
                }
            }
        }
    }

    pub open spec fn spec_remove_or_insert_one(
        &self,
        old_self: Self,
        bucket: nat,
        idx: nat,
    ) -> bool {
        let list = self.free_lists[bucket as int];
        let old_list = old_self.free_lists[bucket as int];
        &&& list@ =~~= old_list@.remove(idx as int)
        &&& old_list@ =~~= list@.push(old_list@[idx as int])
        &&& self.free_lists =~~= old_self.free_lists.update(bucket as int, list)
        &&& self.perms =~~= old_self.perms.remove((bucket, idx))
    }

    pub open spec fn _spec_remove_or_insert_one(
        &self,
        old_self: &Self,
        bucket: nat,
        idx: nat,
        nodeptr: SnpPPtr<Node<()>>,
        node_perm: SnpPointsTo<Node<()>>,
        free_perm: SnpPointsToRaw,
    ) -> bool {
        let size = spec_bit64(bucket as u64) as nat;
        &&& self.wf_bucket(bucket as nat)
        &&& old_self.wf_bucket(bucket as nat)
        &&& Self::valid_bucket(bucket)
        &&& old_self.free_lists.len() == ORDER
        &&& self.free_lists.len() == ORDER
        &&& node_perm@.ptr_not_null_wf(nodeptr)
        &&& node_perm@.snp() === SwSnpMemAttr::spec_default()
        &&& nodeptr.uptr.spec_valid_addr_with(size)
        &&& free_perm@.size() + MIN_ADDR_ALIGN!() == size
        &&& free_perm@.wf_freemem((nodeptr.id() + MIN_ADDR_ALIGN!(), free_perm@.size()))
        &&& free_perm@.snp() === SwSnpMemAttr::spec_default()
    }

    pub open spec fn _spec_remove_or_insert_one2(
        &self,
        old_self: &Self,
        bucket: nat,
        idx: nat,
        nodeptr: SnpPPtr<Node<()>>,
        node_perm: SnpPointsTo<Node<()>>,
        free_perm: SnpPointsToRaw,
    ) -> bool {
        let list = old_self.free_lists[bucket as int];
        let oldlen = list@.len();
        let newlist = self.free_lists[bucket as int];
        //&&& old_self.perms.dom().insert((bucket as nat, oldlen)) =~~= self.perms.dom()
        &&& self.perms.contains_key((bucket as nat, oldlen))
        &&& self.perms.dom().remove((bucket as nat, oldlen)) === old_self.perms.dom()
        &&& old_self.perms =~~= Map::new(
            |key| old_self.perms.contains_key(key),
            |k: (nat, nat)|
                if k.0 != bucket || k.1 < idx {
                    self.perms[k]
                } else {
                    self.perms[(k.0, (k.1 + 1) as nat)]
                },
        )
        &&& self.perms[(bucket, idx)]
            === free_perm/* &&& self.perms =~~= Map::new(
            |key| self.perms.contains_key(key),
            |k: (nat, nat)| if k.0 != bucket || k.1 < idx {old_self.perms[k]} else if k.1 > idx {old_self.perms[(k.0, (k.1 - 1) as nat)]} else {free_perm}
        )*/

        &&& self.free_lists =~~= old_self.free_lists.update(
            bucket as int,
            self.free_lists[bucket as int],
        )
        &&& newlist@ =~~= list@.insert(
            idx as int,
            SpecListItem {
                ptr: nodeptr,
                snp: node_perm@.snp(),
                val: node_perm@.value().get_Some_0().val,
            },
        )  //
        //&&& newlist@ =~~= list@.subrange(0, idx as int).push(SpecListItem {ptr: nodeptr, snp: node_perm@.snp()}) + list@.subrange(idx as int, list@.len() as int)

    }

    pub proof fn proof_remove_or_add_idx(
        &self,
        old_self: &Self,
        bucket: nat,
        idx: nat,
        nodeptr: SnpPPtr<Node<()>>,
        node_perm: SnpPointsTo<Node<()>>,
        free_perm: SnpPointsToRaw,
    )
        requires
            self._spec_remove_or_insert_one(old_self, bucket, idx, nodeptr, node_perm, free_perm),
            self._spec_remove_or_insert_one2(old_self, bucket, idx, nodeptr, node_perm, free_perm),
            idx <= old_self.free_lists[bucket as int]@.len(),
            idx < self.free_lists[bucket as int]@.len(),
        ensures
            old_self.inv() == self.inv(),
    {
        let list = old_self.free_lists[bucket as int];
        let oldlen = list@.len();
        let newlist = self.free_lists[bucket as int];
        let newlen = newlist@.len();
        let left = list@.subrange(0, idx as int).push(
            SpecListItem {
                ptr: nodeptr,
                snp: node_perm@.snp(),
                val: node_perm@.value().get_Some_0().val,
            },
        );
        let right = list@.subrange(idx as int, list@.len() as int);
        assert(left.len() == idx + 1);
        assert(right.len() == oldlen - idx);
        assert(oldlen + 1 == self.free_lists[bucket as int]@.len());
        if old_self.inv() {
            assert(self.inv()) by {
                assert forall|b: nat| b < self.free_lists.len() implies self.wf_bucket(b) by {
                    assert(self.wf_bucket(bucket as nat));
                    assert(old_self.wf_bucket(b));
                }
                assert forall|b: nat, i: nat| #[trigger] self.wf_perm(b, i) by {
                    assert(old_self.wf_perm(b, i));
                    if b < ORDER && self.perms.contains_key((b, i)) {
                        if b != bucket || i < idx {
                            assert(idx <= oldlen);
                            assert((b, i) !== (bucket, oldlen));
                            assert(old_self.perms.contains_key((b, i)));
                            assert(old_self.perms[(b, i)] === self.perms[(b, i)]);
                            assert(self.wf_perm(b, i));
                        } else if idx < i < newlen {
                            assert(old_self.wf_perm(b, (i - 1) as nat));
                            assert(i <= oldlen);
                            assert(old_self.perms.contains_key((b, (i - 1) as nat)));
                            assert(self.wf_perm(b, i));
                        } else {
                            assert(self.perms[(b, i)] === free_perm);
                            assert(self.wf_perm(b, i));
                        }
                    }
                }
            }
        }
        if self.inv() {
            assert(old_self.inv()) by {
                assert forall|b: nat| b < old_self.free_lists.len() implies old_self.wf_bucket(
                    b,
                ) by {
                    assert(self.wf_bucket(b));
                }
                assert forall|b: nat, i: nat| #[trigger] old_self.wf_perm(b, i) by {
                    assert(self.wf_perm(b, i));
                    if b < ORDER && i < old_self.free_lists[b as int]@.len() {
                        assert(i < self.free_lists[b as int]@.len());
                        assert(self.perms.contains_key((b, i)));
                        assert(old_self.perms.contains_key((b, i)));
                        if b != bucket || i < idx {
                            assert(old_self.perms[(b, i as nat)] === self.perms[(b, i)]);
                        } else if idx <= i < oldlen {
                            assert(self.wf_perm(b, i + 1));
                            assert(old_self.perms[(b, i as nat)] === self.perms[(b, i + 1)]);
                        }
                    } else {
                        if self.perms.contains_key((b, i)) {
                            assert(b < ORDER && i < self.free_lists[b as int]@.len());
                            assert(b == bucket);
                            assert(i == oldlen);
                            assert(!old_self.perms.contains_key((b, i)));
                        } else {
                            assert(!old_self.perms.contains_key((b, i)));
                        }
                    }
                }
            }
        }
    }
}

} // verus!
verus! {

pub open spec fn alloc_valid_size(old_size: usize, size: usize) -> bool {
    &&& size >= old_size
    &&& spec_bit64_is_pow_of_2(size as int)
    &&& size >= MIN_ADDR_ALIGN!()
    //&&& size % align == 0

}

} // verus!
verus! {

impl BuddyAllocator {
    pub const fn new() -> (ret: BuddyAllocator)
        ensures
            ret@.wf_before_validate_write(),
            forall|i: int|
                0 <= i < ORDER_USIZE as int ==> ret@.free_lists[i]@.len()
                    == 0,
    //forall |i: int| 0 <= i < ORDER_USIZE as int ==> ret@.free_lists[i].is_Some(),

    {
        let mut free_lists = new_array_linked_list32();
        let tracked mut perms: Map<(nat, nat), SnpPointsToRaw> = Map::tracked_empty();
        let ret = BuddyAllocator { perms: Tracked(perms), free_lists };
        proof {
            assert forall|bucket: nat| bucket < ret@.free_lists.len() implies ret@.wf_bucket(
                bucket,
            ) by {
                //assert(ret@.free_lists[bucket as int].is_Some());
                assert(ret@.free_lists[bucket as int].inv());
                assert(ret@.free_lists[bucket as int].is_constant());
            }
            assert forall|b: nat, i: nat| #[trigger] ret@.wf_perm(b, i) by {}
        }
        ret
    }

    #[inline]
    fn pop(&mut self, bucket: usize) -> (ret: Option<(usize_t, Tracked<SnpPointsToRaw>)>)
        requires
            old(self)@.inv(),
            SpecBuddyAllocator::valid_bucket(bucket as nat),
        ensures
            self@.inv(),
            ret.is_Some() ==> {
                ret.get_Some_0().1@@.wf_freemem(
                    (ret.get_Some_0().0 as int, spec_bit64(bucket as u64) as nat),
                )
            },
            ret.is_Some() ==> self@.spec_pop_or_push_element(old(self)@, bucket as nat),
            ret.is_None() ==> {
                &&& old(self)@ =~~= self@
                &&& self@.free_lists[bucket as int]@.len() == 0
            },
    {
        let ghost old_self = *self;
        let ghost gbucket = bucket as nat;
        proof {
            bit_shl64_pow2_auto();
        }
        proof {
            assert(self@.wf_bucket(gbucket));
        }
        let ghost old_list = old_self.free_lists[gbucket as int];
        let ghost old_len = old_list@.len();
        let mut list = self.free_lists.take(bucket);
        proof {
            assert(old_self@.wf_perm(gbucket, (old_len - 1) as nat));
        }
        let ret = list.pop();
        // TODO: Remove this dirty update back.
        self.free_lists.update(bucket, list);
        match ret {
            Some((nodeptr, tnode_perm)) => {
                let tracked free_perm = self.perms.borrow_mut().tracked_remove(
                    (gbucket, list@.len()),
                );
                proof {
                    assert(old_self@.perms =~~= self@.perms.insert(
                        (gbucket, list@.len()),
                        free_perm,
                    ));
                    assert(old_self@.perms.remove((gbucket, list@.len())) =~~= self@.perms);
                    assert(old_self@.free_lists =~~= self@.free_lists.update(
                        gbucket as int,
                        old_self.free_lists[gbucket as int],
                    ));
                    assert(list@ =~~= old_list@.drop_last());
                    assert(old_list@.last() === SpecListItem {
                        ptr: nodeptr,
                        snp: tnode_perm@@.snp(),
                        val: tnode_perm@@.value().get_Some_0().val,
                    });
                    assert(old_list@.drop_last() =~~= list@);
                    old_self@.proof_add_one_to_bucket(
                        &self@,
                        gbucket,
                        nodeptr,
                        tnode_perm@,
                        free_perm,
                    );
                    assert(self@.wf_perm(gbucket, old_len));
                }
                let tracked node_rperm = tnode_perm.get().trusted_into_raw();
                let tracked ret_perm = node_rperm.trusted_join(free_perm);
                let ret = Some((nodeptr.to_usize(), Tracked(ret_perm)));
                proof {
                    assert(ret.get_Some_0().1.is_constant());
                    assert(ret.get_Some_0().1.wf());
                }
                return ret
            },
            None => {
                proof {
                    assert(old_self@.perms =~~= self@.perms);
                    assert(old_self@.free_lists =~~= self@.free_lists);
                }
                return None
            },
        }
    }

    #[inline]
    fn push(&mut self, bucket: usize, start: usize_t, Tracked(perm): Tracked<SnpPointsToRaw>)
        requires
            bucket.is_constant(),
            start.is_constant(),
            SpecBuddyAllocator::valid_bucket(bucket as nat),
            perm@.wf_freemem((start as int, spec_bit64((bucket as u64)) as nat)),
            old(self)@.inv(),
        ensures
            self@.inv(),
            old(self)@.spec_pop_or_push_element(self@, bucket as nat),
            start == self@.free_lists[bucket as int]@.last().ptr.id(),
    {
        let ghost old_self = *self;
        let tracked (mut node_rperm, free_perm) = perm.trusted_split(MIN_ADDR_ALIGN!() as nat);
        let tracked mut node_perm = node_rperm.trusted_into();
        proof {
            bit_shl64_pow2_auto();
        }
        let nodeptr = SnpPPtr::<Node<()>>::from_usize(start);
        nodeptr.replace(Tracked(&mut node_perm), Node::default());
        proof {
            assert(self@.wf_bucket(bucket as nat));
            let len = self.free_lists[bucket as int]@.len();
            assert(self@.wf_perm(bucket as nat, len));
            self.perms.borrow_mut().tracked_insert((bucket as nat, len), free_perm);
        }
        let mut current_bucket = self.free_lists.take(bucket);
        current_bucket.push(nodeptr, Tracked(node_perm));
        // TODO: Remove this dirty update back.
        self.free_lists.take_end(bucket, current_bucket);
        proof {
            self@.proof_add_one_to_bucket(&old_self@, bucket as nat, nodeptr, node_perm, free_perm);
        }
    }

    pub fn alloc_inner(&mut self, size: usize, align: usize) -> (ret: Option<
        (usize, Tracked<SnpPointsToRaw>),
    >)
        requires
            spec_bit64_is_pow_of_2(align as int),
            0 < (size as int) <= POW2!(31),
            0 < (align as int) <= POW2!(31),
            size.is_constant(),
            align.is_constant(),
            old(self)@.inv(),
        ensures
            self@.inv(),
            ret.is_Some() ==> alloc_valid_ptr(size, ret.get_Some_0()),
            ret.is_Some() ==> ret.get_Some_0().is_constant(),
            ret.is_Some() ==> (spec_align_up(ret.get_Some_0().0 as int, align as int), size as nat)
                === (ret.get_Some_0().0 as int, size as nat),
    {
        let old_size = size;
        let mut size = size;
        if let Some((addr, perm)) = self._alloc_inner(&mut size, align) {
            let prev_addr = addr;
            let addr: usize_t = align_up_by(addr as u64, align as u64) as usize;
            let Tracked(mut ret_perm) = perm;
            if addr != prev_addr {
                return None;
            }
            proof {
                assert(size == ret_perm@.size());
                assert(size >= old_size);
                if old_size < ret_perm@.size() {
                    ret_perm = ret_perm.trusted_split(old_size as nat).0;
                } else {
                    assert(old_size == ret_perm@.size());
                }
            }
            mem_set_zeros(addr, old_size, Tracked(&mut ret_perm));
            Some((addr, Tracked(ret_perm)))
        } else {
            None
        }
    }

    pub fn _alloc_inner(&mut self, size: &mut usize_t, align: usize_t) -> (ret: Option<
        (usize_t, Tracked<SnpPointsToRaw>),
    >)
        requires
            spec_bit64_is_pow_of_2(align as int),
            0 < (*old(size) as int) <= POW2!(31),
            0 < (align as int) <= POW2!(31),
            old(size).is_constant(),
            align.is_constant(),
            old(self)@.inv(),
        ensures
            self@.inv(),
            ret.is_Some() ==> valid_free_ptr(*size, ret.get_Some_0()),
            ret.is_Some() ==> alloc_valid_size(*old(size), *size),
            *size % align == 0,
            size.is_constant(),
    {
        proof {
            bit_shl64_pow2_auto();
        }
        let ghost old_size = *size;
        *size =
        max(max(align as u64, MIN_ADDR_ALIGN as u64), next_power_of_two((*size) as u64)) as usize;
        assert(*size < POW2!(32));
        assert(*size >= MIN_ADDR_ALIGN);
        let size = *size;
        let bucket = pow2_to_bits(size as u64) as usize;
        let mut i = bucket;
        let mut ret = None;
        assert(bucket < 32);
        assert(bucket >= 3);
        while i < ORDER_USIZE
            invariant_except_break
                bucket as int <= i as int <= ORDER,
                SpecBuddyAllocator::valid_bucket(bucket as nat),
                spec_bit64(bucket as u64) == size as int,
                size >= old_size,
                self@.inv(),
                !ret.is_Some(),
                i.is_constant(),
                ret.is_constant(),
                self.is_constant(),
                bucket.is_constant(),
                size.is_constant(),
            ensures
                ret.is_Some() ==> valid_free_ptr(size, ret.get_Some_0()),
                ret.is_Some() ==> alloc_valid_size(old_size, size),
                ret.is_constant(),
                self.is_constant(),
                self@.inv(),
        {
            proof {
                bit_shl64_pow2_auto();
                assert(self@.wf_bucket(i as nat));
            }
            if !self.free_lists.index(i).is_empty() && i > bucket {
                // Split buffers
                let mut j = i;
                while j >= bucket + 1
                    invariant
                        bucket as int <= (i as int) < ORDER,
                        bucket as int <= (j as int) <= i as int,
                        SpecBuddyAllocator::valid_bucket(bucket as nat),
                        spec_bit64(bucket as u64) as int == size as int,
                        self@.inv(),
                        self@.free_lists[j as int]@.len() > 0,
                        i.is_constant(),
                        j.is_constant(),
                        ret.is_constant(),
                        self.is_constant(),
                        bucket.is_constant(),
                        size.is_constant(),
                {
                    proof {
                        bit_shl64_pow2_auto();
                    }
                    let ghost prev_self = *self;
                    if let Some((start1, tperm)) = self.pop(j) {
                        let offset = 1usize << (j - 1);
                        let start2 = start1 + offset;
                        let tracked mut perm1;
                        let tracked mut perm2;
                        proof {
                            let tracked perm = tperm.get();
                            let tracked (p1, p2) = perm.trusted_split(offset as nat);
                            perm1 = p1;
                            perm2 = p2;
                        }
                        self.push(j - 1, start2, Tracked(perm2));
                        self.push(j - 1, start1, Tracked(perm1));
                    }
                    j = j - 1;
                }
            }
            let ghost len = self.free_lists[bucket as int]@.len();
            proof {
                assert(self@.wf_bucket(bucket as nat));
                assert(self@.wf_perm(bucket as nat, (len - 1) as nat));
            }
            if let Some(addr_with_perm) = self.pop(bucket) {
                ret = Some(addr_with_perm);
                break ;
            }
            i = i + 1;
        }
        ret
    }

    //pub fn dealloc(&mut self, addr: u64, layout: Layout) {}
    /// Dealloc a range of memory from the heap
    pub fn dealloc_inner(
        &mut self,
        addr: usize_t,
        size: usize_t,
        Tracked(perm): Tracked<SnpPointsToRaw>,
    )
        requires
            perm@.wf_freemem((addr as int, size as nat)),
            spec_bit64_is_pow_of_2(size as int),
            size < (1usize << ORDER),
            size.is_constant(),
            size > MIN_ADDR_ALIGN!(),
            addr % size == 0,
            old(self)@.inv(),
        ensures
            self@.inv(),
    {
        proof {
            bit_shl64_pow2_auto();
        }
        let mut current_bucket = pow2_to_bits(size as u64) as usize;
        let mut current_addr: usize = addr;
        let tracked mut current_perm = perm;
        let ghost mut current_size = size as usize;
        // Put back into free list
        //self.push(current_bucket, current_addr, Tracked(perm));
        // Merge free buddy lists
        let len = self.free_lists.len();
        while current_bucket + 1 < len
            invariant
                current_bucket.is_constant(),
                len.is_constant(),
                current_addr.is_constant(),
                self.is_constant(),
                current_perm@.wf_freemem((current_addr as int, current_size as nat)),
                current_size == (1u64 << current_bucket as u64),
                current_bucket as int + 1 <= len as int,
                SpecBuddyAllocator::valid_bucket(current_bucket as nat),
                len == self@.free_lists.len(),
                self@.inv(),
                current_addr as u64 % (spec_bit64(current_bucket as u64)) == 0,
            ensures
                current_bucket.is_constant(),
                current_addr.is_constant(),
                self.is_constant(),
                current_perm@.wf_freemem((current_addr as int, current_size as nat)),
                current_size == (1u64 << current_bucket as u64),
                current_bucket as int + 1 <= len as int,
                SpecBuddyAllocator::valid_bucket(current_bucket as nat),
                self@.inv(),
                current_addr as u64 % (spec_bit64(current_bucket as u64)) == 0,
        {
            let buddy = current_addr ^ (1 << current_bucket);
            let ghost prev_self = self@;
            let check = |ptr: SnpPPtr<Node<()>>| -> (ret: bool)
                requires
                    ptr.is_constant(),
                ensures
                    ret == (ptr.id() == buddy as int),
                { ptr.to_usize() == buddy };
            proof {
                assert(self@.wf_bucket(current_bucket as nat));
            }
            let mut list = self.free_lists.take(current_bucket);
            let ghost prev_list = list;
            let removed_items = list.remove_iter(check, 1);
            assert((removed_items.0@.len() == 0) ==> prev_list === list);
            let ghost removed_ids = removed_items.1@;
            let mut removed = removed_items.0;
            self.free_lists.update(current_bucket, list);
            if let Some((ptr, tperm)) = removed.pop() {
                let tracked mut removed_perm;
                proof {
                    proof_buddy(current_addr as u64, current_bucket as u64, current_size as u64);
                    assert(removed_ids.len() == 1);
                    assert(check.ensures((ptr,), true));
                    assert(ptr.id() == buddy as int);
                    let ghost removed_id = removed_ids[0];
                    let tracked mut removed_node_perm;
                    let tracked Tracked(perm) = tperm;
                    removed_node_perm = perm;
                    assert(perm@.pptr() == buddy as int);
                    assert(prev_self.wf_perm(current_bucket as nat, removed_id as nat));
                    assert(prev_self.wf_perm(current_bucket as nat, list@.len() as nat));
                    assert(prev_self.wf_perm(current_bucket as nat, (list@.len() - 1) as nat));
                    assert(prev_self.perms === self@.perms);
                    let tracked removed_free_perm = self.perms.borrow_mut().tracked_remove(
                        (current_bucket as nat, removed_id as nat),
                    );
                    removed_perm =
                    removed_node_perm.trusted_into_raw().trusted_join(removed_free_perm);
                    let key_map = Map::new(
                        |k: (nat, nat)|
                            k.0 < ORDER && k.1 < prev_self.free_lists[k.0 as int]@.len() && k !== (
                                current_bucket as nat,
                                (prev_list@.len() - 1) as nat,
                            ),
                        |k: (nat, nat)|
                            if k.0 != current_bucket as nat || k.1 < removed_id as nat {
                                k
                            } else {
                                (k.0, k.1 + 1)
                            },
                    );
                    assert forall|j1: (nat, nat), j2: (nat, nat)|
                        !builtin::spec_eq(j1, j2) && key_map.dom().contains(j1)
                            && key_map.dom().contains(j2) implies !builtin::spec_eq(
                        key_map.index(j1),
                        key_map.index(j2),
                    ) by {
                        assert(prev_self.wf_perm(j1.0, j1.1));
                    }
                    assert forall|j| key_map.dom().contains(j) implies self@.perms.dom().contains(
                        key_map.index(j),
                    ) by {
                        let (bb, ii) = j;
                        let (b, i) = key_map.index(j);
                        assert(prev_self.wf_perm(b, i));
                        assert(prev_self.wf_perm(bb, ii));
                        assert(prev_self.perms.contains_key((b, i)));
                    }
                    self.perms.borrow_mut().tracked_map_keys_in_place(key_map);
                    assert(self@.inv()) by {
                        assert forall|k|
                            k !== (
                                current_bucket as nat,
                                (prev_list@.len() - 1) as nat,
                            ) implies prev_self.perms.contains_key(k) == self@.perms.contains_key(
                            k,
                        ) by {
                            assert(prev_self.wf_perm(k.0, k.1));
                            if prev_self.perms.contains_key(k) {
                                assert(key_map.contains_key(k));
                            }
                            if self@.perms.contains_key(k) {
                                assert(key_map.contains_key(k));
                                assert(prev_self.perms.contains_key(k));
                            }
                        }
                        assert forall|k|
                            k === (
                                current_bucket as nat,
                                (prev_list@.len() - 1) as nat,
                            ) implies !self@.perms.contains_key(k) by {
                            assert(!key_map.contains_key(k));
                        }
                        assert(prev_self.perms.dom().remove(
                            (current_bucket as nat, (prev_list@.len() - 1) as nat),
                        ) =~~= self@.perms.dom());
                        prev_self.proof_remove_or_add_idx(
                            &self@,
                            current_bucket as nat,
                            removed_id as nat,
                            ptr,
                            removed_node_perm,
                            removed_free_perm,
                        );
                    }
                    assert(removed_perm@.size() == current_size as nat);
                    assert(self@.wf_perm(current_bucket as nat, list@.len() as nat));
                }
                if current_addr > buddy {
                    //assert(buddy as int + current_size  == current_addr as int);
                    current_addr = buddy;
                    proof {
                        current_perm = removed_perm.trusted_join(current_perm);
                    }
                } else {
                    //assert(current_addr as int + current_size == buddy as int);
                    proof {
                        current_perm = current_perm.trusted_join(removed_perm);
                    }
                };
                current_bucket = current_bucket + 1;
                proof {
                    current_size = (1usize << current_bucket as usize);
                }
            } else {
                assert(removed@.len() == 0);
                assert(prev_list =~~= list);
                assert(self@.free_lists =~~= prev_self.free_lists);
                break ;
            }
        }
        self.push(current_bucket, current_addr, Tracked(current_perm));
    }
}

} // verus!
verus! {

impl BuddyAllocator {
    pub fn add_mem(
        &mut self,
        start: &mut usize,
        end: &mut usize,
        Tracked(perm): Tracked<SnpPointsToRaw>,
    )
        requires
            (*old(start)) < (*old(end)),
            old(start).is_constant(),
            old(end).is_constant(),
            old(start).spec_valid_addr_with(1),
            old(end).spec_valid_addr_with(0),
            old(self)@.inv(),
            old(self).is_constant(),
            perm@.wf_freemem(((*old(start) as int), (*old(end) - *old(start)) as nat)),
            perm@.snp() === SwSnpMemAttr::spec_default(),
        ensures
            self@.inv(),
            self.is_constant(),/*self@.inv(),
        GVA::new(*start as int).is_valid_end(),
        GVA::new(*end as int).is_valid_end(),
        spec_is_align_up_by_int(*old(start) as int, MIN_ADDR_ALIGN!() as int, *start as int),
        spec_is_align_down_by_int(*old(end) as int, MIN_ADDR_ALIGN!() as int, *end as int),*/

    {
        let ghost oldstart = *start;
        let ghost oldend = *end;
        proof {
            assert(spec_bit64_is_pow_of_2(MIN_ADDR_ALIGN!() as int));
        }
        *start = align_up_by((*start) as u64, MIN_ADDR_ALIGN as u64) as usize;
        *end = align_down_by((*end).into(), MIN_ADDR_ALIGN.into()).into();
        let current_end = *end;
        let mut current_start = *start;
        proof {
            assert(spec_size::<Node<()>>() == MIN_ADDR_ALIGN!());
        }
        if (current_start >= current_end) {
            return ;
        }
        let tracked (mut removed_start_perm, mut perm2) = perm.trusted_split(
            (current_start - oldstart) as nat,
        );
        let tracked (mut total_perm, mut removed_end_perm) = perm2.trusted_split(
            (current_end - current_start) as nat,
        );
        while (current_start < current_end) && (current_start + MIN_ADDR_ALIGN <= current_end)
            invariant
                current_start as int <= current_end as int,
                current_start.spec_valid_addr_with(0),
                current_end.spec_valid_addr_with(0),
                current_start as usize % MIN_ADDR_ALIGN == 0,
                current_end as usize % MIN_ADDR_ALIGN == 0,
                self@.inv(),
                total_perm@.wf_freemem(
                    ((current_start as int), (current_end - current_start) as nat),
                ),
                total_perm@.snp() === SwSnpMemAttr::spec_default(),
                current_end.is_constant(),
                current_start.is_constant(),
                self.is_constant(),
        {
            let totalsize = current_end - current_start;
            let ghost old_self = *self;
            proof {
                let ghost g_start = current_start as usize;
                //assert(g_start & sub(MIN_ADDR_ALIGN!(), 1usize) == 0usize) by(bit_vector)
                //requires g_start % MIN_ADDR_ALIGN!() == 0usize;
                proof_bit_usize_and_rel_mod(g_start, MIN_ADDR_ALIGN!());
                lemma_get_low_bits_via_bit_op(current_start as u64, MIN_ADDR_ALIGN!() as u64);
                proof_bit_usize_not(g_start);
                lemma_prev_power_of_two(totalsize as u64);
            }
            let size = if current_start != 0 {
                let lowbit = current_start & (!current_start + 1usize);
                min(lowbit as u64, prev_power_of_two(totalsize as u64)) as usize
            } else {
                prev_power_of_two(totalsize as u64) as usize
            };
            let size = min((1usize << (ORDER!() - 1usize) as usize) as u64, size as u64) as usize;
            let tracked (mut cur_perm, mut remain_perm) = total_perm.trusted_split(size as nat);
            proof {
                total_perm = remain_perm;
                bit_shl64_pow2_auto();  // prove size is power of 2 and bucket is valid;
            }
            // Get the bucket id.
            let bucket = pow2_to_bits(size as u64) as usize;
            self.push(bucket, current_start, Tracked(cur_perm));
            current_start = current_start + size;
        }
    }
}

} // verus!

================
File: ./source/verismo/src/allocator/buddy_new.rs
================

use super::*;
use super::buddy::ORDER_USIZE;
use crate::linkedlist::LinkedList;

seq_macro::seq!(N in 0..32 {
verus!{
        //impl Array<LinkedList<()>, ORDER_USIZE> {
        #[verifier(external_body)]
        pub const fn new_array_linked_list32() -> (ret: Array<LinkedList<()>, ORDER_USIZE>)
        ensures
        ret@.len() == ORDER_USIZE as nat,
        forall |i: int| 0 <= i < ret@.len() as int ==> ret@[i]@.len() == 0,
        forall |i: int| 0 <= i < ret@.len() as int ==> ret@[i].inv(),
        //forall |i: int| 0 <= i < ret@.len() as int ==> ret@[i].is_Some(),
        forall |i: int| 0 <= i < ret@.len() as int ==> ret@[i].is_constant(),
        ret.is_constant()
        {
        Array{array: [#(LinkedList::new(),)*]}
        }
}
});

================
File: ./source/verismo/src/main.rs
================

#![no_std] // don't link the Rust standard library
#![no_main] // disable all Rust-level entry points
#![feature(panic_info_message)]
#![allow(unused)]

#[cfg(target_os = "none")]
core::arch::global_asm!(include_str!("entry.s"), options(att_syntax));

use core::panic::PanicInfo;

use verismo::debug::VEarlyPrintAtLevel;
use verismo::snp::ghcb::*;
use vstd::prelude::*;
use vstd::string::*;

#[panic_handler]
#[verifier::external]
fn panic(info: &PanicInfo) -> ! {
    new_strlit("panic:\n").err(Tracked::assume_new());
    match info.message() {
        Some(msg) => {
            if msg.as_str().is_some() {
                StrSlice::from_rust_str(msg.as_str().unwrap()).err(Tracked::assume_new());
            }
        }
        None => todo!(),
    }

    if let Some(location) = info.location() {
        (StrSlice::from_rust_str(location.file()), location.line()).err(Tracked::assume_new());
    } else {
        new_strlit("Panic occurred but no location information available")
            .err(Tracked::assume_new());
    }
    vc_terminate(SM_TERM_UNSUPPORTED, Tracked::assume_new());

    loop {}
}

================
File: ./source/verismo/src/tspec/wellformed.rs
================

use super::*;

verus! {

pub trait WellFormed {
    spec fn wf(&self) -> bool;
}

impl WellFormed for () {
    open spec fn wf(&self) -> bool {
        true
    }
}

impl<T1: WellFormed, T2: WellFormed> WellFormed for (T1, T2) {
    open spec fn wf(&self) -> bool {
        self.0.wf() && self.1.wf()
    }
}

impl<T1: WellFormed, T2: WellFormed, T3: WellFormed> WellFormed for (T1, T2, T3) {
    open spec fn wf(&self) -> bool {
        self.0.wf() && self.1.wf() && self.2.wf()
    }
}

impl<T: WellFormed> WellFormed for Option<T> {
    #[verifier(inline)]
    open spec fn wf(&self) -> bool {
        self.is_Some() ==> self.get_Some_0().wf()
    }
}

impl<T> WellFormed for Ghost<T> {
    #[verifier(inline)]
    open spec fn wf(&self) -> bool {
        true
    }
}

impl<T> WellFormed for Tracked<T> {
    #[verifier(inline)]
    open spec fn wf(&self) -> bool {
        true
    }
}

#[macro_export]
    macro_rules! impl_spec_wf_for_basic {
        ($($type: ty),* $(,)?) => {
            $(verus!{
                impl WellFormed for $type {
                    #[verifier(inline)]
                    open spec fn wf(&self) -> bool
                    {
                        true
                    }
                }
            })*
        }
    }

impl_spec_wf_for_basic!{u64, u32, u16, usize, u8, bool, char}

} // verus!

================
File: ./source/verismo/src/tspec/fnspec.rs
================

use super::ops::*;
use super::*;

verus! {

pub open spec fn fn_vspec_lt<T1: VSpecOrd::<T2>, T2>() -> spec_fn(T1, T2) -> bool {
    |v1: T1, v2: T2| VSpecOrd::<T2>::spec_lt(v1, v2)
}

pub open spec fn fn_vspec_le<T1: VSpecOrd<T2>, T2>() -> spec_fn(T1, T2) -> bool {
    |v1: T1, v2: T2| VSpecOrd::<T2>::spec_le(v1, v2)
}

pub open spec fn fn_vspec_gt<T1: VSpecOrd<T2>, T2>() -> spec_fn(T1, T2) -> bool {
    |v1: T1, v2: T2| VSpecOrd::<T2>::spec_gt(v1, v2)
}

pub open spec fn fn_vspec_ge<T1: VSpecOrd<T2>, T2>() -> spec_fn(T1, T2) -> bool {
    |v1: T1, v2: T2| VSpecOrd::<T2>::spec_ge(v1, v2)
}

pub open spec fn fn_vspec_eq<T1: VSpecEq<T2>, T2>() -> spec_fn(T1, T2) -> bool {
    |v1: T1, v2: T2| VSpecEq::<T2>::spec_eq(v1, v2)
}

pub open spec fn fn_vspec_neg<T1: VSpecNeg>() -> spec_fn(T1) -> T1 {
    |v1| VSpecNeg::spec_neg(v1)
}

pub open spec fn fn_vspec_not<T1: VSpecNot>() -> spec_fn(T1) -> T1 {
    |v1| VSpecNot::spec_not(v1)
}

pub open spec fn fn_vspec_add<T1: VSpecAdd<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| VSpecAdd::<T2, T3>::spec_add(v1, v2)
}

pub open spec fn fn_vspec_sub<T1: VSpecSub<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| VSpecSub::<T2, T3>::spec_sub(v1, v2)
}

pub open spec fn fn_vspec_mul<T1: VSpecMul<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| VSpecMul::<T2, T3>::spec_mul(v1, v2)
}

pub open spec fn fn_vspec_div<T1: VSpecEuclideanDiv<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| VSpecEuclideanDiv::<T2, T3>::spec_euclidean_div(v1, v2)
}

pub open spec fn fn_vspec_rem<T1: VSpecEuclideanMod<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| VSpecEuclideanMod::<T2, T3>::spec_euclidean_mod(v1, v2)
}

pub open spec fn fn_vspec_euclidean_div<T1: VSpecEuclideanDiv<T2, T3>, T2, T3>() -> spec_fn(
    T1,
    T2,
) -> T3 {
    |v1: T1, v2: T2| VSpecEuclideanDiv::<T2, T3>::spec_euclidean_div(v1, v2)
}

pub open spec fn fn_vspec_euclidean_mod<T1: VSpecEuclideanMod::<T2, T3>, T2, T3>() -> spec_fn(
    T1,
    T2,
) -> T3 {
    |v1: T1, v2: T2| VSpecEuclideanMod::<T2, T3>::spec_euclidean_mod(v1, v2)
}

pub open spec fn fn_vspec_bitand<T1: VSpecBitAnd<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| VSpecBitAnd::<T2, T3>::spec_bitand(v1, v2)
}

pub open spec fn fn_vspec_bitor<T1: VSpecBitOr<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| VSpecBitOr::<T2, T3>::spec_bitor(v1, v2)
}

pub open spec fn fn_vspec_bitxor<T1: VSpecBitXor<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| VSpecBitXor::<T2, T3>::spec_bitxor(v1, v2)
}

pub open spec fn fn_vspec_shr<T1: VSpecShr<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| v1 >> v2
}

pub open spec fn fn_vspec_shl<T1: VSpecShl<T2, T3>, T2, T3>() -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| v1 << v2
}

} // verus!
macro_rules! def_builtin_unary_spec_fn {
    ($fname: ident, $op: tt, $t1: ty, $t2: ty) => {
        paste::paste! {verus!{
            pub open spec fn [<fn_ $fname _ $t1 _ $t2>]() -> spec_fn($t1) -> $t2 {
                |v1: $t1| $op v1
            }
        }
}
    };
}

macro_rules! def_builtin_spec_fn {
    ($fname: ident, $op: tt, $t1: ty, $t2: ty, $t3: ty) => {
        paste::paste! {
                                                                                        verus!{
            pub open spec fn [<fn_ $fname _ $t1 _ $t2 _ $t3>]() -> spec_fn($t1, $t2) -> $t3 {
                |v1: $t1, v2: $t2| v1 $op v2
            }
        }
                                                                                    }
    };
}

macro_rules! def_builtin_spec_fns {
    ([$([$fname: ident, $op: tt]),* $(,)*], $t1: ty, $t2: ty, $t3: ty) => {
        $(
            def_builtin_spec_fn!{$fname, $op, $t1, $t2, $t3}
        )*
    };
}

macro_rules! def_builtin_cmp_spec_fns_t1 {
    ([$($t1: ty),*$(,)*], $t2: ty, $t3: ty) => {
        $(
        def_builtin_spec_fns!{[[spec_lt, <], [spec_gt, >], [spec_le, <=], [spec_ge, >=], [spec_eq, ==]], $t1, $t2, $t3}
        )*
    }
}

macro_rules! def_builtin_cmp_spec_fns {
    ($($t2: ty),*$(,)*) => {
        $(
            def_builtin_cmp_spec_fns_t1!{[u128, u64, u32, u16, u8, usize int, nat], $t2, bool}
        )*
    }
}

macro_rules! def_builtin_bop_spec_fns_sized {
    ($($t1: ty),*$(,)*) => {
        $(
        def_builtin_spec_fns!{[
            [spec_add, +], [spec_sub, -], [spec_mul, *]
            ],
            $t1, $t1, int}
        def_builtin_spec_fns!{[
                [spec_euclidean_div, /], [spec_euclidean_mod, %],
                [spec_div, /], [spec_rem, %]
                ],
                $t1, $t1, $t1}
        )*
    }
}

macro_rules! def_builtin_bop_spec_fns_nat {
    () => {
        def_builtin_spec_fns! {[
        [spec_add, +], [spec_mul, *], [spec_euclidean_div, /], [spec_euclidean_mod, %],
        [spec_div, /], [spec_rem, %]
        ],
        nat, nat, nat}
        def_builtin_spec_fns! {[
        [spec_sub, -]
        ],
        nat, nat, int}
    };
}

macro_rules! def_builtin_bit_bop_spec_fns_t1 {
    ($($t1: ty),*$(,)*) => {
        $(
        def_builtin_spec_fns!{[
            [spec_bitand, &], [spec_bitor, |], [spec_bitxor, ^], [spec_shr, >>], [spec_shl, <<],
            ],
            $t1, $t1, $t1}
        )*
    }
}

macro_rules! def_builtin_cmp_spec_fns {
    ($($t2: ty),*$(,)*) => {
        $(
            def_builtin_cmp_spec_fns_t1!{[u128, u64, u32, u16, u8, usize, int, nat], $t2, bool}
        )*
    }
}

macro_rules! def_builtin_not_neg {
    ($($t1: ty),*$(,)*) => {
        $(
        def_builtin_unary_spec_fn!{spec_not, !, $t1, $t1}
        )*
    };
}

def_builtin_cmp_spec_fns! {u128, u64, u32, u16, u8, usize, int, nat}
def_builtin_bop_spec_fns_sized! {u128, u64, u32, u16, u8, usize, int}
def_builtin_bop_spec_fns_nat! {}
def_builtin_not_neg! {u128, u64, u32, u16, u8, usize, bool}
def_builtin_bit_bop_spec_fns_t1! {u128, u64, u32, u16, u8, usize}

verus! {

pub open spec fn fn_spec_add_seq<T>() -> spec_fn(Seq<T>, Seq<T>) -> Seq<T> {
    |v1: Seq<T>, v2: Seq<T>| v1 + v2
}

} // verus!

================
File: ./source/verismo/src/tspec/fmap.rs
================

use super::*;

verus! {

#[derive(SpecGetter, SpecSetter)]
#[verifier::reject_recursive_types(K)]
#[verifier::reject_recursive_types(V)]
pub struct FMap<K, V> {
    map: Map<K, V>,
}

impl<K, V> FMap<K, V> {
    #[verifier(external_body)]
    pub broadcast proof fn axiom_inv(&self, id: K)
        ensures
            (#[trigger] self.spec_map()[id]) === self.spec_map()[id],
            #[trigger] self.spec_map().dom().contains(id),
    {
    }

    #[verifier(inline)]
    pub open spec fn ext_equal(&self, other: Self) -> bool {
        self.spec_map() =~~= (other.spec_map())
    }

    #[verifier(external_body)]
    pub broadcast proof fn axiom_equal(&self, other: Self)
        ensures
            #[trigger] (*self =~~= other) == equal(*self, other),
            #[trigger] self.spec_map() =~~= (other.spec_map()) == equal(*self, other),
    {
    }

    #[verifier(inline)]
    pub open spec fn update(&self, fv: spec_fn(K) -> V) -> Self {
        self.spec_set_map(Map::new(|K| true, |k: K| fv(k)))
    }

    #[verifier(inline)]
    pub open spec fn insert(&self, k: K, v: V) -> Self {
        self.spec_set_map(self.spec_map().insert(k, v))
    }

    #[verifier(inline)]
    pub open spec fn dom(&self) -> Set<K> {
        self.spec_map().dom()
    }

    #[verifier(inline)]
    pub open spec fn union(&self, add: Map<K, V>) -> Self {
        self.spec_set_map(self.spec_map().union_prefer_right(add))
    }

    #[verifier(inline)]
    pub open spec fn spec_index(&self, k: K) -> V {
        self.spec_map().spec_index(k)
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/seqlib/mod.rs
================

use super::*;
mod seq_multiset;
mod subseq;
pub use seq_multiset::*;
pub use subseq::*;

================
File: ./source/verismo/src/tspec/seqlib/subseq.rs
================

use vstd::prelude::*;

verus! {

#[verifier(inline)]
pub open spec fn sub_element<T>(subs: Seq<T>, s: Seq<T>, idx: Seq<int>, k: int) -> bool {
    &&& (0 <= #[trigger] idx[k] < s.len())
    &&& (subs[k] === s[idx[k]])
}

pub open spec fn is_subseq_via_index<T>(subs: Seq<T>, s: Seq<T>, idx: Seq<int>) -> bool {
    &&& (forall|k: int| (0 <= k < idx.len()) ==> sub_element(subs, s, idx, k))
    &&& subs.len() == idx.len()
    &&& subs.len() <= s.len()
}

pub proof fn proof_empty_is_subs<T>(s: Seq<T>)
    ensures
        is_subseq_via_index(Seq::empty(), s, Seq::empty()),
{
}

pub proof fn proof_subs_remove<T>(s: Seq<T>, subs: Seq<T>, subs_idx: Seq<int>, i: int)
    requires
        is_subseq_via_index(subs, s, subs_idx),
        0 <= i < subs_idx.len(),
    ensures
        is_subseq_via_index(subs.remove(i), s, subs_idx.remove(i)),
        subs.remove(i).len() == subs.len() - 1,
{
    let (subs0, subs_idx0) = (subs, subs_idx);
    let subs1 = subs0.remove(i);
    let subs_idx1 = subs_idx0.remove(i);
    assert forall|k: int| (0 <= k < subs_idx1.len()) implies sub_element(
        subs1,
        s,
        subs_idx1,
        k,
    ) by {
        assert(subs_idx1.len() == subs_idx0.len() - 1);
        assert(0 <= k < subs_idx0.len());
        assert(0 <= k < subs0.len());
        assert(0 <= k < subs1.len());
        if k < i {
            assert(sub_element(subs0, s, subs_idx0, k));
            assert(subs_idx0[k] == subs_idx1[k]);
            assert(subs0[k] === subs1[k]);
            assert(sub_element(subs1, s, subs_idx1, k));
        } else {
            assert(subs_idx0[k + 1] == subs_idx1[k]);
            assert(subs0[k + 1] === subs1[k]);
            assert(sub_element(subs0, s, subs_idx0, k + 1));
            assert(sub_element(subs1, s, subs_idx1, k));
        }
    }
    assert(subs1.len() == subs_idx1.len());
    assert(subs1.len() <= s.len());
}

pub proof fn proof_subs_push<T>(s: Seq<T>, subs: Seq<T>, subs_idx: Seq<int>, i: int)
    requires
        is_subseq_via_index(subs, s, subs_idx),
        0 <= i < s.len(),
        subs.len() < s.len(),
    ensures
        is_subseq_via_index(subs.push(s[i]), s, subs_idx.push(i)),
        subs.push(s[i]).len() == subs.len() + 1,
        subs_idx.push(i).len() == subs_idx.len() + 1,
{
    let (subs0, subs_idx0) = (subs, subs_idx);
    let subs1 = subs.push(s[i]);
    let subs_idx1 = subs_idx.push(i);
    assert(subs_idx1.len() == subs_idx0.len() + 1);
    assert forall|k: int| (0 <= k < subs_idx1.len()) implies sub_element(
        subs1,
        s,
        subs_idx1,
        k,
    ) by {
        assert(subs_idx1.len() == subs_idx0.len() + 1);
        if (0 <= k < subs_idx0.len()) {
            assert(0 <= k < subs0.len());
            assert(0 <= k < subs1.len());
            assert(sub_element(subs0, s, subs_idx0, k));
            assert(subs_idx0[k] == subs_idx1[k]);
            assert(subs0[k] === subs1[k]);
            assert(sub_element(subs1, s, subs_idx1, k));
        } else {
            assert(k == subs_idx0.len());
            assert(i == subs_idx1[k]);
            assert(s[i] === subs1[k]);
            assert(sub_element(subs1, s, subs_idx1, k));
        }
    }
    assert(subs1.len() == subs_idx1.len());
    assert(subs1.len() <= s.len());
    assert(is_subseq_via_index(subs1, s, subs_idx1));
}

pub proof fn lemma_remove_keep<T>(
    s: Seq<T>,
    keep: Seq<T>,
    removed: Seq<T>,
    keep_idx: Seq<int>,
    removed_idx: Seq<int>,
    i: int,
)  //-> (ret: (Seq<T>, Seq<T>, Seq<int>, Seq<int>))
    requires
        is_subseq_via_index(keep, s, keep_idx),
        is_subseq_via_index(removed, s, removed_idx),
        0 <= i < keep_idx.len(),
        keep.len() + removed.len() == s.len(),
    ensures
        is_subseq_via_index(keep.remove(i), s, keep_idx.remove(i)),
        is_subseq_via_index(
            removed.push(s[keep_idx[i]]),
            s,
            removed_idx.push(keep_idx[i]),
        ),
//ret === (keep.remove(i), removed.push(s[keep_idx[i]]), keep_idx.remove(i), removed_idx.push(keep_idx[i])),

{
    let keep0 = keep;
    let keep_idx0 = keep_idx;
    let (removed0, removed_idx0) = (removed, removed_idx);
    assert(sub_element(keep0, s, keep_idx0, i));
    assert(0 <= keep_idx0[i] < s.len());
    let removed1 = removed0.push(s[keep_idx0[i]]);
    let removed_idx1 = removed_idx0.push(keep_idx0[i]);
    let keep1 = keep0.remove(i);
    let keep_idx1 = keep_idx0.remove(i);
    proof_subs_remove(s, keep0, keep_idx0, i);
    proof_subs_push(s, removed0, removed_idx0, keep_idx0[i]);
}

pub proof fn proof_remove_keep<T>(
    s: Seq<T>,
    keep: Seq<T>,
    removed: Seq<T>,
    keep_idx: &mut Seq<int>,
    removed_idx: &mut Seq<int>,
    i: int,
)
    requires
        is_subseq_via_index(keep, s, *old(keep_idx)),
        is_subseq_via_index(removed, s, *old(removed_idx)),
        0 <= i < old(keep_idx).len(),
        keep.len() + removed.len() == s.len(),
    ensures
        is_subseq_via_index(keep.remove(i), s, *keep_idx),
        is_subseq_via_index(removed.push(s[old(keep_idx)[i]]), s, *removed_idx),
        *keep_idx === (old(keep_idx).remove(i)),
        *removed_idx === old(removed_idx).push(old(keep_idx)[i]),
{
    let (keep0, keep_idx0) = (keep, *keep_idx);
    let (removed0, removed_idx0) = (removed, *removed_idx);
    assert(sub_element(keep0, s, keep_idx0, i));
    assert(0 <= keep_idx0[i] < s.len());
    let removed1 = removed0.push(s[keep_idx0[i]]);
    let removed_idx1 = removed_idx0.push(keep_idx0[i]);
    let keep1 = keep0.remove(i);
    let keep_idx1 = keep_idx0.remove(i);
    proof_subs_remove(s, keep0, keep_idx0, i);
    proof_subs_push(s, removed0, removed_idx0, keep_idx0[i]);
    *keep_idx = keep_idx1;
    *removed_idx = removed_idx1;
}

} // verus!

================
File: ./source/verismo/src/tspec/seqlib/seq_multiset.rs
================

use vstd::multiset::Multiset;
use vstd::seq_lib::{lemma_multiset_commutative, lemma_seq_union_to_multiset_commutative};

use super::*;
verus! {

pub open spec fn seq_is_sorted<T>(s: Seq<T>, speclt: spec_fn(T, T) -> bool) -> bool {
    forall|i: int, j: int| 0 <= i < j < s.len() ==> !speclt(#[trigger] s[j], #[trigger] s[i])
}

pub proof fn proof_sorted_subrange<T>(
    s: Seq<T>,
    speclt: spec_fn(T, T) -> bool,
    start: int,
    end: int,
)
    requires
        0 <= start <= end <= s.len(),
    ensures
        seq_is_sorted(s, speclt) ==> seq_is_sorted(s.subrange(start, end), speclt),
{
    let ret = s.subrange(start, end);
    if seq_is_sorted(s, speclt) {
        assert forall|i: int, j: int| 0 <= i < j < ret.len() implies !speclt(
            #[trigger] ret[j],
            #[trigger] ret[i],
        ) by {
            assert(!speclt(s[j], s[i]));
        }
    }
}

pub proof fn seq_to_multi_set_to_set<T>(s1: Seq<T>)
    ensures
        s1.to_set() === s1.to_multiset().dom(),
{
    s1.to_multiset_ensures();
    assert(s1.to_set() =~~= s1.to_multiset().dom()) by {
        assert forall|a| s1.contains(a) == s1.to_multiset().dom().contains(a) by {
            assert(s1.contains(a) == (s1.to_multiset().count(a) > 0));
        }
    }
}

pub open spec fn seq_uop<A, B>(s: Seq<A>, op: spec_fn(A) -> B) -> Seq<B> {
    Seq::new(s.len(), |i| op(s[i]))
}

pub proof fn proof_seq_to_multiset_insert<A>(s1: Seq<A>, i: int, v: A)
    requires
        0 <= i <= s1.len(),
    ensures
        s1.insert(i, v).to_multiset() === s1.to_multiset().insert(v),
{
    let s2 = s1.insert(i, v);
    let left = s1.take(i);
    let middle = Seq::empty().push(v);
    let left2 = left + middle;
    let right = s1.skip(i);
    assert(s1 =~~= left + right);
    assert(s2 =~~= left + middle + right);
    assert(s2 === left2 + right);
    if i < s1.len() {
        left.to_multiset_ensures();
        lemma_multiset_commutative(left, middle);
        lemma_multiset_commutative(left2, right);
        lemma_seq_union_to_multiset_commutative(left2, right);
        lemma_seq_union_to_multiset_commutative(left, right);
        lemma_multiset_commutative(left + right, middle);
        lemma_multiset_commutative(left, right);
        (left + right).to_multiset_ensures();
        assert(left + right =~~= s1);
        assert(s1 + middle =~~= s1.push(v));
        s1.to_multiset_ensures();
        assert(s2.to_multiset() === (left2 + right).to_multiset());
        assert((left2 + right).to_multiset() === (right + left2).to_multiset());
        assert(left2.to_multiset() === left.to_multiset().add(middle.to_multiset()));
        // multiset distribution property
        assert((right + left2).to_multiset() =~~= right.to_multiset().add(left.to_multiset()).add(
            middle.to_multiset(),
        ));
        assert(s1.to_multiset().add(middle.to_multiset()) =~~= right.to_multiset().add(
            left.to_multiset(),
        ).add(middle.to_multiset()));
        assert(s2.to_multiset() === (left + right + middle).to_multiset());
    } else {
        assert(s2 =~~= s1.push(v));
        s1.to_multiset_ensures();
        assert(s1.insert(i, v).to_multiset() === s1.to_multiset().insert(v));
    }
}

//#[verifier(external_body)]
pub proof fn proof_seq_to_seq_eq_multiset<A, B>(s1: Seq<A>, s2: Seq<A>, op: spec_fn(A) -> B)
    requires
        s1.to_multiset() === s2.to_multiset(),
        s1.len() == s2.len(),
    ensures
        seq_uop(s1, op).to_multiset() === seq_uop(s2, op).to_multiset(),
    decreases s1.len(),
{
    if s1.len() > 0 {
        let v = s1.last();
        let ss1 = s1.drop_last();
        assert(s1 =~~= ss1.push(v));
        s1.to_multiset_ensures();
        s2.to_multiset_ensures();
        assert(s1[s1.len() - 1] == v);
        assert(s1.contains(v));
        assert(s1.to_multiset().count(v) > 0);
        assert(s2.contains(v));
        let i = choose|i| s2[i] === v && 0 <= i < s2.len();
        let ss2 = s2.remove(i);
        assert(s2 =~~= ss2.insert(i, v));
        assert(seq_uop(s2, op) =~~= seq_uop(ss2, op).insert(i, op(v)));
        proof_seq_to_multiset_insert(seq_uop(ss2, op), i, op(v));
        assert(seq_uop(ss1, op).push(op(v)) =~~= seq_uop(s1, op));
        seq_uop(ss1, op).to_multiset_ensures();
        proof_seq_to_multiset_insert(ss2, i, v);
        ss1.to_multiset_ensures();
        assert(ss2.to_multiset().insert(v) =~~= s2.to_multiset());
        assert(ss1.to_multiset().insert(v) =~~= s1.to_multiset());
        assert(ss2.to_multiset() =~~= s2.to_multiset().remove(v));
        assert(ss1.to_multiset() =~~= s1.to_multiset().remove(v));
        assert(ss1.to_multiset() =~~= ss2.to_multiset());
        proof_seq_to_seq_eq_multiset(ss1, ss2, op);
    } else {
        assert(s1 =~~= s2);
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/security/sectype.rs
================

use core::marker::PhantomData;

use super::*;
use crate::tspec::cast::VTypeCast;
use crate::tspec::*;

seq_macro::seq! {N in 1..=4 {
verus! {

#[verifier::reject_recursive_types(T)]
#[verifier::reject_recursive_types(M)]
#[repr(C)]
pub struct SecType<T, M> {
    val: T,
    view: Ghost<SpecSecType<T, M>>,
}

#[is_variant]
pub enum DataLabel {
    Symbol,
    Unknown,
    TrustedRandom,
    Secret,
}

#[verifier::reject_recursive_types(T)]
#[verifier::reject_recursive_types(M)]
#[derive(SpecSetter, SpecGetter)]
pub ghost struct SpecSecType<T, M> {
    pub val: T,
    pub _unused: Option<M>,
    pub valsets: Map<nat, Set<T>>,  // vmpl1 -> guess space
    // When force_secret[vmpl] ==> valsets[vmpl]!=1,
    // This is used to ensure upgrade function and downgrade functions.
    // Symbol might be secret or non-secret depending on the valset.
    // Allowed upgrade:
    // others == dependent or concrete
    // TrustedRandom ==> Symbol,
    // Dependent && is_full() ===> Symbol,
    // secret1 + secret2  ==> dependent1,
    // secret1 * secret2  ==> dependent2,
    // unknown + secret2 ==> dependent3,
    // unknown - depenent ==> dependent4,
    // TrustedRandom ==> Concrete,
    // concrete op concrete ==> concrete,
    // concrete op others ==> dep,
    // dependent1 && len() == 1 ==> concrete,
    pub labels: Map<nat, DataLabel>,
}

pub trait IsFullSecret {
    spec fn is_fullsecret_to(&self, vmpl: nat) -> bool;
}

pub trait IsFullSecretToAll {
    spec fn is_fullsecret(&self) -> bool;
}

impl<T: IsFullSecret> IsFullSecretToAll for T {
    open spec fn is_fullsecret(&self) -> bool {
        #(&&& self.is_fullsecret_to(N))*
    }
}

impl<T, M> ExecStruct for SecType<T, M> {

}

//impl<T, M> ExecStruct for SpecSecType<T, M>{}
pub trait SecMemType<T, M> {
    spec fn view(&self) -> SpecSecType<T, M>;
}

impl<T, M> SecMemType<T, M> for SecType<T, M> {
    closed spec fn view(&self) -> SpecSecType<T, M> {
        self.view@.spec_set_val(self.val)
    }
}

impl<T: core::clone::Clone, M> core::clone::Clone for SecType<T, M> {
    #[verifier(external_body)]
    fn clone(&self) -> (ret: Self)
        ensures
            *self === ret,
    {
        SecType { val: self.val.clone(), view: Ghost(self@) }
    }
}

impl<T: core::marker::Copy, M> core::marker::Copy for SecType<T, M> {

}

impl<T, M> SecType<T, M> {
    /// Iff valset is full or the data is a trusted random val.
    pub spec fn spec_new(val: SpecSecType<T, M>) -> Self;

    pub open spec fn call_self(self) -> Self {
        self
    }

    #[verifier(external_body)]
    pub broadcast proof fn axiom_spec_new(val: SpecSecType<T, M>)
        ensures
            (#[trigger] Self::spec_new(val))@ === val,
    {
    }

    #[verifier(external_body)]
    pub broadcast proof fn axiom_ext_equal(val: SecType<T, M>, val2: SecType<T, M>)
        ensures
            (val@ === val2@) == (val === val2),
    {
    }

    #[inline(always)]
    #[verifier(external_body)]
    pub exec fn upgrade_secret(&mut self, Ghost(vmpl): Ghost<nat>)
        requires
            old(self)@.valsets[vmpl] =~~= Set::full() || old(self)@.labels[vmpl].is_TrustedRandom(),
        ensures
            self@ == old(self)@.spec_set_labels(old(self)@.labels.insert(vmpl, DataLabel::Secret)),
    {
    }

    #[inline(always)]
    #[verifier(external_body)]
    // TODO: enable precondition checking
    // making a unknown data to public to vmpl/hv.
    // Data with other labels cannot become public through this API.
    pub exec fn downgrade_security(&mut self, Ghost(vmpl): Ghost<nat>)
        requires
            old(self).wf_value(),
        ensures
            self@ == old(self)@.spec_set_labels(old(self)@.labels.insert(vmpl, DataLabel::Symbol)),
            self@ == old(self)@.spec_set_valsets(
                old(self)@.valsets.insert(vmpl, self@.valsets[vmpl]),
            ),
            self@.valsets[vmpl].len() == 1,
            self@.valsets[vmpl] =~~= set![self@.val],
            self.wf_value(),
    {
    }

    pub exec fn declassify(
        &mut self,
    )/*requires
            !old(self)@.labels[1].is_Secret(),
            !old(self)@.labels[2].is_Secret(),
            !old(self)@.labels[3].is_Secret(),
            !old(self)@.labels[4].is_Secret(),*/

        requires
            old(self).wf_value(),
        ensures
            self@.labels[1].is_Symbol(),
            self@.labels[2].is_Symbol(),
            self@.labels[3].is_Symbol(),
            self@.labels[4].is_Symbol(),
            self@.is_constant(),
            self@ == old(self)@.spec_set_valsets(self@.valsets).spec_set_labels(self@.labels),
            self.wf_value(),
    {
        self.downgrade_security(Ghost(1));
        self.downgrade_security(Ghost(2));
        self.downgrade_security(Ghost(3));
        self.downgrade_security(Ghost(4));
    }

    #[inline(always)]
    #[verifier(external_body)]
    pub exec fn reveal_value(self) -> (ret: T)
        requires
            self.is_constant(),
        ensures
            self@.val == ret,
    {
        self.val
    }

    pub closed spec fn wf_value(&self) -> bool {
        &&& self@.wf_value()
        &&& self.val === self@.val
    }
}

impl<T, M> IsFullSecret for SpecSecType<T, M> {
    open spec fn is_fullsecret_to(&self, vmpl: nat) -> bool {
        self.valsets[vmpl] =~~= Set::full()
    }
}

impl<T, M> IsFullSecret for SecType<T, M> {
    open spec fn is_fullsecret_to(&self, vmpl: nat) -> bool {
        self@.is_fullsecret_to(vmpl)
    }
}

impl<T, M> IsConstant for SpecSecType<T, M> {
    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        &&& self.valsets[vmpl].len() == 1
        &&& self.valsets[vmpl] =~~= set![self.val]
        &&& self.labels[vmpl].is_Symbol()
        &&& self.wf_value()
    }
   open spec fn is_constant(&self) -> bool {
        #(
            &&& self.is_constant_to(N)
        )*
    }
}

impl<T, M> SpecSecType<T, M> {
    pub proof fn proof_constant(&self)
        requires
            self.is_constant(),
        ensures
            *self === SpecSecType::constant(self.val),
    {
        let expected = SpecSecType::<T, M>::constant(self.val);
        assert(self.valsets =~~= expected.valsets) by {
            assert(expected.valsets.dom() =~~= self.valsets.dom());
            assert forall|vmpl: nat| expected.valsets.contains_key(vmpl) implies self.valsets[vmpl]
                === expected.valsets[vmpl] by {
                assert(0 < vmpl <= 4);
                assert(self.valsets.contains_key(vmpl));
                assert(self.valsets[vmpl] =~~= expected.valsets[vmpl]);
            }
        }
        assert(self.labels =~~= expected.labels) by {
            assert(expected.labels.dom() =~~= self.labels.dom());
            assert forall|vmpl: nat|
                expected.labels.contains_key(vmpl) == self.labels.contains_key(vmpl) by {};
            assert forall|vmpl: nat| expected.labels.contains_key(vmpl) implies self.labels[vmpl]
                === expected.labels[vmpl] by {
                assert(0 < vmpl <= 4);
                assert(self.labels.contains_key(vmpl));
                assert(self.labels[vmpl] =~~= expected.labels[vmpl]);
            }
        }
        assert(self.val =~~= expected.val);
    }
}

impl<T, M> SecType<T, M> {
    pub proof fn proof_constant_wf(val: T)
        ensures
            SecType::spec_constant(val).wf(),
    {
    }
}

impl<T, M> IsConstant for SecType<T, M> {
    #[verifier(inline)]
    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        &&& self@.is_constant_to(vmpl)
        &&& self.wf_value()
    }

    open spec fn is_constant(&self) -> bool {
        &&& self@.is_constant()
        &&& self.wf_value()
    }
}

impl<T, M> SpecSecType<T, M> {
    pub open spec fn call_self(self) -> Self {
        self
    }

    pub open spec fn wf_vmpl(
        valsets: Map<nat, Set<T>>,
        labels: Map<nat, DataLabel>,
        vmpl: nat,
    ) -> bool {
        //&&& valsets[vmpl] =~~= Set::full() ==> labels[vmpl].is_Symbol()
        &&& labels[vmpl].is_TrustedRandom() ==> valsets[vmpl] =~~= Set::full()
        &&& labels[vmpl].is_Secret() ==> valsets[vmpl] =~~= Set::full()
        &&& labels.contains_key(vmpl)
        &&& valsets.contains_key(vmpl)
        &&& valsets[vmpl].len() > 0
        &&& valsets[vmpl].finite()
    }

    pub open spec fn wf_value(&self) -> bool {
        #(
            &&& Self::wf_vmpl(self.valsets, self.labels, N)
        )*
        &&& self._unused === None
        &&& self.valsets.dom() =~~= set![1,2,3,4]
        &&& self.labels.dom() =~~= set![1,2,3,4]
    }

    pub open spec fn bop_new<Rhs, T2>(
        self,
        rhs: SpecSecType<Rhs, M>,
        op: spec_fn(T, Rhs) -> T2,
    ) -> SpecSecType<T2, M> {
        SpecSecType {
            val: op(self.val, rhs.val),
            _unused: rhs._unused,
            valsets: Map::new(
                |vmpl| 1 <= vmpl <= 4,
                |vmpl| set_op(self.valsets[vmpl], rhs.valsets[vmpl], op),
            ),
            labels: Map::new(|vmpl| 1 <= vmpl <= 4, |vmpl| DataLabel::Symbol),
        }
    }

    pub proof fn proof_bop_new<Rhs, T2>(
        self,
        rhs: SpecSecType<Rhs, M>,
        op: spec_fn(T, Rhs) -> T2,
    ) -> (ret: SpecSecType<T2, M>)
        requires
            self.wf_value(),
            rhs.wf_value(),
    //Set::<T2>::full().finite(),

        ensures
            ret === self.bop_new(rhs, op),
            ret.wf_value(),
            self._is_constant() && rhs._is_constant() ==> ret._is_constant(),
    {
        let ret = self.bop_new(rhs, op);
        assert forall|i| 1 <= i <= 4 implies ret.valsets[i].len() <= self.valsets[i].len()
            * rhs.valsets[i].len() && ret.valsets[i].len() >= 1 && ret.valsets[i].finite()
            && #[trigger] SpecSecType::<T2, M>::wf_vmpl(ret.valsets, ret.labels, i) by {
            lemma_setop_len(self.valsets[i], rhs.valsets[i], op);
            assert(ret.valsets[i].len() <= self.valsets[i].len() * rhs.valsets[i].len());
            assert(ret.valsets[i].len() >= 1);
            assert(ret.valsets[i].finite());
            assert(SpecSecType::<T2, M>::wf_vmpl(ret.valsets, ret.labels, i));
            //assert(set_op(self.valsets[i], rhs.valsets[i], op).len() <= 1);
        }
        assert(ret.wf_value());
        if self._is_constant() && rhs._is_constant() {
            assert(ret._is_constant()) by {
                assert forall|i| 1 <= i <= 4 implies (#[trigger] ret.valsets[i]).len() == 1 by {
                    lemma_setop_len(self.valsets[i], rhs.valsets[i], op);
                    assert(1 * 1 == 1);
                }
            }
        }
        ret
    }

    pub proof fn proof_uop_valset<T2>(self, op: spec_fn(T) -> T2) -> (ret: SpecSecType<T2, M>)
        requires
            self.wf_value(),
    //Set::<T2>::full().finite(),

        ensures
            ret === self.uop_new(op),
            ret.wf_value(),
            self._is_constant() ==> ret._is_constant(),
    {
        self.proof_bop_new::<T, T2>(SpecSecType::constant(arbitrary()), uop_to_bop(op))
    }

    #[verifier(inline)]
    pub open spec fn uop_new<T2>(self, op: spec_fn(T) -> T2) -> SpecSecType<T2, M> {
        self.bop_new(SpecSecType::constant(arbitrary::<T>()), uop_to_bop(op))
    }

    pub closed spec fn secval_eq_at(&self, rhs: Self, vmpl: nat) -> bool {
        &&& (#[trigger] self.valsets[vmpl]).len() == rhs.valsets[vmpl].len()
    }

    pub open spec fn sec_eq(&self, rhs: Self) -> bool {
        #(
            &&& self.valsets[N].len() == rhs.valsets[N].len()
        )*
    }

    pub open spec fn _is_constant(&self) -> bool {
        &&& self.sec_eq(Self::constant(Set::<T>::full().choose()))
    }

    // Create a constant value
    pub open spec fn constant(val: T) -> Self {
        SpecSecType {
            val,
            _unused: None,
            valsets: Map::new(|vmpl| 1 <= vmpl <= 4, |vmpl| Set::<T>::empty().insert(val)),
            labels: Map::new(|vmpl| 1 <= vmpl <= 4, |vmpl| DataLabel::Symbol),
        }
    }
}

impl<T, M> SecType<T, M> {
    #[inline(always)]
    pub const fn constant(val: T) -> (ret: Self)
        ensures
            ret@ === SpecSecType::constant(val),
            ret.is_constant(),
            ret === SecType::spec_constant(val),
    {
        Self { val, view: Ghost(SpecSecType::constant(val)) }
    }

    #[inline(always)]
    pub const fn new(val: T) -> (ret: Self)
        ensures
            ret@ === SpecSecType::constant(val),
            ret.is_constant(),
    {
        Self::constant(val)
    }

    #[verifier(inline)]
    pub open spec fn spec_constant(val: T) -> Self {
        Self::spec_new(SpecSecType::constant(val))
    }
}

} // verus!
}
}
#[macro_use]
macro_rules! impl_binary_ops_trait_spec_fn {
    ($trt: tt, $baset: ty, $rhs: ty, $out: ty, $fname: ident, $spec_fn: ident) => {
        paste::paste! {
                verus!{
            impl<M> crate::tspec::ops::[<V $trt>]<SpecSecType<$rhs, M>, SpecSecType<$out, M>> for SpecSecType<$baset, M>{
                open spec fn [<$fname>](self, rhs: SpecSecType<$rhs, M>) -> SpecSecType<$out, M> {
                    self.bop_new(rhs, $spec_fn())
                }
            }

            impl<M> crate::tspec::ops::[<V $trt>]<SecType<$rhs, M>, SecType<$out, M>> for SecType<$baset, M>{
                #[verifier(inline)]
                open spec fn [<$fname>](self, rhs: SecType<$rhs, M>) -> SecType<$out, M> {
                    SecType::spec_new(self@.$fname(rhs@))
                }
            }
        }
            }
    };
}

#[macro_use]
macro_rules! impl_binary_ops_trait {
    ($trt: tt, $baset: ty, $rhs: ty, $out: ty, $fname: ident) => {
        paste::paste! {
            impl_binary_ops_trait_spec_fn!{
                $trt, $baset, $rhs, $out, $fname, [<fn_ $fname _ $baset _ $rhs _ $out>]
            }
        }
    };
}
// pattern for i64, u64, int, nat
#[macro_use]
macro_rules! impl_ord_for_all_types {
    ($baset: ty) => {
        verus! {
            impl<M> IntOrd for SpecSecType<$baset, M>
            {
                #[verifier(inline)]
                open spec fn ord_int(&self) -> int {
                    self.val as int
                }
            }

            impl<M> IntOrd for SecType<$baset, M>
            {
                #[verifier(inline)]
                open spec fn ord_int(&self) -> int {
                    self@.ord_int()
                }
            }
        }
    };
}

#[macro_use]
macro_rules! impl_cmp_ops_for_stype {
    ($baset: ty, $rhs: ty, [$([$fname: ident, $op: tt, $trt: ident]),* $(,)*]) => {
        paste::paste! {verus!{$(
            impl<M> SpecSecType<$baset, M> {
                #[verifier(inline)]
                pub open spec fn [<is_secure_ $fname>](self, other: SpecSecType<$rhs, M>) -> bool {
                    self.sec_eq(other)
                }

                pub proof fn [<lemma_const_is_secure_ $fname>](self, other: SpecSecType<$rhs, M>)
                ensures
                    self.is_constant() && other.is_constant() ==> self.[<is_secure_ $fname>](other),
                {}
            }

            impl<M> vops::$trt<SecType<$rhs, M>> for SecType<$baset, M> {
                #[inline(always)]
                exec fn $fname(&self, rhs: &SecType<$rhs, M>) -> (ret: bool)
                requires
                    self@.sec_eq(rhs@),
                    self.wf_value(),
                    rhs.wf_value(),
                ensures
                    (self@.val $op rhs@.val) == ret,
                {
                    self.val $op rhs.val
                }
            }

            impl<M> vops::$trt<$rhs> for SecType<$baset, M> {
                #[inline(always)]
                exec fn $fname(&self, rhs: &$rhs) -> (ret: bool)
                requires
                    self@.sec_eq(Self::spec_constant(*rhs)@),
                    self.wf_value(),
                ensures
                    (self@.val $op Self::spec_constant(*rhs)@.val) == ret,
                {
                    self.$fname(&Self::constant(*rhs))
                }
            }

            impl<M> SecType<$baset, M> {
                #[inline(always)]
                pub exec fn [<sec_ $fname>](&self, rhs: &SecType<$rhs, M>) -> (ret: SecType<bool, M>)
                requires
                    self.wf_value(),
                    rhs.wf_value(),
                ensures
                    ret@ === self@.bop_new(rhs@, |val1: $baset, val2: $rhs| val1 $op val2),
                    ret@ === self@.bop_new(rhs@, [<fn_spec_ $fname _ $baset _ $rhs _ bool>]()),
                    ret.wf_value(),
                {
                    proof {
                        self@.proof_bop_new(rhs@, [<fn_spec_ $fname _ $baset _ $rhs _ bool>]());
                    }
                    SecType {
                        val: self.val $op rhs.val,
                        view: Ghost(self@.bop_new(rhs@, [<fn_spec_ $fname _ $baset _ $rhs _ bool>]()))
                    }
                }
            }
        )*}
}
    };
}

#[macro_use]
macro_rules! impl_exe_bops_for_stype {
    ($baset: ty, [$([$fname: ident, $op: tt, $trt: ident, $specout: ty, ($check:tt $val: expr), $use_cast: ident]),* $(,)*]) => {
        paste::paste! {verus!{$(
            impl<M> SecType<$baset, M> {
                #[inline(always)]
                fn [<_ $fname>](self, other: Self) -> (ret: Self)
                requires
                    self.wf_value(),
                    (self@.val $op other@.val) as $baset == self@.val $op other@.val,
                    other@.val $check $val,
                ensures
                    ret@ === (self@ $op other@).$use_cast(),
                    (self $op other)@ === (self@ $op other@),
                    ret@.val == self@.val $op other@.val,
                    //ret@ === (self $op other)@.vspec_cast_to(),
                {
                    proof {
                        assert(VTypeCast::<SpecSecType<$baset, M>>::vspec_cast_to((self $op other)@) ===
                        VTypeCast::<SpecSecType<$baset, M>>::vspec_cast_to(self@ $op other@)) by {
                            assert(self $op other === SecType::spec_new(self@ $op other@));
                            let v1 = (self $op other)@;
                            let v2 = self@ $op other@;
                            assert(v1 === v2);
                        }
                    }
                    let ghost view: SpecSecType<$baset, M> = (self@ $op other@).$use_cast();
                    SecType {
                        val: self.val $op other.val,
                        view: Ghost(view),
                    }
                }
            }

            impl<M> core::ops::$trt<SecType<$baset, M>> for SecType<$baset, M> {
                type Output = Self;
                #[inline(always)]
                exec fn $fname(self, other: Self) -> (ret: Self)
                requires
                    self.wf_value(),
                    other.wf_value(),
                    (self@.val $op other@.val) as $baset == self@.val $op other@.val,
                    other@.val $check $val
                ensures
                    ret@ === (self@ $op other@).$use_cast(),
                    ret@.val == self@.val $op other@.val,
                    (self.is_constant() && other.is_constant()) ==> ret.is_constant(),
                    ret.wf_value(),
                {
                    proof {
                        self@.proof_bop_new(other@, [<fn_spec_ $fname _ $baset _ $baset _ $specout>]());
                        let ret: SpecSecType<$baset, M> = (self@ $op other@).proof_uop_valset(fn_vspec_cast_to());
                    }
                    self.[<_ $fname>](other)
                }
            }

            impl<M> core::ops::[<$trt Assign>]<SecType<$baset, M>> for SecType<$baset, M> {
                fn [<$fname _assign>](&mut self, other: SecType<$baset, M>)
                requires
                    old(self).wf_value(),
                    other.wf_value(),
                    (old(self)@.val $op other@.val) as $baset == old(self)@.val $op other@.val,
                    other@.val $check $val
                ensures
                    (old(self) $op other)@.$use_cast() === self@,
                    (old(self).is_constant() && other.is_constant()) ==> self.is_constant(),
                    self.wf_value(),
                {
                    *self = core::ops::$trt::<SecType<$baset, M>>::$fname(*self, other);
                }
            }

            impl<M> core::ops::$trt<SecType<$baset, M>> for $baset {
                type Output = Self;
                #[inline(always)]
                exec fn $fname(self, other: SecType<$baset, M>) -> (ret: Self)
                requires
                    other.wf_value(),
                    other.is_constant(),
                    (self $op other@.val) as $baset == self $op other@.val,
                    other@.val $check $val
                ensures
                    ret == self $op other@.val
                {
                    SecType::constant(self).$fname(other).reveal_value()
                }
            }

            impl<M> core::ops::$trt<$baset> for SecType<$baset, M> {
                type Output = Self;
                #[inline(always)]
                exec fn $fname(self, other: $baset) -> (ret: Self)
                requires
                    self.wf_value(),
                    (self@.val $op other) as $baset == self@.val $op other,
                    other $check $val
                ensures
                    (self@ $op SpecSecType::constant(other)).$use_cast() === ret@,
                    (self.is_constant()) ==> ret.is_constant(),
                    ret.wf_value(),
                {
                    self.$fname(Self::constant(other))
                }
            }
        )*

        }
}
    };
}

#[macro_use]
macro_rules! impl_exe_not_for_stype {
    ($baset: ty, [$([$fname: ident, $op: tt, $trt: ident]),* $(,)*]) => {
        paste::paste! {
                verus!{
        $(impl<M> crate::tspec::ops::[<VSpec $trt>] for SpecSecType<$baset, M> {
            open spec fn [<spec_ $fname>](self) -> Self {
                self.uop_new(fnspec::[<fn_spec_ $fname _ $baset _ $baset>]())
            }
        }

        impl<M> crate::tspec::ops::[<VSpec $trt>] for SecType<$baset, M> {
            #[verifier(inline)]
            open spec fn [<spec_ $fname>](self) -> SecType<$baset, M> {
                SecType::spec_new(self@.[<spec_ $fname>]())
            }
        }

        impl<M> [<V $trt>] for SecType<$baset, M> {
            type Output = SecType<$baset, M>;
            open spec fn [<requires_ $fname>](self) -> bool {
                self.wf_value()
            }

            open spec fn [<ensures_ $fname>](self, ret: Self) -> bool {
                &&& ret@  === self@.[<spec_ $fname>]()
                &&& self.is_constant() ==> ret.is_constant()
                &&& ret.wf_value()
            }

            #[inline(always)]
            exec fn $fname(self) -> (ret: Self)
            {
                proof {
                    (self@).proof_uop_valset([<fn_spec_ $fname _ $baset _ $baset>]());
                }
                self.[<_ $fname>]()
            }
        }

        impl<M> SecType<$baset, M> {
            #[inline(always)]
            exec fn [<_ $fname>](self) -> (ret: Self)
            ensures
                ret@ === self@.[<spec_ $fname>](),
            {
                Self {
                    val: $op self.val,
                    view: Ghost(self@.[<spec_ $fname>]()),
                }
            }
        }
        )*
        }
            }
    };
}

#[macro_export]
macro_rules! impl_exe_cast_to_sectype {
    ($baset: ty, [$($out: ty),*$(,)*]) => {
        verus!{
        impl<M> core::convert::Into<$baset> for SecType<$baset, M> {
            // Required method
            //#[verifier(external_body)]
            fn into(self) -> (ret: $baset)
            requires
                self.wf_value(),
                self.is_constant(),
            ensures
                ret == self@.val,
                ret === self.vspec_cast_to(),
            {
                self.val as $baset
            }
        }
        $(impl<M> core::convert::Into<SecType<$out, M>> for SecType<$baset, M> {
            // Required method
            #[verifier(external_body)]
            fn into(self) -> (ret: SecType<$out, M>)
            requires
                self.wf_value(),
            ensures
                ret === self.vspec_cast_to(),
                ret@ === self@.vspec_cast_to(),
                ret.wf_value(),
                self.is_constant() ==> ret.is_constant()
            {
                SecType{
                    val: self.val as $out,
                    view: Ghost(self@.vspec_cast_to()),
                }
            }
        }

        impl<M> core::convert::Into<$out> for SecType<$baset, M> {
            // Required method
            fn into(self) -> (ret: $out)
            requires
                self.wf_value(),
                self.is_constant(),
            ensures
                ret == self@.val as $out,
            {
                self.val as $out
            }
        }
        impl<M> core::convert::Into<SecType<$out, M>> for $baset {
            // Required method
            fn into(self) -> (ret: SecType<$out, M>)
            ensures
                ret === self.vspec_cast_to(),
                ret@ == SpecSecType::<$out, M>::constant(self as $out),
                ret.is_constant(),
            {
                SecType{
                    val: self as $out,
                    view: Ghost(SpecSecType::constant(self as $out)),
                }
            }
        }
    )*

    impl<M> core::convert::Into<SecType<$baset, M>> for $baset {
        // Required method
        fn into(self) -> (ret: SecType<$baset, M>)
        requires
            self.wf()
        ensures
            ret === self.vspec_cast_to(),
            ret@ === SpecSecType::<$baset, M>::constant(self),
            ret.is_constant(),
        {
            SecType{
                val: self,
                view: Ghost(SpecSecType::constant(self)),
            }
        }
    }
    }
    };
}

#[macro_export]
macro_rules! impl_exe_default {
    ($($baset: ty),*$(,)*) => {
    $(verus!{
    impl<M> Default for SecType<$baset, M>  {
        fn default() -> (ret: Self)
        ensures
            ret@ == SpecSecType::<$baset, M>::spec_default(),
            ret.is_constant(),
        {
            Self::constant(0)
        }
    }

    impl<M> SpecDefault for SpecSecType<$baset, M>  {
        #[verifier(inline)]
        open spec fn spec_default() -> Self {
            Self::constant(0)
        }
    }

    impl<M> SpecDefault for SecType<$baset, M>  {
        #[verifier(inline)]
        open spec fn spec_default() -> Self {
            Self::spec_constant(0)
        }
    }
    }
)*
    }
}

verus! {

impl<T1: VTypeCast<T2>, T2, M> VTypeCast<SpecSecType<T2, M>> for SpecSecType<T1, M> {
    open spec fn vspec_cast_to(self) -> SpecSecType<T2, M> {
        self.uop_new(fn_vspec_cast_to())
    }
}

impl<T1: VTypeCast<T2>, T2, M> VTypeCast<SecType<T2, M>> for SecType<T1, M> {
    #[verifier(inline)]
    open spec fn vspec_cast_to(self) -> SecType<T2, M> {
        SecType::spec_new(self@.vspec_cast_to())
    }
}

} // verus!
#[macro_export]
macro_rules! impl_cast_to_sectype {
    ($baset: ty, $out: ty) => {
        verus! {
            impl<M> VTypeCast<SpecSecType<$out, M>> for $baset {
                #[verifier(inline)]
                open spec fn vspec_cast_to(self) -> SpecSecType<$out, M> {
                    SpecSecType::constant(self.vspec_cast_to())
                }
            }

            impl<M> VTypeCast<SecType<$out, M>> for $baset {
                #[verifier(inline)]
                open spec fn vspec_cast_to(self) -> SecType<$out, M> {
                    SecType::spec_new(self.vspec_cast_to())
                }
            }
        }
    };
}

#[macro_export]
macro_rules! impl_cast_to_basics {
    ($baset: ty, $out: ty) => {
        verus! {
            impl<M> VTypeCast<$out> for SecType<$baset, M> {
                #[verifier(inline)]
                open spec fn vspec_cast_to(self) -> $out {
                    self@.vspec_cast_to()
                }
            }

            impl<M> VTypeCast<$out> for SpecSecType<$baset, M> {
                #[verifier(inline)]
                open spec fn vspec_cast_to(self) -> $out {
                    VTypeCast::<$out>::vspec_cast_to(self.val)
                }
            }
        }
    };
}

#[macro_export]
macro_rules! impl_cast_to {
    ($fromty: ty, [$($out: ty),*$(,)?]) => {
        $(
            impl_cast_to_basics!($fromty, $out);
            impl_cast_to_sectype!($fromty, $out);
        )*
    }
}

#[macro_export]
macro_rules! impl_exe_ops_for_stype {
    ($($baset: ty),* $(,)?) => {
        $(
            impl_cmp_ops_for_stype!($baset, $baset, [[gt, >, VGt], [lt, <, VLt], [le, <=, VLe], [ge, >=, VGe], [eq, ==, VEq]]);
            impl_exe_bops_for_stype!($baset,
                [
                    [add, +, Add, int, (>= 0), vspec_cast_to],
                    [sub, -, Sub, int, (>= 0), vspec_cast_to],
                    [mul, *, Mul, int, (>= 0), vspec_cast_to],
                    [div, /, Div, $baset, (!= 0), call_self],
                    [rem, %, Rem, $baset, (!= 0), call_self],
                    [shr, >>, Shr, $baset, (< (8 * spec_size::<$baset>())), call_self],
                    [shl, <<, Shl, $baset, (< (8 * spec_size::<$baset>())), call_self],
                    [bitxor, ^, BitXor, $baset, (>= 0), call_self],
                    [bitor, |, BitOr, $baset, (>= 0), call_self],
                    [bitand, &, BitAnd, $baset, (>= 0), call_self]
                ]);
            impl_exe_not_for_stype!($baset, [[not, !, Not]]);
        )*
    }
}

#[macro_export]
macro_rules! impl_spec_ops_for_stype {
    ($($baset: ty),*$(,)?) =>
    {
        $(
        impl_ord_for_all_types!($baset);
        impl_cast_to!($baset, [int, nat, usize, u64, u32, u16, u8, Seq<u8>]);
        impl_binary_ops_trait!(SpecAdd, $baset, $baset, int, spec_add);
        impl_binary_ops_trait!(SpecSub, $baset, $baset, int, spec_sub);
        impl_binary_ops_trait!(SpecMul, $baset, $baset, int, spec_mul);
        impl_binary_ops_trait!(SpecBitOr, $baset, $baset, $baset, spec_bitor);
        impl_binary_ops_trait!(SpecBitAnd, $baset, $baset, $baset, spec_bitand);
        impl_binary_ops_trait!(SpecBitXor, $baset, $baset, $baset, spec_bitxor);
        impl_binary_ops_trait!(SpecEuclideanDiv, $baset, $baset, $baset, spec_euclidean_div);
        impl_binary_ops_trait!(SpecEuclideanMod, $baset, $baset, $baset, spec_euclidean_mod);
        impl_binary_ops_trait!(SpecShl, $baset, $baset, $baset, spec_shl);
        impl_binary_ops_trait!(SpecShr, $baset, $baset, $baset, spec_shr);
        )*
    }
}

#[macro_export]
macro_rules! impl_spec_ops_for_seq {
    ($($baset: ty),*$(,)?) =>
    {
        $(
        //impl_cast_to!($baset, [usize, u64, u32, u16, u8, Seq<u8>]);
        impl_binary_ops_trait_spec_fn!(SpecAdd, $baset, $baset, $baset, spec_add, fn_spec_add_seq);
        )*
    }
}

// Pattern for nat
#[macro_export]
macro_rules! impl_ops_for_snat {
    ($($baset: ty,)*) =>
    {
        $(
        impl_ord_for_all_types!($baset);
        impl_cast_to!($baset, [int, nat, usize, u64, u32, u16, u8]);
        impl_binary_ops_trait!(SpecAdd, $baset, $baset, nat, spec_add);
        impl_binary_ops_trait!(SpecSub, $baset, $baset, int, spec_sub);
        impl_binary_ops_trait!(SpecMul, $baset, $baset, nat, spec_mul);
        impl_binary_ops_trait!(SpecEuclideanDiv, $baset, $baset, $baset, spec_euclidean_div);
        impl_binary_ops_trait!(SpecEuclideanMod, $baset, $baset, $baset, spec_euclidean_mod);
        )*
    }
}

// Pattern for sint
#[macro_export]
macro_rules! impl_ops_for_sint {
    ($($baset: ty,)*) =>
    {
        $(
        impl_ord_for_all_types!($baset);
        impl_cast_to!($baset, [int, nat, usize, u64, u32, u16, u8]);
        impl_binary_ops_trait!(SpecAdd, $baset, $baset, int, spec_add);
        impl_binary_ops_trait!(SpecSub, $baset, $baset, int, spec_sub);
        impl_binary_ops_trait!(SpecMul, $baset, $baset, int, spec_mul);
        impl_binary_ops_trait!(SpecEuclideanDiv, $baset, $baset, $baset, spec_euclidean_div);
        impl_binary_ops_trait!(SpecEuclideanMod, $baset, $baset, $baset, spec_euclidean_mod);
        )*
    }
}

/// verismo macro will replace basic types to secure types.
/// Using type alias ensures the macro will not replace the type.
#[macro_export]
macro_rules! impl_secure_type {
    ($memattr: ty, $($p: tt)*) =>
    {
        $($p )* u64_s = SecType<u64, $memattr>;
        $($p )* u32_s = SecType<u32, $memattr>;
        $($p )* u16_s = SecType<u16, $memattr>;
        $($p )* u8_s = SecType<u8, $memattr>;
        $($p )* usize_s = SecType<usize, $memattr>;
        $($p )* int_s = SecType<int, $memattr>;
        $($p )* nat_s = SecType<nat, $memattr>;
        $($p )* bool_s = SecType<bool, $memattr>;
        //$($p )* SecSeqByte = SecType<Seq<u8>, $memattr>;
        $($p )* SecSeqByte = Seq<SpecSecType<u8, $memattr>>;

        $($p )* u64_t = u64;
        $($p )* u32_t = u32;
        $($p )* u16_t = u16;
        $($p )* u8_t = u8;
        $($p )* usize_t = usize;
        $($p )* int_t = int;
        $($p )* nat_t = nat;
        $($p )* bool_t = bool;
    }
}

verus! {

impl<T> SecType<T, ()> {
    pub open spec fn wf(&self) -> bool {
        self.wf_value()
    }
}

} // verus!
impl_exe_cast_to_sectype!(u64, [usize, u32, u16, u8]);
impl_exe_cast_to_sectype!(u32, [usize, u64, u16, u8]);
impl_exe_cast_to_sectype!(u16, [usize, u64, u32, u8]);
impl_exe_cast_to_sectype!(u8, [usize, u64, u32, u16]);
impl_exe_cast_to_sectype!(usize, [u64, u32, u16, u8]);
impl_exe_default!(u8, u16, u32, u64, usize);
impl_exe_ops_for_stype! {u8, u16, u32, u64, usize}
impl_exe_not_for_stype!(bool, [[not, !, Not]]);
impl_spec_ops_for_stype! {u8, u16, u32, u64, usize}

impl_ops_for_snat! {nat,}
impl_ops_for_sint! {int,}

verus! {

pub trait VNot {
    type Output;

    spec fn requires_not(self) -> bool where Self: core::marker::Sized;

    spec fn ensures_not(self, ret: Self::Output) -> bool where Self: core::marker::Sized;

    fn not(self) -> (ret: Self::Output) where Self: core::marker::Sized
        requires
            self.requires_not(),
        ensures
            self.ensures_not(ret),
    ;
}

impl VNot for bool {
    type Output = bool;

    open spec fn requires_not(self) -> bool {
        true
    }

    open spec fn ensures_not(self, ret: bool) -> bool {
        self == !ret
    }

    fn not(self) -> bool {
        !self
    }
}

} // verus!
//impl_spec_ops_for_seq! {Seq<u8>}

================
File: ./source/verismo/src/tspec/security/sectype_test.rs
================

//impl_secure_type!{(), type}
use core::ops::*;

use super::*;

impl_secure_type! {(), type}
use vops::VEq;

mod p {
    use super::*;
    verus! {

// assert by cannot exist with broadcast forall with trait bound.
pub proof fn proof_test1(v1: u64, v2: u64)
    requires
        v1 < 10,
        v2 < 10,
    ensures
        v1 * v2 < 100,
{
    assert(v1 * v2 < 100) by (nonlinear_arith)
        requires
            v1 < 10,
            v2 < 10,
    ;
}

pub proof fn proof_test_bits2(v1: u64, v2: u64)
    requires
        v1 < 10,
        v2 < 10,
    ensures
        v1 & v2 < 10,
{
    assert(v1 & v2 < 10) by (bit_vector)
        requires
            v1 < 10,
            v2 < 10,
    ;
}

} // verus!
}

verismo! {
    fn test_add (v1: u64_s, v2: u64_s) -> (ret: u64_s)
    requires
        v1 + v2 <= MAXU64!(),
    {
        v1.add(v2)
    }

    fn test1(v1: u64_s, v2: u64_s) -> (ret: u64_s)
    requires
        v1 < 10,
        v2 < 10,
    ensures
        v1 * v2 < 100,
    {
        proof {p::proof_test1(v1 as u64, v2 as u64)}
        v1.add(v2)
    }

    fn test2 (v1: u64_s, v2: u64_s) -> (ret: u64_s)
    requires
        v1 * v2 <= MAXU64!(),
    {
        let v = 11;
        assert(v1 >= 0);
        assert(v2 >= 0);
        assert(v1 >= 0) by {
            assert(v1>=0)
        }
        v
    }
}

verismo! {
    proof fn proof_u64_s(v1: u64_s, v2: u64_s)
    requires
        v1 > v2,
    ensures
        (v1 + v2)@.val == (v1@.val + v2@.val),
        (v1 + v2)@.valsets[1] =~~= set_op(v1@.valsets[1], v2@.valsets[1], |v1: u64, v2: u64| (v1 + v2)),
        //(v1 + v2 - v2)@.val == (v1@.val) as u64
    {}

    /*proof fn test_bit(v1: u64_s, v2: u64_s)
    requires
        v2 == 11,
    ensures
        v1 >> v2 == v1 / (1u64_s << v2)
    {
        assert(v2 < 64);
        // bit-vector does not support call fn
        //assert(v1 << v2 == v1 / (1u64_s << v2)) by(bit_vector)
        //requires v2@.val < 64u64;

        assert(v1 >> v2 == v1 / (1u64_s << v2)) by {
            let val1: u64 = v1@.val;
            let val2: u64 = v2@.val;
            /*assert((v1 >> v2)@.val == val1 >> val2);
            assert(1u64_s@.val == 1u64);
            assert( (1u64_s << v2)@.val === (1u64 << val2));*/
            assert((v1 / (1u64_s << v2))@.val == val1 / (1u64 << val2));
            assert(val1 >> val2 == val1 / (1u64 << val2)) by(bit_vector)
            requires val2 == 11u64;
        }
    }*/
}

verismo_non_secret! {
    fn test_bits2(v1: u64_s, v2: u64_s) -> (ret: u64_s)
    requires
        v1 < 10,
        v2 < 10,
    ensures
        v1 & v2 < 10,
    {
        proof {p::proof_test_bits2(v1 as u64, v2 as u64)}

        if v1 & v2 == 4 {
            proof {
                assert(v1 & v2 == 4);
            }
        }

        if v1 & v2 != 4 {
            proof {
                assert(v1 & v2 != 4);
            }
        }
        v1 & v2
    }
}

verismo! {
    fn test_not(v1: u64_s) -> (ret: u64_s)
    requires
        v1 == 10,
    ensures
        ret@.val == !((v1@.val - 1) as u64),
    {
        let mask = v1 - 1;
        let ret = (!mask);
        ret
    }

    fn test_add2(v1: u64) -> (ret: u64)
    requires
        v1 == 0xff
    ensures
        ret == 0x100
    {
        v1 + 1
    }

    fn test_cast(v1: u64) -> (ret: u32)
    requires
        v1 == 0xff,
    ensures
        ret == 0xff,
        v1@.val == 0xff,
        ret@.val == 0xff,
    {
        v1 as u32
    }
}

================
File: ./source/verismo/src/tspec/security/mod.rs
================

#[macro_use]
mod sectype;
mod sectype_test;
mod seq;
pub use sectype::*;

use super::*;
use crate::*;

================
File: ./source/verismo/src/tspec/security/seq.rs
================

use super::*;

verus! {

pub open spec fn fn_spec_to_seq_index<T: VTypeCast<Seq<u8>>>(i: int) -> spec_fn(T) -> u8 {
    |v: T| VTypeCast::<Seq<u8>>::vspec_cast_to(v).index(i)
}

impl<T: VTypeCast<Seq<u8>>, M> VTypeCast<Seq<SpecSecType<u8, M>>> for SecType<T, M> {
    open spec fn vspec_cast_to(self) -> Seq<SpecSecType<u8, M>> {
        Seq::new(spec_size::<T>(), |i| self@.uop_new(fn_spec_to_seq_index(i)))
    }
}

impl<M> VTypeCast<Seq<SpecSecType<u8, M>>> for () {
    open spec fn vspec_cast_to(self) -> Seq<SpecSecType<u8, M>> {
        Seq::empty()
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/range_set.rs
================

use vstd::prelude::*;

use super::*;

verus! {

pub open spec fn range_to_set(first: int, n: nat) -> Set<int> {
    Set::new(|i: int| first <= i < first + (n as int))
}

pub open spec fn range(first: int, end: int) -> (int, nat) {
    if end > first {
        (first, (end - first) as nat)
    } else {
        (first, 0)
    }
}

#[verifier(inline)]
pub open spec fn range2set(range: (int, nat)) -> Set<int> {
    Set::new(|i: int| within_range(i, range))
}

pub trait VRange {
    spec fn to_set(self) -> Set<int> where Self: core::marker::Sized;

    spec fn end(self) -> int where Self: core::marker::Sized;
}

impl VRange for (int, nat) {
    #[verifier(inline)]
    open spec fn to_set(self) -> Set<int> {
        range2set(self)
    }

    #[verifier(inline)]
    open spec fn end(self) -> int {
        self.0 + self.1
    }
}

#[verifier(inline)]
pub open spec fn within_range(x: int, range: (int, nat)) -> bool {
    range.0 <= x < range.0 + range.1
}

#[verifier(inline)]
pub open spec fn inside_range(x: (int, nat), range: (int, nat)) -> bool {
    &&& range.0 <= x.0 < range.0 + range.1
    &&& x.0 + x.1 <= range.0 + range.1
}

pub open spec fn inside_ranges(r2: (int, nat), rs: Set<(int, nat)>) -> bool {
    exists|r1| #[trigger] rs.contains(r1) && inside_range(r2, r1)
}

#[verifier(inline)]
pub open spec fn after_range(x: (int, nat), range: (int, nat)) -> bool {
    &&& x.0 >= range.0 + range.1
}

pub proof fn lemma_to_set(first: int, n: nat) -> (ret: Set<int>)
    ensures
        ret.len() == n,
        ret.finite(),
        ret === range_to_set(first, n),
    decreases n,
{
    let ret = range_to_set(first, n);
    if n == 0 {
        assert forall|v: int| !range_to_set(first, n).contains(v) by {
            assert(!(first < v && v < first));
        }
        assert(range_to_set(first, n) =~= Set::empty());
        //assert(ret=~~=(Set::empty()));
    }
    if n > 0 {
        let prev = lemma_to_set(first, (n - 1) as nat);
        assert(range_to_set(first, n) =~~= (range_to_set(first, (n - 1) as nat).insert(
            first + (n - 1),
        ))) by {
            assert forall|v| range_to_set(first, n).contains(v) implies range_to_set(
                first,
                (n - 1) as nat,
            ).contains(v) || v === (first + (n - 1)) by {
                assert(first <= v < first + n);
                if (first <= v < first + n - 1) {
                    assert(range_to_set(first, (n - 1) as nat).contains(v));
                } else {
                    assert(v == first + n - 1);
                    assert(v == (first + (n - 1)));
                    assert(v == (first + (n - 1)));
                    assert(v === first + (n - 1));
                }
            }
            assert forall|v|
                range_to_set(first, (n - 1) as nat).contains(v) || v === (first + (n
                    - 1)) implies range_to_set(first, n).contains(v) by {}
        }
    }
    ret
}

pub open spec fn range_disjoint(f1: int, n1: nat, f2: int, n2: nat) -> bool {
    ||| n1 == 0
    ||| n2 == 0
    ||| (f1 >= f2 + n2 as int)
    ||| (f2 >= f1 + n1 as int)
}

pub open spec fn range_disjoint_(r1: (int, nat), r2: (int, nat)) -> bool {
    range_disjoint(r1.0, r1.1, r2.0, r2.1)
}

pub open spec fn ranges_disjoint(rs: Set<(int, nat)>, r2: (int, nat)) -> bool {
    forall|r1| #[trigger] rs.contains(r1) ==> range_disjoint_(r1, r2)
}

pub proof fn proof_range_set_disjoint(r1: (int, nat), r2: (int, nat)) -> (ret: bool)
    ensures
        ret === range_disjoint_(r1, r2),
        ret === range2set(r1).disjoint(range2set(r2)),
{
    lemma_range_set_disjoint(r1.0, r1.1, r2.0, r2.1)
}

pub proof fn lemma_ranges_disjoint_insert(r2: (int, nat), range: (int, nat), rs: Set<(int, nat)>)
    requires
        range_disjoint_(r2, range),
    ensures
        ranges_disjoint(rs, r2) == ranges_disjoint(rs.insert(range), r2),
{
    let rs2 = rs.insert(range);
    assert forall|r|
        inside_range(r, r2) && r.1 != 0 && ranges_disjoint(rs, r) implies ranges_disjoint(
        rs2,
        r,
    ) by {
        assert forall|v| rs2.contains(v) implies range_disjoint_(v, r) by {
            if v !== range {
                assert(rs.contains(v));
            } else {
                assert(range_disjoint_(r2, v));
                assert(range_disjoint_(v, r));
            }
        }
    }
    assert forall|r|
        inside_range(r, r2) && r.1 != 0 && ranges_disjoint(
            rs.insert(range),
            r,
        ) implies ranges_disjoint(rs, r) by {
        assert forall|v| rs.contains(v) implies range_disjoint_(v, r) by {
            assert(rs2.contains(v));
        }
    }
}

pub proof fn lemma_range_set_disjoint(f1: int, n1: nat, f2: int, n2: nat) -> (ret: bool)
    ensures
        ret === range_disjoint(f1, n1, f2, n2),
        ret === range_to_set(f1, n1).disjoint(range_to_set(f2, n2)),
{
    let ret = range_disjoint(f1, n1, f2, n2);
    let set1 = lemma_to_set(f1, n1);
    let set2 = lemma_to_set(f2, n2);
    if ret {
        assert forall|a| set1.contains(a) implies !set2.contains(a) by {}
    } else {
        assert(set1.contains(f1)) by {
            assert(f1 <= f1);
            assert(n1 > 0);
            assert(f1 < f1 + n1 as int);
        }
        assert(set2.contains(f2));
        if !set2.contains(f1) {
            assert(set1.contains(f2));
        }
        if !set1.contains(f2) {
            assert(set2.contains(f1));
        }
        assert(!set1.disjoint(set2));
    }
    ret
}

pub proof fn lemma_range_set_low_high(f1: int, n1: nat, n2: nat)
    requires
        n1 <= n2,
    ensures
        range_to_set(f1, n2) =~~= (range_to_set(f1, n1).union(
            range_to_set(f1 + n1 as int, (n2 - n1) as nat),
        )),
        range_to_set(f1, n1) =~~= (range_to_set(f1, n2).difference(
            range_to_set(f1 + n1 as int, (n2 - n1) as nat),
        )),
{
    let s1 = range_to_set(f1, n2);
    let s2 = range_to_set(f1, n1).union(range_to_set(f1 + n1 as int, (n2 - n1) as nat));
    assert forall|i| s1.contains(i) === s2.contains(i) by {}
}

pub proof fn proof_union_auto<int>()
    ensures
        forall|s1: Set<int>, s2: Set<int>, s3: Set<int>|
            s1.union(s2).union(s3) === s1.union(s2.union(s3)),
        forall|s1: Set<int>, s2: Set<int>| s1.union(s2) === s2.union(s1),
        forall|s1: Set<int>, s2: Set<int>| s1.subset_of(#[trigger] s1.union(s2)),
        forall|s1: Set<int>, s2: Set<int>| s2.subset_of(s1) ==> s1.union(s2) === s1,
{
    assert forall|s1: Set<int>, s2: Set<int>, s3: Set<int>|
        s1.union(s2).union(s3) =~~= (s1.union(s2.union(s3))) by {
        assert forall|a|
            s1.union(s2).union(s3).contains(a) === s1.union(s2.union(s3)).contains(a) by {}
    }
    assert forall|s1: Set<int>, s2: Set<int>| s1.union(s2) =~~= (s2.union(s1)) by {
        lemma_union(s1, s2);
    }
    assert forall|s1: Set<int>, s2: Set<int>| s1.subset_of(#[trigger] s1.union(s2)) by {
        lemma_union(s1, s2);
    }
    assert forall|s1: Set<int>, s2: Set<int>| s2.subset_of(s1) implies s1.union(s2) =~~= (s1) by {
        assert forall|a| s1.union(s2).contains(a) === #[trigger] s1.contains(a) by {}
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/stream/mod.rs
================

pub mod basic;

use vstd::prelude::*;

use super::size_s::*;
use super::*;

pub type Byte = u8;
pub type EncUnit = u128;
pub use basic::*;
pub type Stream<T> = Seq<T>;
pub type ByteStream = Seq<u8>;

verus! {

impl VSpecShr<nat, ByteStream> for ByteStream {
    #[verifier(inline)]
    open spec fn spec_shr(self, rhs: nat) -> Self {
        self.subrange(rhs as int, self.len() - rhs)
    }
}

pub open spec fn stream_to_data<T: VTypeCast<ByteStream>>(s: ByteStream) -> T {
    choose|ret: T| ret.vspec_cast_to() =~~= s
}

#[verifier(opaque)]
pub open spec fn stream_from_data<T: VTypeCast<ByteStream>>(data: T) -> ByteStream {
    data.vspec_cast_to()
}

pub proof fn lemma_from_data<T: VTypeCast<ByteStream>>(data: T) -> (ret: ByteStream)
    ensures
        ret === stream_from_data(data),
        stream_from_data(stream_to_data::<T>(ret)) === ret,
{
    reveal(stream_from_data);
    stream_from_data(data)
}

} // verus!

================
File: ./source/verismo/src/tspec/stream/basic.rs
================

use super::*;
use crate::*;

verus! {

#[verifier(inline)]
pub open spec fn bool_to_stream(data: bool) -> ByteStream {
    u8_to_stream(
        if data {
            1u8
        } else {
            0u8
        },
    )
}

#[verifier(inline)]
pub open spec fn char_to_stream(data: char) -> ByteStream {
    u8_to_stream(data as u8)
}

//#[verifier(inline)]
pub open spec fn u8_to_stream(data: u8) -> ByteStream {
    Seq::new(
        1,
        |i|
            if i == 0 {
                data
            } else {
                0u8
            },
    )
}

#[verifier(inline)]
pub open spec fn ghost_to_stream<T>(data: Ghost<T>) -> ByteStream {
    Stream::new(0, |i| 0u8)
}

#[verifier(inline)]
pub open spec fn u16_to_stream(data: u16) -> ByteStream {
    (u8_to_stream(data as u8) + u8_to_stream((data / 0x100) as u8))
}

#[verifier(inline)]
pub open spec fn u32_to_stream(data: u32) -> ByteStream {
    (u16_to_stream(data as u16).add(u16_to_stream((data / 0x10000) as u16)))
}

#[verifier(inline)]
pub open spec fn u64_to_stream(data: u64) -> ByteStream {
    u32_to_stream(data as u32).add(u32_to_stream((data / 0x1_0000_0000) as u32))
}

#[verifier(inline)]
pub open spec fn u128_to_stream(data: u128) -> ByteStream {
    u64_to_stream(data as u64).add(u64_to_stream((data / 0x10000_0000) as u64))
}

#[verifier(inline)]
pub open spec fn usize_to_stream(data: usize) -> ByteStream {
    //if arch_word_bits() == 8 {
    u64_to_stream(data as u64)
    //} else {
    //    u32_to_stream(data as u32)
    //}

}

#[verifier(inline)]
pub open spec fn bool_from_stream(data: ByteStream) -> bool {
    data[0] != 0
}

//#[verifier(inline)]
pub open spec fn char_from_stream(data: ByteStream) -> char;

//#[verifier(inline)]
pub open spec fn u8_from_stream(data: ByteStream) -> u8 {
    data[0]
}

#[verifier(inline)]
pub open spec fn u16_from_stream(data: ByteStream) -> u16 {
    (u8_from_stream(data.subrange(0, 1)) + u8_from_stream(data.subrange(1, 2)) * 0x100) as u16
}

#[verifier(inline)]
pub open spec fn u32_from_stream(data: ByteStream) -> u32 {
    (u16_from_stream(data.subrange(0, 2)) + u16_from_stream(data.subrange(2, 4)) * 0x10000) as u32
}

#[verifier(inline)]
pub open spec fn u64_from_stream(data: ByteStream) -> u64 {
    (u32_from_stream(data.subrange(0, 4)) + u32_from_stream(data.subrange(4, 8))
        * 0x10000_0000) as u64
}

#[verifier(inline)]
pub open spec fn u128_from_stream(data: ByteStream) -> u128 {
    (u64_from_stream(data.subrange(0, 8)) + u64_from_stream(data.subrange(8, 16))
        * 0x10000_0000_0000_0000) as u128
}

#[verifier(inline)]
pub open spec fn usize_from_stream(data: ByteStream) -> usize {
    //if arch_word_bits() == 8 {
    u64_from_stream(data) as usize
    //} else {
    //    u32_from_stream(data as u32)
    //}

}

pub proof fn proof_u64_non_zero(data: u64)
    requires
        forall|i| 0 <= i < 8 ==> #[trigger] (u64_to_stream(data)[i]) == 0,
    ensures
        data == 0,
{
    let l = data as u32;
    let h = (data / 0x10000_0000) as u32;
    if data != 0 {
        assert forall|i| 0 <= i < 4 implies #[trigger] (u32_to_stream(l)[i]) == 0 by {
            assert(u64_to_stream(data)[i] == 0);
        }
        proof_u32_non_zero(l);
        assert forall|i| 0 <= i < 4 implies #[trigger] (u32_to_stream(h)[i]) == 0 by {
            assert(u64_to_stream(data)[i + 4] == 0);
        }
        proof_u32_non_zero(h);
    }
}

pub proof fn proof_u32_non_zero(data: u32)
    requires
        forall|i| 0 <= i < 4 ==> #[trigger] (u32_to_stream(data)[i]) == 0,
    ensures
        data == 0,
{
    let l = data as u16;
    let h = (data / 0x10000) as u16;
    assert forall|i| 0 <= i < 2 implies #[trigger] (u16_to_stream(l)[i]) == 0 by {
        assert(u32_to_stream(data)[i] == 0);
    }
    proof_u16_non_zero(l);
    assert forall|i| 0 <= i < 2 implies #[trigger] (u16_to_stream(h)[i]) == 0 by {
        assert(u32_to_stream(data)[i + 2] == 0);
    }
    proof_u16_non_zero(h);
}

pub proof fn proof_u16_non_zero(data: u16)
    requires
        forall|i| 0 <= i < 2 ==> (#[trigger] u16_to_stream(data)[i]) == 0,
    ensures
        data == 0,
{
    let l = data as u8;
    let h = (data / 0x100) as u8;
    assert(u16_to_stream(data)[0] == 0);
    assert(u16_to_stream(data)[1] == 0);
    proof_u8_non_zero(l);
    proof_u8_non_zero(h);
}

pub proof fn proof_u8_non_zero(data: u8)
    requires
        u8_to_stream(data)[0] == 0,
    ensures
        data == 0,
{
}

} // verus!

================
File: ./source/verismo/src/tspec/cast.rs
================

use super::*;
use crate::tspec_e::SecSeqByte;
verus! {

pub trait VTypeCast<T> {
    // verus macro transform a as int to vspec_cast_to::<_, int>(a)
    spec fn vspec_cast_to(self) -> T where Self: core::marker::Sized;
}

pub trait SpecInto<T> {
    spec fn spec_into(self) -> T where Self: core::marker::Sized;
}

pub open spec fn is_castable<T1: IsConstant + SpecSize, T2: IsConstant + SpecSize>(t1: T1) -> bool {
    &&& spec_size::<T1>() == spec_size::<
        T2,
    >()/*&&& exists |t2: T2|
            t1.is_constant_to(1) == t2.is_constant_to(1)
            && t1.is_constant_to(2) == t2.is_constant_to(2)
            && t1.is_constant_to(3) == t2.is_constant_to(3)
            && t1.is_constant_to(4) == t2.is_constant_to(4)*/

}

impl<T2, T1: VTypeCast<T2>> SpecInto<T2> for T1 {
    open spec fn spec_into(self) -> T2 {
        self.vspec_cast_to()
    }
}

#[verifier(external_body)]
pub broadcast proof fn axiom_cast_to_seq_unique<T: VTypeCast<SecSeqByte>>(val: T)
    ensures
        val === VTypeCast::<SecSeqByte>::vspec_cast_to(val).vspec_cast_to(),
{
}

#[verifier(external_body)]
broadcast proof fn axiom_into_conversion_bytes<T1: VTypeCast<SecSeqByte> + IsConstant>(
    b: SecSeqByte,
)
    ensures
        VTypeCast::<T1>::vspec_cast_to(b).vspec_cast_to() =~~= b,
{
}

pub proof fn proof_cast_from_seq_unique<T: VTypeCast<SecSeqByte>>(val: T)
    ensures
        val === VTypeCast::<SecSeqByte>::vspec_cast_to(val).vspec_cast_to(),
{
}

#[verifier(opaque)]
pub open spec fn field_at<T: VTypeCast<SecSeqByte>, F: VTypeCast<SecSeqByte>>(
    val: T,
    offset: nat,
) -> F {
    let bytes: SecSeqByte = val.vspec_cast_to();
    let b = bytes.subrange(offset as int, (offset + spec_size::<F>()) as int);
    b.vspec_cast_to()
}

#[verifier(opaque)]
pub open spec fn field_set<T: VTypeCast<SecSeqByte>, F: VTypeCast<SecSeqByte>>(
    prev_val: T,
    val: T,
    offset: nat,
    f: F,
) -> bool {
    let prev_bytes: SecSeqByte = prev_val.vspec_cast_to();
    let bytes: SecSeqByte = val.vspec_cast_to();
    bytes =~~= prev_bytes.take(offset as int) + f.vspec_cast_to() + prev_bytes.skip(
        (offset + spec_size::<F>()) as int,
    )
}

pub proof fn proof_field_set_at<
    T: SpecSize + IsConstant + VTypeCast<SecSeqByte>,
    F: SpecSize + IsConstant + VTypeCast<SecSeqByte>,
>(prev_val: T, val: T, offset: nat, f: F)
    requires
        offset < spec_size::<T>(),
        field_set(prev_val, val, offset, f),
    ensures
        field_at(val, offset) === f,
{
    reveal(field_set);
    reveal(field_at);
    proof_cast_from_seq_unique(f);
    let prev_bytes: SecSeqByte = prev_val.vspec_cast_to();
    let bytes: SecSeqByte = val.vspec_cast_to();
    let fb: SecSeqByte = f.vspec_cast_to();
    let bytes = (prev_bytes.take(offset as int) + fb + prev_bytes.skip(
        (offset + spec_size::<F>()) as int,
    ));
    assert(bytes.subrange(offset as int, (offset + spec_size::<F>()) as int) =~~= fb);
}

pub broadcast proof fn proof_field_set_constant<
    T: SpecSize + IsConstant + VTypeCast<SecSeqByte>,
    F: SpecSize + IsConstant + VTypeCast<SecSeqByte>,
>(prev_val: T, val: T, offset: nat, f: F)
    requires
        offset < spec_size::<T>(),
        f.is_constant(),
        prev_val.is_constant(),
        #[trigger] field_set(prev_val, val, offset, f),
    ensures
        val.is_constant(),
{
    reveal(field_set);
    let bytes: SecSeqByte = val.vspec_cast_to();
    proof_into_is_constant::<_, SecSeqByte>(prev_val);
    proof_into_is_constant::<_, SecSeqByte>(f);
    assert(bytes.is_constant());
    proof_into_is_constant::<_, T>(bytes)
}

pub proof fn proof_into_is_constant<T1: VTypeCast<T2> + IsConstant, T2: IsConstant>(v: T1)
    ensures
        v.is_constant() <==> VTypeCast::<T2>::vspec_cast_to(v).is_constant(),
        v.is_constant_to(1) <==> VTypeCast::<T2>::vspec_cast_to(v).is_constant_to(1),
        v.is_constant_to(2) <==> VTypeCast::<T2>::vspec_cast_to(v).is_constant_to(2),
        v.is_constant_to(3) <==> VTypeCast::<T2>::vspec_cast_to(v).is_constant_to(3),
        v.is_constant_to(4) <==> VTypeCast::<T2>::vspec_cast_to(v).is_constant_to(4),
{
    proof_into_is_constant_to::<T1, T2>(v, 1);
    proof_into_is_constant_to::<T1, T2>(v, 2);
    proof_into_is_constant_to::<T1, T2>(v, 3);
    proof_into_is_constant_to::<T1, T2>(v, 4);
}

#[verifier(external_body)]
pub proof fn proof_into_is_constant_to<T1: VTypeCast<T2> + IsConstant, T2: IsConstant>(
    v: T1,
    vmpl: nat,
)
    requires
        1 <= vmpl <= 4,
    ensures
        v.is_constant_to(vmpl) <==> VTypeCast::<T2>::vspec_cast_to(v).is_constant_to(vmpl),
{
}

pub proof fn proof_subrange_is_constant_to(b: SecSeqByte, start: int, end: int, vmpl: nat)
    requires
        b.is_constant_to(vmpl),
        0 <= start,
        start <= end,
        end <= b.len(),
    ensures
        b.subrange(start, end).is_constant_to(vmpl),
{
}

pub proof fn proof_bytes_add_is_constant_to(b1: SecSeqByte, b2: SecSeqByte, vmpl: nat)
    ensures
        (b1.is_constant_to(vmpl) && b2.is_constant_to(vmpl)) <==> (b1 + b2).is_constant_to(vmpl),
{
    let b = (b1 + b2);
    if (b1.is_constant_to(vmpl) && b2.is_constant_to(vmpl)) {
        assert forall|i| 0 <= i < b.len() implies b[i].is_constant_to(vmpl) by {
            if i < b1.len() {
                assert(b[i] === b1[i]);
            } else {
                assert(b[i] === b2[i - b1.len()]);
            }
        }
    }
    if b.is_constant_to(vmpl) {
        assert(b1 =~~= b.take(b1.len() as int));
        assert(b2 =~~= b.skip(b1.len() as int));
        proof_subrange_is_constant_to(b, 0, b1.len() as int, vmpl);
        proof_subrange_is_constant_to(b, b1.len() as int, b.len() as int, vmpl);
    }
}

pub proof fn proof_bytes_add_is_constant(b1: SecSeqByte, b2: SecSeqByte)
    ensures
        (b1.is_constant() && b2.is_constant()) <==> (b1 + b2).is_constant(),
{
    proof_bytes_add_is_constant_to(b1, b2, 1);
    proof_bytes_add_is_constant_to(b1, b2, 2);
    proof_bytes_add_is_constant_to(b1, b2, 3);
    proof_bytes_add_is_constant_to(b1, b2, 4);
}

pub open spec fn fn_vspec_cast_to<T1: VTypeCast::<T2>, T2>() -> spec_fn(T1) -> T2 {
    |v: T1| VTypeCast::<T2>::vspec_cast_to(v)
}

} // verus!
macro_rules! impl_typecast_trait_single {
    ($lt:ty, [$($rt: ty,)*]) => {
        $(verus!{
            impl VTypeCast<$rt> for $lt {
                open spec fn vspec_cast_to(self) -> $rt {
                    builtin::spec_cast_integer::<$lt, $rt>(self)
                }
            }
        }
)*
    }
}

macro_rules! impl_typecast_traits {
    ($($lt: ty,)*) => {
        $(
        impl_typecast_trait_single!{$lt, [u8, u16, u32, u64, u128, usize, int, nat,]}
        )*
    }
}
macro_rules! impl_typecast_to_bool_traits {
    ($($lt: ty,)*) => {
        paste::paste!{
            $(verus!{
                impl VTypeCast<bool> for $lt {
                    open spec fn vspec_cast_to(self) -> bool {
                        self != 0
                    }
                }

                impl VTypeCast<$lt> for bool {
                    open spec fn vspec_cast_to(self) -> $lt {
                        choose |ret: $lt| self === ret.vspec_cast_to()
                    }
                }
            }
)*
        }
    }
}

macro_rules! impl_typecast_to_bytes_traits {
    ($($lt: ty,)*) => {
        paste::paste!{
        $(verus!{
            impl VTypeCast<Seq<u8>> for $lt {
                open spec fn vspec_cast_to(self) -> Seq<u8> {
                    stream::basic::[<$lt _to_stream>](self)
                }
            }

            impl VTypeCast<$lt> for Seq<u8> {
                open spec fn vspec_cast_to(self) -> $lt {
                    choose |ret: $lt| self === ret.vspec_cast_to()
                }
            }
        }
)*
        }
    }
}

impl_typecast_to_bytes_traits! {u8, u16, u32, u64, u128, usize, char, bool, }

impl_typecast_traits! {u8, u16, u32, u64, u128, usize, int, nat, }

impl_typecast_to_bool_traits! {u8, u16, u32, u64, u128, usize, int, nat, }

impl_typecast_trait_single! {char, [u8,]}

verus! {

impl VTypeCast<Seq<u8>> for () {
    #[verifier(inline)]
    open spec fn vspec_cast_to(self) -> Seq<u8> {
        Seq::empty()
    }
}

impl<T> VTypeCast<Seq<u8>> for Ghost<T> {
    #[verifier(inline)]
    open spec fn vspec_cast_to(self) -> Seq<u8> {
        Seq::empty()
    }
}

impl VTypeCast<Seq<u8>> for Seq<u8> {
    #[verifier(inline)]
    open spec fn vspec_cast_to(self) -> Seq<u8> {
        self
    }
}

impl<T> VTypeCast<Seq<u8>> for Tracked<T> {
    #[verifier(inline)]
    open spec fn vspec_cast_to(self) -> Seq<u8> {
        Seq::empty()
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/math/pow_s.rs
================

use vstd::prelude::*;

use super::*;

verus! {

#[verifier(inline)]
pub open spec fn spec_pow2_to_bits(val: u64) -> u64 {
    choose|ret: u64| BIT64!(ret) == val && 0 <= ret < 64
}

pub open spec fn spec_int_pow2(offset: int) -> int
    recommends
        offset >= 0,
{
    spec_nat_pow2(offset as nat)
}

pub open spec fn spec_nat_pow2(e: nat) -> int
    decreases e,
{
    if e == 0 {
        1
    } else {
        2 * spec_nat_pow2((e - 1) as nat)
    }
}

seq_macro::seq!(N in 0..64 {
        verus!{
            #[verifier(inline)]
            pub open spec fn spec_bit64_is_pow_of_2(val: int) -> bool {
                #(
                    ||| val == POW2!(N)
                )*
            }
        }
    }
    );

seq_macro::seq!(N in 0..64 {
        verus!{
            #[verifier(inline)]
            pub open spec fn spec_bit64_is_shl_by_bits(val: u64) -> bool {
                #(
                    ||| val == BIT64!(N as u64)
                )*
            }
        }
    }
    );

} // verus!

================
File: ./source/verismo/src/tspec/math/minmax_s.rs
================

use super::*;
verus! {

pub open spec fn spec_max(a: int, b: int) -> int {
    if a > b {
        a
    } else {
        b
    }
}

pub open spec fn spec_min(a: int, b: int) -> int {
    if a > b {
        b
    } else {
        a
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/math/bits_p.rs
================

#[allow(unused_imports)]
use super::*;

#[macro_export]
macro_rules! BIT {
    ($x: expr) => {
        (1 << ($x))
    };
}

#[macro_export]
macro_rules! BIT64 {
    ($x: expr) => {
        (1u64 << (($x) as u64))
    };
}

verus! {
#[verifier(inline)]
pub open spec fn spec_bit64(val: u64) -> u64 {
    1u64 << val
}
}

#[macro_export]
macro_rules! POW2 {
    (0) => {
        0x1u64
    };
    (1) => {
        0x2u64
    };
    (2) => {
        0x4u64
    };
    (3) => {
        0x8u64
    };
    (4) => {
        0x10u64
    };
    (5) => {
        0x20u64
    };
    (6) => {
        0x40u64
    };
    (7) => {
        0x80u64
    };
    (8) => {
        0x100u64
    };
    (9) => {
        0x200u64
    };
    (10) => {
        0x400u64
    };
    (11) => {
        0x800u64
    };
    (12) => {
        0x1000u64
    };
    (13) => {
        0x2000u64
    };
    (14) => {
        0x4000u64
    };
    (15) => {
        0x8000u64
    };
    (16) => {
        0x10000u64
    };
    (17) => {
        0x20000u64
    };
    (18) => {
        0x40000u64
    };
    (19) => {
        0x80000u64
    };
    (20) => {
        0x100000u64
    };
    (21) => {
        0x200000u64
    };
    (22) => {
        0x400000u64
    };
    (23) => {
        0x800000u64
    };
    (24) => {
        0x1000000u64
    };
    (25) => {
        0x2000000u64
    };
    (26) => {
        0x4000000u64
    };
    (27) => {
        0x8000000u64
    };
    (28) => {
        0x10000000u64
    };
    (29) => {
        0x20000000u64
    };
    (30) => {
        0x40000000u64
    };
    (31) => {
        0x80000000u64
    };
    (32) => {
        0x100000000u64
    };
    (33) => {
        0x200000000u64
    };
    (34) => {
        0x400000000u64
    };
    (35) => {
        0x800000000u64
    };
    (36) => {
        0x1000000000u64
    };
    (37) => {
        0x2000000000u64
    };
    (38) => {
        0x4000000000u64
    };
    (39) => {
        0x8000000000u64
    };
    (40) => {
        0x10000000000u64
    };
    (41) => {
        0x20000000000u64
    };
    (42) => {
        0x40000000000u64
    };
    (43) => {
        0x80000000000u64
    };
    (44) => {
        0x100000000000u64
    };
    (45) => {
        0x200000000000u64
    };
    (46) => {
        0x400000000000u64
    };
    (47) => {
        0x800000000000u64
    };
    (48) => {
        0x1000000000000u64
    };
    (49) => {
        0x2000000000000u64
    };
    (50) => {
        0x4000000000000u64
    };
    (51) => {
        0x8000000000000u64
    };
    (52) => {
        0x10000000000000u64
    };
    (53) => {
        0x20000000000000u64
    };
    (54) => {
        0x40000000000000u64
    };
    (55) => {
        0x80000000000000u64
    };
    (56) => {
        0x100000000000000u64
    };
    (57) => {
        0x200000000000000u64
    };
    (58) => {
        0x400000000000000u64
    };
    (59) => {
        0x800000000000000u64
    };
    (60) => {
        0x1000000000000000u64
    };
    (61) => {
        0x2000000000000000u64
    };
    (62) => {
        0x4000000000000000u64
    };
    (63) => {
        0x8000000000000000u64
    };
    ($_:expr) => {
        0u64
    };
}

#[macro_export]
macro_rules! BIT32 {
    ($x: expr) => {
        (1u32 << ($x))
    };
}

#[macro_export]
macro_rules! BIT16 {
    ($x: expr) => {
        (1u16 << ($x))
    };
}

#[macro_export]
macro_rules! BIT8 {
    ($x: expr) => {
        (1u8 << ($x))
    };
}

#[macro_export]
macro_rules! BIT_MASK {
    ($x: expr) => {
        sub((1u64 << ($x)), 1)
    };
}

verus! {
#[verifier(inline)]
pub open spec fn spec_bit_set(val: u64, bit: u64) -> u64 {
    val | BIT64!(bit)
}

#[verifier(inline)]
pub open spec fn spec_bit_clear(val: u64, bit: u64) -> u64 {
    val & (!BIT64!(bit))
}

#[verifier(inline)]
pub open spec fn spec_has_bit_set(val: u64, bit: u64) -> bool {
    #[trigger] BIT64!(bit) == val & BIT64!(bit)
    //&&& 0 <= bit < 64
}

#[verifier(bit_vector)]
pub proof fn proof_bit_check(val: u64, bit: u64)
requires
    bit < 64,
ensures
    spec_has_bit_set(spec_bit_set(val, bit), bit),
    !spec_has_bit_set(spec_bit_clear(val, bit), bit),
{}
}

macro_rules! mask_proof_for_bits_internal {
    [$($N:expr),* $(,)?] => {
        verus!{
        pub open spec fn slow_bit_range_req(bits: u64) -> bool {
            $(
                ||| (bits == $N)
            )*
        }
        pub proof fn slow_bit_mask64_mod_auto(bits: u64)
            requires
                slow_bit_range_req(bits)
            ensures
                forall |a: u64| #![auto] (a & BIT_MASK!(bits)) == a % BIT64!(bits),
                forall |a: u64| #![auto] (a|BIT_MASK!(bits)) == add(sub(a, (a&BIT_MASK!(bits))), BIT_MASK!(bits)),
                forall |a: u64| #![auto] add(a & !(BIT_MASK!(bits)), BIT_MASK!(bits)) >= a,
        {
            bit_shl64_auto();
            bit_and64_auto();
            bit_or64_auto();
            $(
            assert(forall |a: u64| #![auto] (a & BIT_MASK!($N)) == a % BIT64!($N)) by(bit_vector);
            assert(forall |a: u64| #![auto] (a|BIT_MASK!($N)) == add(sub(a, (a&BIT_MASK!($N))), BIT_MASK!($N))) by(bit_vector);
            assert(forall |a: u64| #![auto]  add(a & !(BIT_MASK!($N)), BIT_MASK!($N)) >= a) by(bit_vector);
            )*
        }
    }
    };
}

macro_rules! mask_proof_for_bits {
    [$($tail:tt)*] => {
        mask_proof_for_bits_internal!($($tail)*);
    };
}

seq_macro::seq!(N in 0..64 {
verus!{
#[verifier(bit_vector)]
pub proof fn bit_shl64_auto()
    ensures
        forall |a: u64| #[trigger] (a<<0u64) == a,
        forall |a: u64| a < 64 ==> #[trigger] (1u64<<a) > 0,
        forall |a: u64, b: u64| b < 64 ==> ((a & BIT64!(b) ==  BIT64!(b)) || (a & BIT64!(b) == 0)),
        #(
        BIT64!(N as u64) == POW2!(N),
        )*
{}

pub proof fn bit_shl64_pow2_auto()
    ensures
        #(
        BIT64!(N as u64) == POW2!(N),
        )*
{
    bit_shl64_auto()
}
}
}
);

verus! {
    // Add more when necessary; We may add all between [0,64)
    mask_proof_for_bits!(
        2u64,
        3u64,
        12u64,
    );

    #[verifier(bit_vector)]
    pub const proof fn bit_and64_auto()
        ensures
            forall |a: u64, b: u64| #[trigger] (a&b) == b&a ,
            forall |a: u64, b: u64, c:u64| #[trigger] ((a&b)&c) == a&(b&c),
            forall |a: u64| #[trigger] (a&a) == a,
            forall |a: u64| #[trigger] (a&0) == 0,
            forall |a: u64| #[trigger] (a& 0xffffffffffffffffu64) == a,
            forall |a: u64, b: u64| #[trigger] (a&b) <= b,
            forall |a: u32, b: u32| #[trigger] (a&b) <= b,
            forall |a: u16, b: u16| #[trigger] (a&b) <= b,
            forall |a: u8, b: u8| #[trigger] (a&b) <= b,
    {}


    /*
    #[verifier(bit_vector)]
    pub proof fn bit64_and_mask_mask_auto()
        ensures
            forall |a: u64, bits: u64| #![auto] 0<= bits < 64 ==> (a & BIT_MASK!(bits)) == a % BIT64!(bits)
    {}
    */

    #[verifier(bit_vector)]
    pub proof fn bit64_or_mask_auto()
        ensures
            forall |a: u64, bits: u64| #![auto] 0<= bits < 64 ==> (add(a|BIT_MASK!(bits), 1)) & BIT_MASK!(bits) == 0,
    {}

    #[verifier(bit_vector)]
    pub proof fn bit_or64_auto()
        ensures
            forall |a: u64, b: u64, c: u64| (a & c == c) ==> ((a | b) & c == c),
            forall |a: u64, b: u64| #[trigger] (a|b) == b|a,
            forall |a: u64, b: u64, c:u64| #[trigger] ((a|b)|c) == a|(b|c),
            forall |a: u64| #[trigger] (a|a) == a,
            forall |a: u64| #[trigger] (a|0) == a,
            forall |a: u64| #[trigger] (a| 0xffffffffffffffffu64) == 0xffffffffffffffffu64,
            forall |a: u64, b: u64| #[trigger] (a|b) <= 0xffffffffffffffffu64,
            //forall |a: u64, b: u64| #[trigger] (a|b) <= add(sub(a, a&b), b),
            forall |a: u64, b: u64| #[trigger] (a|b) >= a,
    {}

    /*
    //Resource limit (rlimit) exceeded
    pub proof fn bit_or64_rel_add_auto()
    ensures
        forall |a: u64, b: u64| a < sub(0xffffffffffffffffu64, b) ==> #[trigger] (a|b) <= add(a, b),
    {
        assert forall |a: u64, b: u64|
        #[trigger] (a|b) <= add(a, b)
        by {
            assert((a|b) <= add(a, b)) by(bit_vector)
        }
    }*/

    #[verifier(bit_vector)]
    pub proof fn bit_xor64_auto()
        ensures
            forall |a: u64, b: u64| #[trigger] (a^b) == b^a,
            forall |a: u64, b: u64, c:u64| #[trigger] ((a^b)^c) == a^(b^c),
            forall |a: u64| #[trigger] (a^a) == 0,
            forall |a: u64| #[trigger] (a^0) == a,
            forall |a: u64| #[trigger] (a^ 0xffffffffffffffffu64) == !a,
    {}

    #[verifier(bit_vector)]
    pub proof fn bit_not64_auto()
        ensures
            forall |a: u64| #[trigger] !(!a) == a,
            forall |a: u64| #[trigger] (!a) & a == 0,
            !0u64 == 0xffffffffffffffffu64,
            forall |a: u64| #[trigger] (!a) == sub(MAXU64!(), a),
    {}

    #[verifier(bit_vector)]
    pub proof fn proof_bit_u64_not(a: u64)
    ensures
        (!a) == sub(MAXU64!(), a)
    {}

    #[verifier(bit_vector)]
    pub proof fn proof_bit_usize_not(a: usize)
    ensures
        (!a) == sub(MAXU64!() as usize, a)
    {}

    #[verifier(bit_vector)]
    pub proof fn bit_lsh64_auto()
        ensures
            forall |a: u64| #[trigger] (a>>0u64) == a,
    {}

    #[verifier(bit_vector)]
    pub proof fn bit_property64_auto()
        ensures
            // absorb
            forall |a: u64, b: u64| #[trigger] (a & (a | b)) == a,
            forall |a: u64, b: u64| #[trigger] (a | (a & b)) == a,
            forall |a: u64, b: u64| #[trigger] (a & ((!a) & b)) == 0,
            forall |a: u64, b: u64|  a == 0 || #[trigger] ((!a) & b) != #[trigger] (a | b),
            // distributive
            forall |a: u64, b: u64, c:u64| #[trigger] (a & (b | c)) == (a & b) | (a & c),
            forall |a: u64, b: u64, c:u64| #[trigger] (a & (b ^ c)) == (a & b) ^ (a & c),
            forall |a: u64, b: u64, c:u64| #[trigger] (a | (b & c)) == (a | b) & (a | c),
            // De Morgan
            forall |a: u64, b: u64| #[trigger] (!(a & b)) == !a | !b,
            forall |a: u64, b: u64| #[trigger] (!(a | b)) == !a & !b,
    {
    }

    pub proof fn bit_set_non_zero(val: u64, b: u64)
    requires
        0 <= b < 64,
    ensures
        bits_p::spec_bit_set(val, b) > 0
    {
        assert(bits_p::spec_bit_set(val, b) > 0) by (bit_vector)
        requires
            0 <= b < 64;
    }
}

seq_macro::seq!(N in 0..64 {
verus!{
    pub proof fn bit_rsh64_div_rel(b: u64, a: u64) -> (ret: u64)
    requires
        a < 64,
    ensures
        ret == (b >> a),
        ret * BIT64!(a) <= MAXU64!(),
        (b>>a) == (b / BIT64!(a)),
    {
        let ret = (b >> a);
        #(
            if a == N {
                assert(ret == b / BIT64!(N as u64)) by(bit_vector)
                requires ret == (b >> N);
                assert(b <= MAXU64!());
                bit_shl64_pow2_auto();
                assert(b / POW2!(N) * POW2!(N) <= MAXU64!());
            }
        )*
        ret
    }
}
});

seq_macro::seq!(N in 0..64 {
    verus!{
        pub proof fn bit_lsh64_mul_rel(b: u64, a: u64)
        requires
            a < 64,
            b * BIT64!(a) <= MAXU64!(),
        ensures
            (b<<a) == (b * BIT64!(a)),
        {
            #(
                 if a == N {
                    assert((b<<N) == mul(b, BIT64!(N as u64))) by(bit_vector);
                    bit_shl64_pow2_auto();
                    assert(b * BIT64!(N as u64) <= MAXU64!());
                    assert(mul(b, POW2!(N)) == b * POW2!(N));
                    assert((b<<N) == b * BIT64!(N));
                }
            )*
            assert((b<<a) == b * BIT64!(a));

        }
    }
});

seq_macro::seq!(N in 0..64 {
verus!{
    pub proof fn proof_bit_u64_and_rel_mod(a: u64, b: u64)
    requires
        spec_bit64_is_pow_of_2(b as int),
    ensures
        a & sub(b, 1) == a % b
    {
        #(
            assert(a & sub(POW2!(N), 1) == a % POW2!(N)) by(bit_vector);
        )*
    }

    pub proof fn proof_bit_usize_and_rel_mod(a: usize, b: usize)
    requires
        spec_bit64_is_pow_of_2(b as int),
    ensures
        a & sub(b, 1) == a % b
    {
        #(
            assert(a & sub(POW2!(N) as usize, 1) == a % (POW2!(N)  as usize)) by(bit_vector);
        )*
    }
}
}
);

#[macro_export]
macro_rules! lemma_bits64 {
    () => {
        bit_property64_auto();
        bit_and64_auto();
        bit_or64_auto();
        bit_xor64_auto();
        bit_not64_auto();
        bit_lsh64_auto();
        bit_shl64_auto();
    };
}

================
File: ./source/verismo/src/tspec/math/nonlinear.rs
================

use super::*;

verus! {

#[verifier(nonlinear)]
pub proof fn proof_mod_mod(a: int, b: int)
    requires
        b > 0,
    ensures
        a % b % b == a % b,
{
}

#[verifier(nonlinear)]
pub proof fn lemma_mod(a: int, b: int) -> (ret: int)
    requires
        b > 0,
        a % b == 0,
    ensures
        ret * b == a,
{
    a / b
}

// Z3 is too slow with this proof
//#[verifier(nonlinear)]
#[verifier(external_body)]
pub proof fn lemma_mod_from_mul_rel(a: int, b: int, c: int)
    requires
        b != 0,
        a == b * c,
    ensures
        a % b == 0,
{
}

pub proof fn proof_mod_propogate(a: int, b: int, c: int)
    requires
        b > 0,
        c > 0,
        a % b == 0,
        b % c == 0,
    ensures
        a % c == 0,
{
    let d1 = lemma_mod(a, b);
    let d2 = lemma_mod(b, c);
    assert((d1 * d2) * c == a) by {
        assert(a == d1 * (d2 * c));
        proof_mul_dist(d1, d2, c);
    }
    let d = d1 * d2;
    assert(d * c == a);
    proof_mul_exchange(d, c);
    lemma_mod_from_mul_rel(a, c, d);
}

#[verifier(nonlinear)]
pub proof fn proof_mul_assoc(a: nat, b: nat, c: nat)
    ensures
        (a + b) * c == a * c + b * c,
{
}

#[verifier(nonlinear)]
pub proof fn proof_mul_bound(a: u64, b: u64, aa: int, bb: int)
    requires
        a as int == aa,
        b as int == bb,
        aa > 0,
        bb > 0,
        a * b <= 0xffff_ffff_ffff_ffffu64,
    ensures
        aa * bb == mul(a, b),
        aa * bb > 0,
{
}

#[verifier(nonlinear)]
pub proof fn proof_div_mod_rel(a: int, b: int) -> (ret: int)
    requires
        b != 0,
    ensures
        ret == a / b,
        ret * b + (a % b) == a,
{
    a / b
}

#[verifier(nonlinear)]
pub proof fn proof_mul_dist(a: int, b: int, c: int) -> (ret: int)
    ensures
        ret == a * b * c,
        a * b * c == a * (b * c),
{
    a * b * c
}

#[verifier(nonlinear)]
pub proof fn proof_mul_div_rel(a: int, b: int) -> (ret: int)
    requires
        b != 0,
    ensures
        ret == a * b,
        ret / b == a,
{
    a * b
}

#[verifier(nonlinear)]
pub proof fn proof_mul_exchange(a: int, b: int) -> (ret: int)
    ensures
        ret == a * b,
        ret == b * a,
{
    a * b
}

pub proof fn proof_mod_range(a: int, b: int)
    requires
        a >= 0,
        b > 0,
    ensures
        0 <= a % b < b,
{
}

#[verifier(nonlinear)]
pub proof fn proof_mod_pos_neg_rel(a: int, b: int) -> (ret: int)
    requires
        b != 0,
    ensures
        ret == a % b,
        (b > 0) ==> 0 <= ret < b,
        (b < 0) ==> 0 <= ret < -b,
{
    a % b
}

#[verifier(nonlinear)]
pub proof fn proof_mul_pos_neg_rel(a: int, b: int) -> (ret: int)
    ensures
        ret == a * b,
        ((a > 0 && b > 0) || (a < 0 && b < 0)) ==> ret > 0,
        ((a > 0 && b < 0) || (a < 0 && b > 0)) ==> ret < 0,
        (a == 0 || b == 0) ==> ret == 0,
{
    a * b
}

#[verifier(inline)]
pub open spec fn abs(a: int) -> int {
    if a > 0 {
        a
    } else {
        -a
    }
}

#[verifier(nonlinear)]
pub proof fn proof_div_pos_neg_rel(a: int, b: int) -> (ret: int)
    requires
        b != 0,
    ensures
        ret == a / b,
        (a / b > 0) ==> ((a >= b && b > 0) || (a < 0 && b < 0)),
        ((a >= b && b > 0) || (a < b && b < 0)) ==> (a / b > 0),
        (a / b == 0) == (0 <= a < abs(b)),
{
    a / b
}

} // verus!

================
File: ./source/verismo/src/tspec/math/mod.rs
================

pub mod align_s;
pub mod bits_p;
mod cond_bound;
mod integer;
pub mod minmax_s;
mod nonlinear;
pub mod pow_p;
pub mod pow_s;

pub use align_s::*;
#[macro_use]
pub use bits_p::*;
pub use cond_bound::*;
pub use integer::*;
pub use minmax_s::*;
pub use nonlinear::*;
pub use pow_p::*;
pub use pow_s::*;

use self::minmax_s::*;
use crate::tspec::*;
use crate::*;

================
File: ./source/verismo/src/tspec/math/cond_bound.rs
================

use super::*;

verus! {

pub open spec fn is_upper_bound_satisfy_cond(
    cond_fn: spec_fn(u64) -> bool,
    bound: u64,
    max: u64,
) -> bool {
    &&& cond_fn(bound)
    &&& bound <= max
    &&& forall|b: u64| (#[trigger] cond_fn(b) && b <= max) ==> b <= bound
}

spec fn is_upper_bound(cond_fn: spec_fn(u64) -> bool, bound: u64, max: u64) -> bool {
    &&& bound <= max
    &&& forall|b: u64| (#[trigger] cond_fn(b) && b <= max) ==> b <= bound
}

proof fn lemma_has_conditional_upper_bound(val: u64, cond_fn: spec_fn(u64) -> bool, max: u64)
    requires
        cond_fn(val),
        val <= max,
    ensures
        exists|val| #[trigger] is_upper_bound_satisfy_cond(cond_fn, val, max),
    decreases max - val,
{
    let exist_bound = exists|val| #[trigger] is_upper_bound_satisfy_cond(cond_fn, val, max);
    if !exist_bound {
        assert forall|val| !is_upper_bound_satisfy_cond(cond_fn, val, max) by {}
        assert(cond_fn(val));
        assert(!is_upper_bound_satisfy_cond(cond_fn, val, max));
        assert(!forall|b: u64| #[trigger] cond_fn(b) ==> b <= val);
        assert(exists|b: u64| #[trigger] cond_fn(b) && b <= max && b > val);
        let val2 = choose|b: u64| #[trigger] cond_fn(b) && b <= max && b > val;
        assert(val2 > val);
        assert(val2 <= max);
        lemma_has_conditional_upper_bound(val2, cond_fn, max);
    }
}

pub proof fn proof_has_conditional_upper_bound(cond_fn: spec_fn(u64) -> bool, max: u64)
    ensures
        (exists|val| #[trigger] cond_fn(val) && val <= max) ==> exists|val|
            is_upper_bound_satisfy_cond(cond_fn, val, max),
{
    let exist_bound = exists|val| #[trigger] is_upper_bound_satisfy_cond(cond_fn, val, max);
    let exist_val = exists|val| #[trigger] cond_fn(val) && val <= max;
    if !exist_bound {
        assert forall|val| !is_upper_bound_satisfy_cond(cond_fn, val, max) by {}
        if exist_val {
            let val = choose|val| #[trigger] cond_fn(val) && val <= max;
            lemma_has_conditional_upper_bound(val, cond_fn, max);
            assert(exist_bound);
        }
        assert(!exist_val);
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/math/align_s.rs
================

use super::bits_p::*;
use super::*;

verus! {

pub proof fn lemam_bit_or_mask_bound(val: u64, align: u64) -> (mask: u64)
    requires
        spec_bit64_is_shl_by_bits(align),
        val <= MAXU64!() - align,
    ensures
        mask == sub(align, 1),
        (val | mask) < MAXU64!(),
{
    let mask = sub(align, 1);
    assert((val | mask) < MAXU64!()) by (bit_vector)
        requires
            val <= sub(MAXU64!(), align),
            mask == sub(align, 1),
            spec_bit64_is_shl_by_bits(align),
    ;
    mask
}

seq_macro::seq!(N in 0..64 {
        verus! {
            pub proof fn lemma_bit_and_mod_rel(val: u64, align: u64)
            requires
                spec_bit64_is_shl_by_bits(align),
            ensures
                val % align == (val & sub(align, 1)),
            {
                #(
                    assert(val % BIT64!(N as u64) == (val & sub(BIT64!(N as u64), 1))) by(bit_vector);
                )*
            }
        }
        }
    );

pub proof fn proof_align_down(val: nat, align: nat) -> (ret: (u64, u64, u64))
    requires
        spec_bit64_is_pow_of_2(align as int),
        val as u64 == val,
    ensures
        ret.0 == sub(align as u64, 1),
        ret.1 == spec_pow2_to_bits(align as u64),
        ret.2 == val - val % align,
        ret.2 == val / align * align,
        ret.2 == (val as u64) >> ret.1 << ret.1,
        ret.2 == (val as u64) & (!ret.0),
{
    let bits: u64 = spec_pow2_to_bits(align as u64) as u64;
    let val64 = val as u64;
    let mask = sub(align as u64, 1);
    let ret = val - val % align;
    let align64 = BIT64!(bits);
    bit_shl64_pow2_auto();
    assert(val / align * align == val - val % align) by (nonlinear_arith)
        requires
            align != 0,
    ;
    assert(val64 / align as u64 == val / align) by (nonlinear_arith)
        requires
            align != 0,
            val == val64,
            align == align as u64,
    ;
    bit_rsh64_div_rel(val64, bits);
    bit_lsh64_mul_rel(val64 >> bits, bits);
    assert((val64 >> bits) << bits == val64 / align64 * align64);
    assert(val64 & !sub(BIT64!(bits), 1) == ((val64 >> bits) << bits)) by (bit_vector)
        requires
            bits < 64,
    ;
    (mask, bits, ret as u64)
}

pub proof fn proof_align_up(val: nat, align: nat) -> (ret: (u64, u64, u64, u64))
    requires
        spec_bit64_is_pow_of_2(align as int),
        val as u64 == val,
        val + align <= MAXU64!(),
    ensures
        ret.0 == sub(align as u64, 1),
        ret.1 == spec_pow2_to_bits(align as u64),
        ret.2 == if val % align != 0 {
            val + (align - val % align) as nat
        } else {
            val
        },
        ret.3 == val as u64 | ret.0,
        val % align != 0 ==> (ret.3 + 1) as u64 == ret.3 + 1,
        val as u64 & ret.0 == val % align,
        ret.2 == if val as u64 & ret.0 as u64 != 0 {
            add(ret.3, 1)
        } else {
            val as u64
        },
{
    let bits: u64 = spec_pow2_to_bits(align as u64) as u64;
    let val64 = val as u64;
    let align64 = align as u64;
    let mask = sub(align as u64, 1);
    let ret = val + (align - val % align);
    let tmp = val64 | mask;
    let ret2 = add(tmp, 1);
    bit_shl64_pow2_auto();
    assert(val / align * align == val - val % align) by (nonlinear_arith)
        requires
            align != 0,
    ;
    bit_rsh64_div_rel(val64, bits);
    lemma_bit_and_mod_rel(val64, align64);
    assert(val % align == (val64 % align64));
    lemam_bit_or_mask_bound(val64, align64);
    if val64 & mask != 0 {
        assert(add(val64, sub(align64, val64 & mask)) == ret2) by (bit_vector)
            requires
                align64 == 1u64 << bits,
                mask == sub(align64, 1),
                ret2 == add(tmp, 1),
                tmp == val64 | mask,
                bits < 64,
        ;
        (mask, bits, ret as u64, tmp)
    } else {
        (mask, bits, val as u64, tmp)
    }
}

pub open spec fn spec_align_up(val: int, align: int) -> int {
    let r = val % align;
    val + if r == 0 {
        0
    } else {
        align - r
    }
}

pub open spec fn spec_align_down(val: int, align: int) -> int {
    val - val % align
}

pub proof fn proof_align_is_aligned(val: int, align: int)
    requires
        align > 0,
        val >= 0,
    ensures
        spec_align_up(val, align) % align == 0,
        spec_align_down(val, align) % align == 0,
{
    let k = proof_div_mod_rel(val, align);
    let up = spec_align_up(val, align);
    let down = spec_align_down(val, align);
    assert(down == k * align);
    assert(up == k * align + align || up == k * align);
    assert(k * align + align == (k + 1) * align) by (nonlinear_arith);
    proof_mul_div_rel(k, align);
    proof_mul_div_rel(k + 1, align);
    proof_div_mod_rel(up, align);
    proof_div_mod_rel(down, align);
}

pub open spec fn spec_is_align_up_by_int(val: int, align: int, ret: int) -> bool {
    &&& ret % align == 0
    &&& ret == spec_align_up(val, align)
    &&& val <= ret < val + align
}

pub open spec fn spec_is_align_down_by_int(val: int, align: int, ret: int) -> bool {
    &&& (ret as int) == val as int - val as int % align as int
    &&& ret as int == (val as u64 >> spec_pow2_to_bits(align as u64)) << spec_pow2_to_bits(
        align as u64,
    )
    &&& (ret as int) % (align as int) == 0
}

pub open spec fn spec_valid_align(align: int) -> bool {
    //&&& (align as int) % BLOCK_SIZE!() == 0
    &&& spec_bit64_is_pow_of_2(align)
}

pub proof fn proof_modeq_propogation(a: int, b: int, c: int)
    requires
        b >= 1,
        c >= 1,
        a >= 0,
        a % b == 0,
        b % c == 0,
    ensures
        a % c == 0,
{
    let i = proof_div_mod_rel(a, b);
    let j = proof_div_mod_rel(b, c);
    proof_mul_exchange(i, j);
    proof_mul_exchange(i, b);
    proof_mul_exchange(j, c);
    proof_mul_exchange(j * i, c);
    let aa = proof_mul_div_rel(j * i, c);
    assert(a == b * i);
    assert(b == c * j);
    assert(a == c * j * i);
    proof_mul_dist(c, j, i);
    assert(aa == j * i * c);
    assert(a == aa);
    proof_div_mod_rel(a, c);
}

} // verus!

================
File: ./source/verismo/src/tspec/math/integer.rs
================

use vstd::prelude::*;

use super::*;

verus! {
    pub open spec fn has_bit_closure(input: u64) -> spec_fn(u64) -> bool {
        |b: u64| spec_has_bit_set(input, b)
    }

    #[verifier(inline)]
    pub open spec fn has_bits_until(input: u64, nbits: u64, h: u64) -> bool {
        &&& forall |b: u64| (b <= h  && sub(h, b) < nbits) ==> spec_has_bit_set(input, b)
        &&& forall |b: u64| (h < b < 64) ==> !spec_has_bit_set(input, b)
    }

    #[verifier(inline)]
    pub open spec fn nbits_mask(nbits: u64) -> u64
    recommends
        0 < nbits <= 64,
    {
        (sub(BIT64!(sub(nbits, 1)), 1) << 1) | 1u64
    }
}

seq_macro::seq!(N in 0..64 {
    verus!{
    proof fn lemma_zeroval_bits(input: u64)
    requires
        forall |b: u64| 0 <= b < 64 ==> !spec_has_bit_set(input, b)
    ensures
        input == 0
    {
        assert(input == 0 )by(bit_vector)
        requires
            #(
                !spec_has_bit_set(input, N),
            )*
            true;
    }

    proof fn lemma_has_64bits_until_to_mask(val: u64, h: u64)
    requires
        h < 64,
        has_bits_until(val, 64, h),
    ensures
        val == nbits_mask(add(h, 1))
    {
        assert(val == nbits_mask(add(h, 1))) by(bit_vector)
        requires
            #(
                (N <= h) ==> spec_has_bit_set(val, N),
                (N > h) ==> !spec_has_bit_set(val, N),
            )*
            true;
    }
    }
});

verus! {
    pub open spec fn is_highest_bit(input: u64, bit: u64) -> bool {
        is_upper_bound_satisfy_cond(has_bit_closure(input), bit, 63)
    }

    #[verifier(inline)]
    pub open spec fn spec_fill_ones_exe(input: u64) -> u64 {
        let ret = input;
        let ret = ret | (ret >> 1);
        let ret = ret | (ret >> 2);
        let ret = ret | (ret >> 4);
        let ret = ret | (ret >> 8);
        let ret = ret | (ret >> 16);
        let ret = ret | (ret >> 32);
        ret
    }

    #[verifier(inline)]
    pub open spec fn spec_prev_power_of_two_exe(input: u64) -> u64 {
        if input == 0 {
            0
        } else {
            let ret = spec_fill_ones_exe(input);
            add((ret >> 1), 1)
        }
    }

    #[verifier(inline)]
    pub open spec fn spec_next_power_of_two_exe(input: u64) -> u64 {
        if input <= 1 {
            1
        } else {
            let ret = spec_fill_ones_exe(sub(input,1));
            add(ret, 1)
        }
    }

    #[verifier(inline)]
    pub open spec fn spec_highest_bit(input: u64) -> u64
    recommends
        input != 0,
    {
        choose |b: u64| is_highest_bit(input, b)
    }

    pub proof fn proof_get_highest_bit(input: u64) -> (ret: u64)
    requires
        input != 0,
    ensures
        is_highest_bit(input, ret),
        ret == spec_highest_bit(input),
        0 <= ret < 64,
    {
        let high_bit = choose |b: u64| is_highest_bit(input, b);
        let cond_fn = has_bit_closure(input);
        let exist_high_bit = exists |b: u64| is_highest_bit(input, b);
        let exist_has_bit = exists |b: u64| #[trigger]cond_fn(b) && b <= 63 ;
        if  !exist_high_bit {
            assert forall |b: u64| !#[trigger]is_upper_bound_satisfy_cond(cond_fn, b, 63) by{
                assert(!is_highest_bit(input, b));
                assert(is_highest_bit(input, b) === is_upper_bound_satisfy_cond(cond_fn, b, 63));
                assert(!is_upper_bound_satisfy_cond(cond_fn, b, 63));
            }
            assert(!exists |b: u64| is_upper_bound_satisfy_cond(cond_fn, b, 63));
            proof_has_conditional_upper_bound(cond_fn, 63);
            assert(!exist_has_bit);
            assert forall |b: u64| 0 <= b < 64
            implies !spec_has_bit_set(input, b)
            by{
                assert(!cond_fn(b));
            }
            lemma_zeroval_bits(input);
        }
        assert(exist_high_bit);
        assert(is_highest_bit(input, high_bit));
        high_bit
    }

    pub closed spec fn spec_prev_power_of_two(input: u64) -> u64 {
        if input != 0 {
            let high_bit = spec_highest_bit(input);
            BIT64!(high_bit)
        } else {
            0
        }
    }

    pub closed spec fn spec_next_power_of_two(input: u64) -> u64 {
        if input != 0 {
            let high_bit = spec_highest_bit(input);
            if input != BIT64!(high_bit) {
                BIT64!(add(high_bit, 1))
            } else {
                input
            }
        } else {
            1
        }
    }

    pub open spec fn spec_is_prev_power_of_two(input: nat, ret: nat) -> bool {
        &&& (input != 0) ==> (
            input/2nat < ret <= input &&
            spec_bit64_is_shl_by_bits(ret as u64)
        )
    }

    pub open spec fn spec_is_next_power_of_two(input: nat, ret: nat) -> bool {
        &&& spec_bit64_is_shl_by_bits(ret as u64)
        &&& ret == spec_next_power_of_two_exe(input as u64)
        &&& (input != 0) ==> input <= ret < input * 2
    }

    pub proof fn lemma_fill_ones(input: u64) -> (ret: u64)
    requires
        input != 0,
    ensures
        ret == spec_fill_ones_exe(input),
        (ret >> 1) < MAXU64!(),
        (!ret & input) == 0,
        (ret & input) == input,
        ret / 2 < input,
        ret >= input,
        ret == nbits_mask(add(spec_highest_bit(input),1)),
    {
        let ret = spec_fill_ones_exe(input);
        assert((!ret & input) == 0 && ((ret & input) == input)) by(bit_vector)
        requires
            ret == spec_fill_ones_exe(input);
        let high_bit = proof_get_highest_bit(input);
        assert(1 == BIT64!(0u64)) by(bit_vector);
        assert(has_bits_until(input, 1, high_bit)) by {
            assert forall |b: u64| (high_bit < b < 64)
            implies !spec_has_bit_set(input, b)
            by {
                assert(is_highest_bit(input, high_bit));
                if spec_has_bit_set(input, b) {
                    assert(has_bit_closure(input)(b) && b <= 63);
                    assert(b <= high_bit);
                }
            }
        }
        lemma_fill_ones_bit_steps(input, 1, high_bit, 0);
        bit_shl64_auto();

        assert(ret == fill_ones(input)) by {
            reveal_with_fuel(_fill_ones, 7);
        }
        assert(ret == nbits_mask(add(high_bit,1)));
        assert((ret >> 1) < MAXU64!()) by(bit_vector);
        assert(ret >= 1) by(bit_vector)
        requires
            0 <= high_bit < 64,
            ret == nbits_mask(add(high_bit,1));
        assert((!ret & input) == 0 && (ret / 2 < input) && (ret >= input)) by(bit_vector)
        requires
            input != 0,
            ret == spec_fill_ones_exe(input);
        assert((ret & input) == input) by(bit_vector)
        requires
            input != 0,
            ret == spec_fill_ones_exe(input);
        ret
    }

    pub proof fn lemma_prev_power_of_two(input: u64) -> (ret: u64)
    ensures
        spec_is_prev_power_of_two(input as nat, ret as nat),
        ret == spec_prev_power_of_two_exe(input),
    {
        if input == 0 {
            assert(spec_is_prev_power_of_two(0, 0));
            0
        } else {
            let ret = lemma_fill_ones(input);
            let ret_last = add((ret >> 1u64), 1u64);
            let high_bit = proof_get_highest_bit(input);
            assert(add((ret >> 1u64), 1u64) == BIT64!(high_bit)) by(bit_vector)
            requires
                ret == nbits_mask(add(high_bit, 1)),
                0 <= high_bit < 64;
            assert(spec_bit64_is_shl_by_bits(ret_last)) by(bit_vector)
            requires
                ret == nbits_mask(add(high_bit, 1)),
                ret_last == add((ret >> 1u64), 1u64),
                0 <= high_bit < 64;
            assert(input >> 1 == input / 2) by(bit_vector);
            assert((input >> 1) < ret_last <= input) by(bit_vector)
            requires
                input != 0,
                ret_last == spec_prev_power_of_two_exe(input);
            ret_last
        }
    }

    pub proof fn lemma_next_power_of_two(input: u64) -> (ret: u64)
    requires
        input <= POW2!(63),
    ensures
        spec_bit64_is_shl_by_bits(ret as u64),
        spec_is_next_power_of_two(input as nat, ret as nat),
        ret == spec_next_power_of_two_exe(input),
        input > 1 ==> spec_fill_ones_exe(sub(input, 1)) < MAXU64!(),
    {
        if input <= 1 {
            bit_shl64_pow2_auto();
            assert(spec_bit64_is_shl_by_bits(1));
            1
        } else {
            let input1 = sub(input, 1);
            let fill_ones = lemma_fill_ones(input1);
            let high_bit = proof_get_highest_bit(input1);
            let ret = add(fill_ones, 1);
            assert(fill_ones < MAXU64!()) by(bit_vector)
            requires
                0 < input1 < POW2!(63),
                fill_ones/2 < input1;
            assert(spec_bit64_is_shl_by_bits(ret)) by(bit_vector)
            requires
                fill_ones == nbits_mask(add(high_bit, 1)),
                ret == add(fill_ones, 1u64),
                fill_ones < MAXU64!(),
                0 <= high_bit < 64;
            ret
        }
    }

    spec fn _fill_ones(input: u64, nbits: u64, round: u64) -> u64
    recommends
        round <= 6,
        nbits == BIT64!(round),
    decreases
        6 - round,
    {
        if round < 6 {
            let t = input | (input >> nbits);
            _fill_ones(t, BIT64!(add(round, 1)), add(round, 1))
        } else {
            input
        }
    }

    spec fn fill_ones(input: u64) -> u64 {
        _fill_ones(input, 1, 0)
    }

    proof fn lemma_fill_ones_bit_step(input: u64, nbits: u64, h: u64, round: u64) -> (ret: (u64, u64))
    requires
        nbits == BIT64!(round),
        round < 6,
        h < 64,
        has_bits_until(input, nbits, h),
    ensures
        ret.0 == (input | input >> nbits),
        ret.1 == BIT64!(add(round, 1)),
        has_bits_until(ret.0, ret.1, h),
    {
        assert(0 < nbits < 64) by {
            bit_shl64_auto();
            assert(BIT64!(round) < BIT64!(6u64)) by(bit_vector)
            requires round < 6;
        }
        let nbits2: u64 = BIT64!(add(round, 1));
        assert(nbits2 == add(nbits, nbits)) by(bit_vector)
        requires
            nbits == BIT64!(round),
            nbits2 == BIT64!(add(round, 1)),
            round < 6
        ;
        assert(BIT64!(round) <= 32) by(bit_vector)
        requires round < 6;
        let ret = (input | input >> nbits);
        assert forall |b: u64|
            b <= h  && sub(h, b) < nbits2
        implies
            spec_has_bit_set(ret, b)
        by {
            bit_or64_auto();
            if b <= h  && sub(h, b) < nbits {
                assert(spec_has_bit_set(input, b));
                assert(spec_has_bit_set(ret, b));
            } else {
                assert(spec_has_bit_set(input >> nbits, b)) by(bit_vector)
                requires
                    0 < h < 64,
                    0 < nbits < 64,
                    spec_has_bit_set(input, add(b, nbits)),
                    b <= h,
                    sub(h, add(b, nbits)) < nbits;
                assert(spec_has_bit_set(ret, b));
            }
        }
        assert forall |b: u64|
            h < b < 64
        implies
            !spec_has_bit_set(ret, b)
        by {
            assert(!spec_has_bit_set(input, b));
            assert(!spec_has_bit_set(input >> nbits, b)) by{
                if add(b, nbits) < 64 {
                    assert(!spec_has_bit_set(input >> nbits, b)) by(bit_vector)
                    requires
                        b < 64,
                        nbits < 64,
                        add(b, nbits) < 64,
                        !spec_has_bit_set(input, add(b, nbits)),
                } else {
                    assert(!spec_has_bit_set(input >> nbits, b)) by(bit_vector)
                    requires
                        b < 64,
                        nbits < 64,
                        add(b, nbits) >= 64;
                }
            }
            assert(!spec_has_bit_set(input | (input >> nbits), b)) by(bit_vector)
            requires
                0 < nbits < 64,
                h < b < 64,
                !spec_has_bit_set(input, b),
                !spec_has_bit_set(input >> nbits, b);
        }
        (ret, nbits2)
    }

    proof fn lemma_fill_ones_bit_steps(input: u64, nbits: u64, h: u64, round: u64) -> (ret: u64)
    requires
        round <= 6,
        h < 64,
        has_bits_until(input, nbits, h),
        nbits == BIT64!(round),
    ensures
        ret == _fill_ones(input, nbits, round),
        has_bits_until(ret, 64, h),
        ret == nbits_mask(add(h,1)),
    decreases
        6 - round,
    {
        let ret = _fill_ones(input, nbits, round);
        if round < 6 {
            let (t, nbits2) = lemma_fill_ones_bit_step(input, nbits, h, round);
            lemma_fill_ones_bit_steps(t, nbits2, h, add(round, 1));
            assert(has_bits_until(_fill_ones(t, nbits2, add(round, 1)), 64, h));
            assert(ret == _fill_ones(t, nbits2, add(round, 1)));
            assert(has_bits_until(ret, 64, h));
        } else {
            assert(BIT64!(round) == 64) by(bit_vector)
            requires round == 6;
            assert(has_bits_until(input, 64, h));
            assert(ret == input);
        }
        lemma_has_64bits_until_to_mask(ret, h);
        ret
    }


}

/*
macro_rules! define_integer_ex_fn {
    ($T: ty, $N: expr) => {
        paste::paste! {
    verus!{
    pub open spec fn [<spec_is_leading_zeros_ $T>](val: $T, ret: u32) -> bool {
        let mask: $T = (1 as $T) << ($N - ret - 1) as $T;
        &&& ret as int == spec_leading_zeros(val)
        &&& if ret == $N {
            val == 0
        } else if ret == 0 {
            val & mask == mask
        } else {
            let base: $T = (((1 as $T) << ($N - ret) as $T) as $T- 1 as $T) as $T;
            let leading: $T = !base;
            &&& ret <= $N
            &&& val & leading == 0
            &&& val & mask == mask
        }
    }
    #[verifier::external_fn_specification]
    pub fn [<ex_leading_zeros_ $T>](val: $T) -> (ret: u32)
        ensures [<spec_is_leading_zeros_ $T>](val, ret)
    {
        val.leading_zeros()
    }
    }
    }
}
}

define_integer_ex_fn!(u64, 8);
define_integer_ex_fn!(u32, 4);
define_integer_ex_fn!(u16, 2);
*/

================
File: ./source/verismo/src/tspec/math/pow_p.rs
================

use super::*;

verus! {

pub proof fn proof_bits_to_pow2(bit: u64)
    requires
        0 <= bit < 64,
    ensures
        BIT64!(bit) as int == spec_nat_pow2(bit as nat),
    decreases bit,
{
    if bit > 0 {
        let bit2 = sub(bit, 1);
        proof_bits_to_pow2(bit2);
        let val2: u64 = BIT64!(bit2);
        assert(val2 == spec_nat_pow2(bit2 as nat));
        assert(bit2 <= 62);
        assert(BIT64!(bit2) <= BIT64!(62u64)) by (bit_vector)
            requires
                bit2 <= 62,
        ;
        assert(BIT64!(62u64) == 0x4000_0000_0000_0000u64) by (bit_vector);
        assert(BIT64!(sub(bit, 1)) << 1u64 == BIT64!(bit)) by (bit_vector)
            requires
                0 < bit < 64,
        ;
        assert(val2 << 1u64 == mul(2u64, val2)) by (bit_vector)
            requires
                val2 <= 0x4000_0000_0000_0000u64,
        ;
        assert(spec_nat_pow2(bit as nat) == mul(2, BIT64!(bit2)));
    } else {
        assert(BIT64!(0u64) == 1) by (bit_vector);
    }
}

pub open spec fn spec_is_power_of_2(val: nat) -> bool {
    exists|bit: u64| val == BIT64!(bit)
}

pub open spec fn spec_pow2_to_bits_exe(val: nat) -> nat
    recommends
        val as u64 == val,
        val != 0,
    decreases val,
{
    let val64 = val as u64;
    if val > 1 {
        if val64 >> 1u64 < val64 {
            spec_pow2_to_bits_exe((val64 >> 1u64) as nat) + 1
        } else {
            0
        }
    } else {
        0
    }
}

pub proof fn proof_pow2_to_bits(val: nat) -> (ret: u64)
    requires
        val as u64 == val,
        val != 0,
    ensures
        ret == spec_pow2_to_bits_exe(val),
        ret < 64,
        BIT64!(ret) <= val,
        val < BIT64!(ret) * 2,
        (spec_bit64_is_pow_of_2(val as int) && val > 1) ==> spec_bit64_is_pow_of_2(
            (val as u64 >> 1u64) as int,
        ),
        spec_bit64_is_pow_of_2(val as int) ==> ret == spec_pow2_to_bits(val as u64),
    decreases val,
{
    bit_shl64_pow2_auto();
    let val64 = val as u64;
    let ret = spec_pow2_to_bits_exe(val);
    if spec_bit64_is_pow_of_2(val64 as int) && val > 1 {
        assert(spec_bit64_is_pow_of_2((val64 >> 1u64) as int)) by (bit_vector)
            requires
                spec_bit64_is_pow_of_2(val64 as int),
                val64 > 1,
        ;
    }
    if val > 1 {
        let next = val64 >> 1u64;
        assert(next < val64 && next < BIT64!(63) && next > 0) by (bit_vector)
            requires
                next == val64 >> 1u64,
                val64 > 1,
        ;
        let next_bits = spec_pow2_to_bits_exe(next as nat);
        proof_pow2_to_bits(next as nat);
        assert(next_bits < 63);
        assert(ret < 64);
        let next_bits64 = next_bits as u64;
        let ret_bits64 = next_bits64 + 1;
        bit_rsh64_div_rel(val64, 1);
        return ret as u64;
    } else {
        return 0;
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/size_s.rs
================

use vstd::prelude::*;

use super::*;
use crate::BIT64;

macro_rules! impl_spec_size_for_basic  {
    ($([$baset: ty, $size: literal]),* $(,)*) => {
        $(
        verus!{
            impl SpecSize for $baset {
                #[verifier(inline)]
                open spec fn spec_size_def() -> nat
                {
                    $size
                }
            }
        }
        )*
    };
}
verus! {

pub open spec fn spec_max_count<T>() -> nat;

// pub spec fn spec_field_offset<T>(i: nat) -> nat;
pub trait SpecSize {
    spec fn spec_size_def() -> nat;
}

// For core::mem:sizeof
pub open spec fn spec_size<T>() -> nat;

pub trait ExecStruct {

}

impl ExecStruct for u8 {

}

impl ExecStruct for u16 {

}

impl ExecStruct for u32 {

}

impl ExecStruct for u64 {

}

impl ExecStruct for usize {

}

impl ExecStruct for u128 {

}

impl ExecStruct for char {

}

impl ExecStruct for bool {

}

#[verifier(external_body)]
pub broadcast proof fn axiom_max_count_size_rel<T>()
    ensures
        spec_nat_pow2(spec_size::<T>()) / 2 < (#[trigger] spec_max_count::<T>()) <= spec_nat_pow2(
            (spec_size::<T>()) * 8,
        ),
{
}

#[verifier(external_body)]
pub broadcast proof fn axiom_set_full_max_count_rel<T>()
    ensures
        Set::<T>::full().len() == #[trigger] spec_max_count::<T>(),
{
}

// All executable types should have a size and its set should be finite.
/*#[verifier(external_body)]
    pub broadcast proof fn axiom_exe_set_finite<T>(s: Set<T>)
    ensures
        s.finite()
    {}*/

// Size is undef for types without VTypeCast
// Size == spec_size_def if it has VTypeCast
#[verifier(external_body)]
pub broadcast proof fn axiom_size_from_cast_bytes<T: SpecSize>()
    ensures
        (#[trigger] spec_size::<T>()) == T::spec_size_def(),
{
}

#[verifier(external_body)]
pub broadcast proof fn axiom_size_from_cast_bytes_def<T: SpecSize + VTypeCast<Seq<u8>>>(val: T)
    ensures
        T::spec_size_def() == VTypeCast::<Seq<u8>>::vspec_cast_to(val).len(),
{
}

/*impl<T: VTypeCast<Seq<u8>>> SpecSize for T {
        open spec fn spec_size_def() -> nat
        {
            let x: Self = arbitrary();
            let s: Seq<u8> = x.vspec_cast_to();
            s.len()
        }
    }*/
} // verus!
impl_spec_size_for_basic! {[usize, 8], [u128, 16], [u64, 8], [u32, 4], [u16, 2], [u8, 1], [bool, 1], [char, 1], [(), 0]}

verus! {

impl<T> SpecSize for Ghost<T> {
    #[verifier(inline)]
    open spec fn spec_size_def() -> nat {
        0
    }
}

impl<T> SpecSize for Tracked<T> {
    #[verifier(inline)]
    open spec fn spec_size_def() -> nat {
        0
    }
}

impl<T: SpecSize, M> SpecSize for SecType<T, M> {
    #[verifier(inline)]
    open spec fn spec_size_def() -> nat {
        T::spec_size_def()
    }
}

impl<T> SpecSize for Option<T> {
    closed spec fn spec_size_def() -> nat;
}

} // verus!
verus! {

pub proof fn proof_bytes_len(data: u64, s: Seq<u8>)
    requires
        s =~~= data.vspec_cast_to(),
    ensures
        s.len() == 8,
{
}

pub proof fn proof_bytes_len4(data: u32, s: Seq<u8>)
    requires
        s =~~= data.vspec_cast_to(),
    ensures
        s.len() == 4,
{
}

pub proof fn proof_bytes_len2(data: u16, s: Seq<u8>)
    requires
        s =~~= data.vspec_cast_to(),
    ensures
        s.len() == 2,
{
}

pub proof fn proof_bytes_len1(data: u8, s: Seq<u8>)
    requires
        s =~~= data.vspec_cast_to(),
    ensures
        s.len() == 1,
{
}

pub proof fn proof_bytes_len0(data: usize, s: Seq<u8>)
    requires
        s =~~= data.vspec_cast_to(),
    ensures
        s.len() == 8,
{
}

} // verus!

================
File: ./source/verismo/src/tspec/mod.rs
================

// math -> sec_lib;
// math -> size
// size -> array
// size -> stream
#[macro_use]
mod math;
mod cast;
mod constant;
mod default;
mod fmap;
mod isconst;
mod macros;
mod ops;
mod range_set;
#[macro_use]
mod security;
mod fnspec;
#[macro_use]
mod integer;
pub mod map_lib;
mod seqlib;
mod setlib;
mod size_s;
mod stream;
mod wellformed;

//use builtin::*;
pub use builtin_macros::*;
pub use cast::*;
pub use default::*;
pub use fmap::FMap;
pub use fnspec::*;
pub use integer::*;
pub use isconst::*;
pub use math::*;
pub use ops::*;
pub use range_set::*;
pub use security::*;
pub use seqlib::*;
pub use setlib::*;
pub use size_s::*;
pub use stream::basic::*;
pub use stream::{Byte, ByteStream, Stream, *};
pub use verismo_macro::*;
pub use verismo_verus::*;
pub use vstd::pervasive::{affirm, arbitrary, proof_from_false, spec_affirm, unreached};
pub use vstd::prelude::*;
pub use vstd::slice::SliceAdditionalSpecFns;
pub use vstd::std_specs::option::OptionAdditionalFns;
pub use vstd::std_specs::result::ResultAdditionalSpecFns;
pub use vstd::string::{StrSlice, String, *};
pub use vstd::view::*;
pub use wellformed::*;

verus! {

// const
#[verifier(inline)]
pub open spec fn spec_unused<T>() -> T {
    arbitrary()
}

} // verus!
verus! {

#[is_variant]
pub enum ResultOrErr<RetValue, ErrorID> {
    Ok(RetValue),
    Error(ErrorID),
}

impl<RetValue, ErrorID> ResultOrErr<RetValue, ErrorID> {
    pub open spec fn with_when_err(&self, err_ret: RetValue) -> ResultWithErr<RetValue, ErrorID> {
        match *self {
            ResultOrErr::Ok(ret) => ResultWithErr::Ok(ret),
            ResultOrErr::Error(err) => ResultWithErr::Error(err_ret, err),
        }
    }

    pub open spec fn to_result(&self) -> RetValue {
        match self {
            ResultOrErr::Ok(ret) => *ret,
            ResultOrErr::Error(ret) => spec_unused(),
        }
    }
}

} // verus!
verus! {

#[is_variant]
pub enum ResultWithErr<RetValue, ErrorID> {
    Ok(RetValue),
    Error(RetValue, ErrorID),
}

impl<RetValue, ErrorID> ResultWithErr<RetValue, ErrorID> {
    verus! {
        pub open spec fn with_err<ET2>(&self, err: ET2) -> ResultWithErr<RetValue, ET2> {
            match self {
                ResultWithErr::Ok(ret) => ResultWithErr::Error(*ret, err),
                ResultWithErr::Error(ret, olderr) => ResultWithErr::Error(*ret, err),
            }
        }

        pub open spec fn replace_err<ET2>(&self, err: ET2) -> ResultWithErr<RetValue, ET2> {
            match self {
                ResultWithErr::Ok(ret) => ResultWithErr::Ok(*ret),
                ResultWithErr::Error(ret, olderr) => ResultWithErr::Error(*ret, err),
            }
        }

        pub open spec fn with_ret<RT2>(&self, ret: RT2) -> ResultWithErr<RT2, ErrorID> {
            match self {
                ResultWithErr::Ok(_) => ResultWithErr::Ok(ret),
                ResultWithErr::Error(_, err) => ResultWithErr::Error(ret, *err),
            }
        }

        pub open spec fn to_result(&self) -> RetValue {
            match self {
                ResultWithErr::Ok(ret) => *ret,
                ResultWithErr::Error(ret, _) => *ret,
            }
        }

        pub open spec fn to_err(&self) -> ErrorID {
            match self {
                ResultWithErr::Ok(ret) => spec_unused(),
                ResultWithErr::Error(ret, err) => *err,
            }
        }
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/constant.rs
================

#[macro_export]
macro_rules! def_const_macro {
    ($($name:ident = $value:expr)+) => {
        $(
            #[allow(unused_attributes)]
            #[macro_export]
            macro_rules! $name {
                () => {
                    ($value)
                }
            }
        )+
    };
}

#[macro_export]
macro_rules! macro_const_int {
    ($($(#[$attr:meta])* $vis:vis const $name:ident : $type:ty = $value:expr ;)+) => {
        $(
            builtin_macros::verus!{
            #[allow(unused_attributes)]
            $( #[$attr] )* $vis const $name : $type = $value;
            }
#[allow(unused_attributes)]
            $( #[$attr] )*
            macro_rules! $name {
                () => {
                    builtin::spec_cast_integer::<_, int>($value)
                }
            }
        )+
    };
}

#[macro_export]
macro_rules! macro_const {
    ($($(#[$attr:meta])* $vis:vis const $name:ident : $type:ty = $value:expr ;)+) => {
        $(
            builtin_macros::verus!{
            #[allow(unused_attributes)]
            $( #[$attr] )* $vis const $name : $type = $value;
            }
#[allow(unused_attributes)]
            $( #[$attr] )*
            macro_rules! $name {
                () => {
                    $value
                }
            }
        )+
    };
}

#[macro_export]
macro_rules! macro_def {
    ($var: ident: $val: expr) => {
        #[allow(unused_attributes)]
        #[macro_export]
        macro_rules! $var {
            () => {
                $val
            };
        }
    };
}

// VM constants
// Need to publish those constant if it is used in verification;
// otherwise, the root module will not understand those constant in spec.
//#[allow(unused_variables)]
crate::macro_const_int! {
    #[macro_export]
    pub const MAX_CPU_NUM: u64 = 128u64;
}

================
File: ./source/verismo/src/tspec/map_lib.rs
================

use vstd::prelude::*;
use vstd::set_lib;

use super::*;

verus! {

pub trait MapAsSeq<T> {
    spec fn is_seq(&self, n: nat) -> bool;
}

impl<T> MapAsSeq<T> for Map<nat, T> {
    open spec fn is_seq(&self, n: nat) -> bool {
        forall|k: nat| k < n ==> self.contains_key(k)
    }
}

pub proof fn tracked_seq_remove<T>(tracked m: &mut Map<nat, T>, i: nat, n: nat) -> (tracked ret: T)
    requires
        old(m).is_seq(n),
        i < n,
    ensures
        forall|k: nat| k < i ==> m[k] === old(m)[k] && m.contains_key(k),
        forall|k: nat| i <= k < (n - 1) ==> m[k] === old(m)[k + 1] && m.contains_key(k),
        ret === old(m)[i],
{
    let oldm = *m;
    let tracked ret = m.tracked_remove(i);
    let convert = |j: nat|
        if j < i {
            j
        } else {
            (j + 1) as nat
        };
    let key_map = Map::<nat, nat>::new(|j: nat| 0 <= j < (n - 1), |j: nat| convert(j));
    assert forall|j: nat| key_map.contains_key(j) implies m.contains_key(convert(j)) by {
        assert(oldm.contains_key(j));
        assert(oldm.contains_key(j + 1));
    }
    m.tracked_map_keys_in_place(key_map);
    ret
}

pub proof fn tracked_seq_insert<T>(tracked m: &mut Map<nat, T>, i: nat, tracked v: T, n: nat)
    requires
        old(m).is_seq(n),
        i <= n,
    ensures
        m[i] === v,
        m.contains_key(i),
        forall|k: nat| k < i ==> m[k] === old(m)[k] && m.contains_key(k),
        forall|k: nat|
            i + 1 <= k < (n + 1) ==> m[k] === old(m)[(k - 1) as nat] && m.contains_key(k),
{
    let oldm = *m;
    let tracked ret = m.tracked_insert(n, v);
    let convert = |j: nat|
        if j < i {
            j
        } else if j == i {
            n
        } else {
            (j - 1) as nat
        };
    let key_map = Map::<nat, nat>::new(|j: nat| 0 <= j < n + 1, |j: nat| convert(j));
    assert forall|j: nat| key_map.contains_key(j) implies m.contains_key(convert(j)) by {
        if j < i {
            assert(oldm.contains_key(j));
        }
        if j > i {
            assert(oldm.contains_key((j - 1) as nat));
        }
    }
    m.tracked_map_keys_in_place(key_map);
    ret
}

} // verus!

================
File: ./source/verismo/src/tspec/macros.rs
================

#[macro_export]
macro_rules! AsInt {
    ($val: expr) => {
        spec_cast_integer::<_, int>($val)
    };
}

#[macro_export]
macro_rules! AsNat {
    ($val: expr) => {
        spec_cast_integer::<_, nat>($val)
    };
}

#[macro_export]
macro_rules! addr2const {
    ($addr:expr, $t: ty) => {
        unsafe { &*($addr as *const $t) }
    };
}

#[macro_export]
macro_rules! addr2mut {
    ($addr:expr, $t: ty) => {
        unsafe { &mut *($addr as *mut $t) }
    };
}

#[macro_export]
macro_rules! struct2addr {
    ($data:expr) => {
        unsafe { &$data as *const _ as u64 }
    };
}

#[macro_export]
macro_rules! BITS_RANGE_MASK {
    ($first: expr, $last: expr) => {
        ((BIT!($last - $first + 1) - 1) << $first)
    };
}

#[macro_export]
macro_rules! BIT_FIELD_MAX_VAL {
    ($first: expr, $last: expr) => {
        BIT!($last - $first + 1)
    };
}

#[macro_export]
macro_rules! DEFINE_BITS_FIELD_GET {
    ($name: ident, $first: expr, $last: expr, $ty: tt) => {
        paste::paste! {
                                    #[inline]
                                    pub const fn [<get_ $name>](&self) -> $ty {
                                        ensures(|ret: $ty| ret == self.[<spec_get_ $name>]());
                                        let mask = BITS_RANGE_MASK!($first, $last);
                                        (self.value & mask) >> $first
                                    }
                                    verus!{
            pub open spec fn [<spec_get_ $name>](&self) -> $ty {
                let mask = BITS_RANGE_MASK!($first, $last);
                (self.value & mask) >> $first
            }
        }
                                }
    };
}

#[macro_export]
macro_rules! DEFINE_BIT_FIELD_GET {
    ($name: ident, $bit: expr) => {
        paste::paste! {
                                                    verismo_non_secret!{
                                                        #[inline]
                                                        pub fn [<is_ $name>](&self) -> (ret: bool)
                                                        requires
                                                            $bit < 64
                                                        ensures
                                                            ret === self.[<spec_is_ $name>]()
                                                        {
                                                            bit_check(self.value, $bit)
                                                        }
                                                    }

                                                    verus!{
                #[verifier(inline)]
                pub open spec fn [<spec_is_ $name>](&self) -> bool
                {
                    spec_has_bit_set(self.value.vspec_cast_to(), $bit)
                }
            }
                                                }
    };
}

#[macro_export]
macro_rules! DEFINE_BITS_FIELD_CONST {
    ($tyname: tt, $name: ident, $first: expr, $last: expr, $ty: tt) => {
        paste::paste! {
                                 pub const fn [<set_ $name>](&self, val: $ty) -> Self {
                                     requires([val < BIT_FIELD_MAX_VAL!($first, $last)]);
                                     ensures(|ret: $tyname|
                                         [builtin::equal(ret, self.[<spec_set_ $name>](val))]
                                     );
                                     let mask = BITS_RANGE_MASK!($first, $last);
                                     let value = self.value;
                                     let value = (value & !mask) | (val << $first);
                                     $tyname{value}
                                 }

                                verus!{
         pub open spec fn [<spec_set_ $name>](&self, val: $ty) -> Self {
             let mask = BITS_RANGE_MASK!($first, $last);
             let value = self.value;
             let value = (value & !mask) | (val << ($first as $ty));
             $tyname{value}
         }
         }
                            }
    };
}

#[macro_export]
macro_rules! DEFINE_BITS_FIELD {
    ($tyname: tt, $name: ident, $first: expr, $last: expr, $ty: tt) => {
        DEFINE_BITS_FIELD_CONST!($tyname, $name, $first, $last, $ty);

        DEFINE_BITS_FIELD_GET!($name, $first, $last, $ty);
    };
}

================
File: ./source/verismo/src/tspec/isconst.rs
================

use super::*;

verus! {

pub trait IsConstant {
    spec fn is_constant(&self) -> bool;

    spec fn is_constant_to(&self, vmpl: nat) -> bool;
}

#[verifier(external_body)]
pub proof fn lemma_is_constant_derive<T: IsConstant, T2: IsConstant>(v: T, f: spec_fn(T) -> T2)
    requires
        v.is_constant() ==> f(v).is_constant(),
{
}

#[verifier(external_body)]
pub proof fn axiom_default_const<T: SpecDefault + IsConstant>()
    ensures
        (&#[trigger] T::spec_default()).is_constant(),
{
}

#[verifier(external_body)]
pub broadcast proof fn axiom_const_forall<T: IsConstant>(v: T)
    ensures
        #[trigger] v.is_constant() == v.is_constant_to(1) && v.is_constant_to(2)
            && v.is_constant_to(3) && v.is_constant_to(4),
{
}

} // verus!
#[macro_export]
macro_rules! constant_vars {
    ($($var: ident),* $(,)?) => {
        $(
            $var.is_constant(),
        )*
    }
}
#[macro_export]
macro_rules! impl_spec_constant_for_basic {
    ($($type: ty),* $(,)?) => {
        $(verus!{
            impl IsConstant for $type {
                #[verifier(inline)]
                open spec fn is_constant(&self) -> bool
                {
                    true
                }

                #[verifier(inline)]
                open spec fn is_constant_to(&self, vmpl: nat) -> bool {
                    true
                }
            }
        }
)*
    }
}
verus! {

impl<T: IsConstant> IsConstant for Option<T> {
    #[verifier(inline)]
    open spec fn is_constant(&self) -> bool {
        self.is_Some() ==> self.get_Some_0().is_constant()
    }

    #[verifier(inline)]
    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        self.is_Some() ==> self.get_Some_0().is_constant_to(vmpl)
    }
}

impl IsConstant for () {
    #[verifier(inline)]
    open spec fn is_constant(&self) -> bool {
        true
    }

    #[verifier(inline)]
    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        true
    }
}

impl<T1: IsConstant, T2: IsConstant> IsConstant for (T1, T2) {
    #[verifier(inline)]
    open spec fn is_constant(&self) -> bool {
        self.0.is_constant() && self.1.is_constant()
    }

    #[verifier(inline)]
    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        self.0.is_constant_to(vmpl) && self.1.is_constant_to(vmpl)
    }
}

impl<T> IsConstant for Ghost<T> {
    #[verifier(inline)]
    open spec fn is_constant(&self) -> bool {
        true
    }

    #[verifier(inline)]
    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        true
    }
}

impl<T> IsConstant for Tracked<T> {
    #[verifier(inline)]
    open spec fn is_constant(&self) -> bool {
        true
    }

    #[verifier(inline)]
    open spec fn is_constant_to(&self, vmpl: nat) -> bool {
        true
    }
}

} // verus!
impl_spec_constant_for_basic! {u64, u32, u16, usize, u8, bool, char}

================
File: ./source/verismo/src/tspec/setlib.rs
================

use vstd::prelude::*;
use vstd::set_lib;

use super::*;

verus! {

pub proof fn lemma_ret_is_empty<A>(s: Set<A>) -> (b: bool)
    ensures
        b <==> s.finite() && s.len() == 0,
        b <==> s =~= Set::empty(),
{
    if s.finite() && s.len() == 0 {
        s.lemma_len0_is_empty();
    }
    s =~= Set::empty()
}

pub proof fn lemma_union_self<A>(s1: Set<A>)
    ensures
        s1.union(s1) =~~= s1,
{
}

pub proof fn lemma_union<A>(s1: Set<A>, s2: Set<A>)
    ensures
        s1.subset_of(s1.union(s2)),
        s2.subset_of(s1.union(s2)),
        s1.union(s2) =~~= (s2.union(s1)),
        s1.union(s2) =~~= (s1.union(s2).union(s2)),
{
    let ss1 = s1;
    let ss2 = s1.union(s2);
    assert forall|a: A| ss1.contains(a) implies ss2.contains(a) by {}
}

pub open spec fn convert_set<A, B>(s: Set<A>, f: spec_fn(B) -> A) -> Set<B> {
    Set::new(|a| s.contains(f(a)))
}

pub open spec fn uop_to_bop<T1, T2, T3>(op: spec_fn(T1) -> T3) -> spec_fn(T1, T2) -> T3 {
    |v1: T1, v2: T2| op(v1)
}

#[verifier(inline)]
pub open spec fn set_op<T1, T2, T3>(s1: Set<T1>, s2: Set<T2>, op_fn: spec_fn(T1, T2) -> T3) -> Set<
    T3,
> {
    Set::new(|val: T3| exists|v1, v2| s1.contains(v1) && s2.contains(v2) && val === op_fn(v1, v2))
}

#[verifier(inline)]
pub open spec fn set_uop<T1, T2>(s1: Set<T1>, op_fn: spec_fn(T1) -> T2) -> Set<T2> {
    Set::new(
        |val: T2| exists|v1| s1.contains(v1) && val === op_fn(v1),
    )
    //set_op(s1, Set::empty().insert(arbitrary::<T1>()), uop_to_bop(op_fn))

}

spec fn set_bop_recursive<T1, T2, T3>(
    s1: Set<T1>,
    s2: Set<T2>,
    op_fn: spec_fn(T1, T2) -> T3,
) -> Set<T3>
    recommends
        s1.finite(),
    decreases s1.len(),
{
    if s1.finite() {
        if !s1.is_empty() {
            let ss1 = s1.remove(s1.choose());
            if (ss1.len() < s1.len()) {
                set_bop_recursive(ss1, s2, op_fn).union(set_op1(s1.choose(), s2, op_fn))
            } else {
                // unreacheable
                Set::empty()
            }
        } else {
            Set::empty()
        }
    } else {
        Set::empty()
    }
}

#[verifier(inline)]
spec fn set_op1<T1, T2, T3>(v1: T1, s2: Set<T2>, op_fn: spec_fn(T1, T2) -> T3) -> Set<T3> {
    set_uop(s2, |v2| op_fn(v1, v2))
}

proof fn lemma_setop1<T1, T2, T3>(v1: T1, s2: Set<T2>, op_fn: spec_fn(T1, T2) -> T3) -> (ret: Set<
    T3,
>)
    requires
        s2.finite(),
    ensures
        ret =~~= set_op1(v1, s2, op_fn),
        ret.len() <= s2.len(),
        ret.finite(),
    decreases s2.len(),
{
    let ret = set_op1(v1, s2, op_fn);
    lemma_set_uop_len(s2, |v2| op_fn(v1, v2));
    ret
}

// when op_fn is a loseless function
proof fn lemma_setop_without_loss_disjoint<T1, T2, T3>(
    s1: Set<T1>,
    s2: Set<T2>,
    vv1: T1,
    op_fn: spec_fn(T1, T2) -> T3,
    reverse_op_fn: spec_fn(T3) -> (T1, T2),
)
    requires
        s1.len() > 0,
        s1.contains(vv1),
        forall|v1, v2| reverse_op_fn(#[trigger] op_fn(v1, v2)) == (v1, v2),
    ensures
        set_op1(vv1, s2, op_fn).disjoint(set_op(s1.remove(vv1), s2, op_fn)),
{
    let ss1 = s1.remove(vv1);
    let r1 = set_op(ss1, s2, op_fn);
    let r2 = set_op1(vv1, s2, op_fn);
    assert forall|val|
        (r1.contains(val) ==> !r2.contains(val)) && (r2.contains(val) ==> !r1.contains(val)) by {
        if r1.contains(val) {
            let (v1, v2) = choose|v1, v2|
                ss1.contains(v1) && s2.contains(v2) && val === op_fn(v1, v2);
            assert(ss1.contains(v1));
            assert(vv1 !== v1);
            assert(op_fn(v1, v2) === val);
            assert(reverse_op_fn(val) === (v1, v2));
        }
        if r2.contains(val) {
            let v2 = choose|v2| s2.contains(v2) && val === op_fn(vv1, v2);
            assert(reverse_op_fn(val) === (vv1, v2));
        }
    }
}

proof fn lemma_setop_3_union<T1, T2, T3>(
    s1: Set<T1>,
    s2: Set<T2>,
    op_fn: spec_fn(T1, T2) -> T3,
    vv1: T1,
) -> (ret: (Set<T3>, Set<T3>, Set<T3>))
    requires
        s1.len() > 0,
        s1.contains(vv1),
        s1.finite(),
        s2.finite(),
    ensures
        ret.0 =~~= set_op(s1, s2, op_fn),
        ret.1 =~~= set_op(s1.remove(vv1), s2, op_fn),
        ret.2 =~~= set_op1(vv1, s2, op_fn),
        ret.0 =~~= ret.1.union(ret.2),
{
    let r0 = set_op(s1, s2, op_fn);
    let r1 = set_op(s1.remove(vv1), s2, op_fn);
    let r2 = lemma_setop1(vv1, s2, op_fn);
    let ss1 = s1.remove(vv1);
    assert forall|val| #[trigger] r0.contains(val) == r1.union(r2).contains(val) by {
        if r0.contains(val) {
            let (v1, v2) = choose|v1, v2|
                s1.contains(v1) && s2.contains(v2) && val === op_fn(v1, v2);
            assert(s1.contains(v1) && s2.contains(v2) && val === op_fn(v1, v2));
            if (ss1.contains(v1)) {
                assert(r1.contains(val));
            } else {
                if (v1 != vv1) {
                    assert(ss1.contains(v1));
                } else {
                    assert(r2.contains(val));
                }
            }
        }
        if (r1.contains(val)) {
            let (v1, v2) = choose|v1, v2|
                ss1.contains(v1) && s2.contains(v2) && val === op_fn(v1, v2);
            assert(s1.contains(v1) && s2.contains(v2) && val === op_fn(v1, v2));
            assert(r0.contains(val))
        }
        if (r2.contains(val)) {
            let v2 = choose|v2| s2.contains(v2) && val === op_fn(vv1, v2);
            assert(s1.contains(vv1) && s2.contains(v2) && val === op_fn(vv1, v2));
            assert(r0.contains(val));
        }
    }
    (r0, r1, r2)
}

pub open spec fn exchange_spec_fn<T1, T2, T3>(op_fn: spec_fn(T1, T2) -> T3) -> spec_fn(
    T2,
    T1,
) -> T3 {
    |v1, v2| op_fn(v2, v1)
}

pub proof fn lemma_op_exchange<T1, T2, T3>(
    s1: Set<T1>,
    s2: Set<T2>,
    op_fn: spec_fn(T1, T2) -> T3,
) -> (ret: Set<T3>)
    requires
        s1.len() > 0,
        s1.finite(),
        s2.finite(),
    ensures
        ret =~~= set_op(s1, s2, op_fn),
        ret =~~= set_op(s2, s1, exchange_spec_fn(op_fn)),
{
    set_op(s1, s2, op_fn)
}

pub proof fn lemma_set_uop_len<T1, T2>(s1: Set<T1>, op_fn: spec_fn(T1) -> T2) -> (ret: Set<T2>)
    requires
        s1.finite(),
    ensures
        ret =~~= set_uop(s1, op_fn),
        (ret.len() == 0) == (s1.len() == 0),
        ret.len() <= s1.len(),
        ret.finite(),
    decreases s1.len(),
{
    let ret = set_uop(s1, op_fn);
    if !s1.is_empty() {
        let v1 = s1.choose();
        let prev = lemma_set_uop_len(s1.remove(v1), op_fn);
        assert(ret =~~= prev.insert(op_fn(v1)));
        assert(!ret.is_empty());
    } else {
        assert(s1.is_empty());
        assert forall|v| !ret.contains(v) by {};
        assert(ret =~~= Set::empty());
        assert(ret.is_empty());
    }
    ret
}

proof fn lemma_set_bop_len_recursive<T1, T3, T2>(
    s1: Set<T1>,
    s2: Set<T2>,
    op_fn: spec_fn(T1, T2) -> T3,
) -> (ret: Set<T3>)
    requires
        s1.finite(),
        s2.finite(),
    ensures
        ret =~~= set_bop_recursive(s1, s2, op_fn),
        (ret.len() == 0) == (s1.len() == 0 || s2.len() == 0),
        ret.len() <= s1.len() * s2.len(),
        ret.finite(),
    decreases s1.len(),
{
    let ret = set_bop_recursive(s1, s2, op_fn);
    if !s1.is_empty() {
        let v1 = s1.choose();
        let ss1 = s1.remove(v1);
        let r1 = lemma_set_bop_len_recursive(s1.remove(v1), s2, op_fn);
        let r2 = set_op1(v1, s2, op_fn);
        assert(ret =~~= r1.union(r2));
        let uop_fn = |v2| op_fn(v1, v2);
        assert(set_op1(v1, s2, op_fn) =~~= set_uop(s2, uop_fn));
        lemma_set_uop_len(s2, uop_fn);
        set_lib::lemma_len_union(r1, r2);
        assert(r2.len() <= s2.len()) by {
            lemma_setop1(v1, s2, op_fn);
        }
        assert(ss1.len() * s2.len() + s2.len() == s1.len() * s2.len()) by {
            proof_mul_assoc(ss1.len(), 1, s2.len());
            proof_mul_exchange(s2.len() as int, s1.len() as int);
            proof_mul_exchange(ss1.len() as int, s2.len() as int);
        }
        if s2.is_empty() {
            assert(ret.is_empty());
        } else {
            assert(!ret.is_empty()) by { assert(ret.contains(op_fn(s1.choose(), s2.choose()))) }
        }
    } else {
        assert(s1.len() * s2.len() == 0) by (nonlinear_arith)
            requires
                s1.len() == 0 || s2.len() == 0,
        ;
        assert(ret.is_empty());
    }
    ret
}

proof fn lemma_setop_eq_recursive<T1, T2, T3>(
    s1: Set<T1>,
    s2: Set<T2>,
    op_fn: spec_fn(T1, T2) -> T3,
) -> (ret: Set<T3>)
    requires
        s1.finite(),
        s2.finite(),
    ensures
        ret =~~= set_op(s1, s2, op_fn),
        ret =~~= set_bop_recursive(s1, s2, op_fn),
    decreases s1.len(),
{
    let ret = set_op(s1, s2, op_fn);
    if !s1.is_empty() {
        let prev = lemma_setop_eq_recursive(s1.remove(s1.choose()), s2, op_fn);
        assert(ret =~~= prev.union(set_op1(s1.choose(), s2, op_fn))) by {
            lemma_setop_3_union(s1, s2, op_fn, s1.choose());
        }
    } else {
        assert(ret =~~= Set::empty());
    }
    ret
}

pub proof fn lemma_setop_len<T1, T2, T3>(
    s1: Set<T1>,
    s2: Set<T2>,
    op_fn: spec_fn(T1, T2) -> T3,
) -> (ret: Set<T3>)
    requires
        s1.finite(),
        s2.finite(),
    ensures
        ret =~~= set_op(s1, s2, op_fn),
        (ret.len() == 0) == (s1.len() == 0 || s2.len() == 0),
        ret.len() <= s1.len() * s2.len(),
        ret.finite(),
{
    let ret = set_op(s1, s2, op_fn);
    lemma_setop_eq_recursive(s1, s2, op_fn);
    lemma_set_bop_len_recursive(s1, s2, op_fn);
    ret
}

pub proof fn lemma_seq_add_subrange<T>(s1: Seq<T>, s2: Seq<T>)
    ensures
        (s1 + s2).subrange(s1.len() as int, (s1 + s2).len() as int) =~~= s2,
;

} // verus!

================
File: ./source/verismo/src/tspec/default.rs
================

use super::*;
verus! {

pub trait SpecDefault {
    spec fn spec_default() -> Self where Self: core::marker::Sized;
}

pub open spec fn spec_default_<T: SpecDefault>() -> T {
    <T as SpecDefault>::spec_default()
}

impl SpecDefault for () {
    open spec fn spec_default() -> Self;
}

impl<T> SpecDefault for Ghost<T> {
    open spec fn spec_default() -> Self {
        arbitrary()
    }
}

impl<T> SpecDefault for Tracked<T> {
    open spec fn spec_default() -> Self {
        arbitrary()
    }
}

} // verus!
macro_rules! impl_typecast_to_bytes_traits {
    ($($baset: ty),*$(,)*) => {
        $(verus!{
            impl SpecDefault for $baset {
                open spec fn spec_default() -> Self {
                    0 as $baset
                }
            }
        }
)*
    }
}

impl_typecast_to_bytes_traits! {u64, u32, u16, u8, usize, int, nat}

================
File: ./source/verismo/src/tspec/ops.rs
================

use super::*;

verus! {

pub trait VSpecOrd<Rhs> {
    spec fn spec_lt(self, rhs: Rhs) -> bool where Self: core::marker::Sized;

    spec fn spec_le(self, rhs: Rhs) -> bool where Self: core::marker::Sized;

    spec fn spec_gt(self, rhs: Rhs) -> bool where Self: core::marker::Sized;

    spec fn spec_ge(self, rhs: Rhs) -> bool where Self: core::marker::Sized;
}

pub trait VSpecEq<Rhs> {
    spec fn spec_eq(self, rhs: Rhs) -> bool where Self: core::marker::Sized;
}

pub trait VSpecNeg {
    spec fn spec_neg(self) -> Self where Self: core::marker::Sized;
}

pub trait VSpecNot {
    spec fn spec_not(self) -> Self where Self: core::marker::Sized;
}

pub trait VSpecAdd<Rhs, Output> {
    spec fn spec_add(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecSub<Rhs, Output> {
    spec fn spec_sub(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecMul<Rhs, Output> {
    spec fn spec_mul(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecEuclideanDiv<Rhs, Output> {
    spec fn spec_euclidean_div(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecEuclideanMod<Rhs, Output> {
    spec fn spec_euclidean_mod(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecBitAnd<Rhs, Output> {
    spec fn spec_bitand(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecBitOr<Rhs, Output> {
    spec fn spec_bitor(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecBitXor<Rhs, Output> {
    spec fn spec_bitxor(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecShl<Rhs, Output> {
    spec fn spec_shl(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

pub trait VSpecShr<Rhs, Output> {
    spec fn spec_shr(self, rhs: Rhs) -> Output where Self: core::marker::Sized;
}

#[macro_export]
macro_rules! impl_spec_not_for_basic {
    ($($type: ty),* $(,)?) => {
        $(verus!{
            impl VSpecNot for $type {
                #[verifier(inline)]
                open spec fn spec_not(self) -> Self
                {
                    !self
                }
            }
        })*
    }
}

impl_spec_not_for_basic!{u64, u32, u16, usize, u8, bool}

impl VSpecEq<bool> for bool {
    #[verifier(inline)]
    open spec fn spec_eq(self, rhs: bool) -> bool {
        self == rhs
    }
}

} // verus!

================
File: ./source/verismo/src/tspec/integer.rs
================

use super::*;

verus! {

// Implement Int Value only for a ghost type without width.
pub trait IntValue {
    spec fn as_int(&self) -> int;

    spec fn from_int(val: int) -> Self where Self: core::marker::Sized;
}

pub trait IntOrd {
    spec fn ord_int(&self) -> int;
}

pub trait NotPrimitive {

}

pub trait ToSetTrait {
    spec fn to_set(&self) -> Set<int>;

    fn contains(&self, sval: u64) -> (ret: bool)
        ensures
            self.to_set().contains(sval as int) == ret,
    ;
}

} // verus!
verus! {

impl<T1: IntValue> VSpecAdd<int, T1> for T1 {
    open spec fn spec_add(self, rhs: int) -> T1 {
        T1::from_int(self.as_int() + rhs)
    }
}

impl<T1: IntValue> VSpecSub<int, T1> for T1 {
    open spec fn spec_sub(self, rhs: int) -> T1 {
        T1::from_int(self.as_int() - rhs)
    }
}

impl<T1: IntValue> VSpecSub<T1, int> for T1 {
    open spec fn spec_sub(self, rhs: T1) -> int {
        self.as_int() - rhs.as_int()
    }
}

impl<T1: IntValue> VSpecEuclideanDiv<int, T1> for T1 {
    open spec fn spec_euclidean_div(self, rhs: int) -> T1 {
        T1::from_int(self.as_int() / rhs)
    }
}

impl<T1: IntValue> VSpecEuclideanMod<int, T1> for T1 {
    open spec fn spec_euclidean_mod(self, rhs: int) -> T1 {
        T1::from_int(self.as_int() % rhs)
    }
}

impl<T1: IntValue> VSpecMul<int, T1> for T1 {
    open spec fn spec_mul(self, rhs: int) -> T1 {
        T1::from_int(self.as_int() * rhs)
    }
}

} // verus!
/*
macro_rules! impl_ordint_for_basic_inner {
    ($itype: ty) => {
        verus! {
            impl IntOrd for $itype {
                #[verifier(inline)]
                open spec fn ord_int(&self) -> int {
                    *self as int
                }
            }
        }
    }
}

macro_rules! impl_ordint_for_basic {
    ($($itype: ty),* $(,)?) => {
        $(
            impl_ordint_for_basic_inner!($itype);
        )*
    }
}
*/
macro_rules! impl_cmp_with_basic {
    ($basict: ty, $($fname: ident),* $(,)?) => {
        paste::paste! {verus! {
        impl<T: IntOrd> VSpecOrd<$basict> for T {
            $(
            #[verifier(inline)]
            open spec fn [<$fname>](self, rhs: $basict) -> bool {
                builtin::SpecOrd::$fname(self.ord_int(), rhs as int)
            }
            )*
        }
        }
}
    };
}

macro_rules! impl_spec_eq_with_basic {
    ($basict: ty, $($rhs: ty),* $(,)?) => {
        $(verus!{
        impl VSpecEq<$rhs> for $basict {
            #[verifier(inline)]
            open spec fn spec_eq(self, rhs: $rhs) -> bool {
                builtin::spec_eq(self, rhs)
            }
        }
        }
)*
        verus!{
        impl<T: IntOrd> VSpecEq<$basict> for T {
            #[verifier(inline)]
            open spec fn spec_eq(self, rhs: $basict) -> bool {
                builtin::spec_eq(self.ord_int(), rhs as int)
            }
        }}
    }
}

macro_rules! impl_spec_eq_with_basics {
    ($($basict: ty),* $(,)?) => {
        $(
            impl_spec_eq_with_basic!{$basict, u64, u32, u16, u8, usize, int, nat}
        )*
    }
}

macro_rules! impl_cmp_with_basics {
    ($($basict: ty),*$(,)*) => {
        $(
        impl_cmp_with_basic!{$basict, spec_lt, spec_le, spec_gt, spec_ge}
        )*
    }
}

macro_rules! impl_cmp_with_as_int {
    ($($fname: ident),* $(,)?) => {
        paste::paste! {
                                                                                        verus! {
        impl<T: IntOrd, T2: IntOrd> VSpecOrd<T2> for T {
            $(
            #[verifier(inline)]
            open spec fn [<$fname>](self, rhs: T2) -> bool {
                builtin::SpecOrd::$fname(self.ord_int(), rhs.ord_int())
            }
            )*
        }
        impl<T: IntOrd, T2: IntOrd> VSpecEq<T2> for T {
            #[verifier(inline)]
            open spec fn spec_eq(self, rhs: T2) -> bool {
                builtin::spec_eq(self.ord_int(), rhs.ord_int())
            }
        }
        }
                                                                                    }
    };
}

impl_cmp_with_as_int! {spec_lt, spec_le, spec_gt, spec_ge}

verus! {

pub proof fn lemma_int_value_ord<T: IntOrd>(t1: T, t2: T)
    ensures
        t1.ord_int() > t2.ord_int() ==> t1 > t2,
{
}

} // verus!
impl_cmp_with_basics! {int, nat, u8, u16, u32, u64, usize}
impl_spec_eq_with_basics! {int, nat, u8, u16, u32, u64, usize}

crate::macro_const! {
    #[macro_export]
    pub const MAXU64: u64  = 0xffff_ffff_ffff_ffffu64;
}

verus! {

impl VSpecEq<()> for () {
    #[verifier(inline)]
    open spec fn spec_eq(self, rhs: Self) -> bool {
        true
    }
}

} // verus!

================
File: ./source/verismo/src/mem/rawmem_s.rs
================

use super::*;
use crate::registers::SnpCore;

verus! {

pub tracked struct SnpMemCoreConsole {
    pub memperm: RawMemPerms,
    pub cc: crate::snp::SnpCoreConsole,
}

impl SnpMemCoreConsole {
    pub open spec fn wf(&self) -> bool {
        &&& self.cc.wf()
        &&& self.memperm.wf()
    }

    pub open spec fn wf_core(&self, cpu: nat) -> bool {
        &&& self.wf()
        &&& self.cc.snpcore.cpu() == cpu
    }
}

} // verus!

================
File: ./source/verismo/src/mem/rawmem_p2.rs
================

use super::*;
verus! {

impl RawMemPerms {
    // TODO
    #[verifier(external_body)]
    pub proof fn tracked_split_ranges(tracked &mut self, ranges: Set<(int, nat)>) -> (tracked ret:
        Self) where Self: core::marker::Sized
        requires
            forall|r| ranges.contains(r) && r.1 > 0 ==> old(self).contains_range(r),
            old(self).wf(),
        ensures
            self.wf(),
            ret.wf(),
            forall|r| old(self).contains_range(r) ==> self.contains_except(r, ranges),
            forall|r, snp|
                old(self).contains_snp(r, snp) ==> #[trigger] self.contains_with_snp_except(
                    r,
                    snp,
                    ranges,
                ),
            forall|r| ranges.contains(r) ==> ret.eq_at(*old(self), r),
            forall|r| ranges.contains(r) ==> self.no_range(r),
            forall|r|
                old(self).contains_range(r) && ranges_disjoint(ranges, r) ==> self.eq_at(
                    *old(self),
                    r,
                ),
            forall|r: (int, nat)|
                r.1 > 0 && !old(self).contains_range(r) ==> !self.contains_range(r)
                    && !ret.contains_range(r),
    {
        unimplemented!()
    }
}

} // verus!

================
File: ./source/verismo/src/mem/mod.rs
================

mod rawmem_p;
mod rawmem_p2;

mod rawmem_s;

pub use rawmem_p::*;
pub use rawmem_s::*;

use crate::ptr::{SnpMemAttrTrait, SnpPPtr, SnpPointsToBytes, SnpPointsToData, SnpPointsToRaw};
use crate::tspec::*;

================
File: ./source/verismo/src/mem/rawmem_p.rs
================

use super::*;
use crate::ptr::SwSnpMemAttr;
use crate::tspec_e::SecSeqByte;

verus! {

pub tracked struct RawMemPerms {
    perms: Map<int, SnpPointsToRaw>,
}

impl RawMemPerms {
    /*
        // BUG: verus does not support
        // private element in broadcast forall even in private proof
        #[verifier(external_body)]
        broadcast
        proof fn axiom_map_ext_equal_deep(self, other: Self)
        ensures
            #[trigger](self =~~= other) == (self.perms =~~= other.perms)
        {}*/
    pub closed spec fn ext_equal(self, other: Self) -> bool {
        self.perms =~~= other.perms
    }

    pub open spec fn deep_equal(self, other: Self) -> bool {
        forall|r| #[trigger]
            self.contains_range(r) === #[trigger] other.contains_range(r) && self[r] === other[r]
    }

    pub closed spec fn wf(self) -> bool {
        &&& forall|k: int|
            self.perms.contains_key(k) ==> (#[trigger] self.perms[k])@.range() === (k, 1)
        &&& forall|k: int| self.perms.contains_key(k) ==> (#[trigger] self.perms[k])@.snp.wf()
    }

    pub closed spec fn new(perms: Map<int, SnpPointsToRaw>) -> Self {
        Self { perms }
    }

    pub proof fn tracked_empty() -> (tracked ret: Self)
        ensures
            forall|r| !ret.contains_range(r),
    {
        let tracked ret = Self { perms: Map::tracked_empty() };
        assert forall|r| !ret.contains_range(r) by {
            if r.1 != 0 {
                assert(!ret.perms.contains_key(r.0));
            }
        }
        ret
    }

    pub closed spec fn spec_index(self, range: (int, nat)) -> SnpPointsToBytes {
        let (start, size) = range;
        let s = range2set(range);
        let p = self.perms;
        SnpPointsToBytes {
            pptr: start,
            snp_bytes: Seq::new(size, |i| p[start + i]@.snp_bytes[0]),
            snp: p[start]@.snp,
        }
    }

    // The range should be at least with size 1.
    pub closed spec fn contains_range(self, range: (int, nat)) -> bool {
        //&&& forall |i: int|  within_range(i, range) ==> #[trigger]self.perms.contains_key(i)
        let s = range2set(range);
        let perms = self.perms;
        &&& range.1 > 0
        &&& s.subset_of(perms.dom())
        &&& forall|i: int|
            within_range(i, range) ==> (#[trigger] perms[i])@.snp() === perms[range.0]@.snp()
    }

    pub open spec fn contains_except(&self, range: (int, nat), ranges: Set<(int, nat)>) -> bool {
        forall|r: (int, nat)|
            inside_range(r, range) && ranges_disjoint(ranges, r) && r.1 != 0
                ==> #[trigger] self.contains_range(r)
    }

    pub open spec fn contains_with_snp_except(
        &self,
        range: (int, nat),
        snp: SwSnpMemAttr,
        ranges: Set<(int, nat)>,
    ) -> bool {
        forall|r: (int, nat)|
            (inside_range(r, range) && ranges_disjoint(ranges, r) && r.1 != 0) ==> (
            #[trigger] self.contains_range(r) && self[r].snp() === snp)
    }

    #[verifier(inline)]
    pub open spec fn contains_default_except(
        &self,
        range: (int, nat),
        ranges: Set<(int, nat)>,
    ) -> bool {
        self.contains_with_snp_except(range, SwSnpMemAttr::spec_default(), ranges)
    }

    #[verifier(inline)]
    pub open spec fn contains_init_except(
        &self,
        range: (int, nat),
        ranges: Set<(int, nat)>,
    ) -> bool {
        self.contains_with_snp_except(range, SwSnpMemAttr::init(), ranges)
    }

    #[verifier(inline)]
    pub open spec fn contains_snp(&self, r: (int, nat), snp: SwSnpMemAttr) -> bool {
        &&& self.contains_range(r)
        &&& (self[r]).snp() === snp
    }

    #[verifier(inline)]
    pub open spec fn contains_init_mem(&self, r: (int, nat)) -> bool {
        self.contains_snp(r, SwSnpMemAttr::init())
    }

    #[verifier(inline)]
    pub open spec fn contains_default_mem(&self, r: (int, nat)) -> bool {
        self.contains_snp(r, SwSnpMemAttr::spec_default())
    }

    pub open spec fn contains_default_ranges(&self, ranges: Set<(int, nat)>) -> bool {
        // contains all memory permission validated
        &&& forall|r|
            (ranges.contains(r) && r.1 > 0) ==> #[trigger] self.contains_range(r)
                && self.contains_default_mem(r)
    }

    pub open spec fn no_range(self, range: (int, nat)) -> bool {
        &&& !self.contains_range(range)
        &&& forall|r: (int, nat)| inside_range(r, range) ==> !self.contains_range(r)
    }

    pub closed spec fn dom(self) -> Set<int> {
        self.perms.dom()
    }

    pub closed spec fn remove_range(self, range: (int, nat)) -> Self {
        Self { perms: self.perms.remove_keys(range2set(range)) }
    }

    pub closed spec fn restrict(self, range: (int, nat)) -> Self {
        Self { perms: self.perms.restrict(range.to_set()) }
    }

    pub closed spec fn union(self, other: Self) -> Self {
        Self { perms: self.perms.union_prefer_right(other.perms) }
    }

    #[verifier(inline)]
    pub open spec fn eq_at(self, other: Self, r: (int, nat)) -> bool {
        &&& self.contains_range(r) == other.contains_range(r)
        &&& self.contains_range(r) ==> { self[r] === other[r] }
    }
}

impl RawMemPerms {
    proof fn lemma_restricted_ensures(self, r: (int, nat))
        requires
            r.1 > 0,
        ensures
            forall|r2| inside_range(r2, r) ==> self.eq_at(self.restrict(r), r2),
    {
        assert forall|r2| inside_range(r2, r) implies self.eq_at(self.restrict(r), r2) by {
            self.lemma_restricted(r, r2);
        }
    }

    proof fn lemma_restricted(self, r: (int, nat), r2: (int, nat))
        requires
            r.1 > 0,
            inside_range(r2, r),
        ensures
            self.eq_at(self.restrict(r), r2),
    {
        let restrict = self.restrict(r);
        let perms = restrict.perms;
        let s = r.to_set();
        //assert(perms =~~= self.perms.restrict(r.to_set()));
        assert(s.subset_of(perms.dom()) === s.subset_of(self.perms.dom()));
        assert forall|i: int| within_range(i, r2) implies perms.contains_key(i)
            == self.perms.contains_key(i) && (perms.contains_key(i) ==> (#[trigger] perms[i])
            === self.perms[i]) by {
            assert(r.to_set().contains(i));
            assert(r2.to_set().contains(i));
        }
        if self.contains_range(r2) {
            assert forall|i: int| within_range(i, r2) implies perms.contains_key(i) && (
            perms[i])@.snp() === perms[r2.0]@.snp() by {
                assert(self.perms.contains_key(i));
                assert(self.perms[i]@.snp() === self.perms[r2.0]@.snp());
            }
            assert(perms[r2.0] === self.perms[r2.0]);
            assert(self[r2].bytes() =~~= restrict[r2].bytes());
            assert(self.restrict(r).contains_range(r2));
        }
        if restrict.contains_range(r2) {
            assert forall|i: int| within_range(i, r2) implies (#[trigger] self.perms[i])@.snp()
                === self.perms[r2.0]@.snp() by {
                assert(perms.contains_key(i));
                assert(perms[i]@.snp() === perms[r2.0]@.snp());
            }
            assert(range2set(r2).subset_of(self.perms.dom()));
            assert(self.contains_range(r2));
        }
        assert(self.contains_range(r2) == self.restrict(r).contains_range(r2));
    }

    proof fn lemma_remove_range_not_contains(self, r1: (int, nat), r2: (int, nat))
        requires
            !self.contains_range(r2),
        ensures
            !self.remove_range(r1).contains_range(r2),
    {
        let s2 = r2.to_set();
        let ret = self.remove_range(r1);
        if r2.1 > 0 {
            ret.lemma_restricted(r2, r2);
            self.lemma_restricted(r2, r2);
        }
        if ret.contains_range(r2) {
            assert(self.contains_range(r2)) by {
                assert forall|i: int| range2set(r2).contains(i) implies self.perms.dom().contains(
                    i,
                ) by {
                    assert(ret.perms.contains_key(i));
                    assert(within_range(i, r2));
                }
                assert(range2set(r2).subset_of(self.perms.dom()));
                assert forall|i: int| within_range(i, r2) implies (#[trigger] self.perms[i])@.snp()
                    === self.perms[r2.0]@.snp() by {
                    assert(ret.perms[i]@.snp() === ret.perms[r2.0]@.snp());
                }
            }
        }
    }

    proof fn lemma_remove_range_contains(self, r1: (int, nat), r2: (int, nat))
        requires
            range_disjoint_(r1, r2),
            self.contains_range(r2),
            //self.contains_range(r1),
            r2.1 > 0,
        ensures
            self.remove_range(r1).contains_range(r2),
            self.remove_range(r1)[r2] === self[r2],
    {
        let ret = self.remove_range(r1);
        ret.lemma_restricted(r2, r2);
        self.lemma_restricted(r2, r2);
        proof_range_set_disjoint(r1, r2);
        assert(range2set(r1).disjoint(range2set(r2)));
        assert(self.perms.remove_keys(range2set(r1)).restrict(range2set(r2))
            =~~= self.perms.restrict(range2set(r2)));
        let s2 = range2set(r2);
        assert(s2.contains(r2.0));
        assert(ret.perms.restrict(s2) =~~= self.perms.restrict(s2));
        assert(ret.perms[r2.0] === self.perms[r2.0]);
        assert(ret[r2].snp === self[r2].snp);
        assert(ret[r2].snp_bytes =~~= self[r2].snp_bytes);
    }

    pub proof fn lemma_remove_range_ensures(self, r1: (int, nat), r2: (int, nat)) -> (ret: Self)
        ensures
            ret === self.remove_range(r1),
            inside_range(r2, r1) ==> !ret.contains_range(r2),
            range_disjoint_(r1, r2) && self.contains_range(r2) ==> ret.contains_range(r2) && ret[r2]
                === self[r2],
            !self.contains_range(r2) ==> !ret.contains_range(r2),
    {
        let ret = self.remove_range(r1);
        if r2.1 > 0 {
            if range_disjoint_(r1, r2) && self.contains_range(r2) {
                self.lemma_remove_range_contains(r1, r2);
            }
            if !self.contains_range(r2) {
                self.lemma_remove_range_not_contains(r1, r2);
            }
            if inside_range(r2, r1) {
                assert(!ret.perms.contains_key(r2.0));
            }
        }
        ret
    }

    spec fn ext_eq_contains_except(self, ret: Self, r2: (int, nat), rs: Set<(int, nat)>) -> bool {
        forall|r|
            inside_range(r, r2) && r.1 != 0 && ranges_disjoint(rs, r) ==> (
            #[trigger] self.contains_range(r) == #[trigger] ret.contains_range(r)) && (
            self.contains_range(r) ==> self[r] === ret[r])
    }

    proof fn lemma_except_contains_eq(
        self,
        ret: Self,
        r2: (int, nat),
        snp: SwSnpMemAttr,
        rs: Set<(int, nat)>,
    )
        requires
            self.ext_eq_contains_except(ret, r2, rs),
        ensures
            self.contains_with_snp_except(r2, snp, rs) == ret.contains_with_snp_except(r2, snp, rs),
    {
        if self.contains_with_snp_except(r2, snp, rs) {
            assert forall|r|
                inside_range(r, r2) && r.1 != 0 && ranges_disjoint(
                    rs,
                    r,
                ) implies ret.contains_range(r) && ret[r].snp() === snp by {
                assert(self.contains_range(r));
                assert(ret.contains_range(r));
                assert(self[r] === ret[r]);
            }
            assert(ret.contains_with_snp_except(r2, snp, rs));
        }
        if ret.contains_with_snp_except(r2, snp, rs) {
            assert forall|r|
                inside_range(r, r2) && r.1 != 0 && ranges_disjoint(
                    rs,
                    r,
                ) implies self.contains_range(r) && self[r].snp() === snp by {
                assert(ret.contains_range(r));
                assert(self.contains_range(r));
                assert(self[r] === ret[r]);
            }
            assert(self.contains_with_snp_except(r2, snp, rs));
        }
    }

    pub proof fn proof_remove_range_ensures_except(self, range: (int, nat)) -> (ret: Self)
        ensures
            ret === self.remove_range(range),
            forall|snp, r2, rs|
                range_disjoint_(r2, range) ==> (#[trigger] self.contains_with_snp_except(
                    r2,
                    snp,
                    rs,
                ) == #[trigger] ret.contains_with_snp_except(r2, snp, rs.insert(range))),
            forall|snp, r2, rs|
                range_disjoint_(r2, range) ==> (self.contains_with_snp_except(r2, snp, rs)
                    == ret.contains_with_snp_except(r2, snp, rs)),
            forall|snp, r2, rs|
                inside_range(r2, range) ==> (self.contains_with_snp_except(r2, snp, rs)
                    == self.restrict(range).contains_with_snp_except(r2, snp, rs)),
    {
        let ret = self.remove_range(range);
        assert forall|snp, r2, rs| range_disjoint_(r2, range) implies (
        self.contains_with_snp_except(r2, snp, rs) == ret.contains_with_snp_except(
            r2,
            snp,
            rs,
        )) by {
            assert forall|r| inside_range(r, r2) && r.1 != 0 && ranges_disjoint(rs, r) implies (
            #[trigger] self.contains_range(r) == #[trigger] ret.contains_range(r)) && (
            self.contains_range(r) ==> self[r] === ret[r]) by {
                self.proof_remove_range_ensures(range);
            }
            self.lemma_except_contains_eq(ret, r2, snp, rs);
        }
        assert forall|snp, r2, rs| inside_range(r2, range) implies (self.contains_with_snp_except(
            r2,
            snp,
            rs,
        ) == self.restrict(range).contains_with_snp_except(r2, snp, rs)) by {
            assert forall|r| inside_range(r, r2) && r.1 != 0 && ranges_disjoint(rs, r) implies (
            #[trigger] self.contains_range(r) == #[trigger] self.restrict(range).contains_range(r))
                && (self.contains_range(r) ==> self[r] === self.restrict(range)[r]) by {
                self.proof_remove_range_ensures(range);
            }
            self.lemma_except_contains_eq(self.restrict(range), r2, snp, rs);
        }
        assert forall|snp, r2, rs| range_disjoint_(r2, range) implies (
        #[trigger] self.contains_with_snp_except(r2, snp, rs)
            == #[trigger] ret.contains_with_snp_except(r2, snp, rs.insert(range))) by {
            assert forall|r| inside_range(r, r2) && r.1 != 0 implies ranges_disjoint(rs, r)
                == ranges_disjoint(rs.insert(range), r) by {
                lemma_ranges_disjoint_insert(r, range, rs);
            }
        }
        ret
    }

    pub proof fn proof_remove_range_ensures(self, range: (int, nat)) -> (ret: Self)
        ensures
            ret === self.remove_range(range),
            ret.no_range(range),
            //self.remove_range(range).union(self.restrict(range)) === self,
            forall|r: (int, nat)| range_disjoint_(range, r) ==> self.eq_at(ret, r),
            forall|r: (int, nat)| r.1 > 0 && !self.contains_range(r) ==> !ret.contains_range(r),
            range.1 > 0 ==> forall|r2|
                inside_range(r2, range) ==> self.eq_at(
                    self.restrict(range),
                    r2,
                ),
    //forall |snp, r2, rs| range_disjoint_(r2, range) ==>
    //(self.contains_with_snp_except(r2, snp, rs) == ret.contains_with_snp_except(r2, snp, rs)),
    //forall |snp, r2, rs| inside_range(r2, range) ==>
    //(self.contains_with_snp_except(r2, snp, rs) == self.restrict(range).contains_with_snp_except(r2, snp, rs))

    {
        let ret = self.remove_range(range);
        assert(self.remove_range(range).union(self.restrict(range)).perms =~~= self.perms);
        assert forall|r: (int, nat)|
            r.1 > 0 && range_disjoint_(range, r) && self.contains_range(
                r,
            ) implies ret.contains_range(r) && ret[r] === self[r] by {
            self.lemma_remove_range_ensures(range, r);
        }
        assert forall|r: (int, nat)| r.1 > 0 && !self.contains_range(r) implies !ret.contains_range(
            r,
        ) by {
            self.lemma_remove_range_ensures(range, r);
        }
        assert forall|r: (int, nat)| inside_range(r, range) implies !#[trigger] ret.contains_range(
            r,
        ) by {
            self.lemma_remove_range_ensures(range, r);
        }
        if range.1 > 0 {
            self.lemma_restricted_ensures(range);
        }
        ret
    }

    pub proof fn lemma_contain_range_empty(self)
        ensures
            forall|r: (int, nat)| r.1 == 0 ==> !#[trigger] self.contains_range(r),
    {
    }

    pub proof fn lemma_contain_range_sub(self, r1: (int, nat), r2: (int, nat))
        requires
            self.contains_range(r2),
            inside_range(r1, r2),
            r1.1 > 0,
        ensures
            self.contains_range(r1),
            self[r1].snp() === self[r2].snp(),
    {
        assert forall|i: int| within_range(i, r1) implies #[trigger] self.perms.contains_key(i)
            && self.perms[i]@.snp() === self.perms[r1.0]@.snp() by {
            assert(within_range(i, r2));
            assert(self.perms.contains_key(i));
            assert(self.perms.contains_key(r1.0));
        }
    }

    pub proof fn lemma_contain_except_range_sub(
        self,
        r1: (int, nat),
        r2: (int, nat),
        rs: Set<(int, nat)>,
    )
        requires
            self.contains_except(r2, rs),
            inside_range(r1, r2),
        ensures
            self.contains_except(r1, rs),
    {
        assert forall|r|
            inside_range(r, r1) && r.1 != 0 && ranges_disjoint(
                rs,
                r,
            ) implies #[trigger] self.contains_range(r) by {
            assert(inside_range(r, r2));
            assert(self.contains_except(r2, rs));
            assert(self.contains_range(r));
        }
    }

    pub proof fn lemma_with_except_sub(
        self,
        r1: (int, nat),
        r2: (int, nat),
        snp: SwSnpMemAttr,
        rs: Set<(int, nat)>,
    )
        requires
            self.contains_with_snp_except(r2, snp, rs),
            inside_range(r1, r2) || r1.1 == 0,
        ensures
            self.contains_with_snp_except(r1, snp, rs),
    {
        assert forall|r|
            inside_range(r, r1) && r.1 != 0 && ranges_disjoint(rs, r) implies self.contains_range(r)
            && self.contains_snp(r, snp) by {
            assert(inside_range(r, r2));
            assert(self.contains_range(r));
        }
    }

    pub proof fn proof_contains_range_to_except(self, r1: (int, nat), rs: Set<(int, nat)>)
        requires
            self.contains_range(r1) || r1.1 == 0,
        ensures
            self.contains_except(r1, rs),
            self.contains_with_snp_except(r1, self[r1].snp(), rs),
    {
        let snp = self[r1].snp();
        assert forall|r|
            inside_range(r, r1) && r.1 != 0 && ranges_disjoint(rs, r) implies self.contains_snp(
            r,
            snp,
        ) by {
            assert(inside_range(r, r1));
            assert(self.contains_snp(r1, snp));
            self.lemma_contain_range_sub(r, r1);
        }
    }

    pub proof fn lemma_empty_contains_except(
        self,
        r1: (int, nat),
        snp: SwSnpMemAttr,
        rs: Set<(int, nat)>,
    )
        requires
            inside_ranges(r1, rs) || r1.1 == 0,
        ensures
            self.contains_with_snp_except(r1, snp, rs),
    {
        assert forall|r|
            inside_range(r, r1) && r.1 != 0 && ranges_disjoint(rs, r) implies self.contains_range(r)
            && self.contains_snp(r, snp) by {
            assert(inside_ranges(r1, rs));
            let rr = choose|rr| rs.contains(rr) && inside_range(r1, rr);
            assert(rs.contains(rr));
            assert(inside_range(r1, rr));
            assert(ranges_disjoint(rs, r));
            assert(range_disjoint_(rr, r));
            assert(r1.1 == 0 || rr.1 == 0);
        }
    }

    proof fn _lemma_contain_range_union(self, r1: (int, nat), r2: (int, nat)) -> (r3: (int, nat))
        requires
            self.contains_range(r2),
            self.contains_range(r1),
            self[r2].snp() === self[r1].snp(),
            r1.0 <= r2.0,
            r1.0 + r1.1 >= r2.0,  // r1 and r2 overlaps
            r2.0 + r2.1 >= r1.0 + r1.1,
        ensures
            r3.0 == r1.0,
            r3.0 + r3.1 == r2.0 + r2.1,
            self.contains_range((r3)),
            self[r3].snp() === self[r1].snp(),
    {
        let lower = r1.0;
        let upper = r2.0 + r2.1;
        assert(lower <= upper);
        let ret = (lower, (upper - lower) as nat);
        assert forall|i: int| within_range(i, ret) implies self.perms.contains_key(i) && (
        #[trigger] self.perms[i])@.snp() === self[ret].snp() by {
            assert(r1.0 + r1.1 >= r2.0);
            if (within_range(i, r1)) {
                assert(self.perms.contains_key(i));
                assert(self.perms[i]@.snp() === self[r1].snp());
            } else {
                assert(within_range(i, r2));
                assert(self.perms.contains_key(i));
                assert(self.perms[i]@.snp() === self[r2].snp());
            }
        }
        ret
    }

    pub proof fn lemma_with_except_union(
        self,
        r1: (int, nat),
        r2: (int, nat),
        snp: SwSnpMemAttr,
        rs: Set<(int, nat)>,
    ) -> (r3: (int, nat))
        requires
            self.contains_with_snp_except(r1, snp, rs) || r1.1 == 0,
            self.contains_with_snp_except(r2, snp, rs) || r2.1 == 0,
            r1.0 <= r2.0,
            r1.end() >= r2.0,  // r1 and r2 overlaps
            r2.end() >= r1.0 + r1.1,
        ensures
            r3.0 == r1.0,
            r3.0 + r3.1 == r2.0 + r2.1,
            self.contains_with_snp_except(r3, snp, rs) || r3.1 == 0,
    {
        let lower = r1.0;
        let upper = r2.0 + r2.1;
        assert(lower <= upper);
        let ret = range(lower, upper);
        assert forall|r|
            inside_range(r, ret) && r.1 != 0 && ranges_disjoint(rs, r) implies self.contains_range(
            r,
        ) && self.contains_snp(r, snp) by {
            assert forall|i: int| within_range(i, r) implies self.perms.contains_key(i) && (
            #[trigger] self.perms[i])@.snp() === snp by {
                assert forall|rr| rs.contains(rr) implies range_disjoint_(rr, (i, 1)) by {}
                assert(ranges_disjoint(rs, (i, 1)));
                if (within_range(i, r1)) {
                    assert(r1.1 != 0);
                    assert(inside_range((i, 1), r1));
                    assert(self.contains_range((i, 1)));
                    assert(self.perms.contains_key(i));
                    assert(self.perms[i]@.snp() === snp);
                } else {
                    assert(r2.1 != 0);
                    assert(within_range(i, r2));
                    assert(inside_range((i, 1), r2));
                    assert(self.contains_range((i, 1)));
                    assert(self.perms.contains_key(i));
                    assert(self.perms[i]@.snp() === snp);
                }
            }
            assert(range2set(r).subset_of(self.perms.dom())) by {
                assert forall|i| within_range(i, r) implies self.perms.contains_key(i) by {
                    assert(self.perms[i]@.snp() === snp);
                }
            }
            assert(self.contains_range(r));
        }
        ret
    }

    pub proof fn lemma_contain_range_union(self, r1: (int, nat), r2: (int, nat)) -> (r3: (int, nat))
        requires
            self.contains_range(r2) || r2.1 == 0,
            self.contains_range(r1) || r1.1 == 0,
            self[r2].snp() === self[r1].snp() || r2.1 == 0 || r1.1 == 0,
            r1.0 <= r2.0,
            r1.0 + r1.1 >= r2.0,  // r1 and r2 overlaps
            r2.0 + r2.1 >= r1.0 + r1.1,
        ensures
            r3.0 == r1.0,
            r3.0 + r3.1 == r2.0 + r2.1,
            self.contains_range((r3)) || r3.1 == 0,
            self[r3].snp() === self[r1].snp(),
    {
        let lower = r1.0;
        let upper = r2.0 + r2.1;
        assert(lower <= upper);
        let ret = range(lower, upper);
        if r1.1 > 0 && r2.1 > 0 {
            self._lemma_contain_range_union(r1, r2);
        } else if r1.1 > 0 {
            assert(ret === r1);
        } else {
            assert(ret === r2);
        }
        ret
    }

    proof fn _tracked_insert(tracked &mut self, range: (int, nat), tracked perm: SnpPointsToRaw)
        requires
            old(self).no_range(range),
            old(self).wf(),
            perm@.range() === range,
            perm@.snp.wf(),
            range.1 > 0,
        ensures
            self.wf(),
            self[range].sw_eq(perm@),
            self.contains_range(range),
            *old(self) === self.remove_range(range),
            self.ext_equal((*old(self)).union(self.restrict(range))),
        decreases range.1,
    {
        let oldself = *self;
        let (start, size) = range;
        let last = start + size - 1;
        if size > 1 {
            let prev_range = (start, (size - 1) as nat);
            let tracked (prev_perm, p) = perm.trusted_split((size - 1) as nat);
            self._tracked_insert(prev_range, prev_perm);
            let prevself = *self;
            self.perms.tracked_insert(last, p);
            assert(prev_range.to_set().insert(last) =~~= range.to_set());
            assert(oldself.perms =~~= self.remove_range(range).perms) by {
                prevself.lemma_remove_range_ensures(prev_range, (last, 1));
                assert(!prevself.contains_range((last, 1)));
                assert(!prevself.perms.contains_key(last));
                assert(self.perms.remove(last) =~~= prevself.perms);
                assert(oldself.perms =~~= prevself.remove_range(prev_range).perms);
            }
            assert(self[range].sw_eq(perm@)) by {
                assert(self[range].bytes() =~~= prevself[prev_range].bytes() + p@.bytes());
            }
        } else if size == 1 {
            self.perms.tracked_insert(last, perm);
            assert(self[range].sw_eq(perm@));
            assert(oldself.perms =~~= self.remove_range(range).perms) by {
                assert(!oldself.contains_range((last, 1)));
                //assert(!oldself.perms.contains_key(last));
            }
        }
    }

    pub proof fn tracked_insert(tracked &mut self, range: (int, nat), tracked perm: SnpPointsToRaw)
        requires
            forall|r| inside_range(r, range) ==> !old(self).contains_range(r),
            old(self).wf(),
            perm@.range() === range,
            perm@.snp.wf(),
            range.1 > 0,
        ensures
            self.wf(),
            self[range].sw_eq(perm@),
            self.contains_range(range),
            *old(self) === self.remove_range(
                range,
            ),
    //forall |r: (int, nat)| r.1 > 0 && range_disjoint_(range, r) && self.contains_range(r) ==> old(self).contains_range(r) && old(self)[r] === self[r],
    //forall |r: (int, nat)| r.1 > 0 && !self.contains_range(r) ==> !old(self).contains_range(r)

    {
        let ret = self._tracked_insert(range, perm);
        self.proof_remove_range_ensures(range);
        ret
    }

    proof fn _tracked_remove(tracked &mut self, range: (int, nat)) -> (tracked ret: SnpPointsToRaw)
        requires
            old(self).contains_range(range),
            range.1 > 0,
            old(self).wf(),
        ensures
            ret@.range() === range,
            ret@.sw_eq(old(self)[range]),
            ret@.snp.wf(),
            self.wf(),
            forall|i|
                (!(range.0 <= i < range.0 + range.1) && old(self).perms.contains_key(i))
                    ==> self.perms.contains_key(i),
            forall|i| (range.0 <= i < range.0 + range.1) ==> !self.perms.contains_key(i),
            forall|i|
                !(range.0 <= i < range.0 + range.1) && old(self).perms.contains_key(i)
                    ==> #[trigger] self.perms[i] === old(self).perms[i],
        decreases range.1,
    {
        let (start, size) = range;
        if size > 1 {
            let prev_range = (start, (size - 1) as nat);
            let tracked prev = self._tracked_remove(prev_range);
            let last = start + size - 1;
            //assert(self.perms[last] === old(self).perms[last]);
            let tracked last_bytes = self.perms.tracked_remove(last);
            assert(last_bytes@.snp() === old(self).perms[start]@.snp());
            prev.trusted_join(last_bytes)
        } else {
            self.perms.tracked_remove(start)
        }
    }

    pub proof fn tracked_remove(tracked &mut self, range: (int, nat)) -> (tracked ret:
        SnpPointsToRaw)
        requires
            old(self).contains_range(range),
            range.1 > 0,
            old(self).wf(),
        ensures
            ret@.range() === range,
            ret@.sw_eq(old(self)[range]),
            ret@.snp.wf(),
            self.wf(),
            *self === old(self).remove_range(
                range,
            ),
    //self.no_range(range),
    //forall |r: (int, nat)| inside_range(r, range) ==> !self.contains_range(r),
    //forall |r: (int, nat)| (r.1 > 0 && range_disjoint_(range, r) && old(self).contains_range(r)) ==> self.contains_range(r) && self[r] === old(self)[r],
    //forall |r: (int, nat)| (r.1 > 0 && !old(self).contains_range(r)) ==> !self.contains_range(r)

    {
        self.proof_remove_range_ensures(range);
        let tracked removed = self.perms.tracked_remove_keys(range2set(range));
        let tracked mut removed = RawMemPerms { perms: removed };
        removed._tracked_remove(range)
    }

    pub proof fn tracked_split(tracked &mut self, range: (int, nat)) -> (tracked ret: Self)
        requires
            range.1 > 0,
            old(self).wf(),
        ensures
            ret.wf(),
            self.wf(),
            ret === old(self).restrict(range),
            *self === old(self).remove_range(range),
            self.no_range(range),
    {
        self.proof_remove_range_ensures(range);
        let dom = self.perms.dom();
        let s = range2set(range);
        let toremove = s.intersect(self.perms.dom());
        assert(s.intersect(dom).intersect(dom) =~~= s.intersect(dom));
        assert(self.restrict(range).perms =~~= self.perms.restrict(s.intersect(self.perms.dom())));
        assert(self.perms.remove_keys(s) =~~= self.perms.remove_keys(
            s.intersect(self.perms.dom()),
        ));
        let tracked removed = self.perms.tracked_remove_keys(toremove);
        RawMemPerms { perms: removed }
    }

    pub proof fn tracked_union(tracked &mut self, tracked other: Self) where
        Self: core::marker::Sized,

        requires
            old(self).wf(),
            other.wf(),
        ensures
            *self === (*old(self)).union(other),
            self.wf(),
    {
        self.perms.tracked_union_prefer_right(other.perms);
    }

    #[verifier(external_body)]
    pub proof fn lemma_union_propograte_except(
        mem1: Self,
        mem2: Self,
        range: (int, nat),
        snp: SwSnpMemAttr,
        except1: Set<(int, nat)>,
        except2: Set<(int, nat)>,
    )
        requires
            mem1.contains_with_snp_except(range, snp, except1),
            forall|r| except1.contains(r) ==> mem2.contains_with_snp_except(r, snp, except2),
        ensures
            mem1.union(mem2).contains_with_snp_except(range, snp, except2),
    {
    }
}

} // verus!

================
File: ./source/verismo/src/debug/ghcb_print.rs
================

use alloc::string::ToString;

use vstd::string::*;

use super::*;
use crate::global::*;
use crate::ptr::*;
use crate::registers::*;
use crate::snp::ghcb::*;
use crate::snp::snpcore_console_wf;
use crate::lock::MapRawLockTrait;

verus! {

spec fn ascii_is_num(val: u8) -> bool {
    ||| '0' as u8 <= val <= '9' as u8
    ||| 'a' as u8 <= val <= 'f' as u8
}

fn num_to_char(n: u8) -> (ret: u8)
    requires
        0 <= n < 16,
    ensures
        ascii_is_num(ret),
{
    if n < 10 {
        n + 48
    } else {
        97 + n - 10
    }
}

#[verifier(external_body)]
fn bytes_to_str<const N: usize_t, 'a>(arr: &'a Array<u8_t, N>) -> (ret: StrSlice<'a>)
    ensures
        ret.is_ascii(),
        ret@.len() <= arr@.len(),
        forall|i| 0 <= i < ret@.len() ==> arr@[i] == ret@[i] as u8,
{
    let slice = arr.array.as_slice();
    StrSlice::from_rust_str(core::str::from_utf8(slice).unwrap())
}

} // verus!
verus! {

fn int2bytes(input: u64, base: u64) -> (ret: (Array<u8_t, 66>, usize))
    requires
        base > 1,
        input.is_constant(),
        base.is_constant(),
        1 < base as int <= 16,
    ensures
        ascii_is_num(ret.0@[0]),
        3 <= ret.1 <= 66,
        (forall|k: int| 2 <= k < (ret.1 as int) ==> ascii_is_num(ret.0@[k])),
{
    let mut n = input;
    let mut bytes: Array<u8, 66> = Array::new(0);
    if n == 0 {
        bytes.update(0, '0' as u8);
        bytes.update(1, 'x' as u8);
        bytes.update(2, num_to_char(0));
        return (bytes, 3);
    }
    let mut i: usize = 0;
    proof {
        bit_shl64_pow2_auto();
        assert(1u64 << 0u64 == 1u64) by (bit_vector);
        assert(n <= MAXU64!());
        assert(MAXU64!() / (1u64 << 0u64) == MAXU64!());
    }
    while n > 0 && i < 64
        invariant
            0 <= i as int <= 64,
            1 < base as int <= 16,
            i < 64 ==> n <= MAXU64 / (1u64 << i as u64),
            i == 64 ==> n == 0,
            n == 0 ==> i > 0,
            forall|k| 0 <= k < i ==> ascii_is_num(bytes@[k]),
    {
        proof {
            assert(n as u64 / base as u64 <= n as u64 / 2) by (nonlinear_arith)
                requires
                    n >= 0,
                    base > 1,
            ;
            bit_shl64_pow2_auto();
            if i < 63 {
                let pow1 = (1u64 << i as u64);
                let pow2 = (1u64 << add(i as u64, 1u64));
                assert(pow1 * 2 <= POW2!(63));
                assert(pow2 == (pow1 * 2) as u64);
                assert(pow1 * 2 == (pow1 * 2) as u64);
                assert(MAXU64 / pow2 == MAXU64 / ((pow1 * 2) as u64));
                assume(MAXU64!() / ((pow1 * 2) as u64) == MAXU64!() / pow1 / 2);  // TODO: add nonlinear proof
            }
            assert(MAXU64!() / (1u64 << 63u64) / 2 == 0);
        }
        bytes.update(i, num_to_char((n % base) as u8));
        n = n / base;
        i = i + 1;
    }
    if base == 16 {
        bytes.update(i, 'x' as u8);
    } else if base == 8 {
        bytes.update(i, 'h' as u8);
    } else if base == 2 {
        bytes.update(i, 'b' as u8);
    } else {
        bytes.update(i, '_' as u8);
    }
    bytes.update(i + 1, '0' as u8);
    i = i + 2;
    bytes.reverse(0, i);
    (bytes, i)
}

} // verus!
use vstd::slice::{slice_index_get, slice_subrange};
verus! {

fn bytes2u64(s: &[u8], start: usize_t, size: usize_t) -> (ret: u64_t)
    requires
        start < s@.len(),
        size <= s@.len() - start,
        s@.len() < MAXU64,
        size < 8,
{
    let mut ret: u64_t = 0;
    let mut i = start;
    while i < start + size
        invariant
            start <= i <= start + size,
            start + size <= s@.len(),
            size < 8,
            s@.len() < MAXU64,
    {
        let c: u64 = (*slice_index_get(s, i)) as u64;
        let offset = (i - start) as u64;
        ret = ret | (c << (offset * 8));
        i = i + 1;
    }
    ret
}

fn str2u64(s: &StrSlice, start: usize_t, size: usize_t) -> (ret: u64_t)
    requires
        start < s@.len(),
        size <= s@.len() - start,
        s@.len() < MAXU64,
        size < 8,
        s.is_ascii(),
{
    let mut ret: u64_t = 0;
    let mut i = start;
    while i < start + size
        invariant
            start <= i <= start + size,
            start + size <= s@.len(),
            size < 8,
            s.is_ascii(),
            s@.len() < MAXU64,
    {
        let c: u64 = s.get_ascii(i) as u64;
        let offset = (i - start) as u64;
        ret = ret | (c << (offset * 8));
        i = i + 1;
    }
    ret
}

} // verus!
verus! {

fn ghcb_prints_with_lock<'a>(s: &StrSlice<'a>, Tracked(cc): Tracked<&mut SnpCoreConsole>) -> (ret:
    usize)
    requires
        old(cc).wf(),
        s@.len() < MAXU64,
        s.is_ascii(),
    ensures
        cc.wf(),
        ret == s@.len(),
        GHCBProto::restored_ghcb(cc.snpcore, old(cc).snpcore),
        ret.is_constant(),
        print_ensures_cc(*old(cc), *cc),
{
    let tracked mut console = cc.console.tracked_remove(0);
    let (ret, Tracked(console)) = ghcb_prints_with_lock2(
        s,
        Tracked(&mut cc.snpcore),
        Tracked(console),
    );
    proof {
        cc.console.tracked_insert(0, console);
    }
    ret
}

fn ghcb_prints_with_lock2<'a>(
    s: &StrSlice<'a>,
    Tracked(snpcore): Tracked<&mut SnpCore>,
    Tracked(console): Tracked<SnpPointsToRaw>,
) -> (ret: (usize, Tracked<SnpPointsToRaw>))
    requires
        snpcore_console_wf(*old(snpcore), console),
        s@.len() < MAXU64,
        s.is_ascii(),
    ensures
        snpcore_console_wf(*snpcore, ret.1@),
        ret.0 == s@.len(),
        ret.0.is_constant(),
        GHCBProto::restored_ghcb(*snpcore, *old(snpcore)),
        print_ensures_snp_c(*old(snpcore), (console), *snpcore, ret.1@),
{
    let mut index: usize = 0;
    let n = s.unicode_len();
    let ghost prevcore = *snpcore;
    let tracked perm = snpcore.regs.tracked_borrow(GHCB_REGID());
    fence();
    let ghcb_msr = MSR_GHCB();
    let oldval = ghcb_msr.read(Tracked(perm));
    let ghost oldconsole = console@;
    let tracked mut console = console;
    while index < n
        invariant
            n == s@.len(),
            s@.len() < MAXU64,
            s.is_ascii(),
            index <= n,
            index.is_constant(),
            snpcore_console_wf(*snpcore, console),
            snpcore.only_reg_coremode_updated(prevcore, set![GHCB_REGID()]),
            console@.only_val_updated(oldconsole),
    {
        let len = min(6, n as u64 - index as u64) as usize;
        let val: u64_t = GHCB_HV_DEBUG;
        let val = val | (str2u64(&s, index as usize_t, len as usize_t) << 16u64);
        let tracked mut some_console = Some(console);
        ghcb_msr_send(val, Tracked(&mut some_console), Tracked(snpcore));
        proof {
            console = some_console.tracked_unwrap();
        }
        index = index + len;
    }
    // restore ghcb msr val

    let tracked mut perm = snpcore.regs.tracked_remove(GHCB_REGID());
    ghcb_msr.write(oldval, Tracked(&mut perm));
    fence();
    proof {
        snpcore.regs.tracked_insert(GHCB_REGID(), perm);
    }
    (n as usize, Tracked(console))
}

} // verus!
verus! {

fn ghcb_print_bytes_with_lock<'a>(s: &[u8_t], Tracked(cc): Tracked<&mut SnpCoreConsole>) -> (ret:
    usize)
    requires
        old(cc).wf(),
        s@.len() < MAXU64,
    ensures
        ret == s@.len(),
        ret.is_constant(),
        GHCBProto::restored_ghcb(cc.snpcore, old(cc).snpcore),
        print_ensures_cc(*old(cc), *cc),
{
    let (ret, Tracked(console)) = ghcb_print_bytes_with_lock2(
        s,
        Tracked(&mut cc.snpcore),
        Tracked(cc.console.tracked_remove(0)),
    );
    proof {
        cc.console.tracked_insert(0, console);
    }
    ret
}

fn ghcb_print_bytes_with_lock2<'a>(
    s: &[u8_t],
    Tracked(snpcore): Tracked<&mut SnpCore>,
    Tracked(console): Tracked<SnpPointsToRaw>,
) -> (ret: (usize, Tracked<SnpPointsToRaw>))
    requires
        snpcore_console_wf(*old(snpcore), console),
        s@.len() < MAXU64,
    ensures
        ret.0 == s@.len(),
        ret.0.is_constant(),
        GHCBProto::restored_ghcb(*snpcore, *old(snpcore)),
        print_ensures_snp_c(*old(snpcore), console, *snpcore, ret.1@),
{
    let tracked mut console = console;
    let ghost oldconsole = console;
    let ghost prevcore = *snpcore;
    let tracked perm = snpcore.regs.tracked_borrow(GHCB_REGID());
    let mut index: usize = 0;
    let n = s.len();
    fence();
    let ghcb_msr = MSR_GHCB();
    let oldval = ghcb_msr.read(Tracked(perm));
    while index < n
        invariant
            n == s@.len(),
            s@.len() < MAXU64,
            index <= n,
            index.is_constant(),
            snpcore_console_wf(*snpcore, console),
            snpcore.only_reg_coremode_updated(prevcore, set![GHCB_REGID()]),
            console@.only_val_updated(oldconsole@),
    {
        let len = min(6, n as u64 - index as u64) as usize;
        let val: u64_t = GHCB_HV_DEBUG;
        let val = val | ((bytes2u64(s, index as usize_t, len as usize_t) as u64) << 16u64);
        let tracked mut some_console = Some(console);
        ghcb_msr_send(val, Tracked(&mut some_console), Tracked(snpcore));
        proof {
            console = some_console.tracked_unwrap();
        }
        index = index + len;
    }
    // restore ghcb msr val

    let tracked mut perm = snpcore.regs.tracked_remove(GHCB_REGID());
    ghcb_msr.write(oldval, Tracked(&mut perm));
    fence();
    proof {
        snpcore.regs.tracked_insert(GHCB_REGID(), perm);
    }
    (n as usize, Tracked(console))
}

} // verus!
verus! {

impl<'a> VPrint for StrSlice<'a> {
    open spec fn early_print_requires(&self) -> bool {
        &&& self@.len() < MAXU64 - 64
        &&& self.is_ascii()
    }

    fn early_print2(
        &self,
        Tracked(snpcore): Tracked<&mut SnpCore>,
        Tracked(console): Tracked<SnpPointsToRaw>,
    ) -> (newconsole: Tracked<SnpPointsToRaw>) {
        ghcb_prints_with_lock2(self, Tracked(snpcore), Tracked(console)).1
    }
}

macro_rules! def_sec_int_debug {
    ($($itype: ty),* $(,)?) => {
        $(verus!{
        impl VPrint for $itype {
            open spec fn early_print_requires(&self) -> bool {
                self.is_constant()
            }

            fn early_print2(&self, Tracked(snpcore): Tracked<&mut SnpCore>, Tracked(console): Tracked<SnpPointsToRaw>) -> (newconsole: Tracked<SnpPointsToRaw>)
            {
                let val: u64 = (*self).reveal_value() as u64;
                let (bytes, n) = int2bytes(val, 16);
                ghcb_print_bytes_with_lock2(slice_subrange(bytes.as_slice(), 0, n), Tracked(snpcore), Tracked(console)).1
            }
        }})*
}
}

macro_rules! def_int_debug {
    ($($itype: ty),* $(,)?) => {
        $(verus!{
        impl VPrint for $itype {
            open spec fn early_print_requires(&self) -> bool {
                self.is_constant()
            }

            fn early_print2(&self, Tracked(snpcore): Tracked<&mut SnpCore>, Tracked(console): Tracked<SnpPointsToRaw>) -> (newconsole: Tracked<SnpPointsToRaw>)
            {
                let val: u64 = *self as u64;
                let (bytes, n) = int2bytes(val, 16);
                //bytes_to_str(&bytes).early_print(Tracked(cc))
                ghcb_print_bytes_with_lock2(slice_subrange(bytes.as_slice(), 0, n), Tracked(snpcore), Tracked(console)).1
            }
        }})*
}
}

def_sec_int_debug!{u64_s, u32_s, usize_s, u16_s, u8_s}

def_int_debug!{u64_t, u32_t, usize_t, u16_t, u8_t}

verus!{
    impl VPrint for bool {
        open spec fn early_print_requires(&self) -> bool {
            true
        }

        fn early_print2(&self, Tracked(snpcore): Tracked<&mut SnpCore>, Tracked(console): Tracked<SnpPointsToRaw>) -> (newconsole: Tracked<SnpPointsToRaw>)
        {
            let val: u64 = if *self {1} else {0};
            let (bytes, n) = int2bytes(val, 16);
            ghcb_print_bytes_with_lock2(slice_subrange(bytes.as_slice(), 0, n), Tracked(snpcore), Tracked(console)).1
        }
    }}

impl<T1: VPrint, T2: VPrint> VPrint for (T1, T2) {
    open spec fn early_print_requires(&self) -> bool {
        &&& self.0.early_print_requires()
        &&& self.1.early_print_requires()
    }

    #[inline]
    fn early_print2(
        &self,
        Tracked(snpcore): Tracked<&mut SnpCore>,
        Tracked(console): Tracked<SnpPointsToRaw>,
    ) -> (newconsole: Tracked<SnpPointsToRaw>) {
        proof {
            reveal_strlit(" ");
        }
        let Tracked(console) = self.0.early_print2(Tracked(snpcore), Tracked(console));
        let Tracked(console) = new_strlit(" ").early_print2(Tracked(snpcore), Tracked(console));
        self.1.early_print2(Tracked(snpcore), Tracked(console))
    }
}

impl<T: ?Sized + VPrint> VPrintLock for T {
    open spec fn lock_print_requires(&self, cs: SnpCoreSharedMem) -> bool {
        &&& self.early_print_requires()
        &&& cs.inv()
    }

    #[inline]
    fn print(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) {
        let ghost oldlockperms = cs.lockperms;
        let console_ref = CONSOLE();
        let tracked consolelock = cs.lockperms.tracked_remove(console_ref.lockid());
        let ghost oldconsolelock = consolelock;

        assert(cs.lockperms.inv(cs.snpcore.cpu()));
        assert(console_ref.is_constant());
        let (_, Tracked(console), Tracked(mut consolelock)) = console_ref.acquire(
            Tracked(consolelock),
            Tracked(&cs.snpcore.coreid),
        );
        let tracked console = console.trusted_into_raw();
        assert(console.is_console());
        let Tracked(mut console) = self.early_print2(Tracked(&mut cs.snpcore), Tracked(console));
        console_ref.release(
            Tracked(&mut consolelock),
            Tracked(console.trusted_into()),
            Tracked(&cs.snpcore.coreid),
        );
        proof {
            cs.lockperms.tracked_insert(console_ref.lockid(), consolelock);
            assert(consolelock@.points_to.only_val_updated(oldconsolelock@.points_to));
            //assert(consolelock@.points_to.bytes() =~~= oldconsolelock@.points_to.bytes());
            //assert(consolelock@ === oldconsolelock@);
            assert(cs.lockperms.updated_lock(&oldlockperms, set![console_ref.lockid()]));
        }
    }
}

impl<T: ?Sized + VPrint> VEarlyPrintAtLevel for T {
    open spec fn print_level_requires(&self, cs: SnpCoreConsole) -> bool {
        &&& self.early_print_requires()
        &&& cs.wf()
    }

    open spec fn print_ensures(&self, prevcs: SnpCoreConsole, cs: SnpCoreConsole) -> bool {
        &&& print_ensures_cc(prevcs, cs)
    }

    #[cfg(not(debug_assertions))]
    #[verifier(external_body)]
    fn leak_debug(&self) {
    }

    #[cfg(debug_assertions)]
    #[verifier(external_body)]
    fn leak_debug(&self) {
        self.early_print(Tracked::assume_new());
    }

    #[cfg(not(debug_assertions))]
    fn debug(&self, Tracked(cs): Tracked<&mut SnpCoreConsole>) {
    }

    #[cfg(debug_assertions)]
    fn debug(&self, Tracked(cs): Tracked<&mut SnpCoreConsole>) {
        self.early_print(Tracked(cs));
    }

    fn info(&self, Tracked(cs): Tracked<&mut SnpCoreConsole>) {
        self.early_print(Tracked(cs));
    }

    fn err(&self, Tracked(cs): Tracked<&mut SnpCoreConsole>) {
        self.early_print(Tracked(cs));
    }
}

impl<T: ?Sized + VPrint> VPrintAtLevel for T {
    open spec fn print_level_requires(&self, cs: SnpCoreSharedMem) -> bool {
        &&& self.early_print_requires()
        &&& self.lock_print_requires(cs)
    }

    open spec fn print_ensures(&self, prevcs: SnpCoreSharedMem, cs: SnpCoreSharedMem) -> bool {
        &&& print_ensures_cs(prevcs, cs)
    }

    #[cfg(not(debug_assertions))]
    #[verifier(external_body)]
    fn leak_debug(&self) {
    }

    #[cfg(debug_assertions)]
    #[verifier(external_body)]
    fn leak_debug(&self) {
        self.print(Tracked::assume_new());
    }

    #[cfg(not(debug_assertions))]
    fn debug(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) {
    }

    #[cfg(debug_assertions)]
    fn debug(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) {
        self.print(Tracked(cs));
    }

    fn info(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) {
        self.print(Tracked(cs));
    }

    fn err(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>) {
        self.print(Tracked(cs));
    }
}

} // verus!

================
File: ./source/verismo/src/debug/interface.rs
================

use super::*;
use crate::global::*;
use crate::snp::ghcb::GHCB_REGID;
use crate::snp::{snpcore_console_wf, SnpCoreConsole, SnpCoreSharedMem};

verismo_simple! {
    pub struct Console;

    impl Console {
        pub open spec fn invfn() -> spec_fn(Console) -> bool {
            |c: Console| true
        }
    }
}

verus! {

pub open spec fn print_ensures_cc(cc: SnpCoreConsole, ret: SnpCoreConsole) -> bool {
    &&& print_ensures_snp_c(cc.snpcore, cc.console(), ret.snpcore, ret.console())
    &&& cc.wf()
    &&& ret.wf()
}

pub open spec fn print_ensures_snp_c(
    snpcore: SnpCore,
    console: SnpPointsToRaw,
    retsnpcore: SnpCore,
    retconsole: SnpPointsToRaw,
) -> bool {
    &&& snpcore_console_wf(retsnpcore, retconsole)
    &&& retsnpcore.only_reg_coremode_updated(snpcore, set![GHCB_REGID()])
    &&& retconsole@.only_val_updated(console@)
}

pub open spec fn print_ensures_cs(cs: SnpCoreSharedMem, ret: SnpCoreSharedMem) -> bool {
    &&& ret.inv()
    &&& ret.only_lock_reg_coremode_updated(cs, set![GHCB_REGID()], set![spec_CONSOLE().lockid()])
    &&& ret.lockperms.contains_vlock(spec_CONSOLE())
    &&& forall|id|
        id != spec_CONSOLE().lockid() && cs.lockperms.contains_key(id) ==> (
        #[trigger] ret.lockperms[id]) === cs.lockperms[id] && ret.lockperms.contains_key(id)
    &&& cs.inv()
}

pub trait VPrintCC {
    spec fn early_print_cc_requires(&self) -> bool;

    fn early_print(&self, Tracked(cc): Tracked<&mut SnpCoreConsole>)
        requires
            self.early_print_cc_requires(),
            old(cc).wf(),
        ensures
            print_ensures_cc(*old(cc), *cc),
    ;
}

// Basic traits
pub trait VPrint {
    spec fn early_print_requires(&self) -> bool;

    fn early_print2(
        &self,
        Tracked(snpcore): Tracked<&mut SnpCore>,
        Tracked(console): Tracked<SnpPointsToRaw>,
    ) -> (newconsole: Tracked<SnpPointsToRaw>)
        requires
            self.early_print_requires(),
            snpcore_console_wf(*old(snpcore), console),
        ensures
            print_ensures_snp_c(*old(snpcore), console, *snpcore, newconsole@),
    ;
}

impl<T: ?Sized + VPrint> VPrintCC for T {
    open spec fn early_print_cc_requires(&self) -> bool {
        self.early_print_requires()
    }

    fn early_print(&self, Tracked(cc): Tracked<&mut SnpCoreConsole>) {
        let Tracked(console) = self.early_print2(
            Tracked(&mut cc.snpcore),
            Tracked(cc.console.tracked_remove(0)),
        );
        proof {
            cc.console.tracked_insert(0, console);
        }
    }
}

// derived from VPrint
pub trait VPrintLock {
    spec fn lock_print_requires(&self, cs: SnpCoreSharedMem) -> bool;

    fn print(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>)
        requires
            self.lock_print_requires(*old(cs)),
        ensures
            print_ensures_cs(*old(cs), *cs),
    ;
}

// derived from VPrint
pub trait VEarlyPrintAtLevel {
    spec fn print_level_requires(&self, cs: SnpCoreConsole) -> bool;

    spec fn print_ensures(&self, prevcs: SnpCoreConsole, cs: SnpCoreConsole) -> bool;

    fn leak_debug(&self);

    fn debug(&self, Tracked(cs): Tracked<&mut SnpCoreConsole>)
        requires
            self.print_level_requires(*old(cs)),
        ensures
            print_ensures_cc(*old(cs), *cs),
            self.print_ensures(*old(cs), *cs),
    ;

    fn info(&self, Tracked(cs): Tracked<&mut SnpCoreConsole>)
        requires
            self.print_level_requires(*old(cs)),
        ensures
            print_ensures_cc(*old(cs), *cs),
            self.print_ensures(*old(cs), *cs),
    ;

    fn err(&self, Tracked(cs): Tracked<&mut SnpCoreConsole>)
        requires
            self.print_level_requires(*old(cs)),
        ensures
            print_ensures_cc(*old(cs), *cs),
            self.print_ensures(*old(cs), *cs),
    ;
}

// derived from VPrintLock
pub trait VPrintAtLevel {
    spec fn print_level_requires(&self, cs: SnpCoreSharedMem) -> bool;

    spec fn print_ensures(&self, prevcs: SnpCoreSharedMem, cs: SnpCoreSharedMem) -> bool;

    fn leak_debug(&self);

    fn debug(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>)
        requires
            self.print_level_requires(*old(cs)),
        ensures
            print_ensures_cs(*old(cs), *cs),
            self.print_ensures(*old(cs), *cs),
    ;

    fn info(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>)
        requires
            self.print_level_requires(*old(cs)),
        ensures
            print_ensures_cs(*old(cs), *cs),
            self.print_ensures(*old(cs), *cs),
    ;

    fn err(&self, Tracked(cs): Tracked<&mut SnpCoreSharedMem>)
        requires
            self.print_level_requires(*old(cs)),
        ensures
            print_ensures_cs(*old(cs), *cs),
            self.print_ensures(*old(cs), *cs),
    ;
}

} // verus!

================
File: ./source/verismo/src/debug/slice_print.rs
================

use vstd::slice::slice_index_get;

use super::*;
use crate::snp::{snpcore_console_wf, SnpCoreConsole, SnpCoreSharedMem};

verismo_simple! {
impl<T: VPrint> VPrint for [T] {
    open spec fn early_print_requires(&self) -> bool {
        forall |i| 0 <= i < self@.len() ==> self@[i].early_print_requires()
    }


    fn early_print2(&self, Tracked(snpcore): Tracked<&mut SnpCore>, Tracked(console): Tracked<SnpPointsToRaw>) -> (newconsole: Tracked<SnpPointsToRaw>)
    {
        let table = self;
        let n = usize_s::constant(self.len());
        let n64 = n as u64;
        let tracked mut console = console;
        proof {reveal_strlit("size = ");}
        let Tracked(console) = new_strlit("size = ").early_print2(Tracked(snpcore), Tracked(console));
        let Tracked(console) = n.early_print2(Tracked(snpcore), Tracked(console));
        proof {reveal_strlit("[\n");}
        let Tracked(console) = new_strlit("[\n").early_print2(Tracked(snpcore), Tracked(console));
        let ghost oldsnpcore = *snpcore;
        let ghost oldconsole = console;
        let mut i: usize = 0;
        while i < n
        invariant
            i <= n,
            i.is_constant(),
            n.is_constant(),
            snpcore.cpu() == oldsnpcore.cpu(),
            snpcore.inv(),
            snpcore_console_wf(oldsnpcore, oldconsole),
            print_ensures_snp_c(oldsnpcore, oldconsole, *snpcore, console),
            n == self@.len(),
            forall |i| 0 <= i < self@.len() ==> self@[i].early_print_requires(),
        {
            let Tracked(tmpconsole) =  slice_index_get(self, i.reveal_value()).early_print2(Tracked(snpcore), Tracked(console));
            proof {
                reveal_strlit(" ");
            }
            let Tracked(tmpconsole) = new_strlit(" ").early_print2(Tracked(snpcore), Tracked(tmpconsole));
            proof{console = tmpconsole;}
            i = i + 1;
        }
        proof {
            reveal_strlit("]\n");
        }
        new_strlit("]\n").early_print2(Tracked(snpcore), Tracked(console))
    }
}

impl<T: VPrint + IsConstant + WellFormed, const N: usize_t> VPrint for [T; N] {
    open spec fn early_print_requires(&self) -> bool {
        &&& forall |i| 0 <= i < self@.len() ==> self@[i].early_print_requires()
        &&& self.is_constant()
    }

    #[inline]
    fn early_print2(&self, Tracked(snpcore): Tracked<&mut SnpCore>, Tracked(console): Tracked<SnpPointsToRaw>) -> (newconsole: Tracked<SnpPointsToRaw>)
    {
        self.as_slice().early_print2(Tracked(snpcore), Tracked(console))
    }
}
}

verus! {

// slice does not have a size, and so cannot use derived PrintAtAllLevel trait.
// To use derived traits, we use SlicePrinter to print.
pub struct SlicePrinter<'a, T: IsConstant + WellFormed> {
    pub s: &'a [T],
}

} // verus!
verus! {

impl<'a, T: IsConstant + WellFormed + VPrint> VPrint for SlicePrinter<'a, T> {
    open spec fn early_print_requires(&self) -> bool {
        forall|i| 0 <= i < self.s@.len() ==> self.s@[i].early_print_requires()
    }

    #[inline]
    fn early_print2(
        &self,
        Tracked(snpcore): Tracked<&mut SnpCore>,
        Tracked(console): Tracked<SnpPointsToRaw>,
    ) -> (newconsole: Tracked<SnpPointsToRaw>) {
        self.s.early_print2(Tracked(snpcore), Tracked(console))
    }
}

} // verus!

================
File: ./source/verismo/src/debug/mod.rs
================

mod ghcb_print;
mod interface;
mod slice_print;

pub use interface::*;
pub use slice_print::SlicePrinter;

use crate::lock::*;
use crate::ptr::*;
use crate::registers::SnpCore;
use crate::snp::{SnpCoreConsole, SnpCoreSharedMem};
use crate::tspec::*;
use crate::tspec_e::*;
use crate::*;

================
File: ./source/vstd/Cargo.toml
================

[package]
name = "vstd"
version = "0.1.0"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
state_machines_macros = { path = "../../tools/verus/source/state_machines_macros" }
builtin = { path = "../../tools/verus/source/builtin"}
builtin_macros = { path = "../../tools/verus/source/builtin_macros"}

[lib]
name = "vstd"
path = "src/lib.rs"

[features]
default = ["non_std", "test", "debug", "alloc"]
non_std = []
#no_global_allocator = []
test = []
debug = []
alloc = []
================
File: ./source/vstd/lib.rs
================

//! The "standard library" for [Verus](https://github.com/verus-lang/verus).
//! Contains various utilities and datatypes for proofs,
//! as well as runtime functionality with specifications.
//! For an introduction to Verus, see [the tutorial](https://verus-lang.github.io/verus/guide/).
#![no_std] // don't link the Rust standard library
#![allow(unused_parens)]
#![allow(rustdoc::invalid_rust_codeblocks)]
#![feature(core_intrinsics)]
#![feature(step_trait)]
#![feature(allocator_api)]
extern crate alloc;
pub mod bytes;
pub mod calc_macro;
pub mod map;
pub mod map_lib;
pub mod math;
//pub mod option;
pub mod pervasive;
pub mod relations;
//pub mod result;
pub mod seq;
pub mod seq_lib;
pub mod set;
pub mod set_lib;
pub mod slice;
//pub mod cell;
//pub mod invariant;
//pub mod atomic;
//pub mod atomic_ghost;
pub mod function;
//#[cfg(not(feature = "no_global_allocator"))]
//pub mod ptr;
pub mod layout;
pub mod modes;
pub mod multiset;
pub mod std_specs;
//pub mod state_machine_internal;
//pub mod std_specs;
#[cfg(not(feature = "no_global_allocator"))]
pub mod string;
#[cfg(not(feature = "non_std"))]
pub mod thread;
//#[cfg(not(feature = "no_global_allocator"))]
//pub mod vec;
pub mod view;

// Re-exports all pervasive types, traits, and functions that are commonly used or replace
// regular `core` or `std` definitions.
pub mod prelude;

================
File: ./source/target.json
================

{
        "llvm-target": "x86_64-unknown-none",
        "data-layout": "e-m:e-i64:64-f80:128-n8:16:32:64-S128",
        "arch": "x86_64",
        "target-endian": "little",
        "target-pointer-width": "64",
        "target-c-int-width": "32",
        "os": "none",
        "executables": true,
        "linker-flavor": "ld.lld",
        "linker": "rust-lld",
        "pre-link-args": {
                "ld.lld": [
                    "--script=monitor.ld",
                    "--gc-sections",
                    "-pie"
                ]
            },
        "panic-strategy": "abort",
        "disable-redzone": true,
        "features": "-mmx,-sse,+soft-float",
        "relocation-model": "pic",
        "dynamic-linking": false,
        "position-independent-executables": true
}

================
File: ./source/monitor.ld
================

OUTPUT_FORMAT("elf64-x86-64")
OUTPUT_ARCH(i386:x86-64)
ENTRY(_start)
SECTIONS
{
	. = 0x0;
	_monitor_start = .;
	.text	:
	{
		*(.text .text.*)
		*(.got)
	}
	. = ALIGN(0x1000);
	.data	: 
	{ 
		*(.rodata .rodata.*)
		*(.data .data.*)
		
	}
	.rela	:
	{
		*(.rela .rela.* )
	}
	. = ALIGN(0x1000);
	.bss	: 
	{
		*(.bss .bss.*)
	}
	/DISCARD/	:
	{
		*(.comment*)
		*(.gnu.hash*)
		*(.hash)
	}
	_monitor_end = .;
	.debug	:
	{
		*(.debug .debug* )
	}
}

================
File: ./source/.cargo/config.toml
================

[unstable]
build-std-features = ["compiler-builtins-mem"]
build-std = ["core","alloc", "compiler_builtins"]
unstable-options = true

[build]
target = "target.json"
rustc = "../tools/verus-rustc/target/release/verus-rustc"
incremental = true

[profile.dev]
incremental = true
debug-assertions = true

[profile.release]
incremental = true
debug-assertions = false

[env]
VERUS_PATH = { value = "../tools/verus/source/target-verus/release/verus", relative = true }

================
File: ./source/verismo_main/Cargo.toml
================

[package]
name = "verismo_main"
version = "0.1.0"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[features]
default = ["non_std", "debug"]
non_std = []
no_global_allocator = ["verismo/no_global_allocator"]
debug = ["verismo/debug"]
noverify = ["verismo/noverify"]
verifymodule=["verismo/verifymodule"]

[dependencies]
verismo = {path = "../verismo"}
builtin = { path = "../../tools/verus/source/builtin"}
builtin_macros = { path = "../../tools/verus/source/builtin_macros"}
vstd = {path = "../vstd"}

================
File: ./source/verismo_main/build.rs
================

use std::env;
use std::fs::File;
use std::io::Write;
use std::process::Command;

fn main() {
    // Environment vars during build.
    if cfg!(feature = "noverify") {
        println!("cargo:rustc-env=VERUS_ARGS=--no-verify");
    } else {
        println!("cargo:rustc-env=VERUS_ARGS=--rlimit=8000 --expand-errors --multiple-errors=5 --triggers-silent --time-expanded --no-auto-recommends-check --output-json");
    }

    let module_path = env::var("CARGO_MANIFEST_DIR").unwrap();
    println!("cargo:rustc-env=MODULE_PATH={}", module_path);

    println!("cargo:rerun-if-changed=build.rs");
    println!("cargo:rerun-if-env-changed=MODULE");

    // Post build
    let target_dir = env::var("OUT_DIR").unwrap();
    let work_dir = env::var("CARGO_MANIFEST_DIR").unwrap();
    let executable_path = format!("{}/../../../verismo_main", target_dir); // Adjust the path if necessary
    let igvmgen = work_dir.clone() + "/../../tools/igvm/src/igvm/igvmgen.py";
    let bzimage = work_dir.clone() + "/../../richos/target/arch/x86/boot/bzImage";
    let igvmscript_path = format!("{}/../../../igvm.sh", target_dir);
    println!("cargo:rerun-if-changed={}", igvmgen);
    let igvmout = format!("{}/../../../verismo-rust.bin", target_dir);
    let mut cmd = Command::new("python3");
    cmd.arg(igvmgen);
    cmd.args(["-k", &executable_path]);
    cmd.args(["-o", &igvmout]);
    cmd.args([
        "-vtl=2",
        "-append",
        "root=/dev/sda rw debugpat",
        "-inform",
        "verismo",
        "-boot_mode",
        "x64",
        "-pgtable_level",
        "4",
        "-shared_payload",
        &bzimage,
    ]);

    let cmd_str = format!("{:?}", cmd);

    // Write the command to a shell script file
    let mut file = File::create(igvmscript_path.clone()).expect("Failed to create file");
    writeln!(file, "#!/bin/sh").expect("Failed to write to file");
    writeln!(file, "{}", cmd_str).expect("Failed to write to file");

    // Make the script executable
    Command::new("chmod")
        .arg("+x")
        .arg(igvmscript_path)
        .status()
        .expect("Failed to change file permissions");
}

================
File: ./source/verismo_main/src/entry.s
================

.globl boot_stack
.globl boot_stack_end
.globl stack_guard_page
.globl verismo_start
.globl verismo_end
.globl _start
.globl ap_entry
_start:
verismo_start:
	cld
	cli
	mov %rsi, %rdi
	call bsp_call
	mov stack_guard_page(%rip), %rax
	mov $0, %rcx
	mov $0, %rdx
	.byte 0xF2, 0x0F, 0x01, 0xFF
	lea boot_stack_end(%rip), %rsp
	call bsp_call
.align 0x1000
ap_entry:
	cld
	cli
	call ap_call
ap_loop:
	jmp ap_loop

.section .data
.align 0x1000
stack_guard_page:
.space 4096
boot_stack:
.space 4096
.space 4096
.space 4096
.space 4096
boot_stack_end:

.section .bss
.align 8
monitor_data_addr:
.space 8
verismo_end:
================
File: ./source/verismo_main/src/main.rs
================

#![no_std] // don't link the Rust standard library
#![no_main] // disable all Rust-level entry points
#![feature(panic_info_message)]
#![allow(unused)]

#[cfg(target_os = "none")]
core::arch::global_asm!(include_str!("entry.s"), options(att_syntax));

use core::panic::PanicInfo;

use builtin::*;
use builtin_macros::*;
use verismo::debug::VEarlyPrintAtLevel;
use verismo::snp::ghcb::*;
use vstd::prelude::*;
use vstd::string::*;

#[panic_handler]
fn panic(info: &PanicInfo) -> ! {
    new_strlit("\npanic:\n").err(Tracked::assume_new());

    if let Some(location) = info.location() {
        (location.line()).err(Tracked::assume_new());
    } else {
        new_strlit("Panic occurred but no location information available")
            .err(Tracked::assume_new());
    }

    match info.message() {
        Some(msg) => {
            if msg.as_str().is_some() {
                StrSlice::from_rust_str(msg.as_str().unwrap()).err(Tracked::assume_new());
            }
        }
        None => todo!(),
    }

    vc_terminate(SM_TERM_UNSUPPORTED, Tracked::assume_new());

    loop {}
}

================
File: ./source/doc/src/main.rs
================

fn main() {
    println!("Hello, world!");
}

================
File: ./source/.rustfmt.toml
================

indent_style = "Block"
struct_lit_single_line = true
where_single_line = true
fn_single_line = true
reorder_imports = true
use_small_heuristics = "Max"
imports_granularity = "Module"
group_imports = "StdExternalCrate"
reorder_impl_items = true
#max_width = 300
#fn_single_line=true
#use_small_heuristics="max"
================
File: ./source/rust-toolchain.toml
================

[toolchain]
channel = "nightly-2023-12-22"
components = [ "rust-src", "rustc", "cargo", "rustfmt", "rustc-dev", "llvm-tools-preview" ] # TODO llvm-tools-preview may be unnecessary

================
File: ./source/vops/Cargo.toml
================

[package]
name = "vops"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

================
File: ./source/vops/src/lib.rs
================

#![no_std]

pub trait VLt<Rhs> {
    fn lt(&self, rhs: &Rhs) -> bool
    where
        Self: core::marker::Sized;
}
pub trait VLe<Rhs> {
    fn le(&self, rhs: &Rhs) -> bool
    where
        Self: core::marker::Sized;
}
pub trait VGt<Rhs> {
    fn gt(&self, rhs: &Rhs) -> bool
    where
        Self: core::marker::Sized;
}
pub trait VGe<Rhs> {
    fn ge(&self, rhs: &Rhs) -> bool
    where
        Self: core::marker::Sized;
}
pub trait VEq<Rhs> {
    fn eq(&self, rhs: &Rhs) -> bool
    where
        Self: core::marker::Sized;
}

================
File: ./source/verismo_macro/Cargo.toml
================

[package]
name = "verismo_macro"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
proc-macro = true

[dependencies]
proc-macro2 = "1.0.39"
quote = "1.0"
synstructure = "0.12"
syn = "1.0"
syn_verus = { path="../../tools/verus/dependencies/syn", features = ["full", "visit-mut", "extra-traits"] }
prettyplease_verus = { path="../../tools/verus/dependencies/prettyplease" }

================
File: ./source/verismo_macro/src/def.rs
================

use proc_macro2::{Span, TokenStream};
use quote::quote;
use syn_verus::punctuated::Punctuated;
use syn_verus::spanned::Spanned;
use syn_verus::{
    AngleBracketedGenericArguments, DataStruct, Field, GenericArgument, GenericParam, Generics,
    Ident, Lit, LitInt, Path, PathArguments, PathSegment, TraitBound, TraitBoundModifier, Type, TypeParamBound, TypePath, Visibility,
};

pub fn get_closed_or_open(s: &DataStruct) -> TokenStream {
    let mut close_or_open = quote! {open};
    for (i, field) in s.fields.iter().enumerate() {
        let (_fname, _field_ty) = field_name_ty(&field, i, field.span());
        close_or_open = match &field.vis {
            Visibility::Public(_) => close_or_open,
            _ => {
                quote! {closed}
            }
        };
    }
    close_or_open
}
pub fn new_path(names: &[&str], span: Span) -> Path {
    let mut path_segments = Punctuated::new();
    let mut args = Punctuated::new();

    if names.len() > 1 {
        let next = &names[1..names.len()];
        args.push(GenericArgument::Type(Type::Path(TypePath {
            qself: None,
            path: new_path(next, span),
        })));
        path_segments.push(PathSegment {
            ident: proc_macro2::Ident::new(names[0], span),
            arguments: PathArguments::AngleBracketed(AngleBracketedGenericArguments {
                colon2_token: None,
                lt_token: syn_verus::token::Lt(span),
                args: args,
                gt_token: syn_verus::token::Gt(span),
            }),
        });
    } else {
        path_segments.push(PathSegment {
            ident: proc_macro2::Ident::new(names[0], span),
            arguments: PathArguments::None,
        });
    }
    Path {
        leading_colon: None,
        segments: path_segments,
    }
}
pub fn gen_trait_bound(names: Vec<&str>, span: Span) -> TypeParamBound {
    //let path = TypePath {qself: None, path: new_path(&names.as_slice())};
    TypeParamBound::Trait(TraitBound {
        paren_token: None,
        modifier: TraitBoundModifier::None,
        lifetimes: None,
        path: new_path(&names.as_slice(), span),
    })
}

pub fn add_bound_to_generic(
    generic_for_default: &mut Generics,
    bound: TypeParamBound,
    _span: Span,
) {
    //let mut generic_for_default = generics.clone();
    for gparam in generic_for_default.params.iter_mut() {
        let tmp = gparam.clone();
        match &tmp {
            GenericParam::Type(tparam) => {
                let mut tparam = tparam.clone();
                tparam.bounds.push(bound.clone());
                *gparam = GenericParam::Type(tparam);
            }
            _ => (),
        }
    }
}

pub fn add_bound_to_generic_with_self(
    generic_for_default: &mut Generics,
    bound: Vec<&str>,
    _span: Span,
) {
    //let mut generic_for_default = generics.clone();
    for gparam in generic_for_default.params.iter_mut() {
        let tmp = gparam.clone();
        match &tmp {
            GenericParam::Type(tparam) => {
                let mut tparam = tparam.clone();
                let mut bound = bound.clone();
                let tname = tparam.ident.to_string();
                bound.push(&tname);
                tparam.bounds.push(gen_trait_bound(bound, _span));
                *gparam = GenericParam::Type(tparam);
            }
            _ => (),
        }
    }
}

pub fn type_to_type_generic(ty: &Type) -> Type {
    match ty {
        Type::Path(type_path) => {
            let mut type_path = type_path.clone();
            let _segments_len = type_path.path.segments.len();
            for (_i, segment) in type_path.path.segments.iter_mut().enumerate() {
                if let PathArguments::AngleBracketed(ref mut args) = &mut segment.arguments {
                    let mut new_args = vec![];

                    for arg in &args.args {
                        match arg {
                            GenericArgument::Type(arg_type) => {
                                new_args
                                    .push(GenericArgument::Type(type_to_type_generic(arg_type)));
                            }
                            _ => {
                                new_args.push(arg.clone());
                            }
                        }
                    }

                    // Always add the colon2 token since we want every generic to be in the desired format.
                    segment.arguments =
                        PathArguments::AngleBracketed(syn_verus::AngleBracketedGenericArguments {
                            colon2_token: Some(syn_verus::token::Colon2::default()),
                            ..args.clone()
                        });
                }
            }
            Type::Path(type_path)
        }
        _ => ty.clone(),
    }
}

pub fn is_ghost_or_tracked_type(ty: &Type) -> bool {
    if let Type::Path(tpath) = ty {
        let path_segments = tpath
            .path
            .segments
            .iter()
            .map(|segment| segment.ident.to_string())
            .collect::<Vec<_>>();
        if path_segments.last() == Some(&"Ghost".to_string()) {
            true
        } else if path_segments.last() == Some(&"Tracked".to_string()) {
            true
        } else {
            false
        }
    } else {
        false
    }
}

pub fn join_tokens(
    v: &Vec<proc_macro2::TokenStream>,
    j: proc_macro2::TokenStream,
) -> proc_macro2::TokenStream {
    let mut ret = quote! {};
    for (i, vv) in v.iter().enumerate() {
        ret = if i == 0 {
            vv.clone()
        } else {
            quote! {#ret #j #vv}
        };
    }
    ret
}

pub fn gen_field_calls<T: Fn(&proc_macro2::TokenStream, &Type) -> proc_macro2::TokenStream>(
    fname: &proc_macro2::TokenStream,
    field_ty: &Type,
    tocall: &T,
) -> Vec<proc_macro2::TokenStream> {
    let mut ret = vec![];
    let is_ghost = is_ghost_or_tracked_type(field_ty);
    match field_ty {
        Type::Path(_tpath) => {
            if !is_ghost {
                ret.push(tocall(fname, field_ty))
            }
        }
        Type::Tuple(t) => {
            if t.elems.len() > 0 {
                let _ele_sec_label = quote! {true};
                let mut i = 0;
                for elem in &t.elems {
                    let l = LitInt::new(format!("{}", i).as_str(), elem.span());
                    ret.extend(gen_field_calls(&quote! {#fname.#l}, elem, tocall));
                    i = i + 1;
                }
            }
        }
        _ => ret.push(tocall(fname, field_ty)),
    }
    ret
}

pub fn field_name_ty(field: &Field, i: usize, span: Span) -> (proc_macro2::TokenStream, Type) {
    let field_name = match &field.ident {
        Some(name) => quote! {#name},
        None => {
            let lit = Lit::Int(LitInt::new(format!("{}", i).as_str(), span));
            quote! {#lit}
        }
    };

    (field_name, field.ty.clone())
}


pub fn get_field(fname: &proc_macro2::TokenStream, span: Span) -> Ident {
    let getter_ident = Ident::new(&format!("spec_{}", fname.to_string()), span);
    getter_ident
}

pub fn set_field(fname: &proc_macro2::TokenStream, span: Span) -> Ident {
    syn_verus::Ident::new(&format!("spec_set_{}", fname.to_string()), span)
}

pub fn field_offset(fname: &proc_macro2::TokenStream, span: Span) -> Ident {
    syn_verus::Ident::new(&format!("spec_{}_offset", fname.to_string()), span)
}

pub fn field_offset_exe(fname: &proc_macro2::TokenStream, span: Span) -> Ident {
    syn_verus::Ident::new(&format!("_{}_offset", fname.to_string()), span)
}

pub fn tspec_path() -> TokenStream {
    quote! {crate::tspec}
}

pub fn generic_mod() -> TokenStream {
    let common = tspec_path();
    quote! {#common::size_s}
}

/*
pub fn get_array_len(tyarray: &TypeArray) -> &syn_verus::LitInt {
    if let Expr::Lit(ExprLit {
        lit: Lit::Int(l),
        attrs: _,
    }) = &tyarray.len
    {
        l
    } else {
        panic!("Wrong array size {:#?}", tyarray.len);
    }
}
*/

================
File: ./source/verismo_macro/src/enum_int.rs
================

use proc_macro::TokenStream;
use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, ExprLit, Lit};

pub fn verismo_enum_int_expand(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    let name = input.ident;
    let mut variant_stream = quote! {};
    let mut variant_stream_u64 = quote! {};
    let mut variant_stream_from = quote! {};
    let mut variant_stream_from_u64 = quote! {};
    let mut variant_stream_from_int = quote! {};
    match input.data {
        Data::Enum(enumdata) => {
            let mut default = 0;
            for (_, variant) in enumdata.variants.iter().enumerate() {
                let ident = &variant.ident;
                match &variant.discriminant {
                    Some(discriminant) => {
                        let expr = &discriminant.1;
                        let default_lit = if let syn_verus::Expr::Lit(ExprLit {
                            lit: Lit::Int(l),
                            attrs: _,
                        }) = expr
                        {
                            l
                        } else {
                            unreachable!()
                        };
                        default = default_lit.base10_parse::<u64>().unwrap();
                    }
                    None => {}
                };
                let fields = if variant.fields.len() == 0 {
                    quote! {}
                } else {
                    let mut f = quote! {};
                    for _ in 0..variant.fields.len() {
                        f = quote! {
                            #f _,
                        };
                    }
                    f
                };
                variant_stream = quote! {
                    #variant_stream
                    #name::#ident #fields => {spec_cast_integer::<_, int> (#default)},
                };

                variant_stream_from_int = quote! {
                    #variant_stream_from_int
                    else if val == spec_cast_integer::<_, int>(#default) {
                        Option::Some(#name::#ident #fields)
                    }
                };

                variant_stream_from_u64 = quote! {
                    #variant_stream_from_u64
                    if val == #default {
                        Option::Some(#name::#ident #fields)
                    } else
                };

                variant_stream_u64 = quote! {
                    #variant_stream_u64
                    #name::#ident #fields => {#default},
                };

                if variant.fields.len() == 0 {
                    variant_stream_from = quote! {
                        #variant_stream_from
                        builtin::equal(Self::spec_from_int(spec_cast_integer::<_, int> (#default)).get_Some_0(), #name::#ident),
                    };
                }

                default = default + 1;
            }
        }
        _ => panic!("Only Enum can be annotated with SpecIntEnum"),
    };

    let tspec = crate::def::tspec_path();
    let expand = quote! {
        verus!{
        impl #tspec::IntValue for #name {
            open spec fn as_int(&self) -> int {
                let val = match self {
                    #variant_stream
                };
                val
            }

            open spec fn from_int(val: int) -> Self {
                Self::spec_from_int(val).get_Some_0()
            }
        }
        impl #tspec::IntOrd for #name {
            open spec fn ord_int(&self) -> int {
                self.as_int()
            }
        }

        impl #name {
                pub open spec fn spec_from_int(val: int) -> Option<Self> {
                    if val < 0 {
                        Option::None
                    }
                    #variant_stream_from_int
                    else {
                        Option::None
                    }
                }

                #[inline]
                pub const fn as_u64(&self) -> (ret: u64)
                ensures
                    ret as int == self.as_int()
                {
                    let ret = match self {
                        #variant_stream_u64
                    };
                    ret
                }

                #[inline]
                pub const fn from_u64(val: u64) -> (ret: Option<Self>)
                ensures
                    builtin::equal(ret, Self::spec_from_int(spec_cast_integer::<_, int>(val)))
                {
                    #variant_stream_from_u64
                    {
                    Option::None
                    }
                }
            }
        }
    };

    proc_macro::TokenStream::from(expand)
}

================
File: ./source/verismo_macro/src/property.rs
================

use proc_macro;
use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Ident, Visibility};

use crate::def::{
    add_bound_to_generic, field_name_ty, gen_field_calls, gen_trait_bound, join_tokens,
};

pub fn verismo_derive_property_expand2(
    input: &DeriveInput,
    callname: &str,
    trtstr: &str,
) -> proc_macro2::TokenStream {
    // Used in the quasi-quotation below as `#name`.
    let name = &input.ident;
    let mut cast_generic = input.generics.clone();
    let call = Ident::new(callname, name.span());
    let call2 = Ident::new("is_constant_to", name.span());
    let (_impl_generics, _ty_generics, _where_clause) = input.generics.split_for_impl();
    let mut wf = vec![];
    let mut wf2 = vec![];
    let s = match &input.data {
        Data::Struct(s) => s,
        _ => panic!("Only support struct datatype"),
    };
    let mut close_or_open = quote! {open};
    for (i, field) in s.fields.iter().enumerate() {
        let (fname, field_ty) = field_name_ty(&field, i, name.span());
        close_or_open = match &field.vis {
            Visibility::Public(_) => close_or_open,
            _ => {
                quote! {closed}
            }
        };
        let wf_call = gen_field_calls(
            &quote! {self.#fname},
            &field_ty,
            &|fname, _ftype| quote! {#fname.#call()},
        );
        wf.extend(wf_call);

        let wf2_call = gen_field_calls(
            &quote! {self.#fname},
            &field_ty,
            &|fname, _ftype| quote! {#fname.#call2(vmpl)},
        );
        wf2.extend(wf2_call);
    }
    let w = if wf.len() > 0 {
        join_tokens(&wf, quote! {&&})
    } else {
        quote! {true}
    };
    let w2 = if wf2.len() > 0 {
        join_tokens(&wf2, quote! {&&})
    } else {
        quote! {true}
    };
    let trt = if trtstr.len() > 0 {
        Some(Ident::new(trtstr, name.span()))
    } else {
        None
    };
    let usetrt = if let Some(t) = trt {
        quote! {#t for}
    } else {
        quote! {}
    };
    let pub_or_default = if trtstr.len() > 0 {
        quote! {}
    } else {
        quote! {pub}
    };

    let trt = vec![trtstr];
    add_bound_to_generic(
        &mut cast_generic,
        gen_trait_bound(trt, name.span()),
        name.span(),
    );

    let (impl_generics, ty_generics, where_clause) = cast_generic.split_for_impl();

    let expanded = quote! {
        // The generated axiom for size.
        verus!{
        impl #impl_generics #usetrt #name #ty_generics #where_clause  {
            #pub_or_default #close_or_open spec fn #call(&self) -> bool {
                #w
            }

            #pub_or_default #close_or_open spec fn #call2(&self, vmpl: nat) -> bool {
                #w2
            }
        }
        }
    };
    expanded
}

pub fn verismo_derive_property_expand(
    input: proc_macro::TokenStream,
    calname: &str,
    trt: &str,
) -> proc_macro::TokenStream {
    // Hand the output tokens back to the compiler.
    let input = parse_macro_input!(input as DeriveInput);

    let ret = proc_macro::TokenStream::from(verismo_derive_property_expand2(&input, calname, trt));
    //println!("{}", ret);
    ret
}

================
File: ./source/verismo_macro/src/global.rs
================

use std::collections::hash_map::DefaultHasher;
use std::hash::{Hash, Hasher};

use proc_macro::TokenStream;
use quote::quote;
use syn_verus::spanned::Spanned;
use syn_verus::{parse_macro_input, ItemFn, Signature};

fn string_to_u64(s: &str) -> u64 {
    let mut hasher = DefaultHasher::new();
    s.hash(&mut hasher);
    hasher.finish()
}

pub fn parse_global(
    _attr: TokenStream,
    item: TokenStream,
    non_zero: bool,
    need_extern: bool,
) -> TokenStream {
    let input = parse_macro_input!(item as syn_verus::Item);
    let line = input.span().unwrap().start().line() as u64;
    let file = input.span().unwrap().source_file().path();
    let file = file.as_os_str().to_str().unwrap();
    let fileid = string_to_u64(file);
    let unique_id = fileid / 0x10000 * 0x10000 + line;
    let specmem = quote! {crate::arch::addr::SpecMem<crate::arch::addr::GuestVir>};
    let gdef = quote! {crate::verismo::VeriSMoGhost};
    let gspec = quote! {crate::verismo::VeriSMoSpec};
    let vaddrty = quote! {crate::arch::addr::VAddr};
    let extra = match &input {
        syn_verus::Item::Static(syn_verus::ItemStatic { ident, ty, .. }) => {
            //let file = ident.span().source_file().path().display().to_string();
            let addr_ident =
                syn_verus::Ident::new(&format!("mem_{}", ident.to_string()), ident.span());
            let get_ident =
                syn_verus::Ident::new(&format!("{}_get", ident.to_string()), ident.span());
            let axiom_ident =
                syn_verus::Ident::new(&format!("axiom_{}", ident.to_string()), ident.span());
            quote! {
                verus!{
                impl #gspec {
                    #[verifier(inline)]
                    pub open spec fn #addr_ident() -> #specmem
                    {Self::raw_vmem(spec_cast_integer::<_, int>(#unique_id), #non_zero)}
                    pub spec fn #ident() -> crate::verismo::data::VData<#ty>;
                    #[verifier(external_body)]
                    pub broadcast proof fn #axiom_ident()
                    ensures
                        builtin::equal(#[trigger] #gspec::#ident().spec_vmem(),  #[trigger] #gspec::#addr_ident())
                    {}

                    #[verifier(inline)]
                    pub open spec fn #get_ident(&self) -> #ty
                    { Self::#ident().spec_vget(*self)}
                }
                }

                verus!{
                impl #gdef {
                    #[verifier(external_body)]
                    pub fn #ident() -> crate::verismo::data::VData<#ty>
                    {
                        builtin::ensures(|ret: VData<#ty>|[builtin::equal(ret, #gspec::#ident())]);
                        let ret = unsafe {
                            &#ident as *const _ as u64
                        };
                        let vaddr = #vaddrty::new(ret);
                        VData::new(vaddr)
                    }
                }
            }
            }
        }
        syn_verus::Item::Fn(ItemFn {
            sig: Signature { ident, .. },
            ..
        }) => {
            quote! {
                    verus!{
                    impl #gspec {
                        #[verifier(inline)]
                        pub open spec fn #ident() -> #specmem
                        {
                            Self::raw_vmem(spec_cast_integer::<_, int>(#unique_id), true)
                        }
                        }
                    }

                    verus!{
                    impl #gdef {
                        #[verifier(external_body)]
                        #[inline]
                        pub fn #ident() -> #vaddrty
                        {
                            builtin::ensures(|ret: #vaddrty|[builtin::equal(ret.view(), #gspec::#ident().first())]);
                            let ret = unsafe {
                                &#ident as *const _ as u64
                            };
                            #vaddrty::new(ret)
                        }
                    }
                }
            }
        }
        _ => {
            panic! {"Unsupported items"}
        }
    };

    let expanded = if need_extern {
        quote! {
            extern{ #input }

            #extra
        }
    } else {
        quote! {
            #input

            #extra
        }
    };
    //println!("{}", extra);
    proc_macro::TokenStream::from(expanded)
}

================
File: ./source/verismo_macro/src/lib.rs
================

#![feature(proc_macro_span)]

mod bits;
mod clone;
mod def;
mod default;
mod enum_int;
mod exec_struct;
mod getter;
mod global;
mod new;
//mod op;
mod property;
//mod sec;
mod setter;
//mod size;
mod asm_global;
mod cast_sec;
mod print;
mod snp;
mod spec_eq;
mod spec_size;
mod static_globals;
//mod vdata;

use proc_macro::TokenStream;

#[proc_macro_attribute]
//static with non-zero initialization
pub fn vbit_struct(attribute: TokenStream, item: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    bits::parse_bit_struct(attribute, item)
}

#[proc_macro_attribute]
//static with non-zero initialization
pub fn v_static(attribute: TokenStream, item: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    global::parse_global(attribute, item, true, false)
}

#[proc_macro]
pub fn def_asm_addr_for(input: TokenStream) -> TokenStream {
    asm_global::asm_global(input)
}

#[proc_macro_attribute]
pub fn gen_shared_globals(_attribute: TokenStream, item: TokenStream) -> TokenStream {
    static_globals::gen_shared_globals(item)
}

#[proc_macro_attribute]
//static with non-zero initialization
pub fn v_extern_static(attribute: TokenStream, item: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    global::parse_global(attribute, item, true, true)
}

#[proc_macro_attribute]
// static with zero initialization
pub fn v_zstatic(attribute: TokenStream, item: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    global::parse_global(attribute, item, false, false)
}

/*#[proc_macro_attribute]
pub fn vstateop(attribute: TokenStream, item: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    op::parse_op(attribute, item)
}
*/

#[proc_macro_derive(SpecSize)]
pub fn verismo_size(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    spec_size::verismo_size_expand(input)
}

#[proc_macro_derive(SpecOffset, attributes(def_offset))]
pub fn verismo_defoffset(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    spec_size::verismo_defoffset_expand(input)
}

#[proc_macro_derive(VSpecEq)]
pub fn verismo_eq(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    spec_eq::verismo_eq_expand(input)
}

/*
#[proc_macro_derive(VSecLabel)]
pub fn verismo_security(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    sec::verismo_sec_expand(input)
}
*/

#[proc_macro_derive(WellFormed)]
pub fn verismo_wf(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    //snp::verismo_snp_expand(input, "wf", "WellFormed")
    snp::verismo_snp_expand(input, "wf", "WellFormed")
}

#[proc_macro_derive(IsConstant)]
pub fn verismo_is_constant(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    property::verismo_derive_property_expand(input, "is_constant", "IsConstant")
}

/// Auto generate Clone
#[proc_macro_derive(VClone)]
pub fn verismo_clone(input: TokenStream) -> proc_macro::TokenStream {
    // Parse the input tokens into a syntax tree.
    clone::verismo_clone_expand(input)
}

#[proc_macro_derive(VDefault)]
pub fn verismo_default(input: TokenStream) -> TokenStream {
    default::verismo_default_expand(input)
}

#[proc_macro_derive(VPrint)]
pub fn verismo_print(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    print::verismo_print_expand(input)
}

/// Auto generate VData<T> method for the struct T;
/// Provides VData<T>::<field> -> VData<FieldType>
/*#[proc_macro_derive(VeriSMoData)]
pub fn verismo_data(input: TokenStream) -> proc_macro::TokenStream {
    // Parse the input tokens into a syntax tree.
    vdata::verismo_data_expand(input)
}
*/
/// Auto generate to_stream method for the struct;
/// Provides MemStream<struct>::from_data and SpecType::<struct>::size()
#[proc_macro_derive(VTypeCast)]
pub fn verismo_cast(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    cast_sec::verismo_cast_seq_expand(input, vec!["Seq", "u8"], false)
}

#[proc_macro_derive(VTypeCastSec)]
pub fn verismo_cast_sec(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    cast_sec::verismo_cast_seq_expand(input, vec!["SecSeqByte"], true)
}

/// Auto generate spec_set_#field
/// The spec function will be closed if the field is private
#[proc_macro_derive(SpecSetter, attributes(is_public))]
pub fn verismo_setter(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    setter::verismo_setter_expand(input)
}

/// Auto generate spec_#field
/// The spec function will be closed if the field is private
#[proc_macro_derive(SpecGetter, attributes(is_public))]
pub fn verismo_getter(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    getter::verismo_getter_expand(input)
}

#[proc_macro_derive(SpecIntEnum)]
pub fn verismo_enum_int(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    enum_int::verismo_enum_int_expand(input)
}

#[proc_macro_derive(ExecStruct)]
//static with non-zero initialization
pub fn verismo_exec_struct(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    exec_struct::add_empty_trait_for_struct(input, "ExecStruct")
}

#[proc_macro_derive(NotPrimitive)]
//static with non-zero initialization
pub fn verismo_non_primitive_struct(input: TokenStream) -> TokenStream {
    // Parse the input tokens into a syntax tree.
    exec_struct::add_empty_trait_for_struct(input, "NotPrimitive")
}

================
File: ./source/verismo_macro/src/snp.rs
================

use proc_macro;
use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Ident, Type, Visibility};

use crate::def::{
    add_bound_to_generic, field_name_ty, gen_field_calls, gen_trait_bound, join_tokens,
};

pub fn verismo_snp_expand2(
    input: &DeriveInput,
    callname: &str,
    trtstr: &str,
) -> proc_macro2::TokenStream {
    // Used in the quasi-quotation below as `#name`.
    let name = &input.ident;
    let mut cast_generic = input.generics.clone();
    let call = Ident::new(callname, name.span());
    let _snpcall = Ident::new("snp", name.span());
    let (_impl_generics, _ty_generics, _where_clause) = input.generics.split_for_impl();
    let mut wf = vec![];
    //let mut snp = vec![];
    let mut size = vec![];
    let s = match &input.data {
        Data::Struct(s) => s,
        _ => panic!("Only support struct datatype"),
    };
    let mut close_or_open = quote! {open};
    for (i, field) in s.fields.iter().enumerate() {
        let (fname, field_ty) = field_name_ty(&field, i, name.span());
        close_or_open = match &field.vis {
            Visibility::Public(_) => close_or_open,
            _ => {
                quote! {closed}
            }
        };
        let wf_call = gen_field_calls(
            &quote! {self.#fname},
            &field_ty,
            &|fname, _ftype| quote! {#fname.#call()},
        );
        wf.extend(wf_call);

        /*let snp_call = gen_field_calls(
            &quote! {self.#fname},
            &field_ty,
            &|fname, _ftype| quote! {#fname.#snpcall()},
        );
        snp.extend(snp_call);*/

        let size_call = gen_field_calls(
            &quote! {self.#fname},
            &field_ty,
            &|_fname, ftype: &Type| quote! {spec_size::<#ftype>()},
        );
        size.extend(size_call);
    }
    /*for (i, snp_call) in snp.iter().enumerate() {
        if i > 0 {
            let prev = &snp[i - 1];
            let size_call = &size[i];
            wf.push(quote! {builtin::imply(#size_call > 0, builtin::equal(#snp_call, #prev))});
        }
    }
    let snp_call = if snp.len() > 0 {
        snp[0].clone()
    } else {
        quote! {SnpMemAttr::spec_default()}
    };*/
    let w = if wf.len() > 0 {
        join_tokens(&wf, quote! {&&})
    } else {
        quote! {true}
    };

    let trt = if trtstr.len() > 0 {
        Some(Ident::new(trtstr, name.span()))
    } else {
        None
    };
    let usetrt = if let Some(t) = trt {
        quote! {#t for}
    } else {
        quote! {}
    };
    let pub_or_default = if trtstr.len() > 0 {
        quote! {}
    } else {
        quote! {pub}
    };

    let trt = vec![trtstr];
    add_bound_to_generic(
        &mut cast_generic,
        gen_trait_bound(trt, name.span()),
        name.span(),
    );
    let (impl_generics, ty_generics, where_clause) = cast_generic.split_for_impl();

    let expanded = quote! {
        // The generated axiom for size.
        verus!{
        impl #impl_generics #usetrt #name #ty_generics #where_clause  {
            #pub_or_default #close_or_open spec fn #call(&self) -> bool {
                #w
            }
        }
        /*impl #impl_generics SnpMemAttrTrait for #name #ty_generics #where_clause  {
            #pub_or_default #close_or_open spec fn snp(&self) -> SnpMemAttr {
                #snp_call
            }
        }*/
        }
    };
    //println!("{}", expanded);
    expanded
}

pub fn verismo_snp_expand(
    input: proc_macro::TokenStream,
    calname: &str,
    trt: &str,
) -> proc_macro::TokenStream {
    // Hand the output tokens back to the compiler.
    let input = parse_macro_input!(input as DeriveInput);

    let ret = proc_macro::TokenStream::from(verismo_snp_expand2(&input, calname, trt));
    //println!("{}", ret);
    ret
}

================
File: ./source/verismo_macro/src/bits.rs
================

use proc_macro::TokenStream;
use quote::quote;
use syn_verus::spanned::Spanned;
use syn_verus::{parse_macro_input, AttributeArgs, Ident, Lit, NestedMeta};

use crate::def::field_name_ty;

pub fn parse_bit_struct(attr: TokenStream, item: TokenStream) -> TokenStream {
    let input = parse_macro_input!(item as syn_verus::Item);
    let mut fields_stream = quote! {};
    let args = parse_macro_input!(attr as AttributeArgs);
    let s = match &input {
        syn_verus::Item::Struct(s) => s,
        _ => todo!(),
    };
    if args.len() < 2 {
        panic!("Must provide struct name and the integer type")
    }
    let bitstruct = &args[0];
    let valuetype = &args[1];
    //println!("{} : {}\n", quote! {#bitstruct}, quote! {#valuetype});
    //let bitstruct = Ident::new(&attr.to_string(), s.span());
    let specname = &s.ident;
    let mut specfields = quote! {arbitrary::<#specname>()};
    let mut emptyspec = quote! {arbitrary::<#specname>()};
    let mut specstruct = quote! {};
    let mut max_bits = 0;
    for (i, field) in s.fields.iter().enumerate() {
        let (fname, fty) = field_name_ty(&field, i, s.span());
        let setter = Ident::new(&format!("set_{}", fname), fname.span());
        let getter: Ident = Ident::new(&format!("get_{}", fname), fname.span());
        let bound_getter: Ident = Ident::new(&format!("lemma_bound_{}", fname), fname.span());

        let spec_setter = Ident::new(&format!("spec_set_{}", fname), fname.span());
        let spec_getter = Ident::new(&format!("spec_{}", fname), fname.span());
        let mut bit_start: u64 = 0;
        let mut bit_last: u64 = 0;
        for attr in &field.attrs {
            let name = attr.path.get_ident();
            if name.unwrap().to_string() == "vbits" {
                //bit_start = attr.path.segments[1];
                //bit_last = attr.path.segments[2];
                if let Result::Ok(meta) = attr.parse_meta() {
                    match meta {
                        syn_verus::Meta::Path(_) => todo!(),
                        syn_verus::Meta::List(metalist) => {
                            if let NestedMeta::Lit(Lit::Int(litint)) = &metalist.nested[0] {
                                bit_start = litint.base10_parse::<u64>().unwrap();
                                bit_last = bit_start;
                            }
                            if metalist.nested.len() >= 2 {
                                if let NestedMeta::Lit(Lit::Int(litint)) = &metalist.nested[1] {
                                    bit_last = litint.base10_parse::<u64>().unwrap();
                                }
                            }
                        }
                        syn_verus::Meta::NameValue(_) => todo!(),
                    }
                }
            }
        }
        max_bits = if max_bits < bit_last {
            bit_last
        } else {
            max_bits
        };
        let field_max_val: u64 = (1 << (bit_last - bit_start + 1)) - 1;
        let mask: u64 = field_max_val;
        let set_mask: u64 = field_max_val << bit_start;
        fields_stream = quote! {
            #fields_stream
            verus!{
            #[inline(always)]
            pub const fn #setter(&self, val: #fty) -> (ret: Self)
            requires
                val <= #field_max_val as #fty
            ensures
                builtin::equal(ret.view(), self.view().#spec_setter(val))
            {
                let mask = #set_mask as #fty;
                let value = self.value();
                let ghost oldv = value;
                let value = (value & (!mask)) | (val << (#bit_start as #fty));
                let ret = #bitstruct{value};
                proof {
                    let actual_ret = ret.view();
                    let expected_ret = self.view().#spec_setter(val);
                    assume(builtin::equal(actual_ret, expected_ret));
                }
                ret
            }}

            verus!{
            pub proof fn #bound_getter(&self) -> (ret: #fty)
            ensures
                ret == self.#spec_getter(),
                ret <= #field_max_val as #fty,
                ret >= 0,
            {
                bit_and64_auto();
                self.#spec_getter()
            }

            pub const fn #getter(&self) -> (ret: #fty)
            ensures
                ret == self.view().#spec_getter(),
                self.view().#spec_getter() <= #field_max_val as #fty
            {
                proof {
                    bit_and64_auto();
                }
                let mask = #mask as #fty;
                (self.value() >> (#bit_start as #fty)) & mask
            }
            }

            verus!{
            pub open spec fn #spec_getter(&self) -> #fty {
                let mask = #mask as #fty;
                (self.value as #fty >> (#bit_start as #fty)) & mask
            }
            }
        };

        specfields = quote! {
            #specfields.#spec_setter(self.#spec_getter() as #fty)
        };
        emptyspec = quote! {
            #emptyspec.#spec_setter(0)
        };
        let vis = &field.vis;
        specstruct = quote! {
            #specstruct
            #vis #fname: #fty,
        }
    }
    let vis = &s.vis;
    let max_val: u128 = (1 << max_bits) - 1;
    //println!("max_val = {} max_bits ={}", max_val, max_bits);
    let expanded = quote! {
        verus!{
        #[derive(SpecGetter, SpecSetter)]
        #vis ghost struct #specname {
            #specstruct
        }
        }
        verus!{
        impl #specname {
            pub open spec fn empty() -> #specname {
                #emptyspec
            }

            pub open spec fn new(val: #valuetype) -> #specname;

            #[verifier(external_body)]
            pub broadcast proof fn axiom_new(val: #bitstruct)
            ensures
                builtin::equal(#[trigger]Self::new(val.value), #[trigger]val.view())
            {}

            pub open spec fn to_value(&self) -> #bitstruct;

            #[verifier(external_body)]
            pub broadcast proof fn axiom_into(self)
            ensures
                builtin::equal(#[trigger]self.to_value()@, self),
            {}
            }
        }

        verus!{
        #[derive(Copy, PartialEq, Eq, Structural, ExecStruct, NotPrimitive, SpecSize, VSpecEq, IsConstant, WellFormed)]
        #[repr(C, align(1))]
        pub struct #bitstruct {
            pub value: #valuetype,
        }
        }
        verus!{
        impl Clone for #bitstruct {
            #[verifier(external_body)]
            fn clone(&self) -> (ret: Self)
            ensures
                builtin::equal(*self, ret),
            {
                #bitstruct { value: self.value }
            }
        }
        }


        verus!{
        impl #bitstruct {
            verus! {
                pub open spec fn inv(&self) -> bool {
                    0 <= (self.value as int) <= (#max_val as int)
                }

                #[verifier(external_body)]
                pub broadcast proof fn axiom_inv(&self)
                ensures
                    #[trigger] self.inv(),
                    0 <= (#[trigger]self.value as int) <= (#max_val as int)
                {}
            }

            verus! {
                pub const fn new(val: #valuetype) -> (ret: Self)
                ensures
                    builtin::equal(ret, Self::spec_new(val)),
                    builtin::equal(ret.view(), #specname::new(val)),
                {
                    #bitstruct { value:val}
                }

                pub open spec fn spec_new(val: #valuetype) -> (ret: Self) {
                    #bitstruct { value:val}
                }

                pub broadcast proof fn lemma_new_eq(self)
                ensures
                    builtin::equal(Self::spec_new(self.value), self)
                {}

                pub const fn empty() -> (ret: Self)
                ensures
                    builtin::equal(ret.view(), #specname::empty())
                {
                    let ret = #bitstruct { value: 0 };
                    proof{
                        let val = ret.value;
                        assert forall |mask: #valuetype, offset: #valuetype|
                            #[trigger]((val >> offset) & mask)  == 0 as #valuetype
                        by {
                            assert_bit_vector(val != 0 || ((val >> offset) & mask) == 0);
                        }
                    }
                    ret
                }
            }

            #fields_stream
            verus!{
            pub open spec fn view(&self) -> #specname {
                #specfields
            }
            }
            verus!{
            pub const fn value(&self) -> (ret: #valuetype)
            ensures
                equal(ret, self.value),
                self.value <= #max_val as #valuetype
            {
                proof{
                    assert(self.inv());
                }
                self.value
            }
            }
        }
    }
    };
    //println!("{}", expanded);
    proc_macro::TokenStream::from(expanded)
}

================
File: ./source/verismo_macro/src/spec_eq.rs
================

use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Ident};

use crate::def::{
    add_bound_to_generic_with_self, field_name_ty, gen_field_calls, generic_mod,
    get_closed_or_open, join_tokens,
};

pub fn verismo_eq_expand(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    //let eq_tokens = &crate::eq::verismo_eq_expand2(&input);
    let name = input.ident;
    let generics = input.generics;
    let (_impl_generics, ty_generics, _where_clause) = generics.split_for_impl();
    let _ty_generics_fn = if !generics.params.is_empty() {
        quote!(::#ty_generics)
    } else {
        quote!()
    };
    let _instance = Ident::new("self", name.span());
    let _spec_type = generic_mod();

    let mut spec_eq_def_calls = vec![];
    let s = match input.data {
        Data::Struct(s) => s,
        _ => panic!("Only structs can be annotated with Mem"),
    };
    let s = &s;
    for (i, field) in s.fields.iter().enumerate() {
        let (fname, ftype) = field_name_ty(&field, i, name.span());
        spec_eq_def_calls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|_fname: &proc_macro2::TokenStream, _ftype| quote! {self.#_fname.spec_eq(rhs.#_fname)},
        ));
    }
    let closed_or_open = get_closed_or_open(s);
    let eq = if spec_eq_def_calls.len() > 0 {
        join_tokens(&spec_eq_def_calls, quote! {&&})
    } else {
        quote! {true}
    };
    let mut eq_generic = generics.clone();
    add_bound_to_generic_with_self(&mut eq_generic, vec!["VSpecEq"], name.span());
    let (impl_generics, ty_generics, where_clause) = eq_generic.split_for_impl();
    let expand2 = quote! {
        verus!{
            impl #impl_generics VSpecEq<#name #ty_generics> for #name #ty_generics #where_clause {
                #closed_or_open spec fn spec_eq(self, rhs: Self) -> bool
                {
                    #eq
                }
            }
        }
    };
    proc_macro::TokenStream::from(expand2)
}

================
File: ./source/verismo_macro/src/static_globals.rs
================

use proc_macro::TokenStream;
use quote::quote;
use syn_verus::{
    parse_macro_input, Data, DeriveInput, Expr, Fields, Ident, Type,
};

struct ParsedArgs {
    global_ident: Ident,
    type_ident: Type,
    invfn: Expr,
}

impl syn_verus::parse::Parse for ParsedArgs {
    fn parse(input: syn_verus::parse::ParseStream) -> syn_verus::Result<Self> {
        let content;
        syn_verus::parenthesized!(content in input);
        let global_ident = content.parse::<Ident>()?;
        content.parse::<syn_verus::Token![,]>()?;
        let type_ident = content.parse::<Type>()?;
        content.parse::<syn_verus::Token![,]>()?;
        let initfn_tokens: proc_macro2::TokenStream = content.parse()?;
        let invfn = syn_verus::parse2(initfn_tokens)?;
        Ok(ParsedArgs {
            global_ident,
            type_ident,
            invfn,
        })
    }
}

pub fn gen_shared_globals(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);

    let name = &input.ident;
    let data = &input.data;

    let mut new_variants = Vec::new();
    let mut funcs = Vec::new();

    if let Data::Enum(data_enum) = data {
        for variant in &data_enum.variants {
            if let Fields::Unit = variant.fields {
                let tname_attr = variant
                    .attrs
                    .iter()
                    .find(|attr| attr.path.is_ident("tname"))
                    .expect("Expected #[tname(..)] attribute");
                let args: proc_macro2::TokenStream = tname_attr.tokens.clone().into();
                let parsed_args: ParsedArgs =
                    syn_verus::parse2(args).expect("Failed to parse tname arguments");
                let global_ident = &parsed_args.global_ident;
                let type_ident = &parsed_args.type_ident;
                let invfn = &parsed_args.invfn;
                let variant_name = &variant.ident;
                new_variants.push(variant_name.clone());
                let spec_fn =
                    Ident::new(&format!("spec_{}", variant_name.to_string()), name.span());

                let memrange_fn = Ident::new(
                    &format!("spec_{}_range", variant_name.to_string()),
                    name.span(),
                );
                let lockid_fn = Ident::new(
                    &format!("spec_{}_lockid", variant_name.to_string()),
                    name.span(),
                );
                let contains_fn = Ident::new(
                    &format!("contains_{}", variant_name.to_string()),
                    name.span(),
                );
                let islock_fn = Ident::new(
                    &format!("is_permof_{}", variant_name.to_string()),
                    name.span(),
                );
                let axiom_func_name = Ident::new(
                    &format!("axiom_global_{}", variant_name.to_string().to_uppercase()),
                    name.span(),
                );

                funcs.push(quote! {
                    pub closed spec fn #spec_fn() -> VSpinLock<#type_ident>;
                    #[verifier(inline)]
                    pub open spec fn #memrange_fn() -> (int, nat) {
                        g_range(#name::#variant_name)
                    }
                    #[verifier(inline)]
                    pub open spec fn #lockid_fn() -> int {
                        g_range(#name::#variant_name).0
                    }

                    #[verifier(external_body)]
                    pub broadcast proof fn #axiom_func_name()
                    ensures
                        g_range(#name::#variant_name).1 == spec_size::<#type_ident>(),
                        builtin::equal(#spec_fn().ptr_range(), #memrange_fn()),
                        builtin::equal(#spec_fn().lockid(), #lockid_fn()),
                        #spec_fn().is_constant(),
                    {
                    }

                    #[verifier(external_body)]
                    pub fn #variant_name() -> (ret: &'static VSpinLock<#type_ident>)
                    ensures
                        builtin::equal(*ret, #spec_fn()),
                        ret.lockid() == #lockid_fn(),
                        ret.is_constant(),
                    {
                        &#global_ident
                    }

                    #[verifier(inline)]
                    pub open spec fn #contains_fn(m: crate::lock::LockMap) -> bool {
                        m.contains_lock(#lockid_fn(), #memrange_fn()) &&
                        builtin::equal(m[#lockid_fn()]@.invfn.value_invfn(), #invfn) &&
                        builtin::equal(m[#lockid_fn()]@.points_to.snp(), SwSnpMemAttr::spec_default())
                    }

                    #[verifier(inline)]
                    pub open spec fn #islock_fn(lockperm: crate::lock::LockPermToRaw) -> bool {
                        lockperm.lockid() == #lockid_fn() &&
                        lockperm.ptr_range() == #memrange_fn() &&
                        builtin::equal(lockperm.invfn.value_invfn(), #invfn) &&
                        builtin::equal(lockperm.points_to.snp(), SwSnpMemAttr::spec_default())
                    }
                });
            }
        }
    }

    let expanded = quote! {
        verus!{
        pub enum #name {
            #(#new_variants),*
        }
        }
        verus!{
        #[verifier(external_body)]
        pub broadcast proof fn axiom_global_auto()
        ensures
            forall |v1: #name, v2: #name|
                builtin::imply(!builtin::equal(v1, v2),
                g_range(v1).0 != g_range(v2).0
            ),
            forall |v1: #name, v2: #name|
                builtin::imply(!builtin::equal(v1, v2),
                range_disjoint_(#[trigger]g_range(v1), #[trigger]g_range(v2))),
        {}
        #(#funcs)*
        }
    };
    TokenStream::from(expanded)
}


================
File: ./source/verismo_macro/src/clone.rs
================

use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Lit, LitInt};

pub fn verismo_clone_expand(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    //let strinput = input.to_string();
    //let inputcopy = TokenStream::from_str(&strinput).expect("unreacheable");
    //parse_macro_input!(inputcopy as DeriveInput)
    let input: DeriveInput = parse_macro_input!(input as DeriveInput);
    let name = &input.ident;

    let mut fields_stream = quote! {};
    let generics = &input.generics;
    let (impl_generics, ty_generics, where_clause) = generics.split_for_impl();
    let mut is_named = false;
    match &input.data {
        Data::Struct(s) => {
            for (i, field) in s.fields.iter().enumerate() {
                match &field.ident {
                    Some(fname) => {
                        is_named = true;
                        fields_stream = quote! {
                            #fields_stream
                            #fname: self.#fname,
                        };
                    }
                    None => {
                        let lit = Lit::Int(LitInt::new(format!("{}", i).as_str(), name.span()));
                        fields_stream = quote! {
                            #fields_stream
                            self.#lit,
                        };
                    }
                }
            }
        }
        _ => {
            panic!("Only structs can be annotated with VClone");
        }
    };

    let newstruct = if is_named {
        quote! {
            #name{
                #fields_stream
            }
        }
    } else {
        quote! {
            #name(#fields_stream)
        }
    };
    let expand = quote! {
    verus!{
        impl #impl_generics Clone for #name #ty_generics #where_clause  {
            #[verifier(external_body)]
            fn clone(&self) -> (ret: Self)
            ensures
                builtin::equal(ret, *self)
            { #newstruct }
        }
    }
    };

    //println!("{}", expand);
    //let strtoken = newcode.to_string() + &expand.to_string();
    //proc_macro::TokenStream::from_str(&strtoken).expect("wrong code")
    //proc_macro::TokenStream::from(expand)

    proc_macro::TokenStream::from(expand)
}

================
File: ./source/verismo_macro/src/asm_global.rs
================

use proc_macro::TokenStream;
use quote::quote;
use syn_verus::parse::{Parse, ParseStream};
use syn_verus::{parse_macro_input, Ident, Token};

struct AsmInput {
    func_name: Ident,
    varname: Ident,
}

impl Parse for AsmInput {
    fn parse(input: ParseStream) -> Result<Self, syn_verus::Error> {
        let func_name = input.parse()?;
        input.parse::<Token![=]>()?;
        let varname = input.parse()?;
        Ok(AsmInput { func_name, varname })
    }
}

pub fn asm_global(input: TokenStream) -> TokenStream {
    let AsmInput { func_name, varname } = parse_macro_input!(input);
    let spec_func_name = Ident::new(
        format!("spec_{}", func_name.to_string()).as_str(),
        func_name.span(),
    );
    let asm_str = format!("lea rax, [rip + {}]", varname.to_string());
    let asm_tokens = quote! {
        verus!{
            pub spec fn #spec_func_name() -> int;
        }
        verus!{
            #[verifier(external_body)]
            pub fn #func_name() -> (ret: usize_t)
            ensures
                ret as int == #spec_func_name(),
                ret.is_constant()
            {
                let ret: usize;
                unsafe{
                    core::arch::asm!(
                        #asm_str,
                        lateout("rax") ret,
                    );
                }
                ret as usize
            }
        }
    };

    asm_tokens.into()
}

================
File: ./source/verismo_macro/src/setter.rs
================

use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput};

use crate::def::{field_name_ty, get_field, set_field};
use crate::new::gen_new_fn;

pub fn verismo_setter_expand(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    //let strinput = input.to_string();
    //let inputcopy = TokenStream::from_str(&strinput).expect("unreacheable");
    //parse_macro_input!(inputcopy as DeriveInput)
    let dinput: DeriveInput = parse_macro_input!(input as DeriveInput);
    let newcode = gen_new_fn(&dinput);

    let setcode = _verismo_setter_expand(&dinput);
    proc_macro::TokenStream::from(quote! {#setcode #newcode})
}

pub fn _verismo_setter_expand(input: &DeriveInput) -> proc_macro2::TokenStream {
    let name = &input.ident;

    let mut fields_stream = quote! {};
    let generics = &input.generics;
    let (impl_generics, ty_generics, where_clause) = generics.split_for_impl();
    match &input.data {
        Data::Struct(s) => {
            for (i, field) in s.fields.iter().enumerate() {
                let (fname, _) = field_name_ty(&field, i, name.span());
                let mut set_fields = quote! {};
                let field_ty = &field.ty;
                let setter = set_field(&fname, name.span());

                for (j, field2) in s.fields.iter().enumerate() {
                    let (fname2, _) = field_name_ty(&field2, j, name.span());
                    let get_ident = get_field(&fname2, name.span());
                    if i == j {
                        set_fields = quote! {
                            #set_fields
                            val,
                        };
                    } else {
                        set_fields = quote! {
                            #set_fields
                            self.#get_ident(),
                        };
                    }
                }

                /*let verus_pub_token = match &field.vis {
                    Visibility::Inherited => {
                        quote! {open}
                    }
                    _ => {
                        quote! {open}
                    }
                };*/

                fields_stream = quote! {
                    #fields_stream
                    verus!{
                    pub open spec fn #setter(&self, val: #field_ty) -> Self
                    {Self::spec_new(#set_fields)
                    }}
                };
            }
        }
        _ => panic!("Only structs with named fields can be annotated with Mem"),
    };

    let expand = quote! {
        verus!{
        impl #impl_generics #name #ty_generics #where_clause  {
            #fields_stream
        }
    }
    };
    expand
    //let strtoken = newcode.to_string() + &expand.to_string();
    //proc_macro::TokenStream::from_str(&strtoken).expect("wrong code")
    //proc_macro::TokenStream::from(expand)
}

================
File: ./source/verismo_macro/src/getter.rs
================

use proc_macro::TokenStream;
use proc_macro2::Span;
use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Type, Visibility};

use crate::def::{field_name_ty, get_field};

pub fn gen_getter(
    fname: &proc_macro2::TokenStream,
    ftype: &Type,
    verus_pub_token: proc_macro2::TokenStream,
    span: Span,
) -> proc_macro2::TokenStream {
    let getter_ident = get_field(&fname, span);
    match ftype {
        Type::Path(_type_path) => {
            quote! {
                verus!{
                pub #verus_pub_token spec fn #getter_ident(&self) -> #ftype {
                    self.#fname
                }}
            }
        }
        Type::Reference(_type_path) => {
            quote! {
                verus!{
                pub #verus_pub_token spec fn #getter_ident(&self) -> #ftype {
                    self.#fname
                }}
            }
        }
        Type::Array(_tyarray) => {
            panic! {"Not supported"}
            /*let mut ret = quote!{};
            let len = get_array_len(&tyarray);
            for i in 0..len {
                let subfname = Ident::new(format!("{}_{}", fname, i).as_str(), fname.span());
                let subf = gen_getter(&subfname, &tyarray.elem, verus_pub_token);
                ret  = quote!{
                    #ret
                    #subf
                }
            }
            ret*/
        }
        _ => {
            panic! {"Not supported"}
        }
    }
}

pub fn verismo_getter_expand(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    let name = input.ident;
    let generics = input.generics;
    let (impl_generics, ty_generics, where_clause) = generics.split_for_impl();
    let mut fields_stream = quote! {};

    match input.data {
        Data::Struct(s) => {
            for (i, field) in s.fields.iter().enumerate() {
                let (fname, ftype) = field_name_ty(&field, i, name.span());
                let verus_pub_token = match &field.vis {
                    Visibility::Inherited => {
                        quote! {closed}
                    }
                    _ => {
                        quote! {open}
                    }
                };
                let field_stream = gen_getter(&fname, &ftype, verus_pub_token, name.span());
                fields_stream = quote! {
                    #fields_stream

                    #field_stream
                };
            }
        }
        _ => panic!("Only structs with fields can be annotated with Mem"),
    };

    let expand = quote! {
        verus!{
        impl #impl_generics #name #ty_generics #where_clause {
            #fields_stream
        }
        }
    };
    proc_macro::TokenStream::from(expand)
}

================
File: ./source/verismo_macro/src/spec_size.rs
================

use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Ident, Type};

use crate::def::{
    add_bound_to_generic, field_name_ty, field_offset, field_offset_exe, gen_field_calls,
    gen_trait_bound, generic_mod, get_field, join_tokens, type_to_type_generic,
};

fn convert(ftype: &Type) -> proc_macro2::TokenStream {
    match ftype {
        Type::Reference(_) | Type::Ptr(_) => {
            quote! {8}
        }
        _ => {
            let t = type_to_type_generic(ftype);
            quote! {#t::spec_size_def()}
        }
    }
}

fn convert_exe(ftype: &Type) -> proc_macro2::TokenStream {
    let t = type_to_type_generic(ftype);
    quote! {core::mem::size_of::<#t>()}
}

pub fn verismo_size_expand(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    //let size_tokens = &crate::size::verismo_size_expand2(&input);
    let name = input.ident;
    let generics = input.generics;
    let (_impl_generics, ty_generics, _where_clause) = generics.split_for_impl();
    let _ty_generics_fn = if !generics.params.is_empty() {
        quote!(::#ty_generics)
    } else {
        quote!()
    };
    let _instance = Ident::new("self", name.span());
    let _spec_type = generic_mod();

    let mut spec_size_def_calls = vec![];
    let mut size_def_calls = vec![];

    let s = match input.data {
        Data::Struct(s) => s,
        _ => panic!("Only structs can be annotated with Mem"),
    };
    let s = &s;
    for (i, field) in s.fields.iter().enumerate() {
        let (fname, ftype) = field_name_ty(&field, i, name.span());
        if field
            .attrs
            .iter()
            .any(|attr| attr.path.is_ident("def_offset"))
        {
            let field_name = &field.ident;
            let _field_offset_exe = field_offset_exe(&fname, name.span());
            let _field_copy = Ident::new(
                format!("copy_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let _field_update = Ident::new(
                format!("set_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let _spec_set_fn = Ident::new(
                format!("spec_set_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let _spec_get_field = Ident::new(
                format!("spec_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let _copy_field_param = Ident::new(
                format!("Copy{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let _update_field_param = Ident::new(
                format!("Update{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let _spec_field_offset = field_offset(&fname, name.span());
            let _getter = get_field(&fname, name.span());
            let _axiom_field = Ident::new(
                format!("axiom_field_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );
            let _sizeexe = if size_def_calls.len() > 0 {
                join_tokens(&size_def_calls, quote! {+})
            } else {
                quote! {0usize}
            };
            let _specsize = if spec_size_def_calls.len() > 0 {
                join_tokens(&spec_size_def_calls, quote! {+})
            } else {
                quote! {0}
            };
        }

        spec_size_def_calls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|_fname: &proc_macro2::TokenStream, ftype| convert(ftype),
        ));

        size_def_calls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|_fname: &proc_macro2::TokenStream, ftype| convert_exe(ftype),
        ));
    }
    let size = if spec_size_def_calls.len() > 0 {
        join_tokens(&spec_size_def_calls, quote! {+})
    } else {
        quote! {0}
    };
    let mut size_generic = generics.clone();
    add_bound_to_generic(
        &mut size_generic,
        gen_trait_bound(vec!["SpecSize"], name.span()),
        name.span(),
    );
    let (impl_generics, ty_generics, where_clause) = size_generic.split_for_impl();

    let expand2 = quote! {
        verus!{
            impl #impl_generics SpecSize for #name #ty_generics #where_clause {
                #[verifier(inline)]
                open spec fn spec_size_def() -> (ret: nat)
                {
                    #size
                }
            }
        }
    };
    proc_macro::TokenStream::from(expand2)
}

pub fn verismo_defoffset_expand(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    //let size_tokens = &crate::size::verismo_size_expand2(&input);
    let name = input.ident;
    let generics = input.generics;
    let (impl_generics, ty_generics, _where_clause) = generics.split_for_impl();
    let _ty_generics_fn = if !generics.params.is_empty() {
        quote!(::#ty_generics)
    } else {
        quote!()
    };
    let _instance = Ident::new("self", name.span());
    let _spec_type = generic_mod();

    let mut spec_size_def_calls = vec![];
    let mut size_def_calls = vec![];
    let mut offsetfns = quote!();
    let mut offsetfns_ptr = quote!();
    let mut offsetfns_box = quote!();

    let mut offset_counts: i32 = 0;
    let s = match input.data {
        Data::Struct(s) => s,
        _ => panic!("Only structs can be annotated with Mem"),
    };
    let s = &s;
    for (i, field) in s.fields.iter().enumerate() {
        let (fname, ftype) = field_name_ty(&field, i, name.span());
        if field
            .attrs
            .iter()
            .any(|attr| attr.path.is_ident("def_offset"))
        {
            let field_name = &field.ident;
            let field_offset_exe = field_offset_exe(&fname, name.span());
            let field_copy = Ident::new(
                format!("copy_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let field_update = Ident::new(
                format!("set_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let spec_set_fn = Ident::new(
                format!("spec_set_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let spec_get_field = Ident::new(
                format!("spec_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let copy_field_param = Ident::new(
                format!("Copy{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let update_field_param = Ident::new(
                format!("Update{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );

            let spec_field_offset = field_offset(&fname, name.span());
            let getter = get_field(&fname, name.span());
            let axiom_field = Ident::new(
                format!("axiom_field_{}", field_name.clone().unwrap().to_string()).as_str(),
                name.span(),
            );
            let sizeexe = if size_def_calls.len() > 0 {
                join_tokens(&size_def_calls, quote! {+})
            } else {
                quote! {0usize}
            };
            let specsize = if spec_size_def_calls.len() > 0 {
                join_tokens(&spec_size_def_calls, quote! {+})
            } else {
                quote! {0}
            };
            let offsetfn = quote! {
                pub fn #field_offset_exe() -> (ret: usize)
                ensures
                    ret == Self::#spec_field_offset()
                {
                    // Calculate the offset for the field here
                    // You may want to use std::mem::offset_of!
                    // Example: std::mem::offset_of!(Self, #field_name)
                    #sizeexe
                }

                pub open spec fn #spec_field_offset() -> nat
                {
                    (#specsize) as nat
                }

                #[verifier(external_body)]
                pub broadcast proof fn #axiom_field(&self)
                ensures
                    builtin::equal(self.#getter(), field_at(*self, Self::#spec_field_offset()))
                {}
            };
            offset_counts = offset_counts + 1;
            offsetfns = quote!(
                #offsetfns
                #offsetfn
            );
            offsetfns_ptr = quote! {
                #offsetfns_ptr
                pub fn #field_name(&self) -> (ret: SnpPPtr<#ftype>)
                requires
                    self.not_null(),
                ensures
                    self.id() + #name::#spec_field_offset() == ret.id(),
                    builtin::imply(self.is_constant(), ret.is_constant()),
                {
                    let offset: usize_s = #name::#field_offset_exe().into();
                    SnpPPtr::from_usize(self.uptr.add(offset))
                    //SnpPPtr::from_usize(self.uptr)
                }
            };
            offsetfns_box = quote! {
                #offsetfns_box
                pub struct #copy_field_param;
                impl<'a> crate::vbox::BorrowFnTrait<'a, #copy_field_param, #ftype> for #name {
                    open spec fn spec_borrow_requires(&self, params: #copy_field_param) -> bool {
                        true
                    }

                    open spec fn spec_borrow_ensures(&self, params: #copy_field_param, ret: #ftype) -> bool {
                        builtin::equal(self.#spec_get_field(), ret)
                    }

                    //#[verifier(external_body)]
                    fn box_borrow(&'a self, params: #copy_field_param) -> (ret: #ftype)
                    {
                        self.#field_name.clone()
                    }
                }
                impl #impl_generics crate::vbox::VBox<#name #ty_generics> {
                    #[inline]
                    pub fn #field_copy(&self) -> (ret: #ftype)
                    ensures
                        ret.wf(),
                        builtin::imply(self.snp().is_vmpl0_private(),
                            builtin::equal(ret, self@.#spec_get_field())),
                    {
                        return self.box_borrow(#copy_field_param);
                    }
                }

                pub struct #update_field_param {
                    pub val: #ftype
                }

                impl<'a> crate::vbox::MutFnTrait<'a, #update_field_param, bool> for #name {
                    open spec fn spec_update_requires(&self, params: #update_field_param) -> bool {
                        true
                    }

                    open spec fn spec_update(&self, prev: &Self, params:  #update_field_param, ret: bool) -> bool {
                        builtin::equal(*self, prev.#spec_set_fn(params.val))
                    }

                    fn box_update(&'a mut self, params: #update_field_param) -> (ret: bool)
                    {
                        self.#field_name = params.val;
                        true
                    }
                }

                impl #impl_generics crate::vbox::VBox<#name #ty_generics> {
                    #[inline]
                    pub fn #field_update(&mut self, val: #ftype)
                    ensures
                        self@.spec_update(&old(self)@, #update_field_param{val}, true),
                        self.only_val_updated(*old(self)),
                    {
                        self.box_update(#update_field_param{val});
                    }
                }
            };
        }

        spec_size_def_calls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|_fname: &proc_macro2::TokenStream, ftype| convert(ftype),
        ));

        size_def_calls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|_fname: &proc_macro2::TokenStream, ftype| convert_exe(ftype),
        ));
    }
    let _size = if spec_size_def_calls.len() > 0 {
        join_tokens(&spec_size_def_calls, quote! {+})
    } else {
        quote! {0}
    };
    let mut size_generic = generics.clone();
    add_bound_to_generic(
        &mut size_generic,
        gen_trait_bound(vec!["SpecSize"], name.span()),
        name.span(),
    );
    let (impl_generics, ty_generics, where_clause) = size_generic.split_for_impl();
    offsetfns_ptr = if offset_counts == 0 {
        quote! {}
    } else {
        quote! {
            impl #impl_generics crate::ptr::SnpPPtr<#name #ty_generics> #where_clause {
                #offsetfns_ptr
            }
        }
    };

    offsetfns_box = if offset_counts == 0 {
        quote! {}
    } else {
        //println!("{}", offsetfns_box);
        quote! {
            #offsetfns_box
        }
    };
    let expand2 = quote! {
        verus!{
            impl #impl_generics #name #ty_generics #where_clause {
                #offsetfns
            }
            #offsetfns_ptr

            #offsetfns_box
        }
    };
    proc_macro::TokenStream::from(expand2)
}

================
File: ./source/verismo_macro/src/cast_sec.rs
================

use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Ident, Type};

use crate::def::{
    add_bound_to_generic, field_name_ty, gen_field_calls, gen_trait_bound, generic_mod,
    get_closed_or_open, join_tokens, new_path, type_to_type_generic,
};
fn convert(ftype: &Type) -> proc_macro2::TokenStream {
    let t = type_to_type_generic(ftype);
    quote! {#t::spec_size_def()}
}

pub fn verismo_cast_seq_expand(
    input: proc_macro::TokenStream,
    seqtypenames: Vec<&str>,
    _add_proof: bool,
) -> proc_macro::TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    //let size_tokens = &crate::size::verismo_size_expand2(&input);
    let name = input.ident;
    let generics = input.generics;
    let seqtype = new_path(&seqtypenames, name.span());
    let seqtype0 = Ident::new(seqtypenames[0], name.span());
    let (_impl_generics, ty_generics, _where_clause) = generics.split_for_impl();
    let _ty_generics_fn = if !generics.params.is_empty() {
        quote!(::#ty_generics)
    } else {
        quote!()
    };
    let _instance = Ident::new("self", name.span());
    let _spec_type = generic_mod();
    let mut secseqcalls = vec![];
    let mut cast_proof_calls = vec![];
    let mut spec_size_def_calls = vec![];
    let s = match input.data {
        Data::Struct(s) => s,
        _ => panic!("Only structs can be annotated with Mem"),
    };
    let s = &s;
    for (i, field) in s.fields.iter().enumerate() {
        let (fname, ftype) = field_name_ty(&field, i, name.span());
        secseqcalls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|fname, ftype| match ftype {
                Type::Reference(_) => quote! {VTypeCast::<#seqtype>::vspec_cast_to(0u64)},
                _ => quote! {VTypeCast::<#seqtype>::vspec_cast_to(self.#fname)},
            },
        ));
        cast_proof_calls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|fname, _ftype| quote! {self.#fname.proof_into_is_constant();},
        ));
        spec_size_def_calls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|_fname: &proc_macro2::TokenStream, ftype| convert(ftype),
        ));
    }
    let closed_or_open = get_closed_or_open(s);
    let seq = if secseqcalls.len() > 0 {
        join_tokens(&secseqcalls, quote! {+})
    } else {
        quote! {#seqtype0::empty()}
    };
    let _cast_proof_calls = if cast_proof_calls.len() > 0 {
        join_tokens(&cast_proof_calls, quote! {})
    } else {
        quote! {}
    };
    let mut cast_generic = generics.clone();
    let mut trt = vec!["VTypeCast"];
    trt.extend(seqtypenames);
    add_bound_to_generic(
        &mut cast_generic,
        gen_trait_bound(trt, name.span()),
        name.span(),
    );

    let (impl_generics, ty_generics, where_clause) = cast_generic.split_for_impl();
    let expand2 = quote! {
        verus!{
            impl #impl_generics VTypeCast<#seqtype> for #name #ty_generics #where_clause {
                //#[verifier(inline)]
                #closed_or_open spec fn vspec_cast_to(self) -> #seqtype {
                    #seq
                }
            }
        }
    };
    /*
        impl #impl_generics SpecSize for #name #ty_generics #where_clause {
            #[verifier(inline)]
            open spec fn spec_size_def() -> (ret: nat)
            {
                #size
            }
        }
    }*/
    //#size_tokens
    //println!("{}", expand2);
    proc_macro::TokenStream::from(expand2)
}

================
File: ./source/verismo_macro/src/print.rs
================

use proc_macro;
use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Ident};

use crate::def::{
    add_bound_to_generic, field_name_ty, gen_field_calls, gen_trait_bound, join_tokens,
};

pub fn verismo_print_expand(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    //let size_tokens = &crate::size::verismo_size_expand2(&input);
    let name = input.ident;
    let generics = input.generics;
    let (_impl_generics, ty_generics, _where_clause) = generics.split_for_impl();
    let _ty_generics_fn = if !generics.params.is_empty() {
        quote!(::#ty_generics)
    } else {
        quote!()
    };
    let _instance = Ident::new("self", name.span());

    let mut print_calls = vec![];
    let s = match input.data {
        Data::Struct(s) => s,
        _ => panic!("Only structs can be annotated with Mem"),
    };
    let s = &s;
    for (i, field) in s.fields.iter().enumerate() {
        let (fname, ftype) = field_name_ty(&field, i, name.span());
        print_calls.extend(gen_field_calls(
            &fname,
            &ftype,
            &|_fname: &proc_macro2::TokenStream, _ftype| {
                let fname_str = if i == 0 {
                    "{".to_string() + &_fname.to_string()
                } else {
                    ", ".to_string() + &_fname.to_string()
                } + "=";
                quote! {
                    proof {
                        reveal_strlit(#fname_str);
                    }
                    let Tracked(console) = new_strlit(#fname_str).early_print2(Tracked(snpcore), Tracked(console));
                    let Tracked(console) = self.#_fname.early_print2(Tracked(snpcore), Tracked(console));
                }
            },
        ));
    }
    let print_stmts = if print_calls.len() > 0 {
        join_tokens(&print_calls, quote! {})
    } else {
        quote! {}
    };
    let mut print_generic: syn_verus::Generics = generics.clone();
    add_bound_to_generic(
        &mut print_generic,
        gen_trait_bound(vec!["VPrint"], name.span()),
        name.span(),
    );
    add_bound_to_generic(
        &mut print_generic,
        gen_trait_bound(vec!["IsConstant"], name.span()),
        name.span(),
    );
    let (impl_generics, ty_generics, where_clause) = print_generic.split_for_impl();
    let expand2 = quote! {
        verus!{
            impl #impl_generics VPrint for #name #ty_generics #where_clause {
                #[verifier(inline)]
                open spec fn early_print_requires(&self) -> bool
                {
                    self.is_constant()
                }

                fn early_print2(&self, Tracked(snpcore): Tracked<&mut crate::registers::SnpCore>, Tracked(console): Tracked<SnpPointsToRaw>) -> (newconsole: Tracked<SnpPointsToRaw>)
                {
                    #print_stmts
                    proof {
                        reveal_strlit("}\n");
                    }
                    let Tracked(console) = new_strlit("}\n").early_print2(Tracked(snpcore), Tracked(console));
                    Tracked(console)
                }
            }
        }
    };
    proc_macro::TokenStream::from(expand2)
}

================
File: ./source/verismo_macro/src/new.rs
================

use quote::quote;
use syn_verus::{Data, DeriveInput};

use crate::def::{field_name_ty, get_field};

pub fn gen_new_fn(input: &DeriveInput) -> proc_macro2::TokenStream {
    let name = &input.ident;
    let (impl_generics, ty_generics, where_clause) = input.generics.split_for_impl();
    let s = match &input.data {
        Data::Struct(s) => s,
        _ => {
            panic!();
        }
    };
    let mut new_fields = quote! {};
    let mut new_fields_param = quote! {};
    let mut new_fields_param_body = quote! {};

    for (i, field) in s.fields.iter().enumerate() {
        let (fname, ftype) = field_name_ty(&field, i, name.span());
        new_fields_param = quote! {#new_fields_param #fname: #ftype,};
        new_fields_param_body = quote! {#new_fields_param_body #fname,};
    }

    for (i, field) in s.fields.iter().enumerate() {
        let (fname, _) = field_name_ty(&field, i, name.span());
        let getter = get_field(&fname, name.span());
        new_fields = quote! {
            #new_fields
            builtin::equal((#[trigger] Self::spec_new(#new_fields_param_body)).#getter(), #fname),
        };
    }

    let expand = quote! {
        verus!{
        impl #impl_generics #name #ty_generics #where_clause  {
        verus!{
            #[verifier(external_body)]
            pub open spec fn spec_new(#new_fields_param) -> Self{
                unimplemented!()
            }

            #[verifier(external_body)]
            pub broadcast proof fn axiom_spec_new(#new_fields_param)
            {
                ensures([#new_fields]);
            }
        }
        }
    }
    };
    expand
}

================
File: ./source/verismo_macro/src/exec_struct.rs
================

use proc_macro;
use quote::quote;
use quote::spanned::Spanned;
use syn_verus::{parse_macro_input, Data, DeriveInput};

fn add_empty_trait(input: &DeriveInput, trt: proc_macro2::TokenStream) -> proc_macro2::TokenStream {
    // Used in the quasi-quotation below as `#name`.
    let name = &input.ident;
    let (impl_generics, ty_generics, where_clause) = input.generics.split_for_impl();
    let _s = match &input.data {
        Data::Struct(s) => s,
        _ => panic!("Only support struct datatype"),
    };
    let expanded = quote! {
        // The generated axiom for size.
        verus!{
        impl #impl_generics #trt for #name #ty_generics #where_clause  {}
        }
    };
    expanded
}

pub fn add_empty_trait_for_struct(
    input: proc_macro::TokenStream,
    name: &str,
) -> proc_macro::TokenStream {
    // Hand the output tokens back to the compiler.
    let input = parse_macro_input!(input as DeriveInput);
    let trait_name = proc_macro2::Ident::new(name, input.__span());
    proc_macro::TokenStream::from(add_empty_trait(&input, quote! {#trait_name}))
}

================
File: ./source/verismo_macro/src/default.rs
================

use quote::quote;
use syn_verus::{parse_macro_input, Data, DeriveInput, Type};

use crate::def::{add_bound_to_generic, field_name_ty, gen_trait_bound};

pub fn verismo_default_expand(input: proc_macro::TokenStream) -> proc_macro::TokenStream {
    //let strinput = input.to_string();
    //let inputcopy = TokenStream::from_str(&strinput).expect("unreacheable");
    //parse_macro_input!(inputcopy as DeriveInput)
    let input: DeriveInput = parse_macro_input!(input as DeriveInput);
    let name = &input.ident;

    let mut fields_stream = quote! {};
    let mut spec_fields_stream = quote! {};
    let generics = &input.generics;
    let (_impl_generics, _ty_generics, _where_clause) = generics.split_for_impl();
    let mut is_named = false;
    match &input.data {
        Data::Struct(s) => {
            for (i, field) in s.fields.iter().enumerate() {
                is_named = is_named | field.ident.is_some();
                let (fname, ftype) = field_name_ty(&field, i, name.span());
                let ty = &ftype;

                let ty_with_generic = crate::def::type_to_type_generic(ty);
                match ty {
                    Type::Path(tpath) => {
                        let path_segments = tpath
                            .path
                            .segments
                            .iter()
                            .map(|segment| segment.ident.to_string())
                            .collect::<Vec<_>>();
                        if path_segments.last() == Some(&"Ghost".to_string()) {
                            fields_stream = quote! {
                                #fields_stream
                                #fname: Ghost(arbitrary()),
                            };
                            spec_fields_stream = quote! {
                                #spec_fields_stream
                                #fname: (arbitrary()),
                            };
                        } else if path_segments.last() == Some(&"Tracked".to_string()) {
                            fields_stream = quote! {
                                #fields_stream
                                #fname: Tracked(arbitrary()),
                            };
                            spec_fields_stream = quote! {
                                #spec_fields_stream
                                #fname: (arbitrary()),
                            };
                        } else {
                            fields_stream = quote! {
                                #fields_stream
                                #fname: #ty_with_generic::default(),
                            };
                            spec_fields_stream = quote! {
                                #spec_fields_stream
                                #fname: #ty_with_generic::spec_default(),
                            };
                        }
                    }
                    _ => {
                        /*fields_stream = quote! {
                            #fields_stream
                            #fname: #ty_with_generic::default(),
                        };
                        spec_fields_stream = quote! {
                            #spec_fields_stream
                            #fname: #ty_with_generic::spec_default(),
                        };*/
                        panic! {"unsupported default"}
                    }
                }
            }
        }
        _ => {
            panic!("Only structs can be annotated with VClone");
        }
    };

    let newstruct = if is_named {
        quote! {
            #name{
                #fields_stream
            }
        }
    } else {
        quote! {
            #name(#fields_stream)
        }
    };

    let spec_newstruct = if is_named {
        quote! {
            #name{
                #spec_fields_stream
            }
        }
    } else {
        quote! {
            #name(#spec_fields_stream)
        }
    };
    let mut generic_for_default = generics.clone();
    add_bound_to_generic(
        &mut generic_for_default,
        gen_trait_bound(vec!["Default"], name.span()),
        name.span(),
    );
    add_bound_to_generic(
        &mut generic_for_default,
        gen_trait_bound(vec!["SpecDefault"], name.span()),
        name.span(),
    );
    let (impl_generics, ty_generics, where_clause) = generic_for_default.split_for_impl();
    let expand = quote! {
    verus!{
        impl #impl_generics Default for #name #ty_generics #where_clause  {
            #[verifier(external_body)]
            fn default() -> (ret: Self)
            ensures
                builtin::equal(ret, Self::spec_default()),
                ret.wf(),
            { #newstruct }
        }

        impl #impl_generics SpecDefault for #name #ty_generics #where_clause  {
            open spec fn spec_default() -> Self {
                #spec_newstruct
            }
        }
    }
    };

    //let strtoken = newcode.to_string() + &expand.to_string();
    //proc_macro::TokenStream::from_str(&strtoken).expect("wrong code")
    //proc_macro::TokenStream::from(expand)
    //println!("{}", expand);
    proc_macro::TokenStream::from(expand)
}

================
File: ./README.md
================

# VeriSMo: A formally verified security module for AMD confidential VMs.

This repo includes the code for VeriSMo project.

## Files 📁

- tools/ : includes verifier and compiler tools and scripts.
- deps/ : includes hacl package
- source/ : verismo code
- source/verismo : verified code for verismo
- source/verismo_main : main executable bin, which only defines a unverified Rust panic handler.
- source/verismo/src/arch : model
- source/verismo/src/entry.s : a small and unverified assembly code.
- source/target.json : target configuration


## 0. Pre-requirements
First, Install rust toolchain;
```
tools/install.sh
```

## 1. Install tools 🧰
Then, build verus, verus-rustc (replacing rustc) and igvm tools and dependencies.
```
tools/activate.sh
```

## 2. Verify and Build ✔️ 🛠️

Now, run verification checks and build the binary. 

🍵 This step takes several minutes.

### a. Verify the whole project:
Run

```
make verify -f Makefile.default
``` 

or  

```
cd source/verismo_main; cargo build --release;
```

### b. To verify a single module, which is useful for development:
```
cd source/verismo; VERUS_MODULE=security::monitor cargo build --features verifymodule --release;
```

### c. Understand results:

A fully verified result should have "verified": 2138, "errors": 0,


### d. Build without verification 🛠️

If no changes are made in `source/verismo`, we recommend to build without verification to speed up the build process.

```
make debugbuild -f Makefile.default
``` 

or  

```
cd source/verismo_main; cargo build --feature noverify --release;
```

## 3. Create VM image (skip if you run `make` or `make verify`)
1. Download linux submodule: `git submodule update --init richos/snplinux`
2. Build guest OS and drivers: `make fs -f Makefile.default`
1. Run `sh source/target/target/release/verismo/igvm.sh` to generate the verismo in IGVM format for Hyper-V: `source/target/target/release/verismo/verismo-rust.bin`
2. Run `make fs` to generate a vhdx file  as filesystem for the VM: `richos/test-fs/verismo.vhdx`

## 4. Deploy and run

### Requirements

- Hardware: A AMD SEV-SNP machine.
- OS: Windows with a Hypervisor released after 20230909. Earlier release may not support restricted interrupts in both VMPLs and thus will not work.
- Optional: A Debug machine with windows OS.

### Steps

Move following files to your AMD SEV-SNP machine.
- `source/target/target/release/verismo/verismo-rust.bin`
- `richos/test-fs/verismo.vhdx`
- `tools/vm/*`

#### 1. Create a SNP VM from powershell with admin permission.
```
build-vm.ps1 verismo verismo-rust.bin None verismo.vhdx
```

#### 2. Start the VM

```
start-vm verismo
```

#### 3. Connect to the guest.

GUI-access is not supported and you need to use ssh to login into the guest OS.

The verismo.vhdx includes init process that will open sshd service without password.

Wait for a minute before connecting.
```
ssh root@192.168.0.103
```

#### 4. Talk to VeriSMo module

Once the guest is booted, it has already talked to VeriSMo to wake up AP and update page permissions during its booting.

Inside the guest, we also provided a verismo driver and some tests, to talk to VeriSMo directly if you want.

- verismo.ko: driver
- decode_report: display the binary report to a readable format.
- test.sh: a testing script

```
cd verismo
insmod verismo.ko
sh test.sh
```

#### 5. Optional: You can access the boot log from VeriSMo via a remote debugger.

Refer to windows remote debug to enable both host and hypervisor debugging.
VeriSMo boot log is not accessible from guest OS.


## How build process works 🪄

### Replace rustc with verus-rustc

In source/.cargo/config.toml, we replaced rustc with verus-rustc.
verus-rustc will call `verus` to compile `vstd` (a verus library), `verismo` and `verismo-main` package, and call `rustc` to compile all other packages (hacl, core, etc.).

### Add build.rs to pass additional options to verus tool
See `source/verismo/build.rs`

### Features
- noverify: build source without verification
- verifymodule: if ${VERUS_MODULE} is not empty, only verify the specified module.

### Debug vs Release
Depends on cfg(debug_assertations)
Debug mode prints additional messages for debug purpose via leak_debug;
Release mode erase all leak_debug or debug informations;


## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.

================
File: ./SUPPORT.md
================

# Support

## How to file issues and get help  

This project uses GitHub Issues to track bugs and feature requests. Please search the existing 
issues before filing new issues to avoid duplicates.  For new issues, file your bug or 
feature request as a new Issue.

For help and questions about using this project, please submit issue or repo owner contact us via email. VeriSMo is a research prototype and so the support is limited.

## Microsoft Support Policy  

Support for this VeriSMo project is limited to the resources listed above.

================
File: ./Makefile.default
================

IGVMGEN ?= tools/igvm/igvm/igvmgen.py
RELEASE ?= release
DEBUG ?= debug
IMAGE ?= verismo.bin
TARGET_DIR = ${CURDIR}/source/target/target/
CMD ?= root=/dev/sda rw debugpat
LINUX_OUT= ${CURDIR}/richos/target
LINUX ?= ${CURDIR}/richos/target/arch/x86/boot/bzImage
LINUX_HEADER_DIR=$(realpath $(LINUX_OUT))/mod
LINUX_CONFIG = richos/kernel/config-richos

default: verifyonly

debugbuild: buildonly fs
	sh ${TARGET_DIR}/${DEBUG}/igvm.sh

verify: verifyonly fs
	sh ${TARGET_DIR}/${RELEASE}/igvm.sh

verifyonly:
	cd source/verismo_main &&\
	(cargo build --release 1> verus-stderr.log)


${CURDIR}/source/target/target/release/verismo_main:
	verifyonly

${CURDIR}/source/target/target/${DEBUG}/verismo_main: buildonly

buildonly:
	cd source/verismo_main && cargo build --features noverify 


$(LINUX_OUT):
	mkdir -p $(LINUX_OUT)

$(LINUX_OUT)/.config: $(LINUX_OUT)
	mkdir -p $(LINUX_OUT)
	cp ${LINUX_CONFIG} $(LINUX_OUT)/.config

$(LINUX): richos/snplinux/ $(LINUX_OUT)/.config
	git submodule update --init richos/snplinux || true
	cd richos/snplinux/ && make O=$(LINUX_OUT) CC=gcc-9 -j && make O=$(LINUX_OUT) INSTALL_MOD_PATH=$(LINUX_HEADER_DIR) modules_install

kernel: $(LINUX)

driver: kernel
	cd richos/module && make

fs: driver
	make -C richos/fs test-fs/verismo.vhdx

upload: $(IMAGE)
	sh scripts/upload.sh $(IMAGE)

clean:
	cd source && cargo clean


================
File: ./richos/Makefile
================

all:
	make -C fs test-fs/benchmark.vhd
	make -C fs test-fs/verismo.vhd
	make -C module TARGET=5.11.0-rc5+
	make -C user DIR=$(PWD)/user

================
File: ./richos/module/Makefile
================

obj-m += verismo.o
TARGET?=5.11*
EXTRA_CFLAGS += -fno-pie
ccflags-y += -DDEBUG=1 -fno-pie

LINUX_HEADER_DIR ?= $(CURDIR)/../target/mod
KDIR := $(LINUX_HEADER_DIR)/lib/modules/*/build

all:
	make -C $(KDIR) M=$(PWD) modules 
	mkdir -p ../fs/test-fs/verismo/files/verismo/ && cp verismo.ko ../fs/test-fs/verismo/files/verismo/

clean:
	make -C $(KDIR) M=$(PWD) clean

format:
	clang-format-12 -style file -i *.c

format-check:
	clang-format-12 -style file -n -Werror *.c

.PHONY: clean

================
File: ./richos/module/verismo.mod.c
================

#include <linux/module.h>
#define INCLUDE_VERMAGIC
#include <linux/build-salt.h>
#include <linux/vermagic.h>
#include <linux/compiler.h>

BUILD_SALT;

MODULE_INFO(vermagic, VERMAGIC_STRING);
MODULE_INFO(name, KBUILD_MODNAME);

__visible struct module __this_module
__section(".gnu.linkonce.this_module") = {
	.name = KBUILD_MODNAME,
	.init = init_module,
#ifdef CONFIG_MODULE_UNLOAD
	.exit = cleanup_module,
#endif
	.arch = MODULE_ARCH_INIT,
};

#ifdef CONFIG_RETPOLINE
MODULE_INFO(retpoline, "Y");
#endif

MODULE_INFO(depends, "");


================
File: ./richos/module/verismo.c
================

#include <linux/module.h> // Required for all kernel modules
#include <linux/kernel.h> // Required for kernel logging functions
#include <linux/proc_fs.h> // Required for /proc file system functions
#include <linux/slab.h> // Required for kernel logging functions

#include <asm/msr.h> // msr
#include <asm/io.h> // addr translation
#include <linux/seq_file.h>
#include <linux/kprobes.h>
#include <linux/kallsyms.h>

MODULE_LICENSE("GPL");
u64 kernel_start_addr;
u64 kernel_end_addr;
u64 start_addr;
u64 end_addr;

union snp_vmpl_request {
	struct {
		u64 values[3];
	};
	struct {
		u64 npages : 12;
		u64 gpn : 52;
		u64 op : 32;
		u64 cpu : 32;
		u64 other;
	};
};

enum VMPLCode {
	WakupAp = 0xfffffffe,
	SetPrivate = 0x1,
	SetShared = 0x2,
	Register = 0x3,
	ExtendPcr = 0x4,
	LockKernExe = 0x5,
	Attest = 0x6,
	Secret = 0x7,
	Encrypt = 0x8,
	Decrypt = 0x9,
};

struct LockReq {
	u64 start;
	u64 end;
};

#define VERISMO_ATTEST 0xfffffffdUL
#define VERISMO_OK 0x0UL
#define HVCALL_VTL_CALL (0x0011UL << 52UL)
#define GHCB_INFO_SPECIAL_HYPERCALL 0xf00UL

static u64 snp_send_vmpl_via_ghcb_msr(union snp_vmpl_request *req)
{
	u32 *low, *high;
	u64 msr_val;
	u64 oldghcb;
	msr_val = GHCB_INFO_SPECIAL_HYPERCALL | HVCALL_VTL_CALL;
	low = (u32 *)&msr_val;
	high = low + 1;
	rdmsrl_safe(MSR_AMD64_SEV_ES_GHCB, &oldghcb);
	wrmsrl(MSR_AMD64_SEV_ES_GHCB, msr_val);
	asm volatile("rep; vmmcall\n\r"
		     : "=b"(req->values[0]), "=c"(req->values[1]),
		       "=d"(req->values[2])
		     : "a"(0xfeeddead), "b"(req->values[0]),
		       "c"(req->values[1]), "d"(req->values[2]));
	wrmsrl(MSR_AMD64_SEV_ES_GHCB, oldghcb);
	return 0;
}

static int msg_len = 0;

DEFINE_PER_CPU(void *, cpu_msg);

#define msg this_cpu_read(cpu_msg)

static u8 prev_msg[0x1000];

static void *test_pages = 0;
static int buffer_len = 0x1000;

static unsigned long (*kallsyms_lookup_name_fn)(const char *name);

static inline unsigned long kallsyms_get_sym(const char *name)
{
	unsigned long addr = kallsyms_lookup_name_fn(name);
	if (addr == 0) {
		pr_err("Failed to find address for %s!!", name);
	}
	pr_debug("Export %s at %lx\n", name, addr);
	return addr;
}

void verismo_request(union snp_vmpl_request *req, void *this_msg)
{
	struct LockReq *lock_req = NULL;
	unsigned long long start = 0, end = 0;
    	bool use_out_msg = false;
	req->values[2] = 0;
	switch (req->op) {
	case Attest: {
        use_out_msg = true;
		break;
	}
	case ExtendPcr: {
		break;
	}
	case Register: {
		req->values[0] = virt_to_phys(this_msg);
		break;
	}
	case LockKernExe: {
		req->values[0] = (u64)virt_to_phys(this_msg);
		lock_req = (struct LockReq *)this_msg;
		lock_req->start = (u64)virt_to_phys((void *)kallsyms_get_sym("startup_64")) >> 12;
			//(u64)virt_to_phys((void *)0xffffffff90000000) >> 12;
		lock_req->end =
			(u64)virt_to_phys((void *)kallsyms_get_sym("_etext") + 0xfff) >> 12;
			//(u64)(virt_to_phys((void *)0xffffffff9080092a)
		lock_req += 1;
		lock_req->start =
			(u64)virt_to_phys((void *)kallsyms_get_sym("early_idt_handler_array")) >> 12;
		lock_req->end = 
			(u64)virt_to_phys((void *)kallsyms_get_sym("_einittext") + 0xfff) >> 12;
		//(u64)(virt_to_phys((void *)0xffffffff90d2f7a7) + 0xfff) >> 12;
		lock_req += 1;
		lock_req->start = (u64)(start_addr) >> 12;
		lock_req->end = (u64)(end_addr + 0xfff) >> 12;
		break;
	}
	default: {
		break;
	}
	}
	if ((req->op == SetPrivate) || (req->op == SetShared)) {
		printk("count = %lld\n", req->values[0]);
		req->values[0] = (u64)virt_to_phys(test_pages) +
				 ((req->values[0] % 16) << 12);
	} else if (req->op != WakupAp) {
		req->values[0] = (u64)virt_to_phys(this_msg);
	}

	printk("req %x %llx %llx %llx\n", req->op, req->values[0],
	       req->values[1], req->values[2]);
	start = rdtsc();
	if (req->op != WakupAp) {
		local_irq_disable();
		snp_send_vmpl_via_ghcb_msr(req);
		local_irq_enable();
	}
	end = rdtsc();
	printk("req->op = %x, time = %lld\n", req->op, end - start);
	memcpy(prev_msg, this_msg, 0x1000);
    	if (use_out_msg)
	    print_hex_dump(KERN_INFO, "resp : ", DUMP_PREFIX_NONE, 128, 1, this_msg,
		       128, false);
}

// Function to be called when write to /proc/verismo
ssize_t verismo_write(struct file *f, const char __user *buf, size_t len,
		      loff_t *pos)
{
	char input_buffer[512];
	union snp_vmpl_request req;

	if (copy_from_user(input_buffer, buf, len)) {
		return -EFAULT;
	}
	msg_len = len;
	memset(&req, 0, sizeof(req));
	printk("%s", input_buffer);
	preempt_disable();
	local_irq_disable();
	memset(msg, 0, 4096);
	sscanf(input_buffer, "%lld %lld %s", &req.values[1], &req.values[0],
	       (char *)msg);
	verismo_request(&req, msg);
	local_irq_enable();
	preempt_enable();
	*pos = len;
	return len;
}

static int verismo_seq_show(struct seq_file *seq, void *offset)
{
	seq_write(seq, prev_msg, buffer_len);
	return 0;
}

static int verismo_open(struct inode *inode, struct file *file)
{
	return single_open(file, verismo_seq_show, NULL);
}

// Structure for /proc/hello_world file
struct proc_dir_entry *verismo_proc_file;

// Structure for /proc/hello_world file
static const struct proc_ops verismo_fops = {
	.proc_write = verismo_write,
	.proc_read = seq_read,
	.proc_lseek = seq_lseek,
	.proc_release = single_release,
	.proc_open = verismo_open,
};

static const struct proc_ops verismo_count_fops = {
	.proc_write = verismo_write,
	.proc_read = seq_read,
	.proc_lseek = seq_lseek,
	.proc_release = single_release,
	.proc_open = verismo_open,
};

static void verismo_init_percpu(void *unused)
{
	this_cpu_write(cpu_msg, (void *)get_zeroed_page(GFP_KERNEL));
	union snp_vmpl_request req;
	memset((void *)&req, 0, sizeof(req));
	req.op = Register;
	verismo_request(&req, msg);
}

static void verismo_free_percpu(void *unused)
{
	if (msg) {
		kfree(msg);
		this_cpu_write(cpu_msg, 0);
	}
}

static unsigned long kprobe_lookup(const char *name)
{
	unsigned long addr;
	int err;
	struct kprobe kp = { .symbol_name = name };
	err = register_kprobe(&kp);
	if (err < 0) {
		pr_err("failed to find %s: error(%d)\n", name, err);
		return 0;
	}
	addr = (unsigned long)kp.addr;
	unregister_kprobe(&kp);
	return addr;
}

// Function to be called when the module is loaded
static int __init init_verismo(void)
{
	// Create the /proc/hello_world file
	const struct module *mod = THIS_MODULE;

	kallsyms_lookup_name_fn = (unsigned long (*)(
		const char *name))kprobe_lookup("kallsyms_lookup_name");
	if (!kallsyms_lookup_name_fn)
		return -EINVAL;
	// Get the base address of the module
	start_addr = (u64)virt_to_phys(mod->core_layout.base);
	end_addr = (u64)virt_to_phys(mod->core_layout.base +
				     mod->core_layout.text_size);
	printk("kernel  [%llx: %llx]\n", kernel_start_addr, kernel_end_addr);
	printk("module  [%llx: %llx]\n", start_addr, end_addr);
	verismo_proc_file = proc_create("verismo", 0444, NULL, &verismo_fops);

	test_pages = (void *)__get_free_pages(GFP_KERNEL, 4);
	on_each_cpu(verismo_init_percpu, NULL, true);
	printk("an extra page = %llx", (u64)virt_to_phys(test_pages));
	if (!verismo_proc_file) {
		printk(KERN_ALERT
		       "Error: Could not initialize /proc/verismo\n");
		return -ENOMEM;
	}

	printk(KERN_INFO
	       "verismo[flush]! Usage: req.values[1] req.values[0](count for page) msg\n");
	return 0;
}

// Function to be called when the module is unloaded
static void __exit remove_verismo(void)
{
	// Remove the /proc/hello_world file
	on_each_cpu(verismo_free_percpu, NULL, true);

	if (test_pages) {
		kfree(test_pages);
		test_pages = NULL;
	}

	proc_remove(verismo_proc_file);

	printk(KERN_INFO "Goodbye, verismo module unloaded!\n");
}

module_init(init_verismo);
module_exit(remove_verismo);
================
File: ./richos/module/test.sh
================

echo "Extend PCR"
pcrdata=$(xxd -l 32 -c 32 -p < /dev/random)
echo "4 0 ${pcrdata}" > /proc/verismo
dmesg |tail -n 11|grep "req"
sleep 2
echo "Attest PCR"
bytes32=$(xxd -l 32 -c 32 -p < /dev/random)
echo "6 0 ${bytes32}" > /proc/verismo
dmesg |tail -n 11|grep "req"
cat /proc/verismo > report
/verismo/decode_report ./report
sleep 2
echo "Make page shared"
echo "2 1 test" > /proc/verismo
dmesg |tail -n 11|grep "req"
echo "Make page private"
echo "1 1 test" > /proc/verismo
dmesg |tail -n 11|grep "req"

echo "Lock kernel codes"
echo "5 0 0" > /proc/verismo
dmesg |tail -n 11|grep "req"

================
File: ./richos/user/report.c
================

#include "attestation.h"
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>
#include "report.h"
static void print_byte_array(const char *label, const uint8_t *array, size_t size)
{
#define INDENT	"    "
#define MAX_LINE 80
#define CHARS_PER_BYTE 2

	size_t wrap = size;
	size_t i;
	if (size > MAX_LINE/CHARS_PER_BYTE - sizeof(INDENT))
		wrap = size/2;

	if (label)
		printf("%s:\n", label);

	printf(INDENT);

	if (!array) {
		printf("(null)\n");
		return;
	}

	for ( i = 0; i < size; i++) {
		if (i > 0 && (i % wrap) == 0) {
			printf("\n");
			printf(INDENT);
		}
		printf("%02x", array[i]);
	}

	printf("\n");
}

static void print_tcb_version(const char *label, const union tcb_version *tcb)
{
	if (tcb) {
		printf("%s: %02u%02u%02u%02u%02u%02u%02u%02u\n", label,
		       (unsigned) tcb->boot_loader,
		       (unsigned) tcb->tee,
		       (unsigned) tcb->reserved[0],
		       (unsigned) tcb->reserved[1],
		       (unsigned) tcb->reserved[2],
		       (unsigned) tcb->reserved[3],
		       (unsigned) tcb->snp,
		       (unsigned) tcb->microcode);
		printf(" - Boot Loader SVN:  %2u\n", tcb->boot_loader);
		printf(" - TEE SVN:          %2u\n", tcb->tee);
		printf(" - SNP firmware SVN: %2u\n", tcb->snp);
		printf(" - Microcode SVN:    %2u\n", tcb->microcode);
	}
}

/*
 * Print the report version.
 */
void print_version(struct attestation_report *report)
{
	if (report) {
		printf("Version: %u\n", report->version);
	}
}

/*
 * Print the guest SVN.
 */
void print_guest_svn(struct attestation_report *report)
{
	if (report) {
		printf("Guest SVN: %u\n", report->guest_svn);
	}
}

/*
 * Print the guest policy.
 */
void print_policy(struct attestation_report *report)
{
	if (report) {
		printf("Policy: %#0lx\n", report->policy);
		printf(" - Debugging Allowed:       %s\n", report->policy & POLICY_DEBUG_MASK ? "Yes" : "No");
		printf(" - Migration Agent Allowed: %s\n", report->policy & POLICY_MIGRATE_MA_MASK ? "Yes" : "No");
		printf(" - SMT Allowed:             %s\n", report->policy & POLICY_SMT_MASK ? "Yes" : "No");
		printf(" - Min. ABI Major:          %#lx\n",
		       (report->policy & POLICY_ABI_MAJOR_MASK) >> POLICY_ABI_MAJOR_SHIFT);
		printf(" - Min. ABI Minor:          %#lx\n",
		       (report->policy & POLICY_ABI_MINOR_MASK) >> POLICY_ABI_MINOR_SHIFT);
	}
}

/*
 * Print the family ID (in hex) supplied by the guest.
 */
void print_family_id(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Family ID", report->family_id, sizeof(report->family_id));
	}
}

/*
 * Print the image ID (in hex) supplied by the guest.
 */
void print_image_id(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Image ID", report->image_id, sizeof(report->image_id));
	}
}

/*
 * Print the guest VMPL.
 */
void print_vmpl(struct attestation_report *report)
{
	if (report) {
		printf("VMPL: %u\n", report->vmpl);
	}
}

/*
 * Print the signature algorithm encoding.
 */
void print_signature_algo(struct attestation_report *report)
{
	if (report) {
		printf("Signature Algorithm: %u (%s)\n", report->signature_algo,
		       report->signature_algo == SIG_ALGO_ECDSA_P384_SHA384 ? "ECDSA P-384 with SHA-384"
									    : "Invalid");
	}
}

/*
 * Print the platform version.
 */
void print_platform_version(struct attestation_report *report)
{
	if (report) {
		print_tcb_version("Platform Version", &report->platform_version);
	}
}

/*
 * Print the platform info.
 */
void print_platform_info(struct attestation_report *report)
{
	if (report) {
		printf("Platform Info: %#0lx\n", report->platform_info);
		printf(" - SMT Enabled: %s\n", report->platform_info & PLATFORM_INFO_SMT_EN_MASK ? "Yes" : "No");
	}
}

/*
 * Print the Author key enable bit.
 */
void print_author_key_en(struct attestation_report *report)
{
	if (report) {
		printf("Author Key Enabled: %s\n", report->platform_info & AUTHOR_KEY_EN_MASK ? "Yes" : "No");
	}
}

/*
 * Print the report data (in hex) supplied by the guest.
 */
void print_report_data(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Report Data", report->report_data, sizeof(report->report_data));
	}
}

/*
 * Print the launch measurement (in hex).
 */
void print_measurement(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Measurement", report->measurement, sizeof(report->measurement));
	}
}

/*
 * Print the host data (in hex).
 */
void print_host_data(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Host Data", report->host_data, sizeof(report->host_data));
	}
}

/*
 * Print the digest of the ID key (in hex).
 */
void print_id_key_digest(struct attestation_report *report)
{
	if (report) {
		print_byte_array("ID Key Digest", report->id_key_digest, sizeof(report->id_key_digest));
	}
}

/*
 * Print the digest of the Author key (in hex).
 */
void print_author_key_digest(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Author Key Digest", report->author_key_digest, sizeof(report->author_key_digest));
	}
}

/*
 * Print the report ID (in hex).
 */
void print_report_id(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Report ID", report->report_id, sizeof(report->report_id));
	}
}

/*
 * Print the report ID (in hex) of the migration agent.
 */
void print_migration_agent_report_id(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Migration Agent Report ID", report->report_id_ma, sizeof(report->report_id_ma));
	}
}

/*
 * Print the reported TCB version needed to retreive the VCEK
 * from the AMD KDS.
 */
void print_reported_tcb(struct attestation_report *report)
{
	if (report) {
		print_tcb_version("Reported TCB", &report->platform_version);
	}
}

/*
 * Print the chip ID (in hex).
 */
void print_chip_id(struct attestation_report *report)
{
	if (report) {
		print_byte_array("Chip ID", report->chip_id, sizeof(report->chip_id));
	}
}

/*
 * Print the signature (in hex).
 */
void print_signature(struct attestation_report *report)
{
	if (report) {
		printf("Signature:\n");
		print_byte_array("  R", report->signature.r, sizeof(report->signature.r));
		print_byte_array("  S", report->signature.s, sizeof(report->signature.s));
	}
}

/*
 * Print all fields of the guest report.
 */
void print_report(void *data)
{
        struct attestation_report *report = &((struct msg_report_resp*)data)->report;
	print_version(report);
	print_guest_svn(report);
	print_policy(report);
	print_family_id(report);
	print_image_id(report);
	print_vmpl(report);
	print_signature_algo(report);
	print_platform_version(report);
	print_platform_info(report);
	print_author_key_en(report);
	print_report_data(report);
	print_measurement(report);
	print_host_data(report);
	print_id_key_digest(report);
	print_author_key_digest(report);
	print_report_id(report);
	print_migration_agent_report_id(report);
	print_reported_tcb(report);
	print_chip_id(report);
	print_signature(report);
}
================
File: ./richos/user/Makefile
================

DOCKER_IMAGE?=test-fs/verismo
out=decode_report
DIR?=${PWD}
all:
	docker run -it -v ${DIR}:/usr/verismo/:rw -w /usr/verismo/ ${DOCKER_IMAGE}  make local
	cp ${out} ../fs/test-fs/verismo/files/verismo/
local:
	gcc user_report.c report.c -o ${out}

================
File: ./richos/user/user_report.c
================

#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>
#include "report.h"

int main(int arg, char ** argv){
        if(arg != 2){
		printf("Provide a input attestation file\n");
		return -1;
	}
	struct msg_report_resp report;
	char * filename = argv[1];
	FILE* fp = fopen(filename, "rb");
	fread(&report, sizeof(report), 1, fp);
	print_report(&report);
	fclose(fp);
	return 0;
}
================
File: ./richos/user/report.h
================

/* Copyright (C) 2021 Advanced Micro Devices, Inc. */

#ifndef REPORT_H
#define REPORT_H


#include "attestation.h"

/*
 * Print the report version.
 */
void print_version(struct attestation_report *report);

/*
 * Print the guest SVN.
 */
void print_guest_svn(struct attestation_report *report);

/*
 * Print the guest policy.
 */
void print_policy(struct attestation_report *report);

/*
 * Print the family ID (in hex) supplied by the guest.
 */
void print_family_id(struct attestation_report *report);

/*
 * Print the image ID (in hex) supplied by the guest.
 */
void print_image_id(struct attestation_report *report);

/*
 * Print the guest VMPL.
 */
void print_vmpl(struct attestation_report *report);

/*
 * Print the signature algorithm encoding.
 */
void print_signature_algo(struct attestation_report *report);

/*
 * Print the platform version.
 */
void print_platform_version(struct attestation_report *report);

/*
 * Print the platform info.
 */
void print_platform_info(struct attestation_report *report);

/*
 * Print the Author key enable bit.
 */
void print_author_key_en(struct attestation_report *report);

/*
 * Print the report data (in hex) supplied by the guest.
 */
void print_report_data(struct attestation_report *report);

/*
 * Print the launch measurement (in hex).
 */
void print_measurement(struct attestation_report *report);

/*
 * Print the host data (in hex).
 */
void print_host_data(struct attestation_report *report);

/*
 * Print the digest of the ID key (in hex).
 */
void print_id_key_digest(struct attestation_report *report);

/*
 * Print the digest of the Author key (in hex).
 */
void print_author_key_digest(struct attestation_report *report);

/*
 * Print the report ID (in hex).
 */
void print_report_id(struct attestation_report *report);

/*
 * Print the report ID (in hex) of the migration agent.
 */
void print_migration_agent_report_id(struct attestation_report *report);

/*
 * Print the reported TCB version needed to retreive the VCEK
 * from the AMD KDS.
 */
void print_reported_tcb(struct attestation_report *report);

/*
 * Print the chip ID (in hex).
 */
void print_chip_id(struct attestation_report *report);

/*
 * Print the signature (in hex).
 */
void print_signature(struct attestation_report *report);

/*
 * Print all fields of the guest report.
 */
void print_report(void *report);

#endif	/* REPORT_H */
================
File: ./richos/user/attestation.h
================

#ifndef ATTESTATION_H
#define ATTESTATION_H

#define POLICY_DEBUG_SHIFT	19
#define POLICY_MIGRATE_MA_SHIFT	18
#define POLICY_SMT_SHIFT	16
#define POLICY_ABI_MAJOR_SHIFT	8
#define POLICY_ABI_MINOR_SHIFT	0

#define POLICY_DEBUG_MASK	(1UL << (POLICY_DEBUG_SHIFT))
#define POLICY_MIGRATE_MA_MASK	(1UL << (POLICY_MIGRATE_MA_SHIFT))
#define POLICY_SMT_MASK		(1UL << (POLICY_SMT_SHIFT))
#define POLICY_ABI_MAJOR_MASK	(0xFFUL << (POLICY_ABI_MAJOR_SHIFT))
#define POLICY_ABI_MINOR_MASK	(0xFFUL << (POLICY_ABI_MINOR_SHIFT))

#define SIG_ALGO_ECDSA_P384_SHA384	0x102

#define PLATFORM_INFO_SMT_EN_SHIFT	0
#define PLATFORM_INFO_SMT_EN_MASK	(1UL << (PLATFORM_INFO_SMT_EN_SHIFT))

#define AUTHOR_KEY_EN_SHIFT	0
#define AUTHOR_KEY_EN_MASK	(1UL << (AUTHOR_KEY_EN_SHIFT))
#include <stdint.h>
union tcb_version {
	struct {
		uint8_t boot_loader;
		uint8_t tee;
		uint8_t reserved[4];
		uint8_t snp;
		uint8_t microcode;
	};
	uint64_t raw;
};

struct signature {
	uint8_t r[72];
	uint8_t s[72];
	uint8_t reserved[512-144];
};

struct attestation_report {
	uint32_t          version;			/* 0x000 */
	uint32_t          guest_svn;			/* 0x004 */
	uint64_t          policy;			/* 0x008 */
	uint8_t           family_id[16];		/* 0x010 */
	uint8_t           image_id[16];			/* 0x020 */
	uint32_t          vmpl;				/* 0x030 */
	uint32_t          signature_algo;		/* 0x034 */
	union tcb_version platform_version;		/* 0x038 */
	uint64_t          platform_info;		/* 0x040 */
	uint32_t          flags;			/* 0x048 */
	uint32_t          reserved0;			/* 0x04C */
	uint8_t           report_data[64];		/* 0x050 */
	uint8_t           measurement[48];		/* 0x090 */
	uint8_t           host_data[32];		/* 0x0C0 */
	uint8_t           id_key_digest[48];		/* 0x0E0 */
	uint8_t           author_key_digest[48];	/* 0x110 */
	uint8_t           report_id[32];		/* 0x140 */
	uint8_t           report_id_ma[32];		/* 0x160 */
	union tcb_version reported_tcb;			/* 0x180 */
	uint8_t           reserved1[24];		/* 0x188 */
	uint8_t           chip_id[64];			/* 0x1A0 */
	uint8_t           reserved2[192];		/* 0x1E0 */
	struct signature  signature;			/* 0x2A0 */
};

struct msg_report_resp {
	uint32_t status;
	uint32_t report_size;
	uint8_t  reserved[0x20-0x8];
	struct attestation_report report;
};

#endif	/* ATTESTATION_H */
================
File: ./richos/fs/Makefile
================

FILES-verismo:=
FILES-verismo+=$(CURDIR)/../module/verismo.ko
FILES-verismo+=$(CURDIR)/../module/test.sh
FILES-verismo+=$(CURDIR)/../user

test-fs/%.vhdx: test-fs/% 
	for f in $(FILES-verismo) ; do echo $$f && mkdir -p $</files/verismo/ && cp -r $$f $</files/verismo/; done;
	sh gen.sh $<

test-fs/verismo.vhdx: $(CURDIR)/../module/verismo.ko $(CURDIR)/../user
================
File: ./richos/fs/gen.sh
================

#!/bin/bash

# Directory containing Dockerfile
dir=$1
name=${dir}
# set to docker, podmand or buildah
container_cli=podman

container_export() {
    # Build the image and export the root filesystem
    echo "using ${container_cli}"
    ${container_cli} build -t ${name} ${dir}
    # Create but do not run the container
    container=$(${container_cli} create ${name})
    # export rootfs from ${container_cli}
    echo "created ${container}"
    rm -f ${name}.tar
    ${container_cli} export ${container} -o ${name}.tar
    ${container_cli} rm ${container}
}

resolv_config() {
    # Generate resolv.conf (if needed)
    # resolv.conf is overwrittern after creating a container.
    if ${use_container}; then
    echo "Write resolv.conf"
    echo "search redmond.corp.microsoft.com
    nameserver 10.50.10.50
    nameserver 10.50.50.50
    nameserver 8.8.8.8
    nameserver 10.0.80.11
    nameserver 10.0.80.12
    "> mnt/etc/resolv.conf
    fi
}

# Function to export root filesystem using Buildah
buildah_export() {
    echo "buildah_export"
    buildah bud -t "${name}" "${dir}"
    buildah export "${name}" > ${name}.tar
}

mount_image(){
    # Create root filesystem image
    mkdir -p ./mnt
    sudo umount mnt >> /dev/null
    rm -f ${name}.img
    dd if=/dev/zero of=${name}.img bs=1M count=256
    mkfs.ext4 -F -L linuxroot ${name}.img > /dev/null
    # Mount the image and extract the root filesystem
    sudo mount ${name}.img ./mnt/
    sudo tar -xf $name.tar -C mnt
}


# To Hyper-V disk type
to_vhdx() {
    # Create cpio archive if using inram fs
    # find mnt | cpio -H newc -o > "${name}.cpio"
    sleep 5; sudo umount mnt
    qemu-img convert -O vhdx ${name}.img ${name}.vhdx
    rm -rf mnt ${name}.tar ${name}.img

}

container_export
mount_image
#resolv_config ${use_container_cli}
to_vhdx

================
File: ./richos/fs/test-fs/benchmark/Dockerfile
================

FROM alpine
RUN apk add --update --no-cache openrc procps iproute2 openssh bash 
#openntpd openntpd-openrc
RUN ssh-keygen -A
RUN echo "PermitRootLogin yes" >> /etc/ssh/sshd_config
RUN echo "PermitEmptyPasswords yes" >> /etc/ssh/sshd_config
RUN echo "PasswordAuthentication yes" >> /etc/ssh/sshd_config
RUN passwd -d root

COPY ./files/etc/network/interfaces /etc/network/
COPY ./files/etc/resolv.conf /etc/
# This original script will need tty devices and so remove it.
COPY ./files/etc/inittab /etc/inittab
# Add a init script to show IP address after networking
COPY ./files/etc/init.d/showip /etc/init.d/showip
RUN chmod +x /etc/init.d/showip
RUN sed -i -e "s/ -docker//g" /etc/init.d/* 
RUN sed -i -e "s/ -lxc//g" /etc/init.d/* 
RUN echo "verismo" > /etc/hostname


# Generate Host key
RUN ssh-keygen -A

# Mount special file systems on boot and enable local services:
RUN rc-update add sysfs boot 
RUN rc-update add procfs boot 
RUN rc-update add devfs boot
#RUN rc-update add cgroups boot

# Config networking and run servers.
#RUN rc-update add fsck default
#RUN rc-update add root default
RUN rc-update add localmount default
RUN rc-update add hostname default
# Config datetime
RUN rc-update add openntpd default
RUN rc-update add showip default
RUN rc-update add networking default
RUN rc-update add sshd default

#RUN rc-update add local default
# remove unused files
RUN rm -rf /var/cache/apk/*
RUN rm -rf /usr/share/man/* && rm -rf /usr/share/doc/*

================
File: ./richos/fs/test-fs/benchmark/files/usr/sbin/init.sh
================

mkdir -p /dev/pts
mount devpts /dev/pts -t devpts
mkdir -p /securityfs
mount -t securityfs securityfs /securityfs
mount -t sysfs sysfs /sys
#/usr/bin/tpm2_pcrread sha1:10
#echo "Computed measurement using runtime measure traces:"
#/root/ima-tests/ima_measure /securityfs/ima/binary_runtime_measurements
#echo "VMPL0-vTPM provided PCR value"
#/usr/bin/tpm2_pcrread sha1:10
#mknod /dev/kvm c 10 232
#chmod a+rw /dev/kvm
/sbin/ip link set dev eth0 up
ifup eth1
echo "config addr"
/sbin/ip address add 192.168.0.106/24 dev eth0
echo "config route"
/sbin/ip route add default via 192.168.0.1 dev eth0
echo "set up lo"
/sbin/ip link set dev lo up
echo "config addr for lo"
/sbin/ip address add 127.0.0.1/8 dev lo
echo "sshd started"
/usr/bin/ssh-keygen -A
/usr/sbin/sshd
echo "show addr"
ip addr
echo "show route"
ip route show
hostname vmpl2
#/bin/sh /root/tpm_quote.sh

================
File: ./richos/fs/test-fs/benchmark/files/etc/network/interfaces
================

auto lo
iface lo inet loopback

auto eth0
iface eth0 inet static
	address 192.168.0.103
	netmask 255.255.255.0
	network 192.168.0.0
	gateway 192.168.0.1

auto eth1
iface eth1 inet dhcp
================
File: ./richos/fs/test-fs/benchmark/files/etc/resolv.conf
================

search redmond.corp.microsoft.com
nameserver 10.50.10.50
nameserver 10.50.50.50
nameserver 8.8.8.8
nameserver 10.0.80.11
nameserver 10.0.80.12
================
File: ./richos/fs/test-fs/benchmark/files/etc/inittab
================

# /etc/inittab

::sysinit:/sbin/openrc sysinit
::sysinit:/sbin/openrc boot
::wait:/sbin/openrc default

# Set up a couple of getty's
#tty1::respawn:/sbin/getty 38400 tty1
#tty2::respawn:/sbin/getty 38400 tty2
#tty3::respawn:/sbin/getty 38400 tty3
#tty4::respawn:/sbin/getty 38400 tty4
#tty5::respawn:/sbin/getty 38400 tty5
#tty6::respawn:/sbin/getty 38400 tty6

# Put a getty on the serial port
#ttyS0::respawn:/sbin/getty -L ttyS0 115200 vt100

# Stuff to do for the 3-finger salute
::ctrlaltdel:/sbin/reboot

# Stuff to do before rebooting
::shutdown:/sbin/openrc shutdown
================
File: ./richos/fs/test-fs/verismo/Dockerfile
================

FROM alpine as base

RUN apk add --update --no-cache openrc netcat-openbsd procps iproute2 strace openssh bash openntpd

FROM base as verismo-dev
COPY ./files/verismo /verismo
COPY ./files/verismo/user /verismo/user
COPY ./files/verismo/user/* /verismo/user/
RUN apk add build-base automake
RUN cd /verismo/user; make local

FROM base as verismo
RUN ssh-keygen -A
RUN echo "PermitRootLogin yes" >> /etc/ssh/sshd_config
RUN echo "PermitEmptyPasswords yes" >> /etc/ssh/sshd_config
RUN echo "PasswordAuthentication yes" >> /etc/ssh/sshd_config
RUN passwd -d root
RUN rc-update add sysfs default
RUN rc-update add cgroups default
RUN rc-update add procfs default
RUN rc-update add devfs default
RUN rc-update add networking default
RUN rc-update add sshd default
RUN rc-update add openntpd default
COPY ./files/* /
COPY ./files/etc/* /etc/
COPY ./files/etc/network/* /etc/network/
COPY ./files/etc/init.d/* /etc/init.d/
COPY ./files/verismo/verismo.ko /verismo/verismo.ko
COPY ./files/verismo/test.sh /verismo/test.sh
COPY --from=verismo-dev  /verismo/user/decode_report /verismo/
RUN chmod +x /etc/init.d/verismo && \
    rc-update add verismo default




================
File: ./richos/fs/test-fs/verismo/files/etc/init.d/verismo
================

#!/sbin/openrc-run

description="load verismo"

start() {
    insmod /verismo.ko
}

# stop function will be executed on shutdown
stop() {
    echo "exit"
}

case "$1" in
  start)
    start
  ;;
  stop)
    stop
  ;;
  *)
    echo "invalid argument"
    exit 1
esac

================
File: ./richos/fs/test-fs/verismo/files/etc/network/interfaces
================

auto eth0
iface eth0 inet static
	address 192.168.0.103
	netmask 255.255.255.0
	gateway 192.168.0.1

================
File: ./richos/fs/test-fs/verismo/files/etc/resolv.conf
================

search redmond.corp.microsoft.com # eth1
nameserver 10.50.10.50 # eth1
nameserver 10.50.50.50 # eth1
nameserver 8.8.8.8
nameserver 10.0.80.11
nameserver 10.0.80.12

================
File: ./richos/fs/test-fs/verismo/files/etc/inittab
================

# /etc/inittab

::sysinit:/sbin/openrc sysinit
::sysinit:/sbin/openrc boot
::wait:/sbin/openrc default

# Set up a couple of getty's
#tty1::respawn:/sbin/getty 38400 tty1
#tty2::respawn:/sbin/getty 38400 tty2
#tty3::respawn:/sbin/getty 38400 tty3
#tty4::respawn:/sbin/getty 38400 tty4
#tty5::respawn:/sbin/getty 38400 tty5
#tty6::respawn:/sbin/getty 38400 tty6

# Put a getty on the serial port
#ttyS0::respawn:/sbin/getty -L ttyS0 115200 vt100

# Stuff to do for the 3-finger salute
::ctrlaltdel:/sbin/reboot

# Stuff to do before rebooting
::shutdown:/sbin/openrc shutdown

================
File: ./richos/fs/test-fs/unixbench/Dockerfile
================

FROM test-fs/benchmark
RUN apk add --update git perl build-base && \
    rm -rf /var/cache/apk/* && \
    git clone https://github.com/kdlucas/byte-unixbench.git /app
COPY ./context1.c /app/UnixBench/src/
#RUN echo "http://dl-cdn.alpinelinux.org/alpine/v3.8/community" >> /etc/apk/repositories
#RUN apk add php7 && \
#    rm -rf /var/cache/apk/* && \
#    cd ~ && git clone https://github.com/cloudharmony/unixbench.git

#COPY ./test.sh ~/unixbench/test.sh
#RUN chmod +x test.sh

================
File: ./richos/kernel/config-richos
================

#
# Automatically generated file; DO NOT EDIT.
# Linux/x86 5.11.0-rc5 Kernel Configuration
#
CONFIG_CC_VERSION_TEXT="gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0"
CONFIG_CC_IS_GCC=y
CONFIG_GCC_VERSION=110400
CONFIG_LD_VERSION=238000000
CONFIG_CLANG_VERSION=0
CONFIG_LLD_VERSION=0
CONFIG_CC_CAN_LINK=y
CONFIG_CC_CAN_LINK_STATIC=y
CONFIG_CC_HAS_ASM_GOTO=y
CONFIG_CC_HAS_ASM_GOTO_OUTPUT=y
CONFIG_CC_HAS_ASM_INLINE=y
CONFIG_IRQ_WORK=y
CONFIG_BUILDTIME_TABLE_SORT=y
CONFIG_THREAD_INFO_IN_TASK=y

#
# General setup
#
CONFIG_INIT_ENV_ARG_LIMIT=32
# CONFIG_COMPILE_TEST is not set
CONFIG_LOCALVERSION=""
# CONFIG_LOCALVERSION_AUTO is not set
CONFIG_BUILD_SALT=""
CONFIG_HAVE_KERNEL_GZIP=y
CONFIG_HAVE_KERNEL_BZIP2=y
CONFIG_HAVE_KERNEL_LZMA=y
CONFIG_HAVE_KERNEL_XZ=y
CONFIG_HAVE_KERNEL_LZO=y
CONFIG_HAVE_KERNEL_LZ4=y
CONFIG_HAVE_KERNEL_ZSTD=y
# CONFIG_KERNEL_GZIP is not set
# CONFIG_KERNEL_BZIP2 is not set
# CONFIG_KERNEL_LZMA is not set
CONFIG_KERNEL_XZ=y
# CONFIG_KERNEL_LZO is not set
# CONFIG_KERNEL_LZ4 is not set
# CONFIG_KERNEL_ZSTD is not set
CONFIG_DEFAULT_INIT=""
CONFIG_DEFAULT_HOSTNAME="(none)"
CONFIG_SWAP=y
# CONFIG_SYSVIPC is not set
# CONFIG_POSIX_MQUEUE is not set
# CONFIG_WATCH_QUEUE is not set
CONFIG_CROSS_MEMORY_ATTACH=y
# CONFIG_USELIB is not set
CONFIG_AUDIT=y
CONFIG_HAVE_ARCH_AUDITSYSCALL=y
CONFIG_AUDITSYSCALL=y

#
# IRQ subsystem
#
CONFIG_GENERIC_IRQ_PROBE=y
CONFIG_GENERIC_IRQ_SHOW=y
CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK=y
CONFIG_GENERIC_PENDING_IRQ=y
CONFIG_GENERIC_IRQ_MIGRATION=y
CONFIG_HARDIRQS_SW_RESEND=y
CONFIG_IRQ_DOMAIN=y
CONFIG_IRQ_DOMAIN_HIERARCHY=y
CONFIG_GENERIC_IRQ_MATRIX_ALLOCATOR=y
CONFIG_GENERIC_IRQ_RESERVATION_MODE=y
CONFIG_IRQ_FORCED_THREADING=y
CONFIG_SPARSE_IRQ=y
# CONFIG_GENERIC_IRQ_DEBUGFS is not set
# end of IRQ subsystem

CONFIG_CLOCKSOURCE_WATCHDOG=y
CONFIG_ARCH_CLOCKSOURCE_INIT=y
CONFIG_CLOCKSOURCE_VALIDATE_LAST_CYCLE=y
CONFIG_GENERIC_TIME_VSYSCALL=y
CONFIG_GENERIC_CLOCKEVENTS=y
CONFIG_GENERIC_CLOCKEVENTS_BROADCAST=y
CONFIG_GENERIC_CLOCKEVENTS_MIN_ADJUST=y
CONFIG_GENERIC_CMOS_UPDATE=y
CONFIG_HAVE_POSIX_CPU_TIMERS_TASK_WORK=y
CONFIG_POSIX_CPU_TIMERS_TASK_WORK=y

#
# Timers subsystem
#
CONFIG_TICK_ONESHOT=y
CONFIG_NO_HZ_COMMON=y
# CONFIG_HZ_PERIODIC is not set
CONFIG_NO_HZ_IDLE=y
# CONFIG_NO_HZ_FULL is not set
# CONFIG_NO_HZ is not set
CONFIG_HIGH_RES_TIMERS=y
# end of Timers subsystem

CONFIG_PREEMPT_NONE=y
# CONFIG_PREEMPT_VOLUNTARY is not set
# CONFIG_PREEMPT is not set

#
# CPU/Task time and stats accounting
#
CONFIG_TICK_CPU_ACCOUNTING=y
# CONFIG_VIRT_CPU_ACCOUNTING_GEN is not set
# CONFIG_IRQ_TIME_ACCOUNTING is not set
# CONFIG_BSD_PROCESS_ACCT is not set
CONFIG_TASKSTATS=y
CONFIG_TASK_DELAY_ACCT=y
# CONFIG_TASK_XACCT is not set
# CONFIG_PSI is not set
# end of CPU/Task time and stats accounting

# CONFIG_CPU_ISOLATION is not set

#
# RCU Subsystem
#
CONFIG_TREE_RCU=y
# CONFIG_RCU_EXPERT is not set
CONFIG_SRCU=y
CONFIG_TREE_SRCU=y
CONFIG_RCU_STALL_COMMON=y
CONFIG_RCU_NEED_SEGCBLIST=y
# end of RCU Subsystem

CONFIG_IKCONFIG=y
CONFIG_IKCONFIG_PROC=y
# CONFIG_IKHEADERS is not set
CONFIG_LOG_BUF_SHIFT=17
CONFIG_LOG_CPU_MAX_BUF_SHIFT=12
CONFIG_PRINTK_SAFE_LOG_BUF_SHIFT=13
CONFIG_HAVE_UNSTABLE_SCHED_CLOCK=y

#
# Scheduler features
#
# end of Scheduler features

CONFIG_ARCH_SUPPORTS_NUMA_BALANCING=y
CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH=y
CONFIG_CC_HAS_INT128=y
CONFIG_ARCH_SUPPORTS_INT128=y
# CONFIG_CGROUPS is not set
# CONFIG_NAMESPACES is not set
# CONFIG_CHECKPOINT_RESTORE is not set
# CONFIG_SCHED_AUTOGROUP is not set
# CONFIG_SYSFS_DEPRECATED is not set
# CONFIG_RELAY is not set
# CONFIG_BLK_DEV_INITRD is not set
# CONFIG_BOOT_CONFIG is not set
CONFIG_CC_OPTIMIZE_FOR_PERFORMANCE=y
# CONFIG_CC_OPTIMIZE_FOR_SIZE is not set
CONFIG_LD_ORPHAN_WARN=y
CONFIG_SYSCTL=y
CONFIG_SYSCTL_EXCEPTION_TRACE=y
CONFIG_HAVE_PCSPKR_PLATFORM=y
CONFIG_BPF=y
CONFIG_EXPERT=y
CONFIG_MULTIUSER=y
CONFIG_SGETMASK_SYSCALL=y
CONFIG_SYSFS_SYSCALL=y
CONFIG_FHANDLE=y
CONFIG_POSIX_TIMERS=y
CONFIG_PRINTK=y
CONFIG_PRINTK_NMI=y
CONFIG_BUG=y
CONFIG_ELF_CORE=y
CONFIG_PCSPKR_PLATFORM=y
CONFIG_BASE_FULL=y
CONFIG_FUTEX=y
CONFIG_FUTEX_PI=y
CONFIG_EPOLL=y
CONFIG_SIGNALFD=y
CONFIG_TIMERFD=y
CONFIG_EVENTFD=y
CONFIG_SHMEM=y
CONFIG_AIO=y
CONFIG_IO_URING=y
CONFIG_ADVISE_SYSCALLS=y
# CONFIG_MEMBARRIER is not set
CONFIG_KALLSYMS=y
# CONFIG_KALLSYMS_ALL is not set
CONFIG_KALLSYMS_ABSOLUTE_PERCPU=y
CONFIG_KALLSYMS_BASE_RELATIVE=y
# CONFIG_BPF_SYSCALL is not set
CONFIG_ARCH_WANT_DEFAULT_BPF_JIT=y
# CONFIG_USERFAULTFD is not set
CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE=y
# CONFIG_RSEQ is not set
CONFIG_EMBEDDED=y
CONFIG_HAVE_PERF_EVENTS=y
# CONFIG_PC104 is not set

#
# Kernel Performance Events And Counters
#
CONFIG_PERF_EVENTS=y
# CONFIG_DEBUG_PERF_USE_VMALLOC is not set
# end of Kernel Performance Events And Counters

# CONFIG_VM_EVENT_COUNTERS is not set
# CONFIG_SLUB_DEBUG is not set
# CONFIG_COMPAT_BRK is not set
# CONFIG_SLAB is not set
CONFIG_SLUB=y
# CONFIG_SLOB is not set
# CONFIG_SLAB_MERGE_DEFAULT is not set
# CONFIG_SLAB_FREELIST_RANDOM is not set
# CONFIG_SLAB_FREELIST_HARDENED is not set
# CONFIG_SHUFFLE_PAGE_ALLOCATOR is not set
CONFIG_SLUB_CPU_PARTIAL=y
# CONFIG_PROFILING is not set
# end of General setup

CONFIG_64BIT=y
CONFIG_X86_64=y
CONFIG_X86=y
CONFIG_INSTRUCTION_DECODER=y
CONFIG_OUTPUT_FORMAT="elf64-x86-64"
CONFIG_LOCKDEP_SUPPORT=y
CONFIG_STACKTRACE_SUPPORT=y
CONFIG_MMU=y
CONFIG_ARCH_MMAP_RND_BITS_MIN=28
CONFIG_ARCH_MMAP_RND_BITS_MAX=32
CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MIN=8
CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MAX=16
CONFIG_GENERIC_BUG=y
CONFIG_GENERIC_BUG_RELATIVE_POINTERS=y
CONFIG_GENERIC_CALIBRATE_DELAY=y
CONFIG_ARCH_HAS_CPU_RELAX=y
CONFIG_ARCH_HAS_CACHE_LINE_SIZE=y
CONFIG_ARCH_HAS_FILTER_PGPROT=y
CONFIG_HAVE_SETUP_PER_CPU_AREA=y
CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK=y
CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK=y
CONFIG_ARCH_HIBERNATION_POSSIBLE=y
CONFIG_ARCH_SUSPEND_POSSIBLE=y
CONFIG_ARCH_WANT_GENERAL_HUGETLB=y
CONFIG_ZONE_DMA32=y
CONFIG_AUDIT_ARCH=y
CONFIG_X86_64_SMP=y
CONFIG_ARCH_SUPPORTS_UPROBES=y
CONFIG_FIX_EARLYCON_MEM=y
CONFIG_DYNAMIC_PHYSICAL_MASK=y
CONFIG_PGTABLE_LEVELS=4
CONFIG_CC_HAS_SANE_STACKPROTECTOR=y

#
# Processor type and features
#
# CONFIG_ZONE_DMA is not set
CONFIG_SMP=y
CONFIG_X86_FEATURE_NAMES=y
CONFIG_X86_X2APIC=y
CONFIG_X86_MPPARSE=y
# CONFIG_GOLDFISH is not set
# CONFIG_RETPOLINE is not set
# CONFIG_X86_CPU_RESCTRL is not set
# CONFIG_X86_EXTENDED_PLATFORM is not set
CONFIG_X86_AMD_PLATFORM_DEVICE=y
# CONFIG_SCHED_OMIT_FRAME_POINTER is not set
CONFIG_HYPERVISOR_GUEST=y
CONFIG_PARAVIRT=y
# CONFIG_PARAVIRT_DEBUG is not set
# CONFIG_PARAVIRT_SPINLOCKS is not set
CONFIG_X86_HV_CALLBACK_VECTOR=y
# CONFIG_XEN is not set
# CONFIG_KVM_GUEST is not set
CONFIG_ARCH_CPUIDLE_HALTPOLL=y
# CONFIG_PVH is not set
# CONFIG_PARAVIRT_TIME_ACCOUNTING is not set
# CONFIG_ACRN_GUEST is not set
# CONFIG_MK8 is not set
# CONFIG_MPSC is not set
# CONFIG_MCORE2 is not set
# CONFIG_MATOM is not set
CONFIG_GENERIC_CPU=y
CONFIG_X86_INTERNODE_CACHE_SHIFT=6
CONFIG_X86_L1_CACHE_SHIFT=6
CONFIG_X86_TSC=y
CONFIG_X86_CMPXCHG64=y
CONFIG_X86_CMOV=y
CONFIG_X86_MINIMUM_CPU_FAMILY=64
CONFIG_X86_DEBUGCTLMSR=y
CONFIG_IA32_FEAT_CTL=y
CONFIG_X86_VMX_FEATURE_NAMES=y
# CONFIG_PROCESSOR_SELECT is not set
CONFIG_CPU_SUP_INTEL=y
CONFIG_CPU_SUP_AMD=y
CONFIG_CPU_SUP_HYGON=y
CONFIG_CPU_SUP_CENTAUR=y
CONFIG_CPU_SUP_ZHAOXIN=y
CONFIG_HPET_TIMER=y
CONFIG_HPET_EMULATE_RTC=y
# CONFIG_DMI is not set
# CONFIG_MAXSMP is not set
CONFIG_NR_CPUS_RANGE_BEGIN=2
CONFIG_NR_CPUS_RANGE_END=512
CONFIG_NR_CPUS_DEFAULT=64
CONFIG_NR_CPUS=64
CONFIG_SCHED_SMT=y
CONFIG_SCHED_MC=y
# CONFIG_SCHED_MC_PRIO is not set
CONFIG_X86_LOCAL_APIC=y
CONFIG_X86_IO_APIC=y
# CONFIG_X86_REROUTE_FOR_BROKEN_BOOT_IRQS is not set
# CONFIG_X86_MCE is not set

#
# Performance monitoring
#
# CONFIG_PERF_EVENTS_AMD_POWER is not set
# end of Performance monitoring

# CONFIG_X86_VSYSCALL_EMULATION is not set
# CONFIG_X86_IOPL_IOPERM is not set
# CONFIG_I8K is not set
# CONFIG_MICROCODE is not set
# CONFIG_X86_MSR is not set
# CONFIG_X86_CPUID is not set
# CONFIG_X86_5LEVEL is not set
CONFIG_X86_DIRECT_GBPAGES=y
# CONFIG_X86_CPA_STATISTICS is not set
CONFIG_AMD_MEM_ENCRYPT=y
CONFIG_AMD_MEM_ENCRYPT_ACTIVE_BY_DEFAULT=y
# CONFIG_NUMA is not set
CONFIG_ARCH_SPARSEMEM_ENABLE=y
CONFIG_ARCH_SPARSEMEM_DEFAULT=y
CONFIG_ARCH_SELECT_MEMORY_MODEL=y
CONFIG_ARCH_PROC_KCORE_TEXT=y
CONFIG_ILLEGAL_POINTER_VALUE=0xdead000000000000
# CONFIG_X86_PMEM_LEGACY is not set
# CONFIG_X86_CHECK_BIOS_CORRUPTION is not set
CONFIG_X86_RESERVE_LOW=64
# CONFIG_MTRR is not set
# CONFIG_ARCH_RANDOM is not set
# CONFIG_X86_SMAP is not set
# CONFIG_X86_UMIP is not set
CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS=y
CONFIG_X86_INTEL_TSX_MODE_OFF=y
# CONFIG_X86_INTEL_TSX_MODE_ON is not set
# CONFIG_X86_INTEL_TSX_MODE_AUTO is not set
# CONFIG_X86_SGX is not set
# CONFIG_EFI is not set
# CONFIG_HZ_100 is not set
# CONFIG_HZ_250 is not set
# CONFIG_HZ_300 is not set
CONFIG_HZ_1000=y
CONFIG_HZ=1000
CONFIG_SCHED_HRTICK=y
# CONFIG_KEXEC is not set
# CONFIG_KEXEC_FILE is not set
# CONFIG_CRASH_DUMP is not set
CONFIG_PHYSICAL_START=0x10000000
CONFIG_RELOCATABLE=y
# CONFIG_RANDOMIZE_BASE is not set
CONFIG_PHYSICAL_ALIGN=0x200000
CONFIG_HOTPLUG_CPU=y
# CONFIG_BOOTPARAM_HOTPLUG_CPU0 is not set
# CONFIG_DEBUG_HOTPLUG_CPU0 is not set
# CONFIG_LEGACY_VSYSCALL_EMULATE is not set
CONFIG_LEGACY_VSYSCALL_XONLY=y
# CONFIG_LEGACY_VSYSCALL_NONE is not set
# CONFIG_CMDLINE_BOOL is not set
# CONFIG_MODIFY_LDT_SYSCALL is not set
CONFIG_HAVE_LIVEPATCH=y
# end of Processor type and features

CONFIG_ARCH_HAS_ADD_PAGES=y
CONFIG_ARCH_ENABLE_MEMORY_HOTPLUG=y
CONFIG_ARCH_ENABLE_SPLIT_PMD_PTLOCK=y

#
# Power management and ACPI options
#
# CONFIG_SUSPEND is not set
# CONFIG_HIBERNATION is not set
# CONFIG_PM is not set
# CONFIG_ENERGY_MODEL is not set
CONFIG_ARCH_SUPPORTS_ACPI=y
CONFIG_ACPI=y
CONFIG_ACPI_LEGACY_TABLES_LOOKUP=y
CONFIG_ARCH_MIGHT_HAVE_ACPI_PDC=y
CONFIG_ACPI_SYSTEM_POWER_STATES_SUPPORT=y
# CONFIG_ACPI_DEBUGGER is not set
# CONFIG_ACPI_SPCR_TABLE is not set
CONFIG_ACPI_LPIT=y
# CONFIG_ACPI_REV_OVERRIDE_POSSIBLE is not set
# CONFIG_ACPI_EC_DEBUGFS is not set
# CONFIG_ACPI_AC is not set
# CONFIG_ACPI_BATTERY is not set
# CONFIG_ACPI_TINY_POWER_BUTTON is not set
# CONFIG_ACPI_FAN is not set
# CONFIG_ACPI_DOCK is not set
CONFIG_ACPI_CPU_FREQ_PSS=y
CONFIG_ACPI_PROCESSOR_CSTATE=y
CONFIG_ACPI_PROCESSOR_IDLE=y
CONFIG_ACPI_PROCESSOR=y
CONFIG_ACPI_HOTPLUG_CPU=y
# CONFIG_ACPI_PROCESSOR_AGGREGATOR is not set
# CONFIG_ACPI_THERMAL is not set
CONFIG_ACPI_CUSTOM_DSDT_FILE=""
CONFIG_ARCH_HAS_ACPI_TABLE_UPGRADE=y
CONFIG_ACPI_DEBUG=y
CONFIG_ACPI_CONTAINER=y
# CONFIG_ACPI_SBS is not set
# CONFIG_ACPI_HED is not set
# CONFIG_ACPI_CUSTOM_METHOD is not set
# CONFIG_ACPI_REDUCED_HARDWARE_ONLY is not set
# CONFIG_ACPI_NFIT is not set
CONFIG_HAVE_ACPI_APEI=y
CONFIG_HAVE_ACPI_APEI_NMI=y
# CONFIG_ACPI_APEI is not set
# CONFIG_ACPI_DPTF is not set
# CONFIG_ACPI_CONFIGFS is not set
# CONFIG_PMIC_OPREGION is not set
# CONFIG_X86_PM_TIMER is not set
# CONFIG_SFI is not set

#
# CPU Frequency scaling
#
CONFIG_CPU_FREQ=y
# CONFIG_CPU_FREQ_STAT is not set
CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
# CONFIG_CPU_FREQ_DEFAULT_GOV_POWERSAVE is not set
# CONFIG_CPU_FREQ_DEFAULT_GOV_USERSPACE is not set
# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
# CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
# CONFIG_CPU_FREQ_DEFAULT_GOV_SCHEDUTIL is not set
CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
# CONFIG_CPU_FREQ_GOV_POWERSAVE is not set
# CONFIG_CPU_FREQ_GOV_USERSPACE is not set
# CONFIG_CPU_FREQ_GOV_ONDEMAND is not set
# CONFIG_CPU_FREQ_GOV_CONSERVATIVE is not set
# CONFIG_CPU_FREQ_GOV_SCHEDUTIL is not set

#
# CPU frequency scaling drivers
#
# CONFIG_X86_INTEL_PSTATE is not set
# CONFIG_X86_PCC_CPUFREQ is not set
# CONFIG_X86_ACPI_CPUFREQ is not set
# CONFIG_X86_SPEEDSTEP_CENTRINO is not set
# CONFIG_X86_P4_CLOCKMOD is not set

#
# shared options
#
# end of CPU Frequency scaling

#
# CPU Idle
#
CONFIG_CPU_IDLE=y
# CONFIG_CPU_IDLE_GOV_LADDER is not set
CONFIG_CPU_IDLE_GOV_MENU=y
# CONFIG_CPU_IDLE_GOV_TEO is not set
# end of CPU Idle

# CONFIG_INTEL_IDLE is not set
# end of Power management and ACPI options

#
# Bus options (PCI etc.)
#
# CONFIG_ISA_BUS is not set
# CONFIG_ISA_DMA_API is not set
# CONFIG_X86_SYSFB is not set
# end of Bus options (PCI etc.)

#
# Binary Emulations
#
# CONFIG_IA32_EMULATION is not set
# CONFIG_X86_X32 is not set
# end of Binary Emulations

#
# Firmware Drivers
#
# CONFIG_EDD is not set
# CONFIG_FIRMWARE_MEMMAP is not set
# CONFIG_ISCSI_IBFT is not set
# CONFIG_FW_CFG_SYSFS is not set
# CONFIG_GOOGLE_FIRMWARE is not set

#
# Tegra firmware driver
#
# end of Tegra firmware driver
# end of Firmware Drivers

CONFIG_HAVE_KVM=y
# CONFIG_VIRTUALIZATION is not set
CONFIG_AS_AVX512=y
CONFIG_AS_SHA1_NI=y
CONFIG_AS_SHA256_NI=y
CONFIG_AS_TPAUSE=y

#
# General architecture-dependent options
#
CONFIG_CRASH_CORE=y
CONFIG_HOTPLUG_SMT=y
CONFIG_GENERIC_ENTRY=y
CONFIG_HAVE_OPROFILE=y
CONFIG_OPROFILE_NMI_TIMER=y
CONFIG_KPROBES=y
# CONFIG_JUMP_LABEL is not set
# CONFIG_STATIC_CALL_SELFTEST is not set
CONFIG_OPTPROBES=y
CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS=y
CONFIG_ARCH_USE_BUILTIN_BSWAP=y
CONFIG_KRETPROBES=y
CONFIG_HAVE_IOREMAP_PROT=y
CONFIG_HAVE_KPROBES=y
CONFIG_HAVE_KRETPROBES=y
CONFIG_HAVE_OPTPROBES=y
CONFIG_HAVE_KPROBES_ON_FTRACE=y
CONFIG_HAVE_FUNCTION_ERROR_INJECTION=y
CONFIG_HAVE_NMI=y
CONFIG_HAVE_ARCH_TRACEHOOK=y
CONFIG_HAVE_DMA_CONTIGUOUS=y
CONFIG_GENERIC_SMP_IDLE_THREAD=y
CONFIG_ARCH_HAS_FORTIFY_SOURCE=y
CONFIG_ARCH_HAS_SET_MEMORY=y
CONFIG_ARCH_HAS_SET_DIRECT_MAP=y
CONFIG_HAVE_ARCH_THREAD_STRUCT_WHITELIST=y
CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT=y
CONFIG_HAVE_ASM_MODVERSIONS=y
CONFIG_HAVE_REGS_AND_STACK_ACCESS_API=y
CONFIG_HAVE_RSEQ=y
CONFIG_HAVE_FUNCTION_ARG_ACCESS_API=y
CONFIG_HAVE_HW_BREAKPOINT=y
CONFIG_HAVE_MIXED_BREAKPOINTS_REGS=y
CONFIG_HAVE_USER_RETURN_NOTIFIER=y
CONFIG_HAVE_PERF_EVENTS_NMI=y
CONFIG_HAVE_HARDLOCKUP_DETECTOR_PERF=y
CONFIG_HAVE_PERF_REGS=y
CONFIG_HAVE_PERF_USER_STACK_DUMP=y
CONFIG_HAVE_ARCH_JUMP_LABEL=y
CONFIG_HAVE_ARCH_JUMP_LABEL_RELATIVE=y
CONFIG_MMU_GATHER_TABLE_FREE=y
CONFIG_MMU_GATHER_RCU_TABLE_FREE=y
CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG=y
CONFIG_HAVE_ALIGNED_STRUCT_PAGE=y
CONFIG_HAVE_CMPXCHG_LOCAL=y
CONFIG_HAVE_CMPXCHG_DOUBLE=y
CONFIG_HAVE_ARCH_SECCOMP=y
CONFIG_HAVE_ARCH_SECCOMP_FILTER=y
CONFIG_SECCOMP=y
CONFIG_SECCOMP_FILTER=y
# CONFIG_SECCOMP_CACHE_DEBUG is not set
CONFIG_HAVE_ARCH_STACKLEAK=y
CONFIG_HAVE_STACKPROTECTOR=y
# CONFIG_STACKPROTECTOR is not set
CONFIG_HAVE_ARCH_WITHIN_STACK_FRAMES=y
CONFIG_HAVE_CONTEXT_TRACKING=y
CONFIG_HAVE_CONTEXT_TRACKING_OFFSTACK=y
CONFIG_HAVE_VIRT_CPU_ACCOUNTING_GEN=y
CONFIG_HAVE_IRQ_TIME_ACCOUNTING=y
CONFIG_HAVE_MOVE_PUD=y
CONFIG_HAVE_MOVE_PMD=y
CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE=y
CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD=y
CONFIG_HAVE_ARCH_HUGE_VMAP=y
CONFIG_ARCH_WANT_HUGE_PMD_SHARE=y
CONFIG_HAVE_ARCH_SOFT_DIRTY=y
CONFIG_HAVE_MOD_ARCH_SPECIFIC=y
CONFIG_MODULES_USE_ELF_RELA=y
CONFIG_ARCH_HAS_ELF_RANDOMIZE=y
CONFIG_HAVE_ARCH_MMAP_RND_BITS=y
CONFIG_HAVE_EXIT_THREAD=y
CONFIG_ARCH_MMAP_RND_BITS=28
CONFIG_HAVE_STACK_VALIDATION=y
CONFIG_HAVE_RELIABLE_STACKTRACE=y
# CONFIG_COMPAT_32BIT_TIME is not set
CONFIG_HAVE_ARCH_VMAP_STACK=y
CONFIG_VMAP_STACK=y
CONFIG_ARCH_HAS_STRICT_KERNEL_RWX=y
CONFIG_STRICT_KERNEL_RWX=y
CONFIG_ARCH_HAS_STRICT_MODULE_RWX=y
CONFIG_STRICT_MODULE_RWX=y
CONFIG_HAVE_ARCH_PREL32_RELOCATIONS=y
CONFIG_ARCH_USE_MEMREMAP_PROT=y
# CONFIG_LOCK_EVENT_COUNTS is not set
CONFIG_ARCH_HAS_MEM_ENCRYPT=y
CONFIG_HAVE_STATIC_CALL=y
CONFIG_HAVE_STATIC_CALL_INLINE=y
CONFIG_ARCH_WANT_LD_ORPHAN_WARN=y
CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC=y

#
# GCOV-based kernel profiling
#
# CONFIG_GCOV_KERNEL is not set
CONFIG_ARCH_HAS_GCOV_PROFILE_ALL=y
# end of GCOV-based kernel profiling

CONFIG_HAVE_GCC_PLUGINS=y
# end of General architecture-dependent options

CONFIG_RT_MUTEXES=y
CONFIG_BASE_SMALL=0
CONFIG_MODULES=y
# CONFIG_MODULE_FORCE_LOAD is not set
# CONFIG_MODULE_UNLOAD is not set
# CONFIG_MODVERSIONS is not set
# CONFIG_MODULE_SRCVERSION_ALL is not set
# CONFIG_MODULE_SIG is not set
# CONFIG_MODULE_COMPRESS is not set
# CONFIG_MODULE_ALLOW_MISSING_NAMESPACE_IMPORTS is not set
CONFIG_UNUSED_SYMBOLS=y
CONFIG_MODULES_TREE_LOOKUP=y
CONFIG_BLOCK=y
CONFIG_BLK_SCSI_REQUEST=y
CONFIG_BLK_DEV_BSG=y
# CONFIG_BLK_DEV_BSGLIB is not set
# CONFIG_BLK_DEV_INTEGRITY is not set
# CONFIG_BLK_DEV_ZONED is not set
# CONFIG_BLK_CMDLINE_PARSER is not set
# CONFIG_BLK_WBT is not set
CONFIG_BLK_DEBUG_FS=y
# CONFIG_BLK_SED_OPAL is not set
# CONFIG_BLK_INLINE_ENCRYPTION is not set

#
# Partition Types
#
# CONFIG_PARTITION_ADVANCED is not set
CONFIG_MSDOS_PARTITION=y
CONFIG_EFI_PARTITION=y
# end of Partition Types

#
# IO Schedulers
#
CONFIG_MQ_IOSCHED_DEADLINE=y
CONFIG_MQ_IOSCHED_KYBER=y
# CONFIG_IOSCHED_BFQ is not set
# end of IO Schedulers

CONFIG_ASN1=y
CONFIG_INLINE_SPIN_UNLOCK_IRQ=y
CONFIG_INLINE_READ_UNLOCK=y
CONFIG_INLINE_READ_UNLOCK_IRQ=y
CONFIG_INLINE_WRITE_UNLOCK=y
CONFIG_INLINE_WRITE_UNLOCK_IRQ=y
CONFIG_ARCH_SUPPORTS_ATOMIC_RMW=y
CONFIG_MUTEX_SPIN_ON_OWNER=y
CONFIG_RWSEM_SPIN_ON_OWNER=y
CONFIG_LOCK_SPIN_ON_OWNER=y
CONFIG_ARCH_USE_QUEUED_SPINLOCKS=y
CONFIG_QUEUED_SPINLOCKS=y
CONFIG_ARCH_USE_QUEUED_RWLOCKS=y
CONFIG_QUEUED_RWLOCKS=y
CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE=y
CONFIG_ARCH_HAS_SYNC_CORE_BEFORE_USERMODE=y
CONFIG_ARCH_HAS_SYSCALL_WRAPPER=y

#
# Executable file formats
#
CONFIG_BINFMT_ELF=y
CONFIG_ELFCORE=y
CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS=y
CONFIG_BINFMT_SCRIPT=y
# CONFIG_BINFMT_MISC is not set
CONFIG_COREDUMP=y
# end of Executable file formats

#
# Memory Management options
#
CONFIG_SELECT_MEMORY_MODEL=y
CONFIG_SPARSEMEM_MANUAL=y
CONFIG_SPARSEMEM=y
CONFIG_SPARSEMEM_EXTREME=y
CONFIG_SPARSEMEM_VMEMMAP_ENABLE=y
CONFIG_SPARSEMEM_VMEMMAP=y
CONFIG_HAVE_FAST_GUP=y
# CONFIG_MEMORY_HOTPLUG is not set
CONFIG_SPLIT_PTLOCK_CPUS=4
# CONFIG_COMPACTION is not set
# CONFIG_PAGE_REPORTING is not set
CONFIG_PHYS_ADDR_T_64BIT=y
CONFIG_VIRT_TO_BUS=y
# CONFIG_KSM is not set
CONFIG_DEFAULT_MMAP_MIN_ADDR=4096
# CONFIG_TRANSPARENT_HUGEPAGE is not set
CONFIG_ARCH_WANTS_THP_SWAP=y
# CONFIG_CLEANCACHE is not set
# CONFIG_FRONTSWAP is not set
# CONFIG_CMA is not set
# CONFIG_ZPOOL is not set
# CONFIG_ZBUD is not set
# CONFIG_ZSMALLOC is not set
CONFIG_GENERIC_EARLY_IOREMAP=y
# CONFIG_DEFERRED_STRUCT_PAGE_INIT is not set
# CONFIG_IDLE_PAGE_TRACKING is not set
CONFIG_ARCH_HAS_PTE_DEVMAP=y
CONFIG_ARCH_USES_HIGH_VMA_FLAGS=y
CONFIG_ARCH_HAS_PKEYS=y
# CONFIG_PERCPU_STATS is not set
# CONFIG_GUP_TEST is not set
CONFIG_ARCH_HAS_PTE_SPECIAL=y
# end of Memory Management options

CONFIG_NET=y
CONFIG_SKB_EXTENSIONS=y

#
# Networking options
#
CONFIG_PACKET=y
# CONFIG_PACKET_DIAG is not set
CONFIG_UNIX=y
CONFIG_UNIX_SCM=y
# CONFIG_UNIX_DIAG is not set
# CONFIG_TLS is not set
CONFIG_XFRM=y
CONFIG_XFRM_ALGO=y
CONFIG_XFRM_USER=y
CONFIG_XFRM_SUB_POLICY=y
CONFIG_XFRM_MIGRATE=y
# CONFIG_XFRM_STATISTICS is not set
# CONFIG_NET_KEY is not set
CONFIG_INET=y
# CONFIG_IP_MULTICAST is not set
CONFIG_IP_ADVANCED_ROUTER=y
CONFIG_IP_FIB_TRIE_STATS=y
CONFIG_IP_MULTIPLE_TABLES=y
CONFIG_IP_ROUTE_MULTIPATH=y
CONFIG_IP_ROUTE_VERBOSE=y
# CONFIG_IP_PNP is not set
# CONFIG_NET_IPIP is not set
# CONFIG_NET_IPGRE_DEMUX is not set
CONFIG_SYN_COOKIES=y
# CONFIG_NET_IPVTI is not set
# CONFIG_NET_FOU is not set
# CONFIG_INET_AH is not set
# CONFIG_INET_ESP is not set
# CONFIG_INET_IPCOMP is not set
CONFIG_INET_DIAG=y
CONFIG_INET_TCP_DIAG=y
# CONFIG_INET_UDP_DIAG is not set
# CONFIG_INET_RAW_DIAG is not set
# CONFIG_INET_DIAG_DESTROY is not set
CONFIG_TCP_CONG_ADVANCED=y
CONFIG_TCP_CONG_BIC=y
CONFIG_TCP_CONG_CUBIC=y
CONFIG_TCP_CONG_WESTWOOD=y
CONFIG_TCP_CONG_HTCP=y
# CONFIG_TCP_CONG_HSTCP is not set
# CONFIG_TCP_CONG_HYBLA is not set
# CONFIG_TCP_CONG_VEGAS is not set
# CONFIG_TCP_CONG_NV is not set
# CONFIG_TCP_CONG_SCALABLE is not set
# CONFIG_TCP_CONG_LP is not set
# CONFIG_TCP_CONG_VENO is not set
# CONFIG_TCP_CONG_YEAH is not set
# CONFIG_TCP_CONG_ILLINOIS is not set
# CONFIG_TCP_CONG_DCTCP is not set
# CONFIG_TCP_CONG_CDG is not set
# CONFIG_TCP_CONG_BBR is not set
# CONFIG_DEFAULT_BIC is not set
CONFIG_DEFAULT_CUBIC=y
# CONFIG_DEFAULT_HTCP is not set
# CONFIG_DEFAULT_WESTWOOD is not set
# CONFIG_DEFAULT_RENO is not set
CONFIG_DEFAULT_TCP_CONG="cubic"
CONFIG_TCP_MD5SIG=y
# CONFIG_IPV6 is not set
# CONFIG_NETLABEL is not set
# CONFIG_MPTCP is not set
CONFIG_NETWORK_SECMARK=y
# CONFIG_NETWORK_PHY_TIMESTAMPING is not set
# CONFIG_NETFILTER is not set
# CONFIG_BPFILTER is not set
# CONFIG_IP_DCCP is not set
# CONFIG_IP_SCTP is not set
# CONFIG_RDS is not set
# CONFIG_TIPC is not set
# CONFIG_ATM is not set
# CONFIG_L2TP is not set
CONFIG_STP=y
CONFIG_BRIDGE=y
# CONFIG_BRIDGE_IGMP_SNOOPING is not set
# CONFIG_BRIDGE_MRP is not set
# CONFIG_BRIDGE_CFM is not set
CONFIG_HAVE_NET_DSA=y
# CONFIG_NET_DSA is not set
# CONFIG_VLAN_8021Q is not set
# CONFIG_DECNET is not set
CONFIG_LLC=y
# CONFIG_LLC2 is not set
# CONFIG_ATALK is not set
# CONFIG_X25 is not set
# CONFIG_LAPB is not set
# CONFIG_PHONET is not set
# CONFIG_IEEE802154 is not set
CONFIG_NET_SCHED=y

#
# Queueing/Scheduling
#
# CONFIG_NET_SCH_CBQ is not set
# CONFIG_NET_SCH_HTB is not set
# CONFIG_NET_SCH_HFSC is not set
# CONFIG_NET_SCH_PRIO is not set
# CONFIG_NET_SCH_MULTIQ is not set
# CONFIG_NET_SCH_RED is not set
# CONFIG_NET_SCH_SFB is not set
# CONFIG_NET_SCH_SFQ is not set
# CONFIG_NET_SCH_TEQL is not set
# CONFIG_NET_SCH_TBF is not set
# CONFIG_NET_SCH_CBS is not set
# CONFIG_NET_SCH_ETF is not set
# CONFIG_NET_SCH_TAPRIO is not set
# CONFIG_NET_SCH_GRED is not set
# CONFIG_NET_SCH_DSMARK is not set
# CONFIG_NET_SCH_NETEM is not set
# CONFIG_NET_SCH_DRR is not set
# CONFIG_NET_SCH_MQPRIO is not set
# CONFIG_NET_SCH_SKBPRIO is not set
# CONFIG_NET_SCH_CHOKE is not set
# CONFIG_NET_SCH_QFQ is not set
# CONFIG_NET_SCH_CODEL is not set
# CONFIG_NET_SCH_FQ_CODEL is not set
# CONFIG_NET_SCH_CAKE is not set
# CONFIG_NET_SCH_FQ is not set
# CONFIG_NET_SCH_HHF is not set
# CONFIG_NET_SCH_PIE is not set
# CONFIG_NET_SCH_PLUG is not set
# CONFIG_NET_SCH_ETS is not set
# CONFIG_NET_SCH_DEFAULT is not set

#
# Classification
#
# CONFIG_NET_CLS_BASIC is not set
# CONFIG_NET_CLS_TCINDEX is not set
# CONFIG_NET_CLS_ROUTE4 is not set
# CONFIG_NET_CLS_FW is not set
# CONFIG_NET_CLS_U32 is not set
# CONFIG_NET_CLS_RSVP is not set
# CONFIG_NET_CLS_RSVP6 is not set
# CONFIG_NET_CLS_FLOW is not set
# CONFIG_NET_CLS_BPF is not set
# CONFIG_NET_CLS_FLOWER is not set
# CONFIG_NET_CLS_MATCHALL is not set
# CONFIG_NET_EMATCH is not set
# CONFIG_NET_CLS_ACT is not set
CONFIG_NET_SCH_FIFO=y
# CONFIG_DCB is not set
# CONFIG_DNS_RESOLVER is not set
# CONFIG_BATMAN_ADV is not set
# CONFIG_OPENVSWITCH is not set
CONFIG_VSOCKETS=y
# CONFIG_VSOCKETS_DIAG is not set
CONFIG_VSOCKETS_LOOPBACK=y
CONFIG_VIRTIO_VSOCKETS_COMMON=y
CONFIG_HYPERV_VSOCKETS=y
# CONFIG_NETLINK_DIAG is not set
# CONFIG_MPLS is not set
# CONFIG_NET_NSH is not set
# CONFIG_HSR is not set
CONFIG_NET_SWITCHDEV=y
CONFIG_NET_L3_MASTER_DEV=y
# CONFIG_QRTR is not set
CONFIG_NET_NCSI=y
# CONFIG_NCSI_OEM_CMD_GET_MAC is not set
CONFIG_RPS=y
CONFIG_RFS_ACCEL=y
CONFIG_XPS=y
CONFIG_NET_RX_BUSY_POLL=y
CONFIG_BQL=y
# CONFIG_BPF_JIT is not set
CONFIG_NET_FLOW_LIMIT=y

#
# Network testing
#
# CONFIG_NET_PKTGEN is not set
# end of Network testing
# end of Networking options

# CONFIG_HAMRADIO is not set
# CONFIG_CAN is not set
# CONFIG_BT is not set
# CONFIG_AF_RXRPC is not set
# CONFIG_AF_KCM is not set
CONFIG_FIB_RULES=y
# CONFIG_WIRELESS is not set
# CONFIG_RFKILL is not set
# CONFIG_NET_9P is not set
# CONFIG_CAIF is not set
# CONFIG_CEPH_LIB is not set
# CONFIG_NFC is not set
# CONFIG_PSAMPLE is not set
# CONFIG_NET_IFE is not set
# CONFIG_LWTUNNEL is not set
CONFIG_GRO_CELLS=y
# CONFIG_FAILOVER is not set
# CONFIG_ETHTOOL_NETLINK is not set
CONFIG_HAVE_EBPF_JIT=y

#
# Device Drivers
#
CONFIG_HAVE_EISA=y
# CONFIG_EISA is not set
CONFIG_HAVE_PCI=y
# CONFIG_PCI is not set
# CONFIG_PCCARD is not set

#
# Generic Driver Options
#
# CONFIG_UEVENT_HELPER is not set
CONFIG_DEVTMPFS=y
CONFIG_DEVTMPFS_MOUNT=y
# CONFIG_STANDALONE is not set
# CONFIG_PREVENT_FIRMWARE_BUILD is not set

#
# Firmware loader
#
# CONFIG_FW_LOADER is not set
# end of Firmware loader

# CONFIG_ALLOW_DEV_COREDUMP is not set
# CONFIG_DEBUG_DRIVER is not set
# CONFIG_DEBUG_DEVRES is not set
# CONFIG_DEBUG_TEST_DRIVER_REMOVE is not set
# CONFIG_TEST_ASYNC_DRIVER_PROBE is not set
CONFIG_GENERIC_CPU_AUTOPROBE=y
CONFIG_GENERIC_CPU_VULNERABILITIES=y
CONFIG_DMA_SHARED_BUFFER=y
# CONFIG_DMA_FENCE_TRACE is not set
# end of Generic Driver Options

#
# Bus devices
#
# CONFIG_MHI_BUS is not set
# end of Bus devices

# CONFIG_CONNECTOR is not set
# CONFIG_GNSS is not set
# CONFIG_MTD is not set
# CONFIG_OF is not set
CONFIG_ARCH_MIGHT_HAVE_PC_PARPORT=y
# CONFIG_PARPORT is not set
CONFIG_PNP=y
CONFIG_PNP_DEBUG_MESSAGES=y

#
# Protocols
#
CONFIG_PNPACPI=y
CONFIG_BLK_DEV=y
# CONFIG_BLK_DEV_NULL_BLK is not set
CONFIG_BLK_DEV_LOOP=y
CONFIG_BLK_DEV_LOOP_MIN_COUNT=8
CONFIG_BLK_DEV_CRYPTOLOOP=y
# CONFIG_BLK_DEV_DRBD is not set
# CONFIG_BLK_DEV_NBD is not set
CONFIG_BLK_DEV_RAM=y
CONFIG_BLK_DEV_RAM_COUNT=4
CONFIG_BLK_DEV_RAM_SIZE=1024
# CONFIG_CDROM_PKTCDVD is not set
# CONFIG_ATA_OVER_ETH is not set
# CONFIG_BLK_DEV_RBD is not set

#
# NVME Support
#
# CONFIG_NVME_FC is not set
# CONFIG_NVME_TARGET is not set
# end of NVME Support

#
# Misc devices
#
# CONFIG_DUMMY_IRQ is not set
# CONFIG_ENCLOSURE_SERVICES is not set
# CONFIG_SRAM is not set
# CONFIG_XILINX_SDFEC is not set
# CONFIG_PVPANIC is not set
# CONFIG_C2PORT is not set

#
# EEPROM support
#
# CONFIG_EEPROM_93CX6 is not set
# end of EEPROM support

#
# Texas Instruments shared transport line discipline
#
# end of Texas Instruments shared transport line discipline

#
# Altera FPGA firmware download module (requires I2C)
#
# CONFIG_ECHO is not set
# end of Misc devices

CONFIG_HAVE_IDE=y
# CONFIG_IDE is not set

#
# SCSI device support
#
CONFIG_SCSI_MOD=y
# CONFIG_RAID_ATTRS is not set
CONFIG_SCSI=y
CONFIG_SCSI_DMA=y
CONFIG_SCSI_PROC_FS=y

#
# SCSI support type (disk, tape, CD-ROM)
#
CONFIG_BLK_DEV_SD=y
# CONFIG_CHR_DEV_ST is not set
# CONFIG_BLK_DEV_SR is not set
# CONFIG_CHR_DEV_SG is not set
# CONFIG_CHR_DEV_SCH is not set
# CONFIG_SCSI_CONSTANTS is not set
CONFIG_SCSI_LOGGING=y
# CONFIG_SCSI_SCAN_ASYNC is not set

#
# SCSI Transports
#
# CONFIG_SCSI_SPI_ATTRS is not set
# CONFIG_SCSI_FC_ATTRS is not set
# CONFIG_SCSI_ISCSI_ATTRS is not set
# CONFIG_SCSI_SAS_ATTRS is not set
# CONFIG_SCSI_SAS_LIBSAS is not set
# CONFIG_SCSI_SRP_ATTRS is not set
# end of SCSI Transports

CONFIG_SCSI_LOWLEVEL=y
# CONFIG_ISCSI_TCP is not set
# CONFIG_ISCSI_BOOT_SYSFS is not set
# CONFIG_SCSI_UFSHCD is not set
CONFIG_HYPERV_STORAGE=y
# CONFIG_SCSI_DEBUG is not set
# CONFIG_SCSI_DH is not set
# end of SCSI device support

# CONFIG_ATA is not set
CONFIG_MD=y
# CONFIG_BLK_DEV_MD is not set
# CONFIG_BCACHE is not set
# CONFIG_BLK_DEV_DM is not set
# CONFIG_TARGET_CORE is not set
# CONFIG_MACINTOSH_DRIVERS is not set
CONFIG_NETDEVICES=y
CONFIG_NET_CORE=y
# CONFIG_BONDING is not set
# CONFIG_DUMMY is not set
# CONFIG_WIREGUARD is not set
# CONFIG_EQUALIZER is not set
# CONFIG_NET_TEAM is not set
# CONFIG_MACVLAN is not set
# CONFIG_IPVLAN is not set
# CONFIG_VXLAN is not set
# CONFIG_GENEVE is not set
# CONFIG_BAREUDP is not set
# CONFIG_GTP is not set
# CONFIG_MACSEC is not set
# CONFIG_NETCONSOLE is not set
# CONFIG_TUN is not set
# CONFIG_TUN_VNET_CROSS_LE is not set
CONFIG_VETH=y
# CONFIG_NLMON is not set
# CONFIG_NET_VRF is not set

#
# Distributed Switch Architecture drivers
#
# end of Distributed Switch Architecture drivers

CONFIG_ETHERNET=y
CONFIG_NET_VENDOR_ALACRITECH=y
# CONFIG_ALTERA_TSE is not set
CONFIG_NET_VENDOR_AMAZON=y
CONFIG_NET_VENDOR_AQUANTIA=y
CONFIG_NET_VENDOR_ARC=y
CONFIG_NET_VENDOR_AURORA=y
# CONFIG_AURORA_NB8800 is not set
CONFIG_NET_VENDOR_BROADCOM=y
# CONFIG_B44 is not set
# CONFIG_BCMGENET is not set
# CONFIG_SYSTEMPORT is not set
CONFIG_NET_VENDOR_CADENCE=y
# CONFIG_MACB is not set
CONFIG_NET_VENDOR_CAVIUM=y
CONFIG_NET_VENDOR_CORTINA=y
# CONFIG_DNET is not set
CONFIG_NET_VENDOR_EZCHIP=y
CONFIG_NET_VENDOR_GOOGLE=y
CONFIG_NET_VENDOR_HUAWEI=y
CONFIG_NET_VENDOR_I825XX=y
CONFIG_NET_VENDOR_INTEL=y
CONFIG_NET_VENDOR_MARVELL=y
# CONFIG_MVMDIO is not set
CONFIG_NET_VENDOR_MICREL=y
# CONFIG_KS8842 is not set
# CONFIG_KS8851_MLL is not set
CONFIG_NET_VENDOR_MICROCHIP=y
CONFIG_NET_VENDOR_MICROSEMI=y
CONFIG_NET_VENDOR_NATSEMI=y
CONFIG_NET_VENDOR_NETRONOME=y
CONFIG_NET_VENDOR_NI=y
# CONFIG_NI_XGE_MANAGEMENT_ENET is not set
CONFIG_NET_VENDOR_8390=y
# CONFIG_ETHOC is not set
CONFIG_NET_VENDOR_PENSANDO=y
CONFIG_NET_VENDOR_QUALCOMM=y
# CONFIG_QCOM_EMAC is not set
# CONFIG_RMNET is not set
CONFIG_NET_VENDOR_RENESAS=y
CONFIG_NET_VENDOR_ROCKER=y
CONFIG_NET_VENDOR_SAMSUNG=y
# CONFIG_SXGBE_ETH is not set
CONFIG_NET_VENDOR_SEEQ=y
CONFIG_NET_VENDOR_SOLARFLARE=y
CONFIG_NET_VENDOR_SOCIONEXT=y
CONFIG_NET_VENDOR_STMICRO=y
# CONFIG_STMMAC_ETH is not set
CONFIG_NET_VENDOR_SYNOPSYS=y
# CONFIG_DWC_XLGMAC is not set
CONFIG_NET_VENDOR_VIA=y
CONFIG_NET_VENDOR_WIZNET=y
# CONFIG_WIZNET_W5100 is not set
# CONFIG_WIZNET_W5300 is not set
CONFIG_NET_VENDOR_XILINX=y
# CONFIG_XILINX_AXI_EMAC is not set
# CONFIG_XILINX_LL_TEMAC is not set
# CONFIG_NET_SB1000 is not set
# CONFIG_PHYLIB is not set
# CONFIG_MDIO_DEVICE is not set

#
# PCS device drivers
#
# end of PCS device drivers

# CONFIG_PPP is not set
# CONFIG_SLIP is not set

#
# Host-side USB support is needed for USB Network Adapter support
#
# CONFIG_WLAN is not set
# CONFIG_WAN is not set
# CONFIG_FUJITSU_ES is not set
CONFIG_HYPERV_NET=y
# CONFIG_NETDEVSIM is not set
# CONFIG_NET_FAILOVER is not set
# CONFIG_ISDN is not set
# CONFIG_NVM is not set

#
# Input device support
#
# CONFIG_INPUT is not set

#
# Hardware I/O ports
#
# CONFIG_SERIO is not set
CONFIG_ARCH_MIGHT_HAVE_PC_SERIO=y
# CONFIG_GAMEPORT is not set
# end of Hardware I/O ports
# end of Input device support

#
# Character devices
#
CONFIG_TTY=y
# CONFIG_VT is not set
CONFIG_UNIX98_PTYS=y
CONFIG_LEGACY_PTYS=y
CONFIG_LEGACY_PTY_COUNT=256
# CONFIG_LDISC_AUTOLOAD is not set

#
# Serial drivers
#
CONFIG_SERIAL_8250=y
CONFIG_SERIAL_8250_DEPRECATED_OPTIONS=y
CONFIG_SERIAL_8250_PNP=y
# CONFIG_SERIAL_8250_16550A_VARIANTS is not set
# CONFIG_SERIAL_8250_FINTEK is not set
# CONFIG_SERIAL_8250_CONSOLE is not set
CONFIG_SERIAL_8250_DMA=y
CONFIG_SERIAL_8250_NR_UARTS=4
CONFIG_SERIAL_8250_RUNTIME_UARTS=4
# CONFIG_SERIAL_8250_EXTENDED is not set
# CONFIG_SERIAL_8250_DW is not set
# CONFIG_SERIAL_8250_RT288X is not set

#
# Non-8250 serial port support
#
# CONFIG_SERIAL_UARTLITE is not set
CONFIG_SERIAL_CORE=y
# CONFIG_SERIAL_LANTIQ is not set
# CONFIG_SERIAL_SCCNXP is not set
# CONFIG_SERIAL_BCM63XX is not set
# CONFIG_SERIAL_ALTERA_JTAGUART is not set
# CONFIG_SERIAL_ALTERA_UART is not set
# CONFIG_SERIAL_ARC is not set
# CONFIG_SERIAL_FSL_LPUART is not set
# CONFIG_SERIAL_FSL_LINFLEXUART is not set
# CONFIG_SERIAL_SPRD is not set
# end of Serial drivers

# CONFIG_SERIAL_NONSTANDARD is not set
# CONFIG_N_GSM is not set
CONFIG_NULL_TTY=y
# CONFIG_TRACE_SINK is not set
# CONFIG_SERIAL_DEV_BUS is not set
CONFIG_TTY_PRINTK=y
CONFIG_TTY_PRINTK_LEVEL=6
# CONFIG_VIRTIO_CONSOLE is not set
# CONFIG_IPMI_HANDLER is not set
# CONFIG_HW_RANDOM is not set
# CONFIG_MWAVE is not set
CONFIG_DEVMEM=y
CONFIG_DEVKMEM=y
CONFIG_NVRAM=y
# CONFIG_RAW_DRIVER is not set
# CONFIG_HPET is not set
# CONFIG_HANGCHECK_TIMER is not set
# CONFIG_TCG_TPM is not set
# CONFIG_TELCLOCK is not set
# end of Character devices

# CONFIG_RANDOM_TRUST_BOOTLOADER is not set

#
# I2C support
#
# CONFIG_I2C is not set
# end of I2C support

# CONFIG_I3C is not set
# CONFIG_SPI is not set
# CONFIG_SPMI is not set
# CONFIG_HSI is not set
# CONFIG_PPS is not set

#
# PTP clock support
#
# CONFIG_PTP_1588_CLOCK is not set

#
# Enable PHYLIB and NETWORK_PHY_TIMESTAMPING to see the additional clocks.
#
# end of PTP clock support

CONFIG_PINCTRL=y
# CONFIG_DEBUG_PINCTRL is not set
# CONFIG_PINCTRL_AMD is not set
# CONFIG_PINCTRL_BAYTRAIL is not set
# CONFIG_PINCTRL_CHERRYVIEW is not set
# CONFIG_PINCTRL_LYNXPOINT is not set
# CONFIG_PINCTRL_ALDERLAKE is not set
# CONFIG_PINCTRL_BROXTON is not set
# CONFIG_PINCTRL_CANNONLAKE is not set
# CONFIG_PINCTRL_CEDARFORK is not set
# CONFIG_PINCTRL_DENVERTON is not set
# CONFIG_PINCTRL_ELKHARTLAKE is not set
# CONFIG_PINCTRL_EMMITSBURG is not set
# CONFIG_PINCTRL_GEMINILAKE is not set
# CONFIG_PINCTRL_ICELAKE is not set
# CONFIG_PINCTRL_JASPERLAKE is not set
# CONFIG_PINCTRL_LAKEFIELD is not set
# CONFIG_PINCTRL_LEWISBURG is not set
# CONFIG_PINCTRL_SUNRISEPOINT is not set
# CONFIG_PINCTRL_TIGERLAKE is not set

#
# Renesas pinctrl drivers
#
# end of Renesas pinctrl drivers

# CONFIG_GPIOLIB is not set
# CONFIG_W1 is not set
# CONFIG_POWER_RESET is not set
# CONFIG_POWER_SUPPLY is not set
# CONFIG_HWMON is not set
CONFIG_THERMAL=y
# CONFIG_THERMAL_NETLINK is not set
# CONFIG_THERMAL_STATISTICS is not set
CONFIG_THERMAL_EMERGENCY_POWEROFF_DELAY_MS=0
# CONFIG_THERMAL_WRITABLE_TRIPS is not set
CONFIG_THERMAL_DEFAULT_GOV_STEP_WISE=y
# CONFIG_THERMAL_DEFAULT_GOV_FAIR_SHARE is not set
# CONFIG_THERMAL_DEFAULT_GOV_USER_SPACE is not set
# CONFIG_THERMAL_GOV_FAIR_SHARE is not set
CONFIG_THERMAL_GOV_STEP_WISE=y
# CONFIG_THERMAL_GOV_BANG_BANG is not set
# CONFIG_THERMAL_GOV_USER_SPACE is not set
# CONFIG_DEVFREQ_THERMAL is not set
# CONFIG_THERMAL_EMULATION is not set

#
# Intel thermal drivers
#
# CONFIG_INTEL_POWERCLAMP is not set

#
# ACPI INT340X thermal drivers
#
# end of ACPI INT340X thermal drivers
# end of Intel thermal drivers

# CONFIG_WATCHDOG is not set
CONFIG_SSB_POSSIBLE=y
# CONFIG_SSB is not set
CONFIG_BCMA_POSSIBLE=y
# CONFIG_BCMA is not set

#
# Multifunction device drivers
#
# CONFIG_MFD_MADERA is not set
# CONFIG_HTC_PASIC3 is not set
# CONFIG_MFD_INTEL_LPSS_ACPI is not set
# CONFIG_MFD_KEMPLD is not set
# CONFIG_MFD_MT6397 is not set
# CONFIG_MFD_SM501 is not set
# CONFIG_ABX500_CORE is not set
# CONFIG_MFD_SYSCON is not set
# CONFIG_MFD_TI_AM335X_TSCADC is not set
# CONFIG_MFD_TQMX86 is not set
# end of Multifunction device drivers

# CONFIG_REGULATOR is not set
# CONFIG_MEDIA_CEC_SUPPORT is not set
# CONFIG_MEDIA_SUPPORT is not set

#
# Graphics support
#
# CONFIG_DRM is not set

#
# ARM devices
#
# end of ARM devices

#
# Frame buffer Devices
#
# CONFIG_FB is not set
# end of Frame buffer Devices

#
# Backlight & LCD device support
#
# CONFIG_LCD_CLASS_DEVICE is not set
# CONFIG_BACKLIGHT_CLASS_DEVICE is not set
# end of Backlight & LCD device support
# end of Graphics support

# CONFIG_SOUND is not set
CONFIG_USB_OHCI_LITTLE_ENDIAN=y
# CONFIG_USB_SUPPORT is not set
# CONFIG_MMC is not set
# CONFIG_MEMSTICK is not set
# CONFIG_NEW_LEDS is not set
# CONFIG_ACCESSIBILITY is not set
# CONFIG_INFINIBAND is not set
CONFIG_EDAC_ATOMIC_SCRUB=y
CONFIG_EDAC_SUPPORT=y
CONFIG_RTC_LIB=y
CONFIG_RTC_MC146818_LIB=y
CONFIG_RTC_CLASS=y
CONFIG_RTC_HCTOSYS=y
CONFIG_RTC_HCTOSYS_DEVICE="rtc0"
CONFIG_RTC_SYSTOHC=y
CONFIG_RTC_SYSTOHC_DEVICE="rtc0"
# CONFIG_RTC_DEBUG is not set
CONFIG_RTC_NVMEM=y

#
# RTC interfaces
#
CONFIG_RTC_INTF_SYSFS=y
CONFIG_RTC_INTF_PROC=y
CONFIG_RTC_INTF_DEV=y
# CONFIG_RTC_INTF_DEV_UIE_EMUL is not set
# CONFIG_RTC_DRV_TEST is not set

#
# I2C RTC drivers
#

#
# SPI RTC drivers
#

#
# SPI and I2C RTC drivers
#

#
# Platform RTC drivers
#
CONFIG_RTC_DRV_CMOS=y
# CONFIG_RTC_DRV_DS1286 is not set
# CONFIG_RTC_DRV_DS1511 is not set
# CONFIG_RTC_DRV_DS1553 is not set
# CONFIG_RTC_DRV_DS1685_FAMILY is not set
# CONFIG_RTC_DRV_DS1742 is not set
# CONFIG_RTC_DRV_DS2404 is not set
# CONFIG_RTC_DRV_STK17TA8 is not set
# CONFIG_RTC_DRV_M48T86 is not set
# CONFIG_RTC_DRV_M48T35 is not set
# CONFIG_RTC_DRV_M48T59 is not set
# CONFIG_RTC_DRV_MSM6242 is not set
# CONFIG_RTC_DRV_BQ4802 is not set
# CONFIG_RTC_DRV_RP5C01 is not set
# CONFIG_RTC_DRV_V3020 is not set

#
# on-CPU RTC drivers
#
# CONFIG_RTC_DRV_FTRTC010 is not set

#
# HID Sensor RTC drivers
#
CONFIG_DMADEVICES=y
CONFIG_DMADEVICES_DEBUG=y
# CONFIG_DMADEVICES_VDEBUG is not set

#
# DMA Devices
#
CONFIG_DMA_ENGINE=y
CONFIG_DMA_VIRTUAL_CHANNELS=y
CONFIG_DMA_ACPI=y
CONFIG_ALTERA_MSGDMA=y
CONFIG_INTEL_IDMA64=y
CONFIG_XILINX_ZYNQMP_DPDMA=y
CONFIG_QCOM_HIDMA_MGMT=y
CONFIG_QCOM_HIDMA=y
CONFIG_DW_DMAC_CORE=y
CONFIG_DW_DMAC=y
# CONFIG_SF_PDMA is not set

#
# DMA Clients
#
CONFIG_ASYNC_TX_DMA=y
CONFIG_DMATEST=y
CONFIG_DMA_ENGINE_RAID=y

#
# DMABUF options
#
# CONFIG_SYNC_FILE is not set
# CONFIG_UDMABUF is not set
# CONFIG_DMABUF_MOVE_NOTIFY is not set
# CONFIG_DMABUF_SELFTESTS is not set
CONFIG_DMABUF_HEAPS=y
CONFIG_DMABUF_HEAPS_SYSTEM=y
# end of DMABUF options

# CONFIG_AUXDISPLAY is not set
# CONFIG_UIO is not set
# CONFIG_VIRT_DRIVERS is not set
# CONFIG_VIRTIO_MENU is not set
# CONFIG_VDPA is not set
# CONFIG_VHOST_MENU is not set

#
# Microsoft Hyper-V guest support
#
CONFIG_HYPERV=y
CONFIG_HYPERV_TIMER=y
# CONFIG_HYPERV_BALLOON is not set
# end of Microsoft Hyper-V guest support

# CONFIG_GREYBUS is not set
# CONFIG_STAGING is not set
# CONFIG_X86_PLATFORM_DEVICES is not set
# CONFIG_CHROME_PLATFORMS is not set
# CONFIG_MELLANOX_PLATFORM is not set
# CONFIG_SURFACE_PLATFORMS is not set
CONFIG_HAVE_CLK=y
CONFIG_CLKDEV_LOOKUP=y
CONFIG_HAVE_CLK_PREPARE=y
CONFIG_COMMON_CLK=y
# CONFIG_HWSPINLOCK is not set

#
# Clock Source drivers
#
CONFIG_CLKEVT_I8253=y
CONFIG_I8253_LOCK=y
CONFIG_CLKBLD_I8253=y
# end of Clock Source drivers

CONFIG_MAILBOX=y
CONFIG_PCC=y
# CONFIG_ALTERA_MBOX is not set
# CONFIG_IOMMU_SUPPORT is not set

#
# Remoteproc drivers
#
# CONFIG_REMOTEPROC is not set
# end of Remoteproc drivers

#
# Rpmsg drivers
#
# CONFIG_RPMSG_QCOM_GLINK_RPM is not set
# CONFIG_RPMSG_VIRTIO is not set
# end of Rpmsg drivers

# CONFIG_SOUNDWIRE is not set

#
# SOC (System On Chip) specific Drivers
#

#
# Amlogic SoC drivers
#
# end of Amlogic SoC drivers

#
# Broadcom SoC drivers
#
# end of Broadcom SoC drivers

#
# NXP/Freescale QorIQ SoC drivers
#
# end of NXP/Freescale QorIQ SoC drivers

#
# i.MX SoC drivers
#
# end of i.MX SoC drivers

#
# Enable LiteX SoC Builder specific drivers
#
# end of Enable LiteX SoC Builder specific drivers

#
# Qualcomm SoC drivers
#
# end of Qualcomm SoC drivers

# CONFIG_SOC_TI is not set

#
# Xilinx SoC drivers
#
# CONFIG_XILINX_VCU is not set
# end of Xilinx SoC drivers
# end of SOC (System On Chip) specific Drivers

CONFIG_PM_DEVFREQ=y

#
# DEVFREQ Governors
#
# CONFIG_DEVFREQ_GOV_SIMPLE_ONDEMAND is not set
CONFIG_DEVFREQ_GOV_PERFORMANCE=y
# CONFIG_DEVFREQ_GOV_POWERSAVE is not set
# CONFIG_DEVFREQ_GOV_USERSPACE is not set
# CONFIG_DEVFREQ_GOV_PASSIVE is not set

#
# DEVFREQ Drivers
#
# CONFIG_PM_DEVFREQ_EVENT is not set
# CONFIG_EXTCON is not set
# CONFIG_MEMORY is not set
# CONFIG_IIO is not set
# CONFIG_PWM is not set

#
# IRQ chip support
#
# end of IRQ chip support

# CONFIG_IPACK_BUS is not set
# CONFIG_RESET_CONTROLLER is not set

#
# PHY Subsystem
#
# CONFIG_GENERIC_PHY is not set
# CONFIG_BCM_KONA_USB2_PHY is not set
# CONFIG_PHY_PXA_28NM_HSIC is not set
# CONFIG_PHY_PXA_28NM_USB2 is not set
# CONFIG_PHY_INTEL_LGM_EMMC is not set
# end of PHY Subsystem

# CONFIG_POWERCAP is not set
# CONFIG_MCB is not set

#
# Performance monitor support
#
# end of Performance monitor support

# CONFIG_RAS is not set

#
# Android
#
# CONFIG_ANDROID is not set
# end of Android

# CONFIG_LIBNVDIMM is not set
# CONFIG_DAX is not set
CONFIG_NVMEM=y
CONFIG_NVMEM_SYSFS=y

#
# HW tracing support
#
# CONFIG_STM is not set
# CONFIG_INTEL_TH is not set
# end of HW tracing support

# CONFIG_FPGA is not set
# CONFIG_TEE is not set
CONFIG_PM_OPP=y
# CONFIG_UNISYS_VISORBUS is not set
# CONFIG_SIOX is not set
# CONFIG_SLIMBUS is not set
# CONFIG_INTERCONNECT is not set
# CONFIG_COUNTER is not set
# CONFIG_MOST is not set
# end of Device Drivers

#
# File systems
#
CONFIG_DCACHE_WORD_ACCESS=y
# CONFIG_VALIDATE_FS_PARSER is not set
CONFIG_FS_IOMAP=y
# CONFIG_EXT2_FS is not set
# CONFIG_EXT3_FS is not set
CONFIG_EXT4_FS=y
CONFIG_EXT4_USE_FOR_EXT2=y
CONFIG_EXT4_FS_POSIX_ACL=y
CONFIG_EXT4_FS_SECURITY=y
# CONFIG_EXT4_DEBUG is not set
CONFIG_JBD2=y
# CONFIG_JBD2_DEBUG is not set
CONFIG_FS_MBCACHE=y
# CONFIG_REISERFS_FS is not set
# CONFIG_JFS_FS is not set
# CONFIG_XFS_FS is not set
# CONFIG_GFS2_FS is not set
# CONFIG_OCFS2_FS is not set
# CONFIG_BTRFS_FS is not set
# CONFIG_NILFS2_FS is not set
# CONFIG_F2FS_FS is not set
# CONFIG_FS_DAX is not set
CONFIG_FS_POSIX_ACL=y
CONFIG_EXPORTFS=y
# CONFIG_EXPORTFS_BLOCK_OPS is not set
CONFIG_FILE_LOCKING=y
CONFIG_MANDATORY_FILE_LOCKING=y
# CONFIG_FS_ENCRYPTION is not set
# CONFIG_FS_VERITY is not set
CONFIG_FSNOTIFY=y
# CONFIG_DNOTIFY is not set
# CONFIG_INOTIFY_USER is not set
# CONFIG_FANOTIFY is not set
# CONFIG_QUOTA is not set
# CONFIG_AUTOFS4_FS is not set
# CONFIG_AUTOFS_FS is not set
CONFIG_FUSE_FS=y
CONFIG_CUSE=y
# CONFIG_VIRTIO_FS is not set
CONFIG_OVERLAY_FS=y
# CONFIG_OVERLAY_FS_REDIRECT_DIR is not set
CONFIG_OVERLAY_FS_REDIRECT_ALWAYS_FOLLOW=y
# CONFIG_OVERLAY_FS_INDEX is not set
# CONFIG_OVERLAY_FS_XINO_AUTO is not set
# CONFIG_OVERLAY_FS_METACOPY is not set

#
# Caches
#
# CONFIG_FSCACHE is not set
# end of Caches

#
# CD-ROM/DVD Filesystems
#
# CONFIG_ISO9660_FS is not set
# CONFIG_UDF_FS is not set
# end of CD-ROM/DVD Filesystems

#
# DOS/FAT/EXFAT/NT Filesystems
#
# CONFIG_MSDOS_FS is not set
# CONFIG_VFAT_FS is not set
# CONFIG_EXFAT_FS is not set
# CONFIG_NTFS_FS is not set
# end of DOS/FAT/EXFAT/NT Filesystems

#
# Pseudo filesystems
#
CONFIG_PROC_FS=y
CONFIG_PROC_KCORE=y
CONFIG_PROC_SYSCTL=y
CONFIG_PROC_PAGE_MONITOR=y
CONFIG_PROC_CHILDREN=y
CONFIG_PROC_PID_ARCH_STATUS=y
CONFIG_KERNFS=y
CONFIG_SYSFS=y
CONFIG_TMPFS=y
CONFIG_TMPFS_POSIX_ACL=y
CONFIG_TMPFS_XATTR=y
# CONFIG_TMPFS_INODE64 is not set
CONFIG_HUGETLBFS=y
CONFIG_HUGETLB_PAGE=y
CONFIG_MEMFD_CREATE=y
CONFIG_ARCH_HAS_GIGANTIC_PAGE=y
CONFIG_CONFIGFS_FS=y
# end of Pseudo filesystems

# CONFIG_MISC_FILESYSTEMS is not set
# CONFIG_NETWORK_FILESYSTEMS is not set
CONFIG_NLS=y
CONFIG_NLS_DEFAULT="iso8859-1"
# CONFIG_NLS_CODEPAGE_437 is not set
# CONFIG_NLS_CODEPAGE_737 is not set
# CONFIG_NLS_CODEPAGE_775 is not set
# CONFIG_NLS_CODEPAGE_850 is not set
# CONFIG_NLS_CODEPAGE_852 is not set
# CONFIG_NLS_CODEPAGE_855 is not set
# CONFIG_NLS_CODEPAGE_857 is not set
# CONFIG_NLS_CODEPAGE_860 is not set
# CONFIG_NLS_CODEPAGE_861 is not set
# CONFIG_NLS_CODEPAGE_862 is not set
# CONFIG_NLS_CODEPAGE_863 is not set
# CONFIG_NLS_CODEPAGE_864 is not set
# CONFIG_NLS_CODEPAGE_865 is not set
# CONFIG_NLS_CODEPAGE_866 is not set
# CONFIG_NLS_CODEPAGE_869 is not set
# CONFIG_NLS_CODEPAGE_936 is not set
# CONFIG_NLS_CODEPAGE_950 is not set
# CONFIG_NLS_CODEPAGE_932 is not set
# CONFIG_NLS_CODEPAGE_949 is not set
# CONFIG_NLS_CODEPAGE_874 is not set
# CONFIG_NLS_ISO8859_8 is not set
# CONFIG_NLS_CODEPAGE_1250 is not set
# CONFIG_NLS_CODEPAGE_1251 is not set
# CONFIG_NLS_ASCII is not set
# CONFIG_NLS_ISO8859_1 is not set
# CONFIG_NLS_ISO8859_2 is not set
# CONFIG_NLS_ISO8859_3 is not set
# CONFIG_NLS_ISO8859_4 is not set
# CONFIG_NLS_ISO8859_5 is not set
# CONFIG_NLS_ISO8859_6 is not set
# CONFIG_NLS_ISO8859_7 is not set
# CONFIG_NLS_ISO8859_9 is not set
# CONFIG_NLS_ISO8859_13 is not set
# CONFIG_NLS_ISO8859_14 is not set
# CONFIG_NLS_ISO8859_15 is not set
# CONFIG_NLS_KOI8_R is not set
# CONFIG_NLS_KOI8_U is not set
# CONFIG_NLS_MAC_ROMAN is not set
# CONFIG_NLS_MAC_CELTIC is not set
# CONFIG_NLS_MAC_CENTEURO is not set
# CONFIG_NLS_MAC_CROATIAN is not set
# CONFIG_NLS_MAC_CYRILLIC is not set
# CONFIG_NLS_MAC_GAELIC is not set
# CONFIG_NLS_MAC_GREEK is not set
# CONFIG_NLS_MAC_ICELAND is not set
# CONFIG_NLS_MAC_INUIT is not set
# CONFIG_NLS_MAC_ROMANIAN is not set
# CONFIG_NLS_MAC_TURKISH is not set
# CONFIG_NLS_UTF8 is not set
# CONFIG_DLM is not set
# CONFIG_UNICODE is not set
CONFIG_IO_WQ=y
# end of File systems

#
# Security options
#
CONFIG_KEYS=y
# CONFIG_KEYS_REQUEST_CACHE is not set
# CONFIG_PERSISTENT_KEYRINGS is not set
# CONFIG_ENCRYPTED_KEYS is not set
# CONFIG_KEY_DH_OPERATIONS is not set
# CONFIG_SECURITY_DMESG_RESTRICT is not set
CONFIG_SECURITY=y
CONFIG_SECURITYFS=y
CONFIG_SECURITY_NETWORK=y
CONFIG_PAGE_TABLE_ISOLATION=y
# CONFIG_SECURITY_NETWORK_XFRM is not set
CONFIG_SECURITY_PATH=y
CONFIG_LSM_MMAP_MIN_ADDR=65536
CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR=y
# CONFIG_HARDENED_USERCOPY is not set
# CONFIG_FORTIFY_SOURCE is not set
# CONFIG_STATIC_USERMODEHELPER is not set
CONFIG_SECURITY_SELINUX=y
# CONFIG_SECURITY_SELINUX_BOOTPARAM is not set
# CONFIG_SECURITY_SELINUX_DISABLE is not set
CONFIG_SECURITY_SELINUX_DEVELOP=y
CONFIG_SECURITY_SELINUX_AVC_STATS=y
CONFIG_SECURITY_SELINUX_CHECKREQPROT_VALUE=0
CONFIG_SECURITY_SELINUX_SIDTAB_HASH_BITS=9
CONFIG_SECURITY_SELINUX_SID2STR_CACHE_SIZE=256
# CONFIG_SECURITY_SMACK is not set
# CONFIG_SECURITY_TOMOYO is not set
CONFIG_SECURITY_APPARMOR=y
CONFIG_SECURITY_APPARMOR_HASH=y
CONFIG_SECURITY_APPARMOR_HASH_DEFAULT=y
# CONFIG_SECURITY_APPARMOR_DEBUG is not set
# CONFIG_SECURITY_LOADPIN is not set
# CONFIG_SECURITY_YAMA is not set
# CONFIG_SECURITY_SAFESETID is not set
# CONFIG_SECURITY_LOCKDOWN_LSM is not set
# CONFIG_INTEGRITY is not set
# CONFIG_DEFAULT_SECURITY_SELINUX is not set
# CONFIG_DEFAULT_SECURITY_APPARMOR is not set
CONFIG_DEFAULT_SECURITY_DAC=y
CONFIG_LSM="lockdown,yama,loadpin,safesetid,integrity,bpf"

#
# Kernel hardening options
#

#
# Memory initialization
#
CONFIG_INIT_STACK_NONE=y
# CONFIG_INIT_ON_ALLOC_DEFAULT_ON is not set
# CONFIG_INIT_ON_FREE_DEFAULT_ON is not set
# end of Memory initialization
# end of Kernel hardening options
# end of Security options

CONFIG_CRYPTO=y

#
# Crypto core or helper
#
CONFIG_CRYPTO_ALGAPI=y
CONFIG_CRYPTO_ALGAPI2=y
CONFIG_CRYPTO_AEAD=y
CONFIG_CRYPTO_AEAD2=y
CONFIG_CRYPTO_SKCIPHER=y
CONFIG_CRYPTO_SKCIPHER2=y
CONFIG_CRYPTO_HASH=y
CONFIG_CRYPTO_HASH2=y
CONFIG_CRYPTO_RNG=y
CONFIG_CRYPTO_RNG2=y
CONFIG_CRYPTO_RNG_DEFAULT=y
CONFIG_CRYPTO_AKCIPHER2=y
CONFIG_CRYPTO_AKCIPHER=y
CONFIG_CRYPTO_KPP2=y
CONFIG_CRYPTO_ACOMP2=y
CONFIG_CRYPTO_MANAGER=y
CONFIG_CRYPTO_MANAGER2=y
# CONFIG_CRYPTO_USER is not set
CONFIG_CRYPTO_MANAGER_DISABLE_TESTS=y
CONFIG_CRYPTO_GF128MUL=y
CONFIG_CRYPTO_NULL=y
CONFIG_CRYPTO_NULL2=y
# CONFIG_CRYPTO_PCRYPT is not set
CONFIG_CRYPTO_CRYPTD=y
CONFIG_CRYPTO_AUTHENC=y
# CONFIG_CRYPTO_TEST is not set
CONFIG_CRYPTO_SIMD=y
CONFIG_CRYPTO_GLUE_HELPER_X86=y

#
# Public-key cryptography
#
CONFIG_CRYPTO_RSA=y
# CONFIG_CRYPTO_DH is not set
# CONFIG_CRYPTO_ECDH is not set
# CONFIG_CRYPTO_ECRDSA is not set
# CONFIG_CRYPTO_SM2 is not set
# CONFIG_CRYPTO_CURVE25519 is not set
# CONFIG_CRYPTO_CURVE25519_X86 is not set

#
# Authenticated Encryption with Associated Data
#
# CONFIG_CRYPTO_CCM is not set
CONFIG_CRYPTO_GCM=y
# CONFIG_CRYPTO_CHACHA20POLY1305 is not set
# CONFIG_CRYPTO_AEGIS128 is not set
# CONFIG_CRYPTO_AEGIS128_AESNI_SSE2 is not set
# CONFIG_CRYPTO_SEQIV is not set
CONFIG_CRYPTO_ECHAINIV=y

#
# Block modes
#
CONFIG_CRYPTO_CBC=y
# CONFIG_CRYPTO_CFB is not set
CONFIG_CRYPTO_CTR=y
# CONFIG_CRYPTO_CTS is not set
# CONFIG_CRYPTO_ECB is not set
# CONFIG_CRYPTO_LRW is not set
# CONFIG_CRYPTO_OFB is not set
# CONFIG_CRYPTO_PCBC is not set
# CONFIG_CRYPTO_XTS is not set
# CONFIG_CRYPTO_KEYWRAP is not set
# CONFIG_CRYPTO_NHPOLY1305_SSE2 is not set
# CONFIG_CRYPTO_NHPOLY1305_AVX2 is not set
# CONFIG_CRYPTO_ADIANTUM is not set
CONFIG_CRYPTO_ESSIV=y

#
# Hash modes
#
# CONFIG_CRYPTO_CMAC is not set
CONFIG_CRYPTO_HMAC=y
# CONFIG_CRYPTO_XCBC is not set
# CONFIG_CRYPTO_VMAC is not set

#
# Digest
#
CONFIG_CRYPTO_CRC32C=y
# CONFIG_CRYPTO_CRC32C_INTEL is not set
# CONFIG_CRYPTO_CRC32 is not set
# CONFIG_CRYPTO_CRC32_PCLMUL is not set
# CONFIG_CRYPTO_XXHASH is not set
# CONFIG_CRYPTO_BLAKE2B is not set
# CONFIG_CRYPTO_BLAKE2S is not set
# CONFIG_CRYPTO_BLAKE2S_X86 is not set
# CONFIG_CRYPTO_CRCT10DIF is not set
CONFIG_CRYPTO_GHASH=y
# CONFIG_CRYPTO_POLY1305 is not set
# CONFIG_CRYPTO_POLY1305_X86_64 is not set
# CONFIG_CRYPTO_MD4 is not set
CONFIG_CRYPTO_MD5=y
# CONFIG_CRYPTO_MICHAEL_MIC is not set
# CONFIG_CRYPTO_RMD128 is not set
# CONFIG_CRYPTO_RMD160 is not set
# CONFIG_CRYPTO_RMD256 is not set
# CONFIG_CRYPTO_RMD320 is not set
CONFIG_CRYPTO_SHA1=y
# CONFIG_CRYPTO_SHA1_SSSE3 is not set
# CONFIG_CRYPTO_SHA256_SSSE3 is not set
# CONFIG_CRYPTO_SHA512_SSSE3 is not set
CONFIG_CRYPTO_SHA256=y
# CONFIG_CRYPTO_SHA512 is not set
# CONFIG_CRYPTO_SHA3 is not set
# CONFIG_CRYPTO_SM3 is not set
# CONFIG_CRYPTO_STREEBOG is not set
# CONFIG_CRYPTO_TGR192 is not set
# CONFIG_CRYPTO_WP512 is not set
# CONFIG_CRYPTO_GHASH_CLMUL_NI_INTEL is not set

#
# Ciphers
#
CONFIG_CRYPTO_AES=y
CONFIG_CRYPTO_AES_TI=y
CONFIG_CRYPTO_AES_NI_INTEL=y
# CONFIG_CRYPTO_BLOWFISH is not set
# CONFIG_CRYPTO_BLOWFISH_X86_64 is not set
# CONFIG_CRYPTO_CAMELLIA is not set
# CONFIG_CRYPTO_CAMELLIA_X86_64 is not set
# CONFIG_CRYPTO_CAMELLIA_AESNI_AVX_X86_64 is not set
# CONFIG_CRYPTO_CAMELLIA_AESNI_AVX2_X86_64 is not set
# CONFIG_CRYPTO_CAST5 is not set
# CONFIG_CRYPTO_CAST5_AVX_X86_64 is not set
# CONFIG_CRYPTO_CAST6 is not set
# CONFIG_CRYPTO_CAST6_AVX_X86_64 is not set
# CONFIG_CRYPTO_DES is not set
# CONFIG_CRYPTO_DES3_EDE_X86_64 is not set
# CONFIG_CRYPTO_FCRYPT is not set
# CONFIG_CRYPTO_SALSA20 is not set
# CONFIG_CRYPTO_CHACHA20 is not set
# CONFIG_CRYPTO_CHACHA20_X86_64 is not set
# CONFIG_CRYPTO_SERPENT is not set
# CONFIG_CRYPTO_SERPENT_SSE2_X86_64 is not set
# CONFIG_CRYPTO_SERPENT_AVX_X86_64 is not set
# CONFIG_CRYPTO_SERPENT_AVX2_X86_64 is not set
# CONFIG_CRYPTO_SM4 is not set
# CONFIG_CRYPTO_TWOFISH is not set
# CONFIG_CRYPTO_TWOFISH_X86_64 is not set
# CONFIG_CRYPTO_TWOFISH_X86_64_3WAY is not set
# CONFIG_CRYPTO_TWOFISH_AVX_X86_64 is not set

#
# Compression
#
# CONFIG_CRYPTO_DEFLATE is not set
# CONFIG_CRYPTO_LZO is not set
# CONFIG_CRYPTO_842 is not set
# CONFIG_CRYPTO_LZ4 is not set
# CONFIG_CRYPTO_LZ4HC is not set
# CONFIG_CRYPTO_ZSTD is not set

#
# Random Number Generation
#
# CONFIG_CRYPTO_ANSI_CPRNG is not set
CONFIG_CRYPTO_DRBG_MENU=y
CONFIG_CRYPTO_DRBG_HMAC=y
# CONFIG_CRYPTO_DRBG_HASH is not set
# CONFIG_CRYPTO_DRBG_CTR is not set
CONFIG_CRYPTO_DRBG=y
CONFIG_CRYPTO_JITTERENTROPY=y
# CONFIG_CRYPTO_USER_API_HASH is not set
# CONFIG_CRYPTO_USER_API_SKCIPHER is not set
# CONFIG_CRYPTO_USER_API_RNG is not set
# CONFIG_CRYPTO_USER_API_AEAD is not set
CONFIG_CRYPTO_HASH_INFO=y

#
# Crypto library routines
#
CONFIG_CRYPTO_LIB_AES=y
# CONFIG_CRYPTO_LIB_BLAKE2S is not set
# CONFIG_CRYPTO_LIB_CHACHA is not set
# CONFIG_CRYPTO_LIB_CURVE25519 is not set
CONFIG_CRYPTO_LIB_POLY1305_RSIZE=11
# CONFIG_CRYPTO_LIB_POLY1305 is not set
# CONFIG_CRYPTO_LIB_CHACHA20POLY1305 is not set
CONFIG_CRYPTO_LIB_SHA256=y
CONFIG_CRYPTO_HW=y
# CONFIG_CRYPTO_DEV_PADLOCK is not set
CONFIG_CRYPTO_DEV_CCP=y
# CONFIG_CRYPTO_DEV_CCP_DD is not set
CONFIG_CRYPTO_DEV_SEV_GUEST_DD=y
# CONFIG_CRYPTO_DEV_AMLOGIC_GXL is not set
CONFIG_ASYMMETRIC_KEY_TYPE=y
CONFIG_ASYMMETRIC_PUBLIC_KEY_SUBTYPE=y
CONFIG_X509_CERTIFICATE_PARSER=y
# CONFIG_PKCS8_PRIVATE_KEY_PARSER is not set
# CONFIG_PKCS7_MESSAGE_PARSER is not set

#
# Certificates for signature checking
#
# CONFIG_SYSTEM_TRUSTED_KEYRING is not set
# CONFIG_SYSTEM_BLACKLIST_KEYRING is not set
# end of Certificates for signature checking

#
# Library routines
#
# CONFIG_PACKING is not set
CONFIG_BITREVERSE=y
CONFIG_GENERIC_STRNCPY_FROM_USER=y
CONFIG_GENERIC_STRNLEN_USER=y
CONFIG_GENERIC_NET_UTILS=y
CONFIG_GENERIC_FIND_FIRST_BIT=y
# CONFIG_CORDIC is not set
# CONFIG_PRIME_NUMBERS is not set
CONFIG_RATIONAL=y
CONFIG_GENERIC_PCI_IOMAP=y
CONFIG_GENERIC_IOMAP=y
CONFIG_ARCH_USE_CMPXCHG_LOCKREF=y
CONFIG_ARCH_HAS_FAST_MULTIPLIER=y
CONFIG_ARCH_USE_SYM_ANNOTATIONS=y
# CONFIG_CRC_CCITT is not set
CONFIG_CRC16=y
# CONFIG_CRC_T10DIF is not set
# CONFIG_CRC_ITU_T is not set
CONFIG_CRC32=y
# CONFIG_CRC32_SELFTEST is not set
CONFIG_CRC32_SLICEBY8=y
# CONFIG_CRC32_SLICEBY4 is not set
# CONFIG_CRC32_SARWATE is not set
# CONFIG_CRC32_BIT is not set
# CONFIG_CRC64 is not set
# CONFIG_CRC4 is not set
# CONFIG_CRC7 is not set
CONFIG_LIBCRC32C=y
# CONFIG_CRC8 is not set
# CONFIG_RANDOM32_SELFTEST is not set
CONFIG_ZLIB_INFLATE=y
CONFIG_ZLIB_DEFLATE=y
CONFIG_XZ_DEC=y
CONFIG_XZ_DEC_X86=y
CONFIG_XZ_DEC_POWERPC=y
CONFIG_XZ_DEC_IA64=y
CONFIG_XZ_DEC_ARM=y
CONFIG_XZ_DEC_ARMTHUMB=y
CONFIG_XZ_DEC_SPARC=y
CONFIG_XZ_DEC_BCJ=y
# CONFIG_XZ_DEC_TEST is not set
CONFIG_GENERIC_ALLOCATOR=y
CONFIG_ASSOCIATIVE_ARRAY=y
CONFIG_HAS_IOMEM=y
CONFIG_HAS_IOPORT_MAP=y
CONFIG_HAS_DMA=y
CONFIG_NEED_SG_DMA_LENGTH=y
CONFIG_NEED_DMA_MAP_STATE=y
CONFIG_ARCH_DMA_ADDR_T_64BIT=y
CONFIG_ARCH_HAS_FORCE_DMA_UNENCRYPTED=y
CONFIG_SWIOTLB=y
CONFIG_DMA_COHERENT_POOL=y
# CONFIG_DMA_API_DEBUG is not set
# CONFIG_DMA_MAP_BENCHMARK is not set
CONFIG_SGL_ALLOC=y
CONFIG_CPU_RMAP=y
CONFIG_DQL=y
CONFIG_NLATTR=y
CONFIG_CLZ_TAB=y
# CONFIG_IRQ_POLL is not set
CONFIG_MPILIB=y
CONFIG_OID_REGISTRY=y
CONFIG_UCS2_STRING=y
CONFIG_HAVE_GENERIC_VDSO=y
CONFIG_GENERIC_GETTIMEOFDAY=y
CONFIG_GENERIC_VDSO_TIME_NS=y
CONFIG_SG_POOL=y
CONFIG_ARCH_HAS_PMEM_API=y
CONFIG_ARCH_HAS_UACCESS_FLUSHCACHE=y
CONFIG_ARCH_HAS_COPY_MC=y
CONFIG_ARCH_STACKWALK=y
CONFIG_SBITMAP=y
# CONFIG_STRING_SELFTEST is not set
# end of Library routines

#
# Kernel hacking
#

#
# printk and dmesg options
#
CONFIG_PRINTK_TIME=y
# CONFIG_PRINTK_CALLER is not set
CONFIG_CONSOLE_LOGLEVEL_DEFAULT=7
CONFIG_CONSOLE_LOGLEVEL_QUIET=4
CONFIG_MESSAGE_LOGLEVEL_DEFAULT=4
CONFIG_BOOT_PRINTK_DELAY=y
CONFIG_DYNAMIC_DEBUG=y
CONFIG_DYNAMIC_DEBUG_CORE=y
CONFIG_SYMBOLIC_ERRNAME=y
CONFIG_DEBUG_BUGVERBOSE=y
# end of printk and dmesg options

#
# Compile-time checks and compiler options
#
CONFIG_DEBUG_INFO=y
# CONFIG_DEBUG_INFO_REDUCED is not set
# CONFIG_DEBUG_INFO_COMPRESSED is not set
# CONFIG_DEBUG_INFO_SPLIT is not set
# CONFIG_DEBUG_INFO_DWARF4 is not set
# CONFIG_DEBUG_INFO_BTF is not set
# CONFIG_GDB_SCRIPTS is not set
CONFIG_FRAME_WARN=1024
# CONFIG_STRIP_ASM_SYMS is not set
# CONFIG_READABLE_ASM is not set
# CONFIG_HEADERS_INSTALL is not set
# CONFIG_DEBUG_SECTION_MISMATCH is not set
# CONFIG_SECTION_MISMATCH_WARN_ONLY is not set
# CONFIG_DEBUG_FORCE_FUNCTION_ALIGN_32B is not set
CONFIG_FRAME_POINTER=y
CONFIG_STACK_VALIDATION=y
# CONFIG_DEBUG_FORCE_WEAK_PER_CPU is not set
# end of Compile-time checks and compiler options

#
# Generic Kernel Debugging Instruments
#
# CONFIG_MAGIC_SYSRQ is not set
CONFIG_DEBUG_FS=y
CONFIG_DEBUG_FS_ALLOW_ALL=y
# CONFIG_DEBUG_FS_DISALLOW_MOUNT is not set
# CONFIG_DEBUG_FS_ALLOW_NONE is not set
CONFIG_HAVE_ARCH_KGDB=y
# CONFIG_KGDB is not set
CONFIG_ARCH_HAS_UBSAN_SANITIZE_ALL=y
# CONFIG_UBSAN is not set
CONFIG_HAVE_ARCH_KCSAN=y
CONFIG_HAVE_KCSAN_COMPILER=y
# CONFIG_KCSAN is not set
# end of Generic Kernel Debugging Instruments

CONFIG_DEBUG_KERNEL=y
# CONFIG_DEBUG_MISC is not set

#
# Memory Debugging
#
# CONFIG_PAGE_EXTENSION is not set
# CONFIG_DEBUG_PAGEALLOC is not set
# CONFIG_PAGE_OWNER is not set
# CONFIG_PAGE_POISONING is not set
# CONFIG_DEBUG_RODATA_TEST is not set
CONFIG_ARCH_HAS_DEBUG_WX=y
# CONFIG_DEBUG_WX is not set
CONFIG_GENERIC_PTDUMP=y
# CONFIG_PTDUMP_DEBUGFS is not set
# CONFIG_DEBUG_OBJECTS is not set
# CONFIG_SLUB_STATS is not set
CONFIG_HAVE_DEBUG_KMEMLEAK=y
# CONFIG_DEBUG_KMEMLEAK is not set
# CONFIG_DEBUG_STACK_USAGE is not set
# CONFIG_SCHED_STACK_END_CHECK is not set
CONFIG_ARCH_HAS_DEBUG_VM_PGTABLE=y
# CONFIG_DEBUG_VM is not set
# CONFIG_DEBUG_VM_PGTABLE is not set
CONFIG_ARCH_HAS_DEBUG_VIRTUAL=y
# CONFIG_DEBUG_VIRTUAL is not set
# CONFIG_DEBUG_MEMORY_INIT is not set
# CONFIG_DEBUG_PER_CPU_MAPS is not set
CONFIG_ARCH_SUPPORTS_KMAP_LOCAL_FORCE_MAP=y
# CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP is not set
CONFIG_HAVE_ARCH_KASAN=y
CONFIG_HAVE_ARCH_KASAN_VMALLOC=y
CONFIG_CC_HAS_KASAN_GENERIC=y
CONFIG_CC_HAS_WORKING_NOSANITIZE_ADDRESS=y
# CONFIG_KASAN is not set
# end of Memory Debugging

# CONFIG_DEBUG_SHIRQ is not set

#
# Debug Oops, Lockups and Hangs
#
# CONFIG_PANIC_ON_OOPS is not set
CONFIG_PANIC_ON_OOPS_VALUE=0
CONFIG_PANIC_TIMEOUT=0
# CONFIG_SOFTLOCKUP_DETECTOR is not set
CONFIG_HARDLOCKUP_CHECK_TIMESTAMP=y
# CONFIG_HARDLOCKUP_DETECTOR is not set
# CONFIG_DETECT_HUNG_TASK is not set
# CONFIG_WQ_WATCHDOG is not set
# CONFIG_TEST_LOCKUP is not set
# end of Debug Oops, Lockups and Hangs

#
# Scheduler Debugging
#
CONFIG_SCHED_DEBUG=y
CONFIG_SCHED_INFO=y
# CONFIG_SCHEDSTATS is not set
# end of Scheduler Debugging

# CONFIG_DEBUG_TIMEKEEPING is not set

#
# Lock Debugging (spinlocks, mutexes, etc...)
#
CONFIG_LOCK_DEBUGGING_SUPPORT=y
# CONFIG_PROVE_LOCKING is not set
# CONFIG_LOCK_STAT is not set
# CONFIG_DEBUG_RT_MUTEXES is not set
# CONFIG_DEBUG_SPINLOCK is not set
# CONFIG_DEBUG_MUTEXES is not set
# CONFIG_DEBUG_WW_MUTEX_SLOWPATH is not set
# CONFIG_DEBUG_RWSEMS is not set
# CONFIG_DEBUG_LOCK_ALLOC is not set
# CONFIG_DEBUG_ATOMIC_SLEEP is not set
# CONFIG_DEBUG_LOCKING_API_SELFTESTS is not set
# CONFIG_LOCK_TORTURE_TEST is not set
# CONFIG_WW_MUTEX_SELFTEST is not set
# CONFIG_SCF_TORTURE_TEST is not set
# CONFIG_CSD_LOCK_WAIT_DEBUG is not set
# end of Lock Debugging (spinlocks, mutexes, etc...)

CONFIG_STACKTRACE=y
# CONFIG_WARN_ALL_UNSEEDED_RANDOM is not set
# CONFIG_DEBUG_KOBJECT is not set

#
# Debug kernel data structures
#
# CONFIG_DEBUG_LIST is not set
# CONFIG_DEBUG_PLIST is not set
# CONFIG_DEBUG_SG is not set
# CONFIG_DEBUG_NOTIFIERS is not set
# CONFIG_BUG_ON_DATA_CORRUPTION is not set
# end of Debug kernel data structures

# CONFIG_DEBUG_CREDENTIALS is not set

#
# RCU Debugging
#
# CONFIG_RCU_SCALE_TEST is not set
# CONFIG_RCU_TORTURE_TEST is not set
# CONFIG_RCU_REF_SCALE_TEST is not set
CONFIG_RCU_CPU_STALL_TIMEOUT=21
# CONFIG_RCU_TRACE is not set
# CONFIG_RCU_EQS_DEBUG is not set
# end of RCU Debugging

# CONFIG_DEBUG_WQ_FORCE_RR_CPU is not set
# CONFIG_DEBUG_BLOCK_EXT_DEVT is not set
# CONFIG_CPU_HOTPLUG_STATE_CONTROL is not set
# CONFIG_LATENCYTOP is not set
CONFIG_USER_STACKTRACE_SUPPORT=y
CONFIG_HAVE_FUNCTION_TRACER=y
CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y
CONFIG_HAVE_DYNAMIC_FTRACE=y
CONFIG_HAVE_DYNAMIC_FTRACE_WITH_REGS=y
CONFIG_HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS=y
CONFIG_HAVE_DYNAMIC_FTRACE_WITH_ARGS=y
CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y
CONFIG_HAVE_SYSCALL_TRACEPOINTS=y
CONFIG_HAVE_FENTRY=y
CONFIG_HAVE_C_RECORDMCOUNT=y
CONFIG_TRACING_SUPPORT=y
# CONFIG_FTRACE is not set
# CONFIG_SAMPLES is not set
CONFIG_ARCH_HAS_DEVMEM_IS_ALLOWED=y
# CONFIG_STRICT_DEVMEM is not set

#
# x86 Debugging
#
CONFIG_TRACE_IRQFLAGS_SUPPORT=y
CONFIG_TRACE_IRQFLAGS_NMI_SUPPORT=y
# CONFIG_X86_VERBOSE_BOOTUP is not set
# CONFIG_EARLY_PRINTK is not set
# CONFIG_DEBUG_TLBFLUSH is not set
CONFIG_HAVE_MMIOTRACE_SUPPORT=y
# CONFIG_X86_DECODER_SELFTEST is not set
CONFIG_IO_DELAY_0X80=y
# CONFIG_IO_DELAY_0XED is not set
# CONFIG_IO_DELAY_UDELAY is not set
# CONFIG_IO_DELAY_NONE is not set
# CONFIG_DEBUG_BOOT_PARAMS is not set
# CONFIG_CPA_DEBUG is not set
# CONFIG_DEBUG_ENTRY is not set
# CONFIG_DEBUG_NMI_SELFTEST is not set
# CONFIG_X86_DEBUG_FPU is not set
# CONFIG_UNWINDER_ORC is not set
CONFIG_UNWINDER_FRAME_POINTER=y
# CONFIG_UNWINDER_GUESS is not set
# end of x86 Debugging

#
# Kernel Testing and Coverage
#
# CONFIG_KUNIT is not set
# CONFIG_NOTIFIER_ERROR_INJECTION is not set
CONFIG_FUNCTION_ERROR_INJECTION=y
# CONFIG_FAULT_INJECTION is not set
CONFIG_ARCH_HAS_KCOV=y
CONFIG_CC_HAS_SANCOV_TRACE_PC=y
# CONFIG_KCOV is not set
# CONFIG_RUNTIME_TESTING_MENU is not set
# CONFIG_MEMTEST is not set
# CONFIG_HYPERV_TESTING is not set
# end of Kernel Testing and Coverage
# end of Kernel hacking

================
File: ./scripts/clean.sh
================

rm ./target/target/release/.cargo-lock
rm ./target/release/.cargo-lock

================
File: ./scripts/erase.py
================

import os;
def find(input_string, char):  
    count = 0  
    for c in input_string:  
        if c == char:  
            count += 1  
    return count
def create_file_and_dir(file_name):  
    # Create the directory if it doesn't exist 
    path = os.path.dirname(file_name); 
    if not os.path.exists(path):  
        os.makedirs(path)  
      
    # Create the file in the directory  
    file_path = os.path.join(path, file_name)  
    with open(file_path, 'w') as file:  
        file.write("")  
def find_from_set(input, keywords):
        for k in keywords:
                if k in input:
                        i = input.find(k)
                        if input[i-1].isalpha() or input[i+len(k)].isalpha() or input[i-1]=="_" or input[i+len(k)]=="_":
                                continue
                        print(input, k)
                        return k
        return None
def get_spaces(line):
        out = ""
        for c in line:
                if c.isspace():
                        out += c
                else:
                        break
        return out
key2 = ["proof", "open spec fn", "closed spec fn"]
key1 = ["invariant", "ensures", "requires", "proof", "spec fn"]
def erase(infile: str, outfile:str):
        ghost_count = 0
        exe_count = 0
        ghost_lines = []
        alllines = []
        exe_lines = []
        lines = []
        ghost = False
        exe_fn = 0
        stack = ["",""]
        brackets = 0
        count = 0
        with open(infile, "r") as f:
                for line in f:
                        if line.startswith("use") or line.startswith("//") or line.startswith("pub use"):
                                continue
                        if "ghost" in line:
                                ghost_lines.append(line)
                                continue                            
                        count += 1
                        left = find(line, "{")
                        right = find(line, "}")
                        if stack[-1] in ["invariant", "ensures", "requires"]:
                                stack.append(get_spaces(line))
                                #print("stack", count, stack, get_spaces(line))
                        if stack[-1] in key2 or stack[-1] == "{":
                                while left > right:
                                        stack.append("{")
                                        left = left -1
                                while right > left:
                                        stack.pop()
                                        right = right -1
                        keyword = find_from_set(line, ["invariant", "ensures", "requires", "proof", "open spec fn"])
                        if ghost:
                                if stack[-1] in key2:
                                        ghost = False
                                        stack.pop()
                                        ghost_count = ghost_count + 1
                                        ghost_lines.append(line)
                                        #print("proof ends", line)
                                        continue

                                if  stack[-2] in ["invariant", "ensures", "requires"]:
                                        if not line.startswith(stack[-1]):
                                                ghost = False
                                                stack.pop()
                                                stack.pop()
                                                #print("after pop", stack)
                        if keyword in key2:
                                ghost_count = ghost_count + 1
                                ghost = True
                                stack.append(keyword)
                                while left > right:
                                        stack.append("{")
                                        left = left -1
                                while right > left:
                                        stack.pop()
                                        right = right -1
                                #print("proof starts at", count)
                        elif keyword:
                                ghost = True
                                stack.append(keyword)
                        else:
                                exe_count =  exe_count + 1
                        if ghost:
                                #print("ghost:",line)
                                ghost_lines.append(line)
                        else:
                                #print("exe:",line)
                                exe_lines.append(line)
        ghost_count = len(ghost_lines)
        exe_count = count - ghost_count

        #print("ghost line:", len(ghost_lines), "exe:", count -  len(ghost_lines))
        create_file_and_dir(outfile)
        with open(outfile, "w+") as f:
                f.write("".join(exe_lines))
        return (count, ghost_count, exe_count)


DIRNAME = "/root/snp/verismo/source/monitor_mod/src"

def get_all_files_recursively(directory):  
    all_files = []  
  
    for root, dirs, files in os.walk(directory):  
        for file in files:  
            file_path = os.path.join(root, file)  
            all_files.append(file_path[len(directory)+1:])  
  
    return all_files

def get_files():
        files = []
        filelist = get_all_files_recursively(DIRNAME)
        #for filename in os.listdir(DIRNAME):
        for filename in filelist: 
                if filename.endswith(".rs"):  
                        files.append(filename)
        return files
files = get_files()
arch_count = 0
exe_count = 0
  
for filename in files:
        if ("resource" not in filename) and ("arch" not in filename) and (not filename.startswith("output")) and (not filename.endswith("s.rs")) and (not "spec" in filename) and (not "pervasive" in filename) and (not filename.endswith("p.rs")) and "unverified" not in filename:
                print(filename)
                infile = os.path.join(DIRNAME, filename)
                exe_file = os.path.join(os.path.join(DIRNAME, "output"), filename)
                count, ghost, exe = erase(infile, exe_file)
                arch_count += count
                if "arch" in filename or "resource" in filename:
                        exe = 0
                exe_count += exe

for filename in files:
        allline = []
        allfile_noheader = os.path.join(os.path.join(DIRNAME, "output-all"), filename)
        infile = os.path.join(DIRNAME, filename)
        with open(infile, "r") as f:
                for line in f:
                        if line.startswith("use") or line.startswith("//") or line.startswith("pub use"):
                                continue
                        allline.append(line)
        create_file_and_dir(allfile_noheader)
        with open(allfile_noheader, "w+") as f:
                f.write("".join(allline))
#erase("/root/snp/verismo/source/monitor_mod/src/verismo/alloc/exe.rs", "/root/snp/verismo/source/monitor_mod/src/output/verismo/alloc/exe.rs")
print(arch_count, exe_count)

import subprocess
DIRNAME1 = DIRNAME+"/output"
DIRNAME2 = DIRNAME

print("Rust: alloc")
subprocess.run(["cloc", DIRNAME1+"/verismo/alloc"]) 
subprocess.run(["cloc", DIRNAME2+"/verismo/alloc"]) 

print("Rust:Smart pointer")
subprocess.run(["cloc", DIRNAME1+"/verismo/data", DIRNAME1+"/verismo/global"]) 
subprocess.run(["cloc", DIRNAME2+"/verismo/data", DIRNAME2+"/verismo/global"]) 


print("Rust:Lock")
subprocess.run(["cloc", DIRNAME1+"/verismo/lock"]) 
subprocess.run(["cloc", DIRNAME2+"/verismo/lock"]) 

print("Rust: tspec")
subprocess.run(["cloc", DIRNAME1+"/tspec"]) 
subprocess.run(["cloc", DIRNAME2+"/tspec"]) 

print("Rust:mem")
subprocess.run(["cloc", DIRNAME1+"/verismo/mem", DIRNAME1+"/verismo/snp/rmp", DIRNAME1+"/verismo/state"])
cmd =["cloc", DIRNAME2+"/verismo/mem", DIRNAME2+"/verismo/snp/rmp", DIRNAME2+"/verismo/state"]
subprocess.run(cmd) 
print(cmd)
print("Rust: init")
subprocess.run(["cloc", DIRNAME1+"/verismo/idt", DIRNAME1+"/verismo/init", DIRNAME1+"/verismo/boot",DIRNAME1+"/verismo/mshyper",DIRNAME1+"/verismo/msr", DIRNAME1+"/verismo/snp/cpuid", DIRNAME1+"/verismo/snp/cpu"]) 
subprocess.run(["cloc",DIRNAME1+"/verismo/idt", DIRNAME2+"/verismo/init", DIRNAME2+"/verismo/boot",DIRNAME2+"/verismo/mshyper",DIRNAME2+"/verismo/msr", DIRNAME2+"/verismo/snp/cpuid", DIRNAME2+"/verismo/snp/cpu"])

print("Rust: vmpl")
subprocess.run(["cloc", DIRNAME1+"/verismo/vmpl", DIRNAME1+"/verismo/snp/secret", DIRNAME1+"/crypto", DIRNAME1+"/verismo/snp/ghcb"]) 
subprocess.run(["cloc", DIRNAME2+"/verismo/vmpl", DIRNAME2+"/verismo/snp/secret", DIRNAME1+"/crypto", DIRNAME2+"/verismo/snp/ghcb"])

================
File: ./SECURITY.md
================

<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). 

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
  * Full paths of source file(s) related to the manifestation of the issue
  * The location of the affected source code (tag/branch/commit or direct URL)
  * Any special configuration required to reproduce the issue
  * Step-by-step instructions to reproduce the issue
  * Proof-of-concept or exploit code (if possible)
  * Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).

<!-- END MICROSOFT SECURITY.MD BLOCK -->

