================
File: ./CODE_OF_CONDUCT.md
================

# Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns

================
File: ./pmsafe/Cargo.toml
================

[package]
name = "pmsafe"
version = "0.1.0"
edition = "2021"

[dependencies]
proc-macro2 = "1.0.39"
syn = { version = "2.0.66", features = ["extra-traits"] }
quote = "1.0"

[lib]
proc-macro = true
================
File: ./pmsafe/Cargo.lock
================

# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 3

[[package]]
name = "pmsafe"
version = "0.1.0"
dependencies = [
 "proc-macro2",
 "quote",
 "syn",
]

[[package]]
name = "proc-macro2"
version = "1.0.85"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "22244ce15aa966053a896d1accb3a6e68469b97c7f33f284b99f0d576879fc23"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "quote"
version = "1.0.36"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0fa76aaf39101c457836aec0ce2316dbdc3ab723cdda1c6bd4e6ad4208acaca7"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "syn"
version = "2.0.66"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c42f3f41a2de00b01c0aaad383c5a45241efc8b2d1eda5661812fda5f3cdcff5"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "unicode-ident"
version = "1.0.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3354b9ac3fae1ff6755cb6db53683adb661634f67557942dea4facebec0fee4b"

================
File: ./pmsafe/src/lib.rs
================

//! This crate defines three proc macros to derive PM-safety-related
//! traits:
//! - PmSafe, a marker trait indicating whether a structure is safe 
//!   to write to PM
//! - PmSized, a trait that helps calculate the size of structures
//!   so they can be safely read from PM
//! - Plus several related auxiliary traits that help us work around
//!   limitations in Verus and Rust.
//! The macros themselves are documented in pmsafe_macros.rs.

extern crate proc_macro;
use proc_macro::TokenStream;

use crate::pmsafe_macros::*;

mod pmsafe_macros;

#[proc_macro_derive(PmSafe)]
pub fn derive_pmsafe(input: TokenStream) -> TokenStream {
    let ast: syn::DeriveInput = syn::parse(input).unwrap();
    check_pmsafe(&ast)
}

#[proc_macro_derive(PmSized)]
pub fn pmsized(input: TokenStream) -> TokenStream {
    let ast: syn::DeriveInput = syn::parse(input).unwrap();
    generate_pmsized(&ast)
}

#[proc_macro]
pub fn pmsized_primitive(input: TokenStream) -> TokenStream {
    let ty: syn::Type = syn::parse(input).unwrap();
    generate_pmsized_primitive(&ty)
}
================
File: ./pmsafe/src/pmsafe_macros.rs
================

//! This file contains the implementation of derive macros
//! for PmSafe and PmSized. These implementations are TRUSTED
//! and must be manually audited.

use proc_macro::TokenStream;
use syn::{self, spanned::Spanned};
use quote::{quote, quote_spanned};

// This function is used by the PmSafe derive macro to check whether 
// a deriving type is, in fact, PmSafe. This requires two main checks:
// 1. The type must be repr(C). This is not really a strict requirement for 
//    writing to PM -- as long as we know the size of a type, it may have any
//    in-memory layout when writing to PM -- but it makes it easier to determine
//    the size, and other safety-related traits require this as well.
// 2. All fields of the deriving type must be PmSafe. PmSafe primitive types are
//    defined in storage_node/src/pmem/traits_t.rs. 
// 
// The repr(C) requirement is checked by checking the attributes of the deriving type.
// The PmSafe fields requirement is performed by adding trivial trait bounds to
// the unsafe implementation of PmSafe generated for the deriving type.
// For example, the generated implementation of PmSafe for the following type:
// ```
// struct Foo {
//      val1: u8,
//      val2: u16,
//      val3: u32
// }
// ```
// would look like:
// ```
// unsafe impl PmSafe for Foo
//      where u8: PmSafe, u16: PmSafe, u32: PmSafe {}
// ```
// These trait bounds are easily checkable by the compiler. Compilation will
// fail if we attempt to derive PmSafe on a struct with a field of type, e.g., 
// *const u8, as the bound `const *u8: PmSafe` is not met.
pub fn check_pmsafe(ast: &syn::DeriveInput) -> TokenStream {
    let name = &ast.ident;

    let attrs = &ast.attrs;
    // Check that the structure is repr(C)
    if let Err(e) = check_repr_c(name, attrs) {
        return e;
    }

    let data = &ast.data;
    // Obtain a list of the types of the fields in the structure.
    let mut types = match get_types(name, data) {
        Ok(types) => types,
        Err(e) => return e,
    };
    // not strictly necessary, but makes the expanded macro look nicer
    types.dedup();

    let gen = quote! {
        unsafe impl PmSafe for #name 
            where 
            #( #types: PmSafe, )*
        {}
    };
    gen.into()
}

// This function checks whether the struct has the repr(C) attribute so that we can
// trigger a compiler error if it doesn't. The repr(C) attribute ensures that the 
// structure has a consistent layout in memory, which is useful when reading and writing
// values to PM.
pub fn check_repr_c(name: &syn::Ident, attrs: &Vec<syn::Attribute>) ->  Result<(), TokenStream>  
{
    // look for an attribute with "repr(C)"
    for attr in attrs {
        let meta = &attr.meta;
        match meta {
            syn::Meta::List(list) => {
                if list.path.is_ident("repr") {
                    let tokens = proc_macro::TokenStream::from(list.tokens.clone());
                    for token in tokens.into_iter() {
                        match token {
                            proc_macro::TokenTree::Ident(ident) => {
                                if ident.to_string() == "C" {
                                    return Ok(());
                                }
                            }
                            _ => {}
                        }
                    }
                }
            }
            _ => {}
        }
    }
    Err(quote_spanned! {
        name.span() =>
        compile_error!("PmSafe can only be derived for types with repr(C)");
    }.into())
}

// This function obtains a list of the types of the fields of a structure. We do not
// attempt to process the field names to keep things simple.
pub fn get_types<'a>(name: &'a syn::Ident, data: &'a syn::Data) -> Result<Vec<&'a syn::Type>, TokenStream> 
{
    let mut type_vec = Vec::new();
    match data {
        syn::Data::Struct(data) => {
            let fields = &data.fields; 
            match fields {
                syn::Fields::Named(fields) => {
                    for field in fields.named.iter() {
                        let ty = &field.ty;
                        type_vec.push(ty);
                    }
                    Ok(type_vec)
                }
                _ => Err(
                    quote_spanned! {
                        name.span() =>
                        compile_error!("PmSafe can only be derived for structs with named fields");
                    }.into()
                )
            }
        }
        _ => {
            Err(quote_spanned! {
                name.span() =>
                compile_error!("PmSafe can only be derived for structs");
            }.into())
        }
    }
}

// This function generates an implementation of the PmSized trait for the PmSized
// derive macro. It also generates implementations for SpecPmSized, ConstPmSized,
// UnsafeSpecPmSized, and two compile-time assertions to check that we calculate
// the size of each type correctly.
pub fn generate_pmsized(ast: &syn::DeriveInput) -> TokenStream {
    let name = &ast.ident;
   
    // PmSized structures must be repr(C), or the size/align calculation will not be correct.
    // repr(Rust) structures do not have a standardized, deterministic memory layout.
    let attrs = &ast.attrs;
    if let Err(e) = check_repr_c(name, attrs) {
        return e;
    }

    let data = &ast.data;
    let types = match get_types(name, data) {
        Ok(types) => types,
        Err(e) => return e,
    };

    // The size of a repr(C) struct is determined by the following algorithm, from the Rust reference:
    // https://doc.rust-lang.org/reference/type-layout.html#reprc-structs
    // "Start with a current offset of 0 bytes.
    // 
    // For each field in declaration order in the struct, first determine the size and alignment of the field.
    // If the current offset is not a multiple of the field's alignment, then add padding bytes to the current
    // offset until it is a multiple of the field's alignment. The offset for the field is what the current offset is
    // now. Then increase the current offset by the size of the field."
    // 
    // The required padding is calculated by the const fn `padding_needed`, which is implemented in the pmem module
    // and verified. We use the associated constants from ConstPmSized to obtain the alignment and size of the struct.
    //
    // Ideally the result of this code would be verified to match the result of the spec size code generated below,
    // but it is not currently possible to have const trait fn implementations in Rust, Verus' support for const 
    // trait impls is not mature enough and runs into panics in this project, so the const exec fns that calculate 
    // struct size can't be visible to the verifier. We could generate a non-associated constant fn for every 
    // struct that derives the trait, but generating such functions is tricky and ugly. 
    let mut exec_tokens_vec = Vec::new();
    for ty in types.iter() {
        let new_tokens = quote! {
            let offset: usize = offset + <#ty>::SIZE + padding_needed(offset, <#ty>::ALIGN); 
        };
        exec_tokens_vec.push(new_tokens);
    }

    // We generate the size of a repr(C) struct in spec code using the same approach as in exec code, except we use 
    // spec functions to obtain the size, alignment, and padding needed. 
    let mut spec_tokens_vec = Vec::new();
    for ty in types.iter() {
        let new_tokens = quote! {
            let offset: ::builtin::int = offset + <#ty>::spec_size_of() + spec_padding_needed(offset, <#ty>::spec_align_of()); 
        };
        spec_tokens_vec.push(new_tokens);
    }

    // The alignment of a repr(C) struct is the alignment of the most-aligned field in it (i.e. the field with the largest
    // alignment). We currently unroll all of the fields and check which has the largest alignment without using a loop;
    // to make the generated code more concise, we could put the alignments in an array and use a while loop over it 
    // (for loops are not supported in const contexts).
    let mut exec_align_vec = Vec::new();
    for ty in types.iter() {
        let new_tokens = quote! {
            if largest_alignment <= <#ty>::ALIGN {
                largest_alignment = <#ty>::ALIGN;
            }
        };
        exec_align_vec.push(new_tokens);
    }

    // Since the executable implementation of alignment calculation requires a mutable value and/or a loop, it's not 
    // straightforward to generate an identical spec function for it. Instead, we just create a sequence of all of the 
    // alignments and find the maximum. If we ever want to prove that the alignment calculation is correct, the exec
    // side code generation will have to include proof code.
    let spec_alignment = quote! {
        let alignment_seq = seq![#(<#types>::spec_align_of(),)*];
        alignment_seq.max()
    };

    // This is the name of the constant that will perform the compile-time assertion that the calculated size of the struct
    // is equal to the real size. This is not an associated constant in an external trait implementation because the compiler 
    // will optimize the check out if it lives in an associated constant that is never used in any methods. In constrast,
    // it will always be evaluated if it is a standalone constant.
    let size_check = syn::Ident::new(&format!("SIZE_CHECK_{}", name.to_string().to_uppercase()), name.span());
    let align_check = syn::Ident::new(&format!("ALIGN_CHECK_{}", name.to_string().to_uppercase()), name.span());

    let gen = quote! {
        ::builtin_macros::verus!(

            impl SpecPmSized for #name {
                open spec fn spec_size_of() -> ::builtin::int 
                {
                    let offset: ::builtin::int = 0;
                    #( #spec_tokens_vec )*
                    offset
                }      

                open spec fn spec_align_of() -> ::builtin::int 
                {
                    #spec_alignment
                }
            }  
        );

        unsafe impl PmSized for #name {
            fn size_of() -> usize { Self::SIZE }
            fn align_of() -> usize { Self::ALIGN }
        }

        unsafe impl ConstPmSized for #name {
            const SIZE: usize = {
                let offset: usize = 0;
                #( #exec_tokens_vec )*
                offset
            };
            const ALIGN: usize = {
                let mut largest_alignment: usize = 0;
                #( #exec_align_vec )*
                largest_alignment
            };
            
    }

    const #size_check: usize = (core::mem::size_of::<#name>() == <#name>::SIZE) as usize - 1;
    const #align_check: usize = (core::mem::align_of::<#name>() == <#name>::ALIGN) as usize - 1;

    unsafe impl UnsafeSpecPmSized for #name {}


    };
    gen.into()
}

// For most types, alignment is the same as size on x86, EXCEPT for 
// u128/i128, which have an alignment of 8 bytes.
const BOOL_SIZE: usize = 1;
const CHAR_SIZE: usize = 4;
const I8_SIZE: usize = 1;
const I16_SIZE: usize = 2;
const I32_SIZE: usize = 4;
const I64_SIZE: usize = 8;
const I128_SIZE: usize = 16;
const I128_ALIGNMENT: usize = 8;
const ISIZE_SIZE: usize = 8;
const U8_SIZE: usize = 1;
const U16_SIZE: usize = 2;
const U32_SIZE: usize = 4;
const U64_SIZE: usize = 8;
const U128_SIZE: usize = 16;
const U128_ALIGNMENT: usize = 8;
const USIZE_SIZE: usize = 8;

// This function is called by the pmsized_primitive! proc macro and generates an 
// implementation of PmSized, ConstPmSized, SpecPmSized, and UnsafeSpecPmSized
// primitive types. The verifier needs to be aware of their size and alignment at 
// verification time, so we provide this in the constants above and generate 
// implementations based on the values of these constants. The constants don't need
// to be audited, since the compile-time assertion will ensure they are correct,
// but we do need to manually ensure that the match statement in this function
// maps each type to the correct constant.
pub fn generate_pmsized_primitive(ty: &syn::Type) -> TokenStream {
    let (size, align, ty_name) = match ty {
        syn::Type::Path(type_path) => {
            match type_path.path.get_ident() {
                Some(ident) => {
                    let ty_name = ident.to_string();
                    let (size, align) = match ty_name.as_str() {
                        "bool" => (BOOL_SIZE, BOOL_SIZE),
                        "char" => (CHAR_SIZE, CHAR_SIZE),
                        "i8" => (I8_SIZE, I8_SIZE),
                        "i16" => (I16_SIZE, I16_SIZE),
                        "i32" => (I32_SIZE, I32_SIZE),
                        "i64" => (I64_SIZE, I64_SIZE),
                        "i128" => (I128_SIZE, I128_ALIGNMENT),
                        "isize" => (ISIZE_SIZE, ISIZE_SIZE),
                        "u8" => (U8_SIZE, U8_SIZE),
                        "u16" => (U16_SIZE, U16_SIZE),
                        "u32" => (U32_SIZE, U32_SIZE),
                        "u64" => (U64_SIZE, U64_SIZE),
                        "u128" => (U128_SIZE, U128_ALIGNMENT),
                        "usize" => (USIZE_SIZE, USIZE_SIZE),
                        _ => {
                            return quote_spanned! {
                                ty.span() =>
                                compile_error!("pmsized_primitive can only be used on primitive types");
                            }.into()
                        }
                        
                    };
                    (size, align, ty_name)
                }
                None => return quote_spanned! {
                    ty.span() =>
                    compile_error!("pmsized_primitive can only be used on primitive types");
                }.into()
            }
        }
        _ => return quote_spanned! {
            ty.span() =>
            compile_error!("pmsized_primitive can only be used on primitive types");
        }.into()
    };

    let size_check = syn::Ident::new(&format!("SIZE_CHECK_{}", ty_name.to_string().to_uppercase()), ty.span());
    let align_check = syn::Ident::new(&format!("ALIGN_CHECK_{}", ty_name.to_string().to_uppercase()), ty.span());

    // Primitive types have hardcoded size and alignment values
    let gen = quote!{
        ::builtin_macros::verus!(
            impl SpecPmSized for #ty {
                open spec fn spec_size_of() -> ::builtin::int { #size as ::builtin::int }
                open spec fn spec_align_of() -> ::builtin::int { #align as ::builtin::int }
            }
        );

        unsafe impl PmSized for #ty {
            fn size_of() -> usize { Self::SIZE }
            fn align_of() -> usize { Self::ALIGN }
        }

        unsafe impl ConstPmSized for #ty {
            const SIZE: usize = #size;
            const ALIGN: usize = #align;
        }

        const #size_check: usize = (core::mem::size_of::<#ty>() == <#ty>::SIZE) as usize - 1;
        const #align_check: usize = (core::mem::align_of::<#ty>() == <#ty>::ALIGN) as usize - 1;

        unsafe impl UnsafeSpecPmSized for #ty {}
    };
    gen.into()
}

================
File: ./pmemlog/Cargo.toml
================

[package]
name = "pmemlog"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
builtin_macros = { git = "https://github.com/verus-lang/verus.git", rev="a53f39271666ac7dc9f455b6267da4c49a5f75c6" }
builtin = { git = "https://github.com/verus-lang/verus.git", rev="a53f39271666ac7dc9f455b6267da4c49a5f75c6" }
vstd = { git = "https://github.com/verus-lang/verus.git", rev="a53f39271666ac7dc9f455b6267da4c49a5f75c6" }
crc64fast = "1.0.0"

================
File: ./pmemlog/Cargo.lock
================

# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 3

[[package]]
name = "builtin"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git?rev=309a0eca409eeec23d3f7142ca27fb3151c79025#309a0eca409eeec23d3f7142ca27fb3151c79025"

[[package]]
name = "builtin_macros"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git?rev=309a0eca409eeec23d3f7142ca27fb3151c79025#309a0eca409eeec23d3f7142ca27fb3151c79025"
dependencies = [
 "prettyplease_verus",
 "proc-macro2",
 "quote",
 "syn",
 "syn_verus",
 "synstructure",
]

[[package]]
name = "crc64fast"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bd3d10c557924c719a61e583165e31a0d8131b20415d6858983e67d1eb8a82bd"

[[package]]
name = "pmemlog"
version = "0.1.0"
dependencies = [
 "builtin",
 "builtin_macros",
 "crc64fast",
 "vstd",
]

[[package]]
name = "prettyplease_verus"
version = "0.1.15"
source = "git+https://github.com/verus-lang/verus.git?rev=309a0eca409eeec23d3f7142ca27fb3151c79025#309a0eca409eeec23d3f7142ca27fb3151c79025"
dependencies = [
 "proc-macro2",
 "syn_verus",
]

[[package]]
name = "proc-macro2"
version = "1.0.66"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "18fb31db3f9bddb2ea821cde30a9f70117e3f119938b5ee630b7403aa6e2ead9"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "quote"
version = "1.0.32"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "50f3b39ccfb720540debaa0164757101c08ecb8d326b15358ce76a62c7e85965"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "syn"
version = "1.0.109"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72b64191b275b66ffe2469e8af2c1cfe3bafa67b529ead792a6d0160888b4237"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "syn_verus"
version = "1.0.95"
source = "git+https://github.com/verus-lang/verus.git?rev=309a0eca409eeec23d3f7142ca27fb3151c79025#309a0eca409eeec23d3f7142ca27fb3151c79025"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "synstructure"
version = "0.12.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f36bdaa60a83aca3921b5259d5400cbf5e90fc51931376a9bd4a0eb79aa7210f"
dependencies = [
 "proc-macro2",
 "quote",
 "syn",
 "unicode-xid",
]

[[package]]
name = "unicode-ident"
version = "1.0.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "301abaae475aa91687eb82514b328ab47a211a533026cb25fc3e519b86adfc3c"

[[package]]
name = "unicode-xid"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f962df74c8c05a667b5ee8bcf162993134c104e96440b663c8daa176dc772d8c"

[[package]]
name = "vstd"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git?rev=309a0eca409eeec23d3f7142ca27fb3151c79025#309a0eca409eeec23d3f7142ca27fb3151c79025"
dependencies = [
 "builtin",
 "builtin_macros",
]

================
File: ./pmemlog/src/infinitelog_t.rs
================

/*

  This file models the abstraction of an infinite log that our log
  implementation is supposed to implement.

*/

use crate::pmemspec_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;
use vstd::set::*;

verus! {
    #[verifier::ext_equal]
    pub struct AbstractInfiniteLogState {
        pub head: int,
        pub log: Seq<u8>,
        pub capacity: int,
    }

    impl AbstractInfiniteLogState {
        pub open spec fn initialize(capacity: int) -> Self {
            Self{ head: 0int, log: Seq::<u8>::empty(), capacity: capacity }
        }

        pub open spec fn append(self, bytes: Seq<u8>) -> Self {
            Self{ head: self.head, log: self.log + bytes, capacity: self.capacity }
        }

        pub open spec fn advance_head(self, new_head: int) -> Self
        {
            if self.head <= new_head <= self.head + self.log.len() {
                let new_log = self.log.subrange(new_head - self.head, self.log.len() as int);
                Self{ head: new_head, log: new_log, capacity: self.capacity }
            }
            else {
                self
            }
        }
    }

}

================
File: ./pmemlog/src/main_t.rs
================

use std::fmt::Write;

use crate::infinitelog_t::*;
use crate::logimpl_v::*;
use crate::pmemspec_t::*;
use crate::sccf::CheckPermission;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;
use vstd::set::*;
use vstd::slice::*;

verus! {

    pub open spec fn recovery_view() -> (result: FnSpec(Seq<u8>) -> Option<AbstractInfiniteLogState>)
    {
        |c| UntrustedLogImpl::recover(c)
    }

    pub open spec fn read_correct_modulo_corruption(bytes: Seq<u8>, true_bytes: Seq<u8>,
                                                    impervious_to_corruption: bool) -> bool
    {
        if impervious_to_corruption {
            bytes == true_bytes
        }
        else {
            exists |addrs: Seq<int>| {
                &&& all_elements_unique(addrs)
                &&& #[trigger] maybe_corrupted(bytes, true_bytes, addrs)
            }
        }
    }

    /// A `TrustedPermission` indicates what states of persistent
    /// memory are permitted. The struct isn't public, so it can't be
    /// created outside of this file. As a further defense against one
    /// being created outside this file, its fields aren't public, and
    /// the constructor `TrustedPermission::new` isn't public.

    struct TrustedPermission {
        ghost is_state_allowable: FnSpec(Seq<u8>) -> bool
    }

    impl CheckPermission<Seq<u8>> for TrustedPermission {
        closed spec fn check_permission(&self, state: Seq<u8>) -> bool {
            (self.is_state_allowable)(state)
        }
    }

    impl TrustedPermission {
        proof fn new(cur: Seq<u8>, next: FnSpec(AbstractInfiniteLogState, AbstractInfiniteLogState) -> bool)
                     -> (tracked perm: Self)
            ensures
                forall |s| #[trigger] perm.check_permission(s) <==>
                    crate::sccf::is_state_allowable(cur, s, recovery_view(), next)
        {
            Self { is_state_allowable: |s| crate::sccf::is_state_allowable(cur, s, recovery_view(), next) }
        }
    }

    /// A `InfiniteLogImpl` wraps an `UntrustedLogImpl` to provide the
    /// executable interface that turns a persistent memory region
    /// into an effectively infinite log. It provides a simple
    /// interface to higher-level code.
    pub struct InfiniteLogImpl<PM: PersistentMemory> {
        untrusted_log_impl: UntrustedLogImpl,
        wrpm: WriteRestrictedPersistentMemory<TrustedPermission, PM>,
    }

    #[derive(Debug)]
    pub enum InfiniteLogErr {
        InsufficientSpaceForSetup { required_space: u64 },
        CantReadBeforeHead { head: u64 },
        CantReadPastTail { tail: u64 },
        InsufficientSpaceForAppend { available_space: u64 },
        CRCMismatch,
        CantAdvanceHeadPositionBeforeHead { head: u64 },
        CantAdvanceHeadPositionBeyondTail { tail: u64 },
    }

    impl <PM: PersistentMemory> InfiniteLogImpl<PM> {
        pub closed spec fn view(self) -> Option<AbstractInfiniteLogState>
        {
            recovery_view()(self.wrpm@)
        }

        pub closed spec fn constants(self) -> PersistentMemoryConstants
        {
            self.wrpm.constants()
        }

        pub closed spec fn valid(self) -> bool {
            &&& self.untrusted_log_impl.inv(&self.wrpm)
            &&& recovery_view()(self.wrpm@).is_Some()
        }

        /// This static function takes a `PersistentMemory` and writes
        /// to it such that its state represents an empty log starting
        /// at head position 0. This function is meant to be called
        /// exactly once per log, to create and initialize it.
        pub exec fn setup(pm: &mut PM, device_size: u64) -> (result: Result<u64, InfiniteLogErr>)
            requires
                old(pm).inv(),
                old(pm)@.len() == device_size
            ensures
                pm.inv(),
                pm.constants() == old(pm).constants(),
                pm@.len() == device_size,
                match result {
                    Ok(log_capacity) =>
                        recovery_view()(pm@) == Some(AbstractInfiniteLogState::initialize(log_capacity as int)),
                    Err(InfiniteLogErr::InsufficientSpaceForSetup{ required_space }) => device_size < required_space,
                    _ => false
                }
        {
            UntrustedLogImpl::untrusted_setup(pm, device_size)
        }

        /// This static function takes a `PersistentMemory` and wraps
        /// it into an `InfiniteLogImpl`. It's meant to be called after
        /// setting up the persistent memory or after crashing and
        /// restarting.
        pub exec fn start(pm: PM, device_size: u64) -> (result: Result<InfiniteLogImpl<PM>, InfiniteLogErr>)
            requires
                pm.inv(),
                pm@.len() == device_size,
                recovery_view()(pm@).is_Some()
            ensures
                match result {
                    Ok(trusted_log_impl) => {
                        &&& trusted_log_impl.valid()
                        &&& trusted_log_impl@ == recovery_view()(pm@)
                        &&& trusted_log_impl.constants() == pm.constants()
                    },
                    Err(InfiniteLogErr::CRCMismatch) => !pm.constants().impervious_to_corruption,
                    _ => false
                }
        {
            // The untrusted `start` routine may write to persistent memory, as long
            // as it keeps its abstraction as a log unchanged.
            let mut wrpm = WriteRestrictedPersistentMemory::new(pm);
            let tracked perm = TrustedPermission::new(pm@, |s1, s2| false);
            match UntrustedLogImpl::untrusted_start(&mut wrpm, device_size, Tracked(&perm)) {
                Ok(untrusted_log_impl) => Ok(InfiniteLogImpl { untrusted_log_impl, wrpm }),
                Err(e) => Err(e)
            }
        }

        /// This function appends to the log and returns the offset at
        /// which the append happened.
        pub exec fn append(&mut self, bytes_to_append: &Vec<u8>) -> (result: Result<u64, InfiniteLogErr>)
            requires
                old(self).valid()
            ensures
                self.valid(),
                self.constants() == old(self).constants(),
                match result {
                    Ok(offset) =>
                        match (old(self)@, self@) {
                            (Some(old_log), Some(new_log)) => {
                                &&& offset as nat == old_log.log.len() + old_log.head
                                &&& new_log == old_log.append(bytes_to_append@)
                            },
                            _ => false
                        },
                    Err(InfiniteLogErr::InsufficientSpaceForAppend{ available_space }) => {
                        &&& self@ == old(self)@
                        &&& available_space < bytes_to_append.len()
                        &&& {
                               let log = old(self)@.unwrap();
                               ||| available_space == log.capacity - log.log.len()
                               ||| available_space == u64::MAX - log.head - log.log.len()
                           }
                    },
                    _ => false
                }
        {
            // For crash safety, we must restrict the untrusted code's
            // writes to persistent memory. We must only let it write
            // such that, if a crash happens in the middle of a write,
            // the view of the persistent state is either the current
            // state or the current state with `bytes_to_append`
            // appended.

            let tracked perm = TrustedPermission::new(self.wrpm@,
                |s1: AbstractInfiniteLogState, s2| s2 == s1.append(bytes_to_append@));
            self.untrusted_log_impl.untrusted_append(&mut self.wrpm, bytes_to_append, Tracked(&perm))
        }

        /// This function advances the head index of the log.
        pub exec fn advance_head(&mut self, new_head: u64) -> (result: Result<(), InfiniteLogErr>)
            requires
                old(self).valid()
            ensures
                self.valid(),
                self.constants() == old(self).constants(),
                match result {
                    Ok(offset) => {
                        match (old(self)@, self@) {
                            (Some(old_log), Some(new_log)) => {
                                &&& old_log.head <= new_head <= old_log.head + old_log.log.len()
                                &&& new_log == old_log.advance_head(new_head as int)
                            },
                            _ => false
                        }
                    }
                    Err(InfiniteLogErr::CantAdvanceHeadPositionBeforeHead{ head }) => {
                        &&& self@ == old(self)@
                        &&& head == self@.unwrap().head
                        &&& new_head < head
                    },
                    Err(InfiniteLogErr::CantAdvanceHeadPositionBeyondTail{ tail }) => {
                        &&& self@ == old(self)@
                        &&& tail == self@.unwrap().head + self@.unwrap().log.len()
                        &&& new_head > tail
                    },
                    _ => false
                }
        {
            // For crash safety, we must restrict the untrusted code's
            // writes to persistent memory. We must only let it write
            // such that, if a crash happens in the middle of a write,
            // the view of the persistent state is either the current
            // state or the current state with the head advanced to
            // `new_head`.

            let tracked perm = TrustedPermission::new(self.wrpm@,
                |s1: AbstractInfiniteLogState, s2| s2 == s1.advance_head(new_head as int));
            self.untrusted_log_impl.untrusted_advance_head(&mut self.wrpm, new_head, Tracked(&perm))
        }

        /// This function reads `len` bytes from byte position `pos`
        /// in the log. It returns a vector of those bytes.
        pub exec fn read(&self, pos: u64, len: u64) -> (result: Result<Vec<u8>, InfiniteLogErr>)
            requires
                self.valid(),
                pos + len <= u64::MAX
            ensures
                ({
                    let state = self@.unwrap();
                    let head = state.head;
                    let log = state.log;
                    match result {
                        Ok(bytes) => {
                            let true_bytes = log.subrange(pos - head, pos + len - head);
                            &&& pos >= head
                            &&& pos + len <= head + log.len()
                            &&& read_correct_modulo_corruption(bytes@, true_bytes,
                                                             self.constants().impervious_to_corruption)
                        },
                        Err(InfiniteLogErr::CantReadBeforeHead{ head: head_pos }) => {
                            &&& pos < head
                            &&& head_pos == head
                        },
                        Err(InfiniteLogErr::CantReadPastTail{ tail }) => {
                            &&& pos + len > head + log.len()
                            &&& tail == head + log.len()
                        },
                        _ => false
                    }
                })
        {
            // We don't need to provide permission to write to the
            // persistent memory because the untrusted code is only
            // getting a non-mutable reference to it and thus can't
            // write it. Note that the `UntrustedLogImpl` itself *is*
            // mutable, so it can freely update its in-memory state
            // (e.g., its cache) if it chooses.
            self.untrusted_log_impl.untrusted_read(&self.wrpm, pos, len)
        }

        /// This function returns a tuple consisting of the head and
        /// tail positions of the log.
        pub exec fn get_head_and_tail(&self) -> (result: Result<(u64, u64, u64), InfiniteLogErr>)
            requires
                self.valid()
            ensures
                match result {
                    Ok((result_head, result_tail, result_capacity)) => {
                        let inf_log = self@.unwrap();
                        &&& result_head == inf_log.head
                        &&& result_tail == inf_log.head + inf_log.log.len()
                        &&& result_capacity == inf_log.capacity
                    },
                    Err(_) => false
                }
        {
            // We don't need to provide permission to write to the
            // persistent memory because the untrusted code is only
            // getting a non-mutable reference to it and thus can't
            // write it. Note that the `UntrustedLogImpl` itself *is*
            // mutable, so it can freely update its in-memory state
            // (e.g., its local copy of head and tail) if it chooses.
            self.untrusted_log_impl.untrusted_get_head_and_tail(&self.wrpm)
        }
    }
}

================
File: ./pmemlog/src/logimpl_v.rs
================

use crate::infinitelog_t::*;
use crate::main_t::*;
use crate::math::*;
use crate::pmemspec_t::*;
use crate::sccf::CheckPermission;
use builtin::*;
use builtin_macros::*;
use core::convert::TryInto;
use std::f32::consts::E;
use std::fmt::Write;
use vstd::bytes::*;
use vstd::prelude::*;
use vstd::seq::*;
use vstd::set::*;
use vstd::slice::*;

verus! {

    // entire header structure:
    // bytes 0-7: incorruptible boolean
    // bytes 8-39: header 1
    // bytes 40-71: header 2

    // header version structure:
    // 0-7: header CRC
    // 8-15: logical head
    // 16-23: logical tail
    // 24-31: log size

    pub const incorruptible_bool_pos: u64 = 0;
    pub const header1_pos: u64 = 8;
    pub const header2_pos: u64 = 40;

    // offsets of fields within the header structure
    pub const header_crc_offset: u64 = 0;
    pub const header_head_offset: u64 = 8;
    pub const header_tail_offset: u64 = 16;
    pub const header_log_size_offset: u64 = 24;

    pub const header_size: u64 = 32;

    /// Converts the view of a PM region into its incorruptible Boolean, a view of its header,
    /// and a data region.
    pub open spec fn pm_to_views(pm: Seq<u8>) -> (u64, HeaderView, Seq<u8>)
    {
        let incorruptible_bool = spec_u64_from_le_bytes(pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8));
        // read the CRC, then read the rest of the metadata, then combine them
        let crc1 = spec_u64_from_le_bytes(pm.subrange(header1_pos + header_crc_offset, header1_pos + header_crc_offset + 8));
        let crc2 = spec_u64_from_le_bytes(pm.subrange(header2_pos + header_crc_offset, header2_pos + header_crc_offset + 8));

        let header1_metadata = spec_bytes_to_metadata(pm.subrange(header1_pos + header_head_offset, header1_pos + header_size));
        let header2_metadata = spec_bytes_to_metadata(pm.subrange(header2_pos + header_head_offset, header2_pos + header_size));
        let header_view = HeaderView {
            header1: PersistentHeader {
                crc: crc1,
                metadata: header1_metadata,
            },
            header2: PersistentHeader {
                crc: crc2,
                metadata: header2_metadata,
            }
        };
        let data_view = pm.subrange(contents_offset as int, pm.len() as int);
        (
            incorruptible_bool,
            header_view,
            data_view
        )
    }

    pub open spec fn spec_get_live_header(pm: Seq<u8>) -> PersistentHeader
    {
        let (ib, headers, _) = pm_to_views(pm);
        if ib == cdb0_val {
            headers.header1
        } else {
            headers.header2
        }
    }

    pub open spec fn permissions_depend_only_on_recovery_view<Perm: CheckPermission<Seq<u8>>>(perm: &Perm) -> bool
    {
        forall |s1, s2| recovery_view()(s1) == recovery_view()(s2) ==> perm.check_permission(s1) == perm.check_permission(s2)
    }

    pub proof fn lemma_same_permissions<Perm: CheckPermission<Seq<u8>>>(pm1: Seq<u8>, pm2: Seq<u8>, perm: &Perm)
        requires
            recovery_view()(pm1) =~= recovery_view()(pm2),
            perm.check_permission(pm1),
            permissions_depend_only_on_recovery_view(perm)
        ensures
            perm.check_permission(pm2)
    {}

    /// Proves that a PM region has the given header at the given position. Useful for
    /// associating a region with a header structure when the struct will be used later
    /// in a proof.
    pub proof fn lemma_header_match(pm: Seq<u8>, header_pos: int, header: PersistentHeader)
        requires
            pm.len() > contents_offset,
            header_pos == header1_pos || header_pos == header2_pos,
            spec_bytes_to_header(pm.subrange(header_pos as int, header_pos + header_size)) == header,
        ensures
            ({
                let (_, headers, _) = pm_to_views(pm);
                &&& header_pos == header1_pos ==>
                        headers.header1 == header
                &&& header_pos == header2_pos ==>
                        headers.header2 == header
            })
    {
        assert(pm.subrange(header_pos as int, header_pos + header_size) =~=
                pm.subrange(header_pos + header_crc_offset, header_pos + header_crc_offset + 8) +
                pm.subrange(header_pos + header_head_offset, header_pos + header_size)
            );
        lemma_bytes_combine_into_header(
            pm.subrange(header_pos + header_crc_offset, header_pos + header_crc_offset + 8),
            pm.subrange(header_pos + header_head_offset, header_pos + header_size),
            header
        );
    }

    /// Proves that a given header structure consists of a CRC given in bytes as `crc_bytes` and a metadata structure
    /// given in bytes as `metadata_bytes`.
    pub proof fn lemma_bytes_combine_into_header(crc_bytes: Seq<u8>, metadata_bytes: Seq<u8>, header: PersistentHeader)
        requires
            crc_bytes.len() == 8,
            metadata_bytes.len() == header_size - 8,
            spec_bytes_to_header((crc_bytes + metadata_bytes)) == header,
        ensures
            ({
                let combined_header = PersistentHeader { crc: spec_u64_from_le_bytes(crc_bytes), metadata: spec_bytes_to_metadata(metadata_bytes) };
                header == combined_header
            })
    {
        let crc_val = spec_u64_from_le_bytes(crc_bytes);
        let metadata = spec_bytes_to_metadata(metadata_bytes);
        lemma_seq_addition(crc_bytes, metadata_bytes);

        let combined_header = spec_bytes_to_header((crc_bytes + metadata_bytes));
        assert(combined_header.crc == crc_val);
        assert(metadata == spec_bytes_to_metadata((crc_bytes + metadata_bytes).subrange(header_head_offset as int, header_size as int)));
        assert(combined_header.metadata == metadata);
    }

    /// Converse of lemma_bytes_combine_into_header; proves that the byte representation of a header consists of
    /// the byte representations of its CRC and metadata
    pub proof fn lemma_header_split_into_bytes(crc_bytes: Seq<u8>, metadata_bytes: Seq<u8>, header_bytes: Seq<u8>)
        requires
            crc_bytes.len() == 8,
            metadata_bytes.len() == header_size - 8,
            header_bytes.len() == header_size,
            ({
                let header = PersistentHeader { crc: spec_u64_from_le_bytes(crc_bytes), metadata: spec_bytes_to_metadata(metadata_bytes) };
                spec_bytes_to_header(header_bytes) == header
            }),
        ensures
            crc_bytes + metadata_bytes =~= header_bytes
    {
        lemma_auto_spec_u64_to_from_le_bytes();
        let header = PersistentHeader { crc: spec_u64_from_le_bytes(crc_bytes), metadata: spec_bytes_to_metadata(metadata_bytes) };
        assert(header.crc == spec_u64_from_le_bytes(crc_bytes));
        assert(header_bytes.subrange(header_crc_offset as int, header_crc_offset + 8) =~= spec_u64_to_le_bytes(header.crc));
        assert(crc_bytes =~= spec_u64_to_le_bytes(header.crc));

        assert(header.metadata == spec_bytes_to_metadata(metadata_bytes));
        assert(header.metadata == spec_bytes_to_metadata(header_bytes.subrange(header_head_offset as int, header_size as int)));
        lemma_metadata_bytes_eq(metadata_bytes, header_bytes.subrange(header_head_offset as int, header_size as int), header.metadata);
        assert(header_bytes.subrange(header_head_offset as int, header_size as int) =~= metadata_bytes);

    }

    pub proof fn lemma_seq_addition(bytes1: Seq<u8>, bytes2: Seq<u8>)
        ensures
            ({
                let i = bytes1.len() as int;
                let j = bytes2.len() as int;
                &&& (bytes1 + bytes2).subrange(0, i) =~= bytes1
                &&& (bytes1 + bytes2).subrange(i, i + j) =~= bytes2
            })
    {
        assert(forall |i: int| #![auto] 0 <= i < bytes1.len() ==> (bytes1 + bytes2)[i] == bytes1[i]);
        assert(forall |i: int| #![auto] 0 <= i < bytes2.len() ==> (bytes1 + bytes2)[bytes1.len() + i] == bytes2[i]);
    }

    #[verifier::ext_equal]
    pub struct PersistentHeader {
        pub crc: u64,
        pub metadata: PersistentHeaderMetadata,
    }

    #[verifier::ext_equal]
    pub struct PersistentHeaderMetadata {
        pub head: u64,
        pub tail: u64,
        pub log_size: u64,
    }

    #[verifier::ext_equal]
    pub struct HeaderView {
        pub header1: PersistentHeader,
        pub header2: PersistentHeader,
    }

    /// Spec code only converts byte representations to structures and does not go the other way
    /// to simplify reasoning about persistent structures (although the opposite direction is
    /// implemented in exec code).

    exec fn bytes_to_header(bytes: &[u8]) -> (out: PersistentHeader)
        requires
            bytes@.len() == header_size
        ensures
            out == spec_bytes_to_header(bytes@)
    {
        let crc_bytes = slice_subrange(bytes, header_crc_offset as usize, (header_crc_offset + 8) as usize);
        let metadata_bytes = slice_subrange(bytes, header_head_offset as usize, header_size as usize);

        PersistentHeader {
            crc: u64_from_le_bytes(crc_bytes),
            metadata: bytes_to_metadata(metadata_bytes),
        }
    }

    exec fn header_to_bytes(header: &PersistentHeader) -> (out: Vec<u8>)
        ensures
            header == spec_bytes_to_header(out@),
            spec_u64_from_le_bytes(out@.subrange(header_crc_offset as int, header_crc_offset + 8)) == header.crc,
            spec_bytes_to_metadata(out@.subrange(header_head_offset as int, header_size as int)) == header.metadata,
            out@.len() == header_size
    {
        proof { lemma_auto_spec_u64_to_from_le_bytes(); }

        let mut metadata_bytes = metadata_to_bytes(&header.metadata);
        let mut crc_bytes = u64_to_le_bytes(header.crc);
        let ghost old_metadata_bytes = metadata_bytes@;
        let ghost old_crc_bytes = crc_bytes@;
        crc_bytes.append(&mut metadata_bytes);
        proof {
            lemma_auto_spec_u64_to_from_le_bytes();
            assert(old_crc_bytes =~= crc_bytes@.subrange(header_crc_offset as int, header_crc_offset + 8));
            assert(old_metadata_bytes =~= crc_bytes@.subrange(header_head_offset as int, header_size as int));
        }
        crc_bytes
    }

    exec fn bytes_to_metadata(bytes: &[u8]) -> (out: PersistentHeaderMetadata)
        requires
            bytes@.len() == header_size - 8
        ensures
            out == spec_bytes_to_metadata(bytes@)
    {
        let head_bytes = slice_subrange(bytes, (header_head_offset - 8) as usize, (header_head_offset - 8 + 8) as usize);
        let tail_bytes = slice_subrange(bytes, (header_tail_offset - 8) as usize, (header_tail_offset - 8+ 8) as usize);
        let log_size_bytes = slice_subrange(bytes, (header_log_size_offset - 8) as usize, (header_log_size_offset - 8 + 8) as usize);

        PersistentHeaderMetadata {
            head: u64_from_le_bytes(head_bytes),
            tail: u64_from_le_bytes(tail_bytes),
            log_size: u64_from_le_bytes(log_size_bytes),
        }
    }

    exec fn metadata_to_bytes(metadata: &PersistentHeaderMetadata) -> (out: Vec<u8>)
        ensures
            metadata == spec_bytes_to_metadata(out@),
            out@.len() == header_size - 8,
    {
        let mut bytes: Vec<u8> = Vec::new();
        let ghost old_bytes = bytes@;

        let mut head_bytes = u64_to_le_bytes(metadata.head);
        let ghost old_head_bytes = head_bytes@;
        let mut tail_bytes = u64_to_le_bytes(metadata.tail);
        let ghost old_tail_bytes = tail_bytes@;
        let mut log_size_bytes = u64_to_le_bytes(metadata.log_size);
        let ghost old_log_size_bytes = log_size_bytes@;

        bytes.append(&mut head_bytes);
        bytes.append(&mut tail_bytes);
        bytes.append(&mut log_size_bytes);

        proof {
            lemma_auto_spec_u64_to_from_le_bytes();
            assert(old_bytes == Seq::<u8>::empty());
            assert(old_head_bytes =~= bytes@.subrange(header_head_offset - 8, header_head_offset - 8 + 8));
            assert(old_tail_bytes =~= bytes@.subrange(header_tail_offset - 8, header_tail_offset - 8 + 8));
            assert(old_log_size_bytes =~= bytes@.subrange(header_log_size_offset - 8, header_log_size_offset - 8 + 8));
        }
        bytes
    }

    exec fn crc_and_metadata_bytes_to_header(crc_bytes: &[u8], header_bytes: &[u8]) -> (out: PersistentHeader)
        requires
            crc_bytes@.len() == 8,
            header_bytes@.len() == header_size - 8
        ensures
            out.crc == spec_u64_from_le_bytes(crc_bytes@),
            out.metadata == spec_bytes_to_metadata(header_bytes@)
    {
        let head_bytes = slice_subrange(header_bytes, (header_head_offset - 8) as usize, (header_head_offset + 8 - 8) as usize);
        let tail_bytes = slice_subrange(header_bytes, (header_tail_offset - 8) as usize, (header_tail_offset + 8 - 8) as usize);
        let log_size_bytes = slice_subrange(header_bytes, (header_log_size_offset - 8) as usize, (header_log_size_offset + 8 - 8) as usize);

        PersistentHeader {
            crc: u64_from_le_bytes(crc_bytes),
            metadata: PersistentHeaderMetadata {
                head: u64_from_le_bytes(head_bytes),
                tail: u64_from_le_bytes(tail_bytes),
                log_size: u64_from_le_bytes(log_size_bytes)
            }
        }
    }

    pub open spec(checked) fn spec_bytes_to_metadata(header_seq: Seq<u8>) -> PersistentHeaderMetadata
        recommends
            header_seq.len() == 3*8
    {
        let head = spec_u64_from_le_bytes(header_seq.subrange(header_head_offset - 8, header_head_offset - 8 + 8));
        let tail = spec_u64_from_le_bytes(header_seq.subrange(header_tail_offset - 8, header_tail_offset - 8 + 8));
        let log_size = spec_u64_from_le_bytes(header_seq.subrange(header_log_size_offset - 8, header_log_size_offset - 8 + 8));
        PersistentHeaderMetadata {
            head,
            tail,
            log_size
        }
    }

    /// Proves that two sequences of bytes (assumed to be the subrange of a persistent memory device containing
    /// the PersistentHeaderMetadata) are equivalent if their PersistentHeaderMetadata representations are equivalent
    pub proof fn lemma_metadata_bytes_eq(bytes1: Seq<u8>, bytes2: Seq<u8>, metadata: PersistentHeaderMetadata)
        requires
            bytes1.len() == header_size - 8,
            bytes2.len() == header_size - 8,
            metadata == spec_bytes_to_metadata(bytes1),
            metadata == spec_bytes_to_metadata(bytes2),
        ensures
            bytes1 =~= bytes2
    {
        let metadata1 = spec_bytes_to_metadata(bytes1);
        let metadata2 = spec_bytes_to_metadata(bytes2);

        // TODO: could write a lemma that triggers on from instead of to - might help here
        lemma_auto_spec_u64_to_from_le_bytes();
        assert(spec_u64_to_le_bytes(metadata1.head) == spec_u64_to_le_bytes(metadata2.head));
        assert(metadata1.head == spec_u64_from_le_bytes(bytes1.subrange(header_head_offset - 8, header_head_offset - 8 + 8)));
        assert(metadata2.head == spec_u64_from_le_bytes(bytes2.subrange(header_head_offset - 8, header_head_offset - 8 + 8)));
        assert(bytes1.subrange(header_head_offset - 8, header_head_offset - 8 + 8) =~= bytes2.subrange(header_head_offset - 8, header_head_offset - 8 + 8));

        assert(spec_u64_to_le_bytes(metadata1.tail) == spec_u64_to_le_bytes(metadata2.tail));
        assert(metadata1.tail == spec_u64_from_le_bytes(bytes1.subrange(header_tail_offset - 8, header_tail_offset - 8 + 8)));
        assert(metadata2.tail == spec_u64_from_le_bytes(bytes2.subrange(header_tail_offset - 8, header_tail_offset - 8 + 8)));
        assert(bytes1.subrange(header_tail_offset - 8, header_tail_offset - 8 + 8) =~= bytes2.subrange(header_tail_offset - 8, header_tail_offset - 8 + 8));

        assert(spec_u64_to_le_bytes(metadata1.log_size) == spec_u64_to_le_bytes(metadata2.log_size));
        assert(metadata1.log_size == spec_u64_from_le_bytes(bytes1.subrange(header_log_size_offset - 8, header_log_size_offset - 8 + 8)));
        assert(metadata2.log_size == spec_u64_from_le_bytes(bytes2.subrange(header_log_size_offset - 8, header_log_size_offset - 8 + 8)));
        assert(bytes1.subrange(header_log_size_offset - 8, header_log_size_offset - 8 + 8) =~= bytes2.subrange(header_log_size_offset - 8, header_log_size_offset - 8 + 8));

        assert(bytes1 =~= bytes1.subrange(header_head_offset - 8, header_head_offset - 8 + 8) +
                            bytes1.subrange(header_tail_offset - 8, header_tail_offset - 8 + 8) +
                            bytes1.subrange(header_log_size_offset - 8, header_log_size_offset - 8 + 8));
    }

    pub open spec(checked) fn spec_bytes_to_header(header_seq: Seq<u8>) -> PersistentHeader
        recommends
            header_seq.len() == header_size
    {
        let crc_val = spec_u64_from_le_bytes(header_seq.subrange(header_crc_offset as int, header_crc_offset +8));
        let metadata = spec_bytes_to_metadata(header_seq.subrange(header_head_offset as int, header_size as int));
        PersistentHeader {
            crc: crc_val,
            metadata
        }
    }

    /// Proves that a write to data that does not touch any metadata is crash safe.
    pub proof fn lemma_data_write_is_safe<Perm>(pm: Seq<u8>, bytes: Seq<u8>, write_addr: int, perm: &Perm)
        where
            Perm: CheckPermission<Seq<u8>>,
        requires
            UntrustedLogImpl::recover(pm).is_Some(),
            pm.len() > contents_offset,
            contents_offset <= write_addr < pm.len(),
            perm.check_permission(pm),
            permissions_depend_only_on_recovery_view(perm),
            ({
                // write must be a valid write and not overlap the live log
                let live_header = spec_get_live_header(pm);
                let physical_head = spec_addr_logical_to_physical(live_header.metadata.head as int, live_header.metadata.log_size as int);
                let physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);
                &&& physical_head <= physical_tail ==> {
                    &&& write_addr + bytes.len() <= live_header.metadata.log_size + contents_offset
                    &&& write_addr < physical_head ==> write_addr + bytes.len() <= physical_head
                    &&& (physical_tail <= write_addr || write_addr < physical_head)
                }
                &&& physical_tail < physical_head ==> {
                    &&& physical_tail <= write_addr <= write_addr + bytes.len() < physical_head
                }
            }),
        ensures
            UntrustedLogImpl::recover(pm).is_Some(),
            forall |chunks_flushed| {
                let new_pm = #[trigger] update_contents_to_reflect_partially_flushed_write(
                    pm, write_addr, bytes, chunks_flushed);
                perm.check_permission(new_pm)
            },
            ({
                let new_pm = update_contents_to_reflect_write(pm, write_addr, bytes);
                perm.check_permission(new_pm)
            }),
            update_data_view_postcond(pm, bytes, write_addr),
    {
        let new_pm = update_contents_to_reflect_write(pm, write_addr, bytes);
        lemma_append_data_update_view(pm, bytes, write_addr);
        lemma_same_log_state(pm, new_pm);

        assert forall |chunks_flushed| {
            let new_pm = #[trigger] update_contents_to_reflect_partially_flushed_write(
                pm, write_addr, bytes, chunks_flushed);
            perm.check_permission(new_pm)
        } by {
            let new_pm = update_contents_to_reflect_partially_flushed_write(
                pm, write_addr, bytes, chunks_flushed);
            lemma_append_data_update_view_crash(pm, bytes, write_addr, chunks_flushed);
            lemma_same_log_state(pm, new_pm);
            lemma_same_permissions(pm, new_pm, perm);
        }
    }

    pub open spec fn update_data_view_postcond(pm: Seq<u8>, new_bytes: Seq<u8>, write_addr: int) -> bool
    {
        let new_pm = update_contents_to_reflect_write(pm, write_addr, new_bytes);
        let (old_ib, old_headers, old_data) = pm_to_views(pm);
        let (new_ib, new_headers, new_data) = pm_to_views(new_pm);
        let live_header = spec_get_live_header(pm);
        let physical_head = spec_addr_logical_to_physical(live_header.metadata.head as int, live_header.metadata.log_size as int);
        let physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);
        &&& old_ib == new_ib
        &&& old_headers == new_headers
        &&& new_data.len() == old_data.len()
        &&& new_data.subrange(write_addr - contents_offset, write_addr - contents_offset + new_bytes.len()) =~= new_bytes
        &&& new_data.subrange(0, write_addr - contents_offset) =~= old_data.subrange(0, write_addr - contents_offset)
        &&& new_data.subrange(write_addr - contents_offset + new_bytes.len(), new_data.len() as int) =~=
                old_data.subrange(write_addr - contents_offset + new_bytes.len(), old_data.len() as int)
        &&& UntrustedLogImpl::recover(new_pm).is_Some()

        &&& physical_head < physical_tail ==>
                new_data.subrange(physical_head - contents_offset, physical_tail - contents_offset) =~= old_data.subrange(physical_head - contents_offset, physical_tail - contents_offset)
        &&& physical_tail < physical_head ==> {
                &&& old_data.subrange(physical_head - contents_offset, live_header.metadata.log_size as int) =~= new_data.subrange(physical_head - contents_offset, live_header.metadata.log_size as int)
                &&& old_data.subrange(0, physical_tail - contents_offset) =~= new_data.subrange(0, physical_tail - contents_offset)
        }
    }

    /// Proves that a non-crashing data write updates data bytes but no log metadata.
    pub proof fn lemma_append_data_update_view(pm: Seq<u8>, new_bytes: Seq<u8>, write_addr: int)
        requires
            UntrustedLogImpl::recover(pm).is_Some(),
            pm.len() > contents_offset,
            contents_offset <= write_addr < pm.len(),
            ({
                // write must be a valid write and not overlap the live log
                let live_header = spec_get_live_header(pm);
                let physical_head = spec_addr_logical_to_physical(live_header.metadata.head as int, live_header.metadata.log_size as int);
                let physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);
                &&& physical_head <= physical_tail ==> {
                        &&& write_addr + new_bytes.len() <= live_header.metadata.log_size + contents_offset
                        &&& write_addr < physical_head ==> write_addr + new_bytes.len() <= physical_head
                        &&& (physical_tail <= write_addr || write_addr < physical_head)
                }
                &&& physical_tail < physical_head ==> {
                        &&& physical_tail <= write_addr <= write_addr + new_bytes.len() < physical_head
                }
            }),
        ensures
            UntrustedLogImpl::recover(pm).is_Some(),
            update_data_view_postcond(pm, new_bytes, write_addr),
    {
        let live_header = spec_get_live_header(pm);
        let physical_head = spec_addr_logical_to_physical(live_header.metadata.head as int, live_header.metadata.log_size as int);
        let physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);
        let new_pm = update_contents_to_reflect_write(pm, write_addr, new_bytes);
        lemma_headers_unchanged(pm, new_pm);
        lemma_incorruptible_bool_unchanged(pm, new_pm);
        assert(live_header == spec_get_live_header(new_pm));
        assert(new_pm.subrange(0, write_addr) =~= pm.subrange(0, write_addr));
        assert(new_pm.subrange(write_addr + new_bytes.len(), new_pm.len() as int) =~= pm.subrange(write_addr + new_bytes.len(), pm.len() as int));
        lemma_subrange_equality_implies_subsubrange_equality(pm, new_pm, 0, write_addr);
        lemma_subrange_equality_implies_subsubrange_equality(pm, new_pm, write_addr + new_bytes.len(), new_pm.len() as int);
        if physical_head < physical_tail {
            assert(new_pm.subrange(physical_head as int, physical_tail as int) =~= pm.subrange(physical_head as int, physical_tail as int));
        }
    }

    /// Proves that a crashing data write updates data bytes but no log metadata.
    pub proof fn lemma_append_data_update_view_crash(pm: Seq<u8>, new_bytes: Seq<u8>, write_addr: int, chunks_flushed: Set<int>)
        requires
            UntrustedLogImpl::recover(pm).is_Some(),
            pm.len() > contents_offset,
            contents_offset <= write_addr < pm.len(),
            ({
                // write must be a valid write and not overlap the live log
                let live_header = spec_get_live_header(pm);
                let physical_head = spec_addr_logical_to_physical(live_header.metadata.head as int, live_header.metadata.log_size as int);
                let physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);
                &&& physical_head <= physical_tail ==> write_addr + new_bytes.len() <= live_header.metadata.log_size + contents_offset
                &&& physical_tail < physical_head ==> write_addr + new_bytes.len() < physical_head
            })
        ensures
            UntrustedLogImpl::recover(pm).is_Some(),
            ({
                let new_pm = update_contents_to_reflect_partially_flushed_write(pm, write_addr, new_bytes, chunks_flushed);
                let (old_ib, old_headers, old_data) = pm_to_views(pm);
                let (new_ib, new_headers, new_data) = pm_to_views(new_pm);
                &&& old_ib == new_ib
                &&& old_headers == new_headers
                &&& new_data.len() == old_data.len()
                &&& new_data.subrange(0, write_addr - contents_offset) =~= old_data.subrange(0, write_addr - contents_offset)
                &&& new_data.subrange(write_addr - contents_offset + new_bytes.len(), new_data.len() as int) =~=
                        old_data.subrange(write_addr - contents_offset + new_bytes.len(), old_data.len() as int)
                &&& UntrustedLogImpl::recover(new_pm).is_Some()
            })
    {
        let live_header = spec_get_live_header(pm);
        let physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);
        let new_pm = update_contents_to_reflect_partially_flushed_write(pm, write_addr, new_bytes, chunks_flushed);
        lemma_headers_unchanged(pm, new_pm);
        lemma_incorruptible_bool_unchanged(pm, new_pm);
        assert(new_pm.subrange(0, write_addr) =~= pm.subrange(0, write_addr));
        assert(new_pm.subrange(write_addr + new_bytes.len(), new_pm.len() as int) =~= pm.subrange(write_addr + new_bytes.len(), pm.len() as int));
        lemma_subrange_equality_implies_subsubrange_equality(pm, new_pm, 0, write_addr);
    }

    /// Proves that a non-crashing update to the inactive header does not change any visible PM state.
    pub proof fn lemma_inactive_header_update_view(pm: Seq<u8>, new_header_bytes: Seq<u8>, header_pos: int)
        requires
            UntrustedLogImpl::recover(pm).is_Some(),
            header_pos == header1_pos || header_pos == header2_pos,
            ({
                // the new bytes must be written to the inactive header
                let (old_ib, old_headers, old_data) = pm_to_views(pm);
                &&& old_ib == cdb0_val ==> header_pos == header2_pos
                &&& old_ib == cdb1_val ==> header_pos == header1_pos
            }),
            new_header_bytes.len() == header_size,
            pm.len() > contents_offset,
        ensures
            ({
                let new_pm = update_contents_to_reflect_write(pm, header_pos, new_header_bytes);
                let (old_ib, old_headers, old_data) = pm_to_views(pm);
                let (new_ib, new_headers, new_data) = pm_to_views(new_pm);
                &&& old_ib == new_ib
                &&& old_data =~= old_data
                &&& header_pos == header1_pos ==>
                    old_headers.header2 == new_headers.header2
                &&& header_pos == header2_pos ==>
                    old_headers.header1 == new_headers.header1
                &&& UntrustedLogImpl::recover(new_pm).is_Some()
            })
    {
        let new_pm = update_contents_to_reflect_write(pm, header_pos, new_header_bytes);
        let (new_ib, new_headers, new_data) = pm_to_views(new_pm);
        assert(pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8) =~= new_pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8));
        if header_pos == header1_pos {
            // we wrote to header1, so header2 should have stayed the same
            assert(pm.subrange(header2_pos + header_crc_offset, header2_pos + header_crc_offset + 8) =~=
                new_pm.subrange(header2_pos + header_crc_offset, header2_pos + header_crc_offset + 8));

            assert(pm.subrange(header2_pos + header_head_offset, header2_pos + header_size) =~=
                new_pm.subrange(header2_pos + header_head_offset, header2_pos + header_size));
        } else {
            // we wrote to header2, so header1 should have stayed the same
            assert(pm.subrange(header1_pos + header_crc_offset, header1_pos + header_crc_offset + 8) =~=
                new_pm.subrange(header1_pos + header_crc_offset, header1_pos + header_crc_offset + 8));

            assert(pm.subrange(header1_pos + header_head_offset, header1_pos + header_size) =~=
                new_pm.subrange(header1_pos + header_head_offset, header1_pos + header_size));
        }
    }

    /// Proves that a crashing update to the inactive header does not change any visible PM state.
    pub proof fn lemma_inactive_header_update_view_crash(pm: Seq<u8>, new_header_bytes: Seq<u8>, header_pos: int, chunks_flushed: Set<int>)
        requires
            UntrustedLogImpl::recover(pm).is_Some(),
            header_pos == header1_pos || header_pos == header2_pos,
            ({
                // the new bytes must be written to the inactive header
                let (old_ib, old_headers, old_data) = pm_to_views(pm);
                &&& old_ib == cdb0_val ==> header_pos == header2_pos
                &&& old_ib == cdb1_val ==> header_pos == header1_pos
            }),
            new_header_bytes.len() == header_size,
            pm.len() > contents_offset,
        ensures
            ({
                let new_pm = update_contents_to_reflect_partially_flushed_write(
                    pm, header_pos, new_header_bytes, chunks_flushed);
                let (old_ib, old_headers, old_data) = pm_to_views(pm);
                let (new_ib, new_headers, new_data) = pm_to_views(new_pm);
                &&& old_ib == new_ib
                &&& old_data =~= old_data
                &&& header_pos == header1_pos ==>
                    old_headers.header2 == new_headers.header2
                &&& header_pos == header2_pos ==>
                    old_headers.header1 == new_headers.header1
                &&& UntrustedLogImpl::recover(new_pm).is_Some()
            })
    {
        let new_pm = update_contents_to_reflect_partially_flushed_write(
            pm, header_pos, new_header_bytes, chunks_flushed);
        assert(pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8) =~= new_pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8));
        if header_pos == header1_pos {
            // we wrote to header1, so header2 should have stayed the same
            assert(pm.subrange(header2_pos + header_crc_offset, header2_pos + header_crc_offset + 8) =~=
                new_pm.subrange(header2_pos + header_crc_offset, header2_pos + header_crc_offset + 8));

            assert(pm.subrange(header2_pos + header_head_offset, header2_pos + header_size) =~=
                new_pm.subrange(header2_pos + header_head_offset, header2_pos + header_size));
        } else {
            // we wrote to header2, so header1 should have stayed the same
            assert(pm.subrange(header1_pos + header_crc_offset, header1_pos + header_crc_offset + 8) =~=
                new_pm.subrange(header1_pos + header_crc_offset, header1_pos + header_crc_offset + 8));

            assert(pm.subrange(header1_pos + header_head_offset, header1_pos + header_size) =~=
                new_pm.subrange(header1_pos + header_head_offset, header1_pos + header_size));
        }
    }

    /// Proves that an update to the incorruptible boolean is crash-safe and switches the log's
    /// active header. This lemma does most of the work to prove that untrusted_append is
    /// implemented correctly.
    pub proof fn lemma_append_ib_update<Perm: CheckPermission<Seq<u8>>>(
        pm: Seq<u8>,
        new_ib: u64,
        bytes_to_append: Seq<u8>,
        new_header_bytes: Seq<u8>,
        perm: &Perm
    )
        requires
            pm.len() > contents_offset,
            UntrustedLogImpl::recover(pm).is_Some(),
            new_ib == cdb0_val || new_ib == cdb1_val,
            new_ib == cdb0_val ==>
                pm.subrange(header1_pos as int, header1_pos + header_size) == new_header_bytes,
            new_ib == cdb1_val ==>
                pm.subrange(header2_pos as int, header2_pos + header_size) == new_header_bytes,
            new_header_bytes.subrange(header_crc_offset as int, header_crc_offset + 8) ==
                spec_crc_bytes(new_header_bytes.subrange(header_head_offset as int, header_size as int)),
            ({
                let new_header = spec_bytes_to_header(new_header_bytes);
                let live_header = spec_get_live_header(pm);
                &&& new_header.metadata.tail == live_header.metadata.tail + bytes_to_append.len()
                &&& new_header.metadata.head == live_header.metadata.head
                &&& new_header.metadata.log_size == live_header.metadata.log_size
                &&& new_header.metadata.tail - new_header.metadata.head < new_header.metadata.log_size
            }),
            perm.check_permission(pm),
            permissions_depend_only_on_recovery_view(perm),
            ({
                let live_header = spec_get_live_header(pm);
                let physical_head = spec_addr_logical_to_physical(live_header.metadata.head as int, live_header.metadata.log_size as int);
                let physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);
                let contents_end = (live_header.metadata.log_size + contents_offset) as int;
                let append_size = bytes_to_append.len();
                let len1 = (contents_end - physical_tail);
                let len2 = bytes_to_append.len() - len1;

                &&& physical_tail + append_size >= contents_end ==> {
                    &&& pm.subrange(physical_tail, contents_end) =~= bytes_to_append.subrange(0, len1)
                    &&& pm.subrange(contents_offset as int, contents_offset + len2) =~= bytes_to_append.subrange(len1 as int, append_size as int)
                    &&& bytes_to_append =~= pm.subrange(physical_tail, contents_end) + pm.subrange(contents_offset as int, contents_offset + len2)
                }
                &&& physical_head <= physical_tail && physical_tail + append_size < contents_end ==> {
                    pm.subrange(physical_tail, physical_tail + append_size) =~= bytes_to_append
                }
                &&& physical_tail < physical_head ==> {
                    &&& physical_tail + append_size < physical_head
                    &&& pm.subrange(physical_tail, physical_tail + append_size) =~= bytes_to_append
                }
            }),
            ({
                let old_log_state = UntrustedLogImpl::recover(pm);
                forall |pm_state| #[trigger] perm.check_permission(pm_state) <==> {
                    let log_state = UntrustedLogImpl::recover(pm_state);
                    log_state == old_log_state || log_state == Some(old_log_state.unwrap().append(bytes_to_append))
                }
            }),
        ensures
            ({
                let ib_bytes = spec_u64_to_le_bytes(new_ib);
                let new_pm = update_contents_to_reflect_write(pm, incorruptible_bool_pos as int, ib_bytes);
                let old_log_state = UntrustedLogImpl::recover(pm);
                let new_log_state = UntrustedLogImpl::recover(new_pm);
                let new_live_header = spec_get_live_header(new_pm);
                let (new_pm_ib, _, _) = pm_to_views(new_pm);
                &&& match (old_log_state, new_log_state) {
                        (Some(old_log_state), Some(new_log_state)) => {
                            &&& new_log_state =~= old_log_state.append(bytes_to_append)
                            &&& perm.check_permission(new_pm)
                        }
                        _ => false,
                    }
                &&& new_live_header == spec_bytes_to_header(new_header_bytes)
                &&& new_ib == new_pm_ib
            }),
            forall |chunks_flushed| {
                let new_pm = #[trigger] update_contents_to_reflect_partially_flushed_write(
                    pm, incorruptible_bool_pos as int, spec_u64_to_le_bytes(new_ib), chunks_flushed);
                &&& perm.check_permission(new_pm)
            }
    {
        let ib_bytes = spec_u64_to_le_bytes(new_ib);
        let live_header = spec_get_live_header(pm);
        let append_size = bytes_to_append.len();
        let contents_end = live_header.metadata.log_size + contents_offset;
        let physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);

        lemma_auto_spec_u64_to_from_le_bytes();
        lemma_single_write_crash(pm, incorruptible_bool_pos as int, ib_bytes);
        assert(perm.check_permission(pm));

        let new_pm = update_contents_to_reflect_write(pm, incorruptible_bool_pos as int, ib_bytes);
        lemma_headers_unchanged(pm, new_pm);
        assert(new_pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8) =~= ib_bytes);

        let new_header = spec_bytes_to_header(new_header_bytes);
        let (ib, headers, data) = pm_to_views(new_pm);
        let header_pos = if new_ib == cdb0_val {
            header1_pos
        } else {
            header2_pos
        };
        assert(new_pm.subrange(header_pos as int, header_pos + header_size) =~= new_header_bytes);
        lemma_header_match(new_pm, header_pos as int, new_header);
        lemma_header_correct(new_pm, new_header_bytes, header_pos as int);

        // prove that new pm has the append update
        let new_log_state = UntrustedLogImpl::recover(new_pm);
        let old_log_state = UntrustedLogImpl::recover(pm);

        match (new_log_state, old_log_state) {
            (Some(new_log_state), Some(old_log_state)) => {
                lemma_pm_state_header(new_pm);
                lemma_pm_state_header(pm);

                let old_header = spec_get_live_header(pm);
                let live_header = spec_get_live_header(new_pm);
                assert(live_header == new_header);

                assert(live_header.metadata.head == old_header.metadata.head);
                assert(live_header.metadata.tail == old_header.metadata.tail + bytes_to_append.len());

                let physical_head = spec_addr_logical_to_physical(live_header.metadata.head as int, live_header.metadata.log_size as int);
                let new_physical_tail = spec_addr_logical_to_physical(live_header.metadata.tail as int, live_header.metadata.log_size as int);
                let old_physical_tail = spec_addr_logical_to_physical(old_header.metadata.tail as int, old_header.metadata.log_size as int);
                assert(old_physical_tail == physical_tail);

                let (_, _, old_data) = pm_to_views(pm);
                let (_, _, new_data) = pm_to_views(pm);

                if physical_head <= old_physical_tail {
                    if old_physical_tail + append_size >= contents_end {
                        assert(new_log_state.log =~= new_data.subrange(physical_head - contents_offset, old_physical_tail - contents_offset) +
                                                    new_data.subrange(old_physical_tail - contents_offset, contents_end - contents_offset) +
                                                    new_data.subrange(0, new_physical_tail - contents_offset));
                        assert(new_log_state.log =~= old_data.subrange(physical_head - contents_offset, old_physical_tail - contents_offset) +
                                                    new_data.subrange(old_physical_tail - contents_offset, contents_end - contents_offset) +
                                                    new_data.subrange(0, new_physical_tail - contents_offset));
                        let len1 = (contents_end - old_physical_tail);
                        let len2 = bytes_to_append.len() - len1;
                        assert(bytes_to_append =~= new_data.subrange(old_physical_tail - contents_offset, contents_end - contents_offset) +
                                                    new_data.subrange(0, new_physical_tail - contents_offset));
                        assert(new_log_state.log =~= old_data.subrange(physical_head - contents_offset, old_physical_tail - contents_offset) + bytes_to_append);
                    } else {
                        assert(old_data.subrange(0, old_physical_tail - contents_offset) =~= new_data.subrange(0, old_physical_tail - contents_offset));
                        assert(new_data.subrange(old_physical_tail - contents_offset, old_physical_tail - contents_offset + append_size) =~= bytes_to_append);
                    }
                } else { // physical_tail < physical_head
                    assert(old_physical_tail + append_size < physical_head);
                }
                assert(new_log_state =~= old_log_state.append(bytes_to_append));
                assert(perm.check_permission(new_pm));
            }
            _ => assert(false),
        }
    }

    pub open spec fn live_data_view_eq(old_pm: Seq<u8>, new_pm: Seq<u8>) -> bool
    {
        let (old_ib, old_headers, old_data) = pm_to_views(old_pm);
        let (new_ib, new_headers, new_data) = pm_to_views(new_pm);
        let old_live_header = spec_get_live_header(old_pm);
        let new_live_header = spec_get_live_header(new_pm);
        let physical_head = spec_addr_logical_to_physical(old_live_header.metadata.head as int, old_live_header.metadata.log_size as int);
        let physical_tail = spec_addr_logical_to_physical(old_live_header.metadata.tail as int, old_live_header.metadata.log_size as int);
        let log_size = old_live_header.metadata.log_size;
        let physical_data_head = physical_head - contents_offset;
        let physical_data_tail = physical_tail - contents_offset;

        &&& new_live_header == old_live_header
        &&& physical_head < physical_tail ==>
                old_data.subrange(physical_data_head, physical_data_tail) =~= new_data.subrange(physical_data_head, physical_data_tail)
        &&& physical_tail < physical_head ==> {
                &&& old_data.subrange(physical_data_head as int, log_size as int) =~= new_data.subrange(physical_data_head as int, log_size as int)
                &&& old_data.subrange(0, physical_data_tail as int) =~= new_data.subrange(0, physical_data_tail as int)
        }
        &&& physical_tail == physical_head ==>
                physical_data_head == physical_data_tail
    }

    pub proof fn lemma_same_log_state(old_pm: Seq<u8>, new_pm: Seq<u8>)
        requires
            UntrustedLogImpl::recover(old_pm).is_Some(),
            UntrustedLogImpl::recover(new_pm).is_Some(),
            live_data_view_eq(old_pm, new_pm),
            ({
                let (old_ib, old_headers, old_data) = pm_to_views(old_pm);
                let (new_ib, new_headers, new_data) = pm_to_views(new_pm);
                &&& old_ib == cdb0_val || old_ib == cdb1_val
                &&& old_ib == new_ib
                &&& old_ib == cdb0_val ==> {
                    &&& old_headers.header1 == new_headers.header1
                }
                &&& old_ib == cdb1_val ==> {
                    &&& old_headers.header2 == new_headers.header2
                }
            })
        ensures
            UntrustedLogImpl::recover(old_pm) =~=
                UntrustedLogImpl::recover(new_pm)
    {
        let old_state = UntrustedLogImpl::recover(old_pm);
        let new_state = UntrustedLogImpl::recover(new_pm);
        let (old_ib, old_headers, old_data) = pm_to_views(old_pm);
        let (new_ib, new_headers, new_data) = pm_to_views(new_pm);

        assert(old_state.is_Some());
        assert(new_state.is_Some());
        match (old_state, new_state) {
            (Some(old_state), Some(new_state)) => {
                let (old_live_header, new_live_header) = if old_ib == cdb0_val {
                    (old_headers.header1, new_headers.header1)
                } else {
                    (old_headers.header2, new_headers.header2)
                };

                assert(old_state.head == old_live_header.metadata.head);
                assert(new_state.head == new_live_header.metadata.head);
                assert(old_live_header.metadata.tail == new_live_header.metadata.tail);
                let physical_head = spec_addr_logical_to_physical(old_live_header.metadata.head as int, old_live_header.metadata.log_size as int);
                let physical_tail = spec_addr_logical_to_physical(old_live_header.metadata.tail as int, old_live_header.metadata.log_size as int);
                let contents_end = old_live_header.metadata.log_size + contents_offset;

                if physical_head < physical_tail {
                    assert(old_pm.subrange(physical_head, physical_tail) =~= old_data.subrange(physical_head - contents_offset, physical_tail - contents_offset));
                    assert(old_pm.subrange(physical_head, physical_tail) =~= new_pm.subrange(physical_head, physical_tail));
                } else if physical_tail < physical_head {
                    assert(old_pm.subrange(physical_head, contents_end) =~= old_data.subrange(physical_head - contents_offset, contents_end - contents_offset));
                    assert(old_pm.subrange(contents_offset as int, physical_tail) =~= old_data.subrange(contents_offset - contents_offset, physical_tail - contents_offset));
                    assert(old_pm.subrange(physical_head, contents_end) + old_pm.subrange(contents_offset as int, physical_tail) =~=
                        new_pm.subrange(physical_head, contents_end) + new_pm.subrange(contents_offset as int, physical_tail));
                } else {
                    assert(physical_head == physical_tail);
                    assert(old_state.log.len() == 0);
                    assert(new_state.log.len() == 0);
                }
            }
            _ => assert(false),
        }
    }

    pub proof fn lemma_subrange_equality_implies_index_equality<T>(s1: Seq<T>, s2: Seq<T>, i: int, j: int)
        requires
            0 <= i <= j <= s1.len(),
            j <= s2.len(),
            s1.subrange(i, j) == s2.subrange(i, j)
        ensures
            forall |k| i <= k < j ==> s1[k] == s2[k]
    {
        assert forall |k| i <= k < j implies s1[k] == s2[k] by {
            // Trigger axiom_seq_subrange_index
            assert (s1[k] == s1.subrange(i, j)[k - i]);
            assert (s2[k] == s2.subrange(i, j)[k - i]);
        }
    }

    pub proof fn lemma_subrange_equality_implies_subsubrange_equality<T>(s1: Seq<T>, s2: Seq<T>, i: int, j: int)
        requires
            0 <= i <= j <= s1.len(),
            j <= s2.len(),
            s1.subrange(i, j) == s2.subrange(i, j)
        ensures
            forall |k, m| i <= k <= m <= j ==> s1.subrange(k, m) == s2.subrange(k, m)
    {
        lemma_subrange_equality_implies_index_equality(s1, s2, i, j);
        assert forall |k, m| i <= k <= m <= j implies s1.subrange(k, m) == s2.subrange(k, m) by {
            assert (s1.subrange(k, m) =~= s2.subrange(k, m));
        }
    }

    pub proof fn lemma_subrange_equality_implies_subsubrange_equality_forall<T>()
        ensures
            forall |s1: Seq<T>, s2: Seq<T>, i: int, j: int, k: int, m: int|
                {
                    &&& 0 <= i <= j <= s1.len()
                    &&& j <= s2.len()
                    &&& s1.subrange(i, j) == s2.subrange(i, j)
                    &&& i <= k <= m <= j
                }
                ==> s1.subrange(k, m) == s2.subrange(k, m)
    {
        assert forall |s1: Seq<T>, s2: Seq<T>, i: int, j: int, k: int, m: int|
                   {
                       &&& 0 <= i <= j <= s1.len()
                       &&& j <= s2.len()
                       &&& s1.subrange(i, j) == s2.subrange(i, j)
                       &&& i <= k <= m <= j
                   }
                   implies s1.subrange(k, m) == s2.subrange(k, m) by {
            lemma_subrange_equality_implies_subsubrange_equality(s1, s2, i, j);
        }
    }

    pub proof fn lemma_headers_unchanged(old_pm: Seq<u8>, new_pm: Seq<u8>)
        requires
            old_pm.len() == new_pm.len(),
            old_pm.len() >= contents_offset,
            old_pm.subrange(header1_pos as int, header1_pos + header_size) =~= new_pm.subrange(header1_pos as int, header1_pos + header_size),
            old_pm.subrange(header2_pos as int, header2_pos + header_size) =~= new_pm.subrange(header2_pos as int, header2_pos + header_size),
        ensures
            ({
                let (_, old_headers, _) = pm_to_views(old_pm);
                let (_, new_headers, _) = pm_to_views(new_pm);
                old_headers == new_headers
            })
    {
        lemma_subrange_equality_implies_subsubrange_equality_forall::<u8>();
    }

    pub proof fn lemma_incorruptible_bool_unchanged(old_pm: Seq<u8>, new_pm: Seq<u8>)
        requires
            old_pm.len() == new_pm.len(),
            old_pm.len() >= contents_offset,
            old_pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8) =~= new_pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8)
        ensures
            ({
                let (old_ib, _, _) = pm_to_views(old_pm);
                let (new_ib, _, _) = pm_to_views(new_pm);
                old_ib == new_ib
            })
    {}

    pub proof fn lemma_header_crc_correct(header_bytes: Seq<u8>, crc_bytes: Seq<u8>, metadata_bytes: Seq<u8>)
        requires
            header_bytes.len() == header_size,
            crc_bytes.len() == 8,
            metadata_bytes.len() == header_size - 8,
            crc_bytes =~= spec_crc_bytes(metadata_bytes),
            header_bytes =~= crc_bytes + metadata_bytes
        ensures
            header_bytes.subrange(header_crc_offset as int, header_crc_offset + 8) =~= crc_bytes,
            header_bytes.subrange(header_head_offset as int, header_size as int) =~= metadata_bytes,
            header_bytes.subrange(header_crc_offset as int, header_crc_offset + 8) =~=
                spec_crc_bytes(header_bytes.subrange(header_head_offset as int, header_size as int))
    {
        assert(header_bytes.subrange(header_crc_offset as int, header_crc_offset + 8) =~= crc_bytes);
        assert(header_bytes.subrange(header_head_offset as int, header_size as int) =~= metadata_bytes);
    }

    pub proof fn lemma_header_correct(pm: Seq<u8>, header_bytes: Seq<u8>, header_pos: int)
        requires
            pm.len() > contents_offset,
            header_bytes.len() == header_size,
            header_pos == header1_pos || header_pos == header2_pos,
            header_bytes.subrange(header_crc_offset as int, header_crc_offset + 8) =~=
                spec_crc_bytes(header_bytes.subrange(header_head_offset as int, header_size as int)),
            pm.subrange(header_pos, header_pos + header_size) =~= header_bytes
        ensures
            pm.subrange(header_pos + header_crc_offset, header_pos + header_crc_offset + 8) =~=
                header_bytes.subrange(header_crc_offset as int, header_crc_offset + 8),
            pm.subrange(header_pos + header_head_offset, header_pos + header_size) =~=
                header_bytes.subrange(header_head_offset as int, header_size as int),
            pm.subrange(header_pos + header_crc_offset, header_pos + header_crc_offset + 8) =~=
                spec_crc_bytes(pm.subrange(header_pos + header_head_offset, header_pos + header_size))
    {
        assert(pm.subrange(header_pos + header_crc_offset, header_pos + header_crc_offset + 8) =~=
            header_bytes.subrange(header_crc_offset as int, header_crc_offset + 8));
        assert(pm.subrange(header_pos + header_head_offset, header_pos + header_size) =~=
            header_bytes.subrange(header_head_offset as int, header_size as int));
    }

    pub proof fn lemma_u64_bytes_eq(val1: u64, val2: u64)
        requires
            val1 == val2
        ensures
            spec_u64_to_le_bytes(val1) =~= spec_u64_to_le_bytes(val2)
    {}

    pub proof fn lemma_subrange_eq<T>(bytes1: Seq<T>, bytes2: Seq<T>)
        requires
            bytes1 =~= bytes2
        ensures
            forall |i: int, j: int| 0 <= i < j < bytes1.len() ==> bytes1.subrange(i, j) =~= bytes2.subrange(i, j)
    {}

    /// If our write is persistence_chunk_size-sized and -aligned, then there are only 2 possible
    /// resulting crash states, one with the write and one without.
    pub proof fn lemma_single_write_crash(pm: Seq<u8>, write_addr: int, bytes_to_write: Seq<u8>)
        requires
            bytes_to_write.len() == persistence_chunk_size,
            write_addr % persistence_chunk_size == 0, // currently seems to succeed without nonlinear arith
            0 <= write_addr < pm.len(),
            write_addr + bytes_to_write.len() <= pm.len()
        ensures
            ({
                forall |chunks_flushed: Set<int>| {
                    let new_crash_contents = #[trigger] update_contents_to_reflect_partially_flushed_write(
                        pm, write_addr, bytes_to_write, chunks_flushed);
                    let new_contents = update_contents_to_reflect_write(pm, write_addr, bytes_to_write);
                    new_crash_contents =~= pm || new_crash_contents =~= new_contents
                }
            })
    {}

    pub proof fn lemma_pm_state_header(pm: Seq<u8>)
        requires
            UntrustedLogImpl::recover(pm).is_Some(),
            ({
                let header = spec_get_live_header(pm);
                header.metadata.tail - header.metadata.head < header.metadata.log_size
            })
        ensures
            ({
                let pm_state = UntrustedLogImpl::recover(pm);
                let header = spec_get_live_header(pm);
                match pm_state {
                    Some(pm_state) => {
                        &&& header.metadata.head == pm_state.head
                        &&& pm_state.log.len() == header.metadata.tail - header.metadata.head
                    }
                    None => false
                }
            })
    {
        let pm_state = UntrustedLogImpl::recover(pm);
        let header = spec_get_live_header(pm);
        lemma_mod_range(header.metadata.head as int, header.metadata.log_size as int);
        lemma_mod_range(header.metadata.tail as int, header.metadata.log_size as int);
        let head = header.metadata.head as int;
        let tail = header.metadata.tail as int;
        let log_size = header.metadata.log_size as int;
        let physical_head = spec_addr_logical_to_physical(head, log_size);
        let physical_tail = spec_addr_logical_to_physical(tail, log_size);
        match pm_state {
            Some(pm_state) => {
                if physical_head < physical_tail {
                    // log does not wrap
                    lemma_mod_difference_equal(head, tail, log_size);
                } else if physical_tail < physical_head {
                    // log wraps
                    lemma_mod_wrapped_len(head, tail, log_size);
                } else {
                    // size is 0
                    lemma_mod_equal(head, tail, log_size);
                }
            }
            None => assert(false),
        }
    }

    pub open spec fn spec_addr_logical_to_physical(addr: int, log_size: int) -> int {
        (addr % log_size) + contents_offset
    }

    pub struct UntrustedLogImpl {
        pub incorruptible_bool: u64,
        // header fields are stored separately because of limitations
        // on deriving Copy/Clone for the header structures
        pub header_crc: u64,
        pub head: u64,
        pub tail: u64,
        pub log_size: u64,
    }

    // offset of actual log contents from the beginning of the device
    pub const contents_offset: u64 = header2_pos + header_log_size_offset + 8;

    impl UntrustedLogImpl {

        pub exec fn addr_logical_to_physical(addr: u64, log_size: u64) -> (out: u64)
            requires
                addr <= u64::MAX,
                log_size > 0,
                log_size + contents_offset <= u64::MAX,
            ensures
                out == spec_addr_logical_to_physical(addr as int, log_size as int)
        {
            (addr % log_size) + contents_offset
        }

        pub open spec fn log_state_is_valid(pm: Seq<u8>) -> bool {
            let (ib, headers, data) = pm_to_views(pm);
            let live_header = if ib == cdb0_val {
                headers.header1
            } else {
                headers.header2
            };

            let head = live_header.metadata.head as int;
            let tail = live_header.metadata.tail as int;
            let log_size = live_header.metadata.log_size as int;

            &&& ib == cdb0_val || ib == cdb1_val
            &&& log_size + contents_offset <= u64::MAX
            &&& log_size > 0
            &&& log_size + contents_offset == pm.len()
            &&& tail - head < log_size
            &&& ib == cdb0_val ==> {
                    &&& live_header.crc == spec_u64_from_le_bytes(spec_crc_bytes(pm.subrange(header1_pos + header_head_offset, header1_pos + header_size)))
                    &&& pm.subrange(header1_pos + header_crc_offset, header1_pos + header_crc_offset + 8) =~= spec_crc_bytes(pm.subrange(header1_pos + header_head_offset, header1_pos + header_size))
                }
            &&& ib == cdb1_val ==> {
                &&& live_header.crc == spec_u64_from_le_bytes(spec_crc_bytes(pm.subrange(header2_pos + header_head_offset, header2_pos + header_size)))
                &&& pm.subrange(header2_pos + header_crc_offset, header2_pos + header_crc_offset + 8) =~= spec_crc_bytes(pm.subrange(header2_pos + header_head_offset, header2_pos + header_size))
            }
            &&& head <= tail
        }

        pub open spec fn recover(pm: Seq<u8>) -> Option<AbstractInfiniteLogState>
        {
            let (ib, headers, data) = pm_to_views(pm);
            if !Self::log_state_is_valid(pm) {
                None
            } else {
                let live_header = if ib == cdb0_val {
                    headers.header1
                } else {
                    headers.header2
                };

                let head = live_header.metadata.head as int;
                let tail = live_header.metadata.tail as int;
                let log_size = live_header.metadata.log_size as int;
                let contents_end = log_size + contents_offset;
                let physical_head = spec_addr_logical_to_physical(head, log_size);
                let physical_tail = spec_addr_logical_to_physical(tail, log_size);

                let abstract_log = if physical_head < physical_tail {
                    pm.subrange(physical_head, physical_tail)
                } else if physical_tail < physical_head {
                    let range1 = pm.subrange(physical_head, contents_end);
                    let range2 = pm.subrange(contents_offset as int, physical_tail);
                    range1 + range2
                } else {
                    Seq::empty()
                };

                Some(AbstractInfiniteLogState { head: head, log: abstract_log, capacity: log_size - 1 })
            }
        }

        // This is the invariant that the untrusted log implementation
        // maintains between its local state and the contents of
        // persistent memory.
        pub open spec fn inv_pm_contents(self, contents: Seq<u8>) -> bool
        {
            let (ib, headers, data) = pm_to_views(contents);
            let header_pos = if ib == cdb0_val { header1_pos } else { header2_pos };
            let header = spec_get_live_header(contents);
            let head = header.metadata.head;
            let tail = header.metadata.tail;
            let log_size = header.metadata.log_size;
            &&& ib == cdb0_val || ib == cdb1_val
            &&& spec_crc_bytes(contents.subrange(header_pos + header_head_offset, header_pos + header_size)) ==
                  contents.subrange(header_pos + header_crc_offset, header_pos + header_crc_offset + 8)
            &&& log_size + contents_offset <= u64::MAX
            &&& tail - head < log_size
            &&& log_size + contents_offset == contents.len()
            &&& self.header_crc == header.crc
            &&& self.head == head
            &&& self.tail == tail
            &&& self.log_size == log_size
            &&& self.incorruptible_bool == ib
            &&& match Self::recover(contents) {
                   Some(inf_log) => tail == head + inf_log.log.len(),
                   None => false,
               }
        }

        // This is the invariant that the untrusted log implementation
        // maintains between its local state and the write-restricted
        // persistent memory.
        pub open spec fn inv<Perm, PM>(self, wrpm: &WriteRestrictedPersistentMemory<Perm, PM>) -> bool
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
        {
            &&& wrpm.inv()
            &&& self.inv_pm_contents(wrpm@)
        }

        pub exec fn read_incorruptible_boolean<PM: PersistentMemory>(pm: &PM) -> (result: Result<u64, InfiniteLogErr>)
            requires
                Self::recover(pm@).is_Some(),
                pm.inv(),
                pm@.len() > contents_offset
            ensures
                match result {
                    Ok(ib) => {
                        let (spec_ib, _, _) = pm_to_views(pm@);
                        ib == spec_ib
                    }
                    Err(InfiniteLogErr::CRCMismatch) => !pm.constants().impervious_to_corruption,
                    _ => false,
                }
        {
            let bytes = pm.read(incorruptible_bool_pos, 8);
            let ib = u64_from_le_bytes(bytes.as_slice());
            let ghost addrs = Seq::<int>::new(8, |i: int| i + incorruptible_bool_pos);
            if ib == cdb0_val || ib == cdb1_val {
                proof {
                    let (spec_ib, _, _) = pm_to_views(pm@);
                    lemma_auto_spec_u64_to_from_le_bytes();
                    if !pm.constants().impervious_to_corruption {
                        axiom_corruption_detecting_boolean(ib, spec_ib, addrs);
                    }
                }
                Ok(ib)
            } else {
                Err(InfiniteLogErr::CRCMismatch)
            }
        }

        exec fn update_header<Perm, PM>
        (
            &mut self,
            wrpm: &mut WriteRestrictedPersistentMemory<Perm, PM>,
            Tracked(perm): Tracked<&Perm>,
            new_header_bytes: &Vec<u8>
        )
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
            requires
                permissions_depend_only_on_recovery_view(perm),
                contents_offset < old(wrpm)@.len(),
                old(self).inv(&*old(wrpm)),
                Self::recover(old(wrpm)@).is_Some(),
                new_header_bytes@.subrange(header_crc_offset as int, header_crc_offset + 8) =~=
                    spec_crc_bytes(new_header_bytes@.subrange(header_head_offset as int, header_size as int)),
                new_header_bytes.len() == header_size,
                match Self::recover(old(wrpm)@) {
                    Some(log_state) => perm.check_permission(old(wrpm)@),
                    None => false
                }
            ensures
                self.inv(wrpm),
                Self::recover(wrpm@).is_Some(),
                wrpm.constants() == old(wrpm).constants(),
                match (Self::recover(old(wrpm)@), Self::recover(wrpm@)) {
                    (Some(old_log_state), Some(new_log_state)) => old_log_state =~= new_log_state,
                    _ => false
                },
                ({
                    let (old_pm_ib, old_metadata, old_data) = pm_to_views(old(wrpm)@);
                    let (new_pm_ib, new_metadata, new_data) = pm_to_views(wrpm@);
                    let new_header = spec_bytes_to_header(new_header_bytes@);
                    &&& old_pm_ib == new_pm_ib
                    &&& old_pm_ib == cdb0_val ==> {
                        &&& new_metadata.header1 == old_metadata.header1
                        &&& new_metadata.header2 == new_header
                        &&& wrpm@.subrange(header2_pos + header_crc_offset, header2_pos + header_crc_offset + 8) =~=
                                spec_crc_bytes(wrpm@.subrange(header2_pos + header_head_offset, header2_pos + header_size))
                        &&& wrpm@.subrange(header2_pos as int, header2_pos + header_size) =~= new_header_bytes@
                    }
                    &&& old_pm_ib == cdb1_val ==> {
                        &&& new_metadata.header1 == new_header
                        &&& new_metadata.header2 == old_metadata.header2
                        &&& wrpm@.subrange(header1_pos + header_crc_offset, header1_pos + header_crc_offset + 8) =~=
                                spec_crc_bytes(wrpm@.subrange(header1_pos + header_head_offset, header1_pos + header_size))
                        &&& wrpm@.subrange(header1_pos as int, header1_pos + header_size) =~= new_header_bytes@
                    }
                    &&& old_data =~= new_data
                }),

        {
            let ghost original_wrpm = wrpm@;

            // write to the header that is NOT pointed to by the IB
            let header_pos = if self.incorruptible_bool == cdb0_val {
                header2_pos
            } else {
                header1_pos
            };

            // TODO: we could probably roll all of this into a single lemma that contains all of the proofs
            proof {
                let new_pm = update_contents_to_reflect_write(wrpm@, header_pos as int, new_header_bytes@);
                lemma_inactive_header_update_view(wrpm@, new_header_bytes@, header_pos as int);
                lemma_same_log_state(wrpm@, new_pm);
                assert(Self::recover(wrpm@) =~= Self::recover(new_pm));

                // prove crash consistency
                assert forall |chunks_flushed| {
                    let new_pm = #[trigger] update_contents_to_reflect_partially_flushed_write(
                        wrpm@, header_pos as int, new_header_bytes@, chunks_flushed);
                    perm.check_permission(new_pm)
                } by {
                    let new_pm = update_contents_to_reflect_partially_flushed_write(
                        wrpm@, header_pos as int, new_header_bytes@, chunks_flushed);
                    lemma_inactive_header_update_view_crash(wrpm@, new_header_bytes@, header_pos as int, chunks_flushed);
                    lemma_same_log_state(wrpm@, new_pm);
                    assert(permissions_depend_only_on_recovery_view(perm));
                    lemma_same_permissions(wrpm@, new_pm, perm);
                }
            }
            wrpm.write(header_pos, new_header_bytes.as_slice(), Tracked(perm));
            proof {
                // TODO: clean up once ib update is done. put this all in a lemma
                assert(Self::recover(wrpm@).is_Some());
                let (_, headers, _) = pm_to_views(wrpm@);
                assert(wrpm@.subrange(header_pos as int, header_pos + header_size) =~= new_header_bytes@);
                lemma_header_correct(wrpm@, new_header_bytes@, header_pos as int);

                // live header is unchanged
                let live_header_pos = if header_pos == header1_pos {
                    header2_pos
                } else {
                    assert(header_pos == header2_pos);
                    header1_pos
                };

                // TODO: refactor into a lemma (ideally lemma_header_correct)
                assert(old(wrpm)@.subrange(live_header_pos as int, live_header_pos + header_size) =~=
                        wrpm@.subrange(live_header_pos as int, live_header_pos + header_size));
                assert(old(wrpm)@.subrange(live_header_pos + header_crc_offset, live_header_pos + header_crc_offset + 8) =~=
                    spec_crc_bytes(old(wrpm)@.subrange(live_header_pos + header_head_offset, live_header_pos + header_size)));
                assert(old(wrpm)@.subrange(live_header_pos + header_crc_offset, live_header_pos + header_crc_offset + 8) =~=
                    wrpm@.subrange(live_header_pos + header_crc_offset, live_header_pos + header_crc_offset + 8));
                assert(old(wrpm)@.subrange(live_header_pos + header_head_offset, live_header_pos + header_size) =~=
                    wrpm@.subrange(live_header_pos + header_head_offset, live_header_pos + header_size));

                assert(wrpm@.subrange(live_header_pos + header_crc_offset, live_header_pos + header_crc_offset + 8) =~=
                    spec_crc_bytes(wrpm@.subrange(live_header_pos + header_head_offset, live_header_pos + header_size)));
            }
        }

        // Since untrusted_setup doesn't take a WriteRestrictedPersistentMemory, it is not guaranteed
        // to perform crash-safe updates.
        pub exec fn untrusted_setup<PM>(pm: &mut PM, device_size: u64) -> (result: Result<u64, InfiniteLogErr>)
            where
                PM: PersistentMemory
            requires
                old(pm).inv(),
                old(pm)@.len() == device_size
            ensures
                pm.inv(),
                pm.constants() == old(pm).constants(),
                pm@.len() == device_size,
                match result {
                    Ok(capacity) => Self::recover(pm@) ==
                                Some(AbstractInfiniteLogState::initialize(capacity as int)),
                    Err(InfiniteLogErr::InsufficientSpaceForSetup{ required_space }) => device_size < required_space,
                    _ => false
                }
        {
            if device_size <= contents_offset {
                return Err(InfiniteLogErr::InsufficientSpaceForSetup { required_space: contents_offset + 1 });
            }

            let log_size = device_size - contents_offset;

            let log_header_metadata = PersistentHeaderMetadata {
                head: 0,
                tail: 0,
                log_size
            };
            let metadata_bytes = metadata_to_bytes(&log_header_metadata);
            let crc_bytes = bytes_crc(&metadata_bytes);
            let log_header = PersistentHeader {
                crc: u64_from_le_bytes(crc_bytes.as_slice()),
                metadata: log_header_metadata,
            };
            let header_bytes = header_to_bytes(&log_header);

            let initial_ib_bytes = u64_to_le_bytes(cdb0_val);
            pm.write(header1_pos, header_bytes.as_slice());
            pm.write(incorruptible_bool_pos, initial_ib_bytes.as_slice());

            proof {
                lemma_auto_spec_u64_to_from_le_bytes();
                assert(pm@.subrange(header1_pos as int, header1_pos + header_size) =~= header_bytes@);
                assert(pm@.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8) =~= initial_ib_bytes@);
                lemma_header_split_into_bytes(crc_bytes@, metadata_bytes@, header_bytes@);
                assert(pm@.subrange(header1_pos + header_head_offset, header1_pos + header_size) =~= metadata_bytes@);
                lemma_header_match(pm@, header1_pos as int, log_header);
                let log_state = Self::recover(pm@);
                match log_state {
                    Some(log_state) => {
                        assert(log_state.head == 0);
                        assert(log_state.log == Seq::<u8>::empty());
                        assert(log_state.capacity == log_size - 1);
                    }
                    None => assert(false),
                }
            }

            Ok(log_size - 1)
        }

        pub exec fn untrusted_start<Perm, PM>(wrpm: &mut WriteRestrictedPersistentMemory<Perm, PM>,
                                              device_size: u64,
                                              Tracked(perm): Tracked<&Perm>)
                                              -> (result: Result<UntrustedLogImpl, InfiniteLogErr>)
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
            requires
                Self::recover(old(wrpm)@).is_Some(),
                old(wrpm).inv(),
                old(wrpm)@.len() == device_size,
                header_crc_offset < header_crc_offset + crc_size <= header_head_offset < header_tail_offset < header_log_size_offset,
                // The restriction on writing persistent memory during initialization is
                // that it can't change the interpretation of that memory's contents.
                ({
                    forall |pm_state| #[trigger] perm.check_permission(pm_state) <==>
                        Self::recover(pm_state) ==
                        Self::recover(old(wrpm)@)
                }),
            ensures
                Self::recover(old(wrpm)@) == Self::recover(wrpm@),
                wrpm.constants() == old(wrpm).constants(),
                match result {
                    Ok(log_impl) => log_impl.inv(wrpm),
                    Err(InfiniteLogErr::CRCMismatch) => !wrpm.constants().impervious_to_corruption,
                    _ => false
                }
        {
            let pm = wrpm.get_pm_ref();
            assert (device_size > contents_offset);

            let ib = match Self::read_incorruptible_boolean(pm) {
                Ok(ib) => ib,
                Err(e) => return Err(e)
            };

            let header_pos = if ib == cdb0_val {
                header1_pos
            } else {
                assert(ib == cdb1_val);
                header2_pos
            };
            let crc_bytes = pm.read(header_pos + header_crc_offset, 8);
            let ghost crc_addrs = Seq::<int>::new(8, |i: int| i + header_pos + header_crc_offset);
            let header_bytes = pm.read(header_pos + header_head_offset, header_size - header_head_offset);
            let ghost header_addrs = Seq::<int>::new((header_size - header_head_offset) as nat, |i: int| i + header_pos + header_head_offset);

            let header = if u64_from_le_bytes(bytes_crc(&header_bytes).as_slice()) == u64_from_le_bytes(crc_bytes.as_slice()) {
                proof {
                    lemma_auto_spec_u64_to_from_le_bytes();
                    lemma_u64_bytes_eq(spec_u64_from_le_bytes(spec_crc_bytes(header_bytes@)), spec_u64_from_le_bytes(crc_bytes@));
                    if !wrpm.constants().impervious_to_corruption {
                        axiom_bytes_uncorrupted(
                            header_bytes@,
                            pm@.subrange(header_pos + header_head_offset, header_pos + header_size),
                            header_addrs,
                            crc_bytes@,
                            pm@.subrange(header_pos + header_crc_offset, header_pos + header_crc_offset + 8),
                            crc_addrs,
                        );
                    }
                }
                crc_and_metadata_bytes_to_header(crc_bytes.as_slice(), header_bytes.as_slice())
            } else {
                return Err(InfiniteLogErr::CRCMismatch);
            };

            let head = header.metadata.head;
            let tail = header.metadata.tail;
            let log_size = header.metadata.log_size;
            // check log validity now that we have its uncorrupted metadata
            assert(device_size == log_size + contents_offset);
            assert(head <= tail);
            assert(tail - head < log_size);

            let untrusted_log = UntrustedLogImpl {
                incorruptible_bool: ib,
                header_crc: u64_from_le_bytes(crc_bytes.as_slice()),
                head,
                tail,
                log_size
            };

            proof { lemma_pm_state_header(pm@); }
            Ok(untrusted_log)
        }

        pub exec fn untrusted_append<Perm, PM>(
            &mut self,
            wrpm: &mut WriteRestrictedPersistentMemory<Perm, PM>,
            bytes_to_append: &Vec<u8>,
            Tracked(perm): Tracked<&Perm>
        ) -> (result: Result<u64, InfiniteLogErr>)
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
            requires
                old(self).inv(&*old(wrpm)),
                Self::recover(old(wrpm)@).is_Some(),
                ({
                    let old_log_state = Self::recover(old(wrpm)@);
                    forall |pm_state| #[trigger] perm.check_permission(pm_state) <==> {
                        let log_state = Self::recover(pm_state);
                        log_state == old_log_state || log_state == Some(old_log_state.unwrap().append(bytes_to_append@))
                    }
                }),
            ensures
                self.inv(wrpm),
                wrpm.constants() == old(wrpm).constants(),
                ({
                    let old_log_state = Self::recover(old(wrpm)@);
                    let new_log_state = Self::recover(wrpm@);
                    match (result, old_log_state, new_log_state) {
                        (Ok(offset), Some(old_log_state), Some(new_log_state)) => {
                            &&& offset as nat == old_log_state.log.len() + old_log_state.head
                            &&& new_log_state == old_log_state.append(bytes_to_append@)
                        },
                        (Err(InfiniteLogErr::InsufficientSpaceForAppend{ available_space }), _, _) => {
                            &&& new_log_state == old_log_state
                            &&& available_space < bytes_to_append@.len()
                            &&& {
                                   let log = old_log_state.unwrap();
                                   ||| available_space == log.capacity - log.log.len()
                                   ||| available_space == u64::MAX - log.head - log.log.len()
                               }
                        },
                        (_, _, _) => false
                    }
                }),
        {
            assert(permissions_depend_only_on_recovery_view(perm));

            let pm = wrpm.get_pm_ref();
            let ghost original_pm = wrpm@;

            let physical_head = Self::addr_logical_to_physical(self.head, self.log_size);
            let physical_tail = Self::addr_logical_to_physical(self.tail, self.log_size);
            let contents_end = self.log_size + contents_offset;
            let append_size: u64 = bytes_to_append.len() as u64;
            let old_logical_tail = self.tail;

            if self.tail > u64::MAX - append_size {
                Err(InfiniteLogErr::InsufficientSpaceForAppend{ available_space: u64::MAX - self.tail })
            }
            else if append_size >= self.log_size - (self.tail - self.head) {
                Err(InfiniteLogErr::InsufficientSpaceForAppend{ available_space: self.log_size - 1 - (self.tail - self.head) })
            } else {
                let mut header_metadata =
                    PersistentHeaderMetadata { head: self.head, tail: self.tail, log_size: self.log_size };
                assert(header_metadata == spec_get_live_header(wrpm@).metadata);

                if physical_head <= physical_tail {
                    if physical_tail >= contents_end - append_size {
                        // wrap case
                        self.append_wrap(wrpm, bytes_to_append, &header_metadata, Tracked(perm));
                    } else {
                        // no wrap
                        self.append_no_wrap(wrpm, bytes_to_append, &header_metadata, Tracked(perm));
                    }
                } else { // physical_tail < physical_head
                    if physical_tail + append_size >= physical_head {
                        return Err(InfiniteLogErr::InsufficientSpaceForAppend { available_space: physical_head - physical_tail });
                    }
                    // no wrap
                    self.append_no_wrap(wrpm, bytes_to_append, &header_metadata, Tracked(perm));
                }

                let new_tail = self.tail + append_size;
                header_metadata.tail = new_tail;

                let mut metadata_bytes = metadata_to_bytes(&header_metadata);
                let new_crc_bytes = bytes_crc(&metadata_bytes);
                let new_crc_val = u64_from_le_bytes(new_crc_bytes.as_slice());
                let ghost old_metadata_bytes = metadata_bytes@;
                let mut new_header_bytes = new_crc_bytes;
                new_header_bytes.append(&mut metadata_bytes);

                proof { lemma_header_crc_correct(new_header_bytes@, new_crc_bytes@, old_metadata_bytes); }

                self.update_header(wrpm, Tracked(perm), &new_header_bytes);

                // update incorruptible boolean
                let old_ib = self.incorruptible_bool;
                let new_ib = if old_ib == cdb0_val {
                    cdb1_val
                } else {
                    assert(old_ib == cdb1_val);
                    cdb0_val
                };
                let new_ib_bytes = u64_to_le_bytes(new_ib);

                proof {
                    lemma_append_ib_update(wrpm@, new_ib, bytes_to_append@, new_header_bytes@, perm);
                }

                wrpm.write(incorruptible_bool_pos, new_ib_bytes.as_slice(), Tracked(perm));
                self.incorruptible_bool = new_ib;
                self.tail = new_tail;
                self.header_crc = new_crc_val;

                Ok(old_logical_tail)
            }
        }

        exec fn append_no_wrap<Perm, PM>(
            &mut self,
            wrpm: &mut WriteRestrictedPersistentMemory<Perm, PM>,
            bytes_to_append: &Vec<u8>,
            old_header: &PersistentHeaderMetadata,
            Tracked(perm): Tracked<&Perm>
        )
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
            requires
                permissions_depend_only_on_recovery_view(perm),
                perm.check_permission(old(wrpm)@),
                old(self).inv(&*old(wrpm)),
                Self::recover(old(wrpm)@).is_Some(),
                old_header == spec_get_live_header(old(wrpm)@).metadata,
                // TODO: clean up
                ({
                    let physical_tail = spec_addr_logical_to_physical(old_header.tail as int, old_header.log_size as int);
                    physical_tail + bytes_to_append@.len() < old_header.log_size + contents_offset
                }),
                ({
                    let physical_head = spec_addr_logical_to_physical(old_header.head as int, old_header.log_size as int);
                    let physical_tail = spec_addr_logical_to_physical(old_header.tail as int, old_header.log_size as int);
                    let contents_end = old_header.log_size + contents_offset;
                    &&& physical_head <= physical_tail ==> physical_tail + bytes_to_append@.len() < contents_end
                    &&& physical_tail < physical_head ==> physical_tail <= physical_tail + bytes_to_append@.len() < physical_head
                })
            ensures
                self.inv(wrpm),
                wrpm.constants() == old(wrpm).constants(),
                Self::recover(wrpm@).is_Some(),
                match (Self::recover(old(wrpm)@), Self::recover(wrpm@)) {
                    (Some(old_log_state), Some(new_log_state)) => old_log_state =~= new_log_state,
                    _ => false
                },
                ({
                    let (old_ib, old_headers, old_data) = pm_to_views(old(wrpm)@);
                    let (new_ib, new_headers, new_data) = pm_to_views(wrpm@);
                    let physical_tail = spec_addr_logical_to_physical(old_header.tail as int, old_header.log_size as int);
                    &&& old_ib == new_ib
                    &&& old_headers == new_headers
                    &&& new_data.subrange(physical_tail - contents_offset, physical_tail - contents_offset + bytes_to_append@.len() as int) =~= bytes_to_append@
                    &&& new_data.subrange(0, physical_tail - contents_offset) =~= old_data.subrange(0, physical_tail - contents_offset)
                    &&& new_data.subrange(physical_tail - contents_offset + bytes_to_append@.len(), new_data.len() as int) =~=
                            old_data.subrange(physical_tail - contents_offset + bytes_to_append@.len(), old_data.len() as int)
                })
        {
            let physical_tail = Self::addr_logical_to_physical(old_header.tail, old_header.log_size);
            proof { lemma_data_write_is_safe(wrpm@, bytes_to_append@, physical_tail as int, perm); }
            wrpm.write(physical_tail, bytes_to_append.as_slice(), Tracked(perm));
            proof {
                assert(wrpm@.subrange(0, physical_tail as int) =~= old(wrpm)@.subrange(0, physical_tail as int));
                lemma_subrange_equality_implies_subsubrange_equality(wrpm@, old(wrpm)@, 0, physical_tail as int);
            }
        }

        pub exec fn append_wrap<Perm, PM>(
            &mut self,
            wrpm: &mut WriteRestrictedPersistentMemory<Perm, PM>,
            bytes_to_append: &Vec<u8>,
            old_header: &PersistentHeaderMetadata,
            Tracked(perm): Tracked<&Perm>
        )
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
            requires
                permissions_depend_only_on_recovery_view(perm),
                perm.check_permission(old(wrpm)@),
                old(self).inv(&*old(wrpm)),
                Self::recover(old(wrpm)@).is_Some(),
                old_header == spec_get_live_header(old(wrpm)@).metadata,
                ({
                    let physical_head = spec_addr_logical_to_physical(old_header.head as int, old_header.log_size as int);
                    let physical_tail = spec_addr_logical_to_physical(old_header.tail as int, old_header.log_size as int);
                    let contents_end = old_header.log_size + contents_offset;
                    &&& contents_offset < physical_head
                    &&& physical_tail + bytes_to_append@.len() >= contents_end
                    &&& physical_head <= physical_tail
                    &&& bytes_to_append@.len() <= old_header.log_size - (old_header.tail - old_header.head)
                }),
            ensures
                self.inv(wrpm),
                Self::recover(wrpm@).is_Some(),
                wrpm.constants() == old(wrpm).constants(),
                match (Self::recover(old(wrpm)@), Self::recover(wrpm@)) {
                    (Some(old_log_state), Some(new_log_state)) => old_log_state =~= new_log_state,
                    _ => false
                },
                ({
                    let (old_ib, old_headers, old_data) = pm_to_views(old(wrpm)@);
                    let (new_ib, new_headers, new_data) = pm_to_views(wrpm@);
                    let contents_end = old_header.log_size + contents_offset;
                    let physical_tail = spec_addr_logical_to_physical(old_header.tail as int, old_header.log_size as int);
                    let len1 = (contents_end - physical_tail);
                    let len2 = bytes_to_append@.len() - len1;
                    &&& old_ib == new_ib
                    &&& old_headers == new_headers
                    &&& new_data.subrange(physical_tail - contents_offset, contents_end - contents_offset) =~= bytes_to_append@.subrange(0, len1)
                    &&& new_data.subrange(0, len2 as int) =~= bytes_to_append@.subrange(len1 as int, bytes_to_append@.len() as int)
                    &&& new_data.subrange(len2 as int, physical_tail - contents_offset) =~= old_data.subrange(len2 as int, physical_tail - contents_offset)
                    &&& bytes_to_append@ =~= new_data.subrange(physical_tail - contents_offset, contents_end - contents_offset) + new_data.subrange(0, len2 as int)
                })
        {
            let physical_head = Self::addr_logical_to_physical(old_header.head, old_header.log_size);
            let physical_tail = Self::addr_logical_to_physical(old_header.tail, old_header.log_size);
            let contents_end = old_header.log_size + contents_offset;
            let append_size = bytes_to_append.len();

            let len1 = (contents_end - physical_tail) as usize;
            let len2 = bytes_to_append.len() - len1;
            let append_bytes_slice = bytes_to_append.as_slice();
            let bytes1 = slice_subrange(append_bytes_slice, 0, len1);
            let bytes2 = slice_subrange(append_bytes_slice, len1, append_size);

            proof { lemma_data_write_is_safe(wrpm@, bytes1@, physical_tail as int, perm); }
            wrpm.write(physical_tail, bytes1, Tracked(perm));

            proof { lemma_data_write_is_safe(wrpm@, bytes2@, contents_offset as int, perm); }
            wrpm.write(contents_offset, bytes2, Tracked(perm));

            proof {
                assert(wrpm@.subrange(0, contents_offset as int) =~= old(wrpm)@.subrange(0, contents_offset as int));
                lemma_subrange_equality_implies_subsubrange_equality(wrpm@, old(wrpm)@, 0, contents_offset as int);
            }
        }

        pub exec fn untrusted_advance_head<Perm, PM>(
            &mut self,
            wrpm: &mut WriteRestrictedPersistentMemory<Perm, PM>,
            new_head: u64,
            Tracked(perm): Tracked<&Perm>
        ) -> (result: Result<(), InfiniteLogErr>)
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
            requires
                old(self).inv(&*old(wrpm)),
                Self::recover(old(wrpm)@).is_Some(),
                ({
                    let old_log_state = Self::recover(old(wrpm)@);
                    forall |pm_state| #[trigger] perm.check_permission(pm_state) <==> {
                        let log_state = Self::recover(pm_state);
                        ||| log_state == old_log_state
                        ||| log_state == Some(old_log_state.unwrap().advance_head(new_head as int))
                    }
                })
            ensures
                self.inv(wrpm),
                wrpm.constants() == old(wrpm).constants(),
                ({
                    let old_log_state = Self::recover(old(wrpm)@);
                    let new_log_state = Self::recover(wrpm@);
                    match (result, old_log_state, new_log_state) {
                        (Ok(_), Some(old_log_state), Some(new_log_state)) => {
                            &&& old_log_state.head <= new_head <= old_log_state.head + old_log_state.log.len()
                            &&& new_log_state == old_log_state.advance_head(new_head as int)
                        },
                        (Err(InfiniteLogErr::CantAdvanceHeadPositionBeforeHead{ head }), Some(old_log_state), Some(new_log_state)) => {
                            &&& new_log_state == old_log_state
                            &&& head == old_log_state.head
                            &&& new_head < head
                        },
                        (Err(InfiniteLogErr::CantAdvanceHeadPositionBeyondTail{ tail }), Some(old_log_state), Some(new_log_state)) => {
                            &&& new_log_state == old_log_state
                            &&& tail == old_log_state.head + old_log_state.log.len()
                            &&& new_head > tail
                        },
                        (_, _, _) => false
                    }
                })
        {
            let pm = wrpm.get_pm_ref();
            let ghost original_pm = wrpm@;

            let live_header = PersistentHeader {
                crc: self.header_crc,
                metadata: PersistentHeaderMetadata { head: self.head, tail: self.tail, log_size: self.log_size }
            };

            if new_head < live_header.metadata.head {
                assert(self.header_crc == old(self).header_crc);
                return Err(InfiniteLogErr::CantAdvanceHeadPositionBeforeHead{ head: live_header.metadata.head });
            }

            if new_head > live_header.metadata.tail {
                assert(self.header_crc == old(self).header_crc);
                return Err(InfiniteLogErr::CantAdvanceHeadPositionBeyondTail{ tail: live_header.metadata.tail });
            }

            // copy the header and update it
            let mut new_header = live_header;
            new_header.metadata.head = new_head;
            let mut metadata_bytes = metadata_to_bytes(&new_header.metadata);
            let new_crc_bytes = bytes_crc(&metadata_bytes);
            let new_crc_val = u64_from_le_bytes(new_crc_bytes.as_slice());
            let ghost old_metadata_bytes = metadata_bytes@;
            let mut new_header_bytes = new_crc_bytes;
            new_header_bytes.append(&mut metadata_bytes);

            proof { lemma_header_crc_correct(new_header_bytes@, new_crc_bytes@, old_metadata_bytes); }

            self.update_header(wrpm, Tracked(perm), &new_header_bytes);

            // TODO: put ib update in a lemma
            let old_ib = self.incorruptible_bool;
            let new_ib = if old_ib == cdb0_val {
                cdb1_val
            } else {
                assert(old_ib == cdb1_val);
                cdb0_val
            };
            let new_ib_bytes = u64_to_le_bytes(new_ib);

            proof {
                lemma_auto_spec_u64_to_from_le_bytes();
                lemma_single_write_crash(wrpm@, incorruptible_bool_pos as int, new_ib_bytes@);
                assert(perm.check_permission(old(wrpm)@));
                let new_pm = update_contents_to_reflect_write(wrpm@, incorruptible_bool_pos as int, new_ib_bytes@);
                lemma_headers_unchanged(wrpm@, new_pm);
                assert(new_pm.subrange(incorruptible_bool_pos as int, incorruptible_bool_pos + 8) =~= new_ib_bytes@);

                let new_header = spec_bytes_to_header(new_header_bytes@);
                let (ib, headers, data) = pm_to_views(new_pm);
                let header_pos = if new_ib == cdb0_val {
                    header1_pos
                } else {
                    header2_pos
                };
                assert(new_pm.subrange(header_pos as int, header_pos + header_size) =~= new_header_bytes@);
                lemma_header_match(new_pm, header_pos as int, new_header);
                lemma_header_correct(new_pm, new_header_bytes@, header_pos as int);

                // prove that new pm has the advance head update
                let new_log_state = Self::recover(new_pm);
                let old_log_state = Self::recover(old(wrpm)@);
                match (new_log_state, old_log_state) {
                    (Some(new_log_state), Some(old_log_state)) => {
                        lemma_pm_state_header(new_pm);
                        lemma_pm_state_header(old(wrpm)@);
                        assert(new_log_state =~= old_log_state.advance_head(new_head as int));
                        assert(perm.check_permission(new_pm));
                    }
                    _ => assert(false),
                }
            }

            wrpm.write(incorruptible_bool_pos, new_ib_bytes.as_slice(), Tracked(perm));
            self.incorruptible_bool = new_ib;
            self.head = new_head;
            self.header_crc = new_crc_val;

            Ok(())
        }

        pub exec fn untrusted_read<Perm, PM>(
            &self,
            wrpm: &WriteRestrictedPersistentMemory<Perm, PM>,
            pos: u64,
            len: u64
        ) -> (result: Result<Vec<u8>, InfiniteLogErr>)
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
            requires
                self.inv(wrpm),
                Self::recover(wrpm@).is_Some(),
            ensures
                ({
                    let log = Self::recover(wrpm@).unwrap();
                    match result {
                        Ok(bytes) => {
                            let true_bytes = log.log.subrange(pos - log.head, pos + len - log.head);
                            &&& pos >= log.head
                            &&& pos + len <= log.head + log.log.len()
                            &&& read_correct_modulo_corruption(bytes@, true_bytes,
                                                             wrpm.constants().impervious_to_corruption)
                        },
                        Err(InfiniteLogErr::CantReadBeforeHead{ head: head_pos }) => {
                            &&& pos < log.head
                            &&& head_pos == log.head
                        },
                        Err(InfiniteLogErr::CantReadPastTail{ tail }) => {
                            &&& pos + len > log.head + log.log.len()
                            &&& tail == log.head + log.log.len()
                        },
                        _ => false
                    }
                })
        {
            let pm = wrpm.get_pm_ref();
            let physical_pos = Self::addr_logical_to_physical(pos, self.log_size);
            let contents_end = self.log_size + contents_offset;
            if pos < self.head {
                Err(InfiniteLogErr::CantReadBeforeHead{ head: self.head })
            } else if pos > u64::MAX - len {
                Err(InfiniteLogErr::CantReadPastTail{ tail: self.tail })
            } else if pos + len > self.tail {
                Err(InfiniteLogErr::CantReadPastTail{ tail: self.tail })
            } else {
                proof {
                    // we get a type error if we calculate physical head and tail in non-ghost code and use them here,
                    // so we need to calculate them here for the proof and again later for execution
                    let physical_head = spec_addr_logical_to_physical(self.head as int, self.log_size as int);
                    let physical_tail = spec_addr_logical_to_physical(self.tail as int, self.log_size as int);
                    if physical_head == physical_tail {
                        lemma_mod_equal(self.head as int, self.tail as int, self.log_size as int);
                        assert(len == 0);
                    } else if physical_head < physical_tail {
                        // read cannot wrap around
                        lemma_mod_between(self.log_size as int, self.head as int, self.tail as int, pos as int);
                        lemma_mod_difference_equal(self.head as int, pos as int, self.log_size as int);
                    } else {
                        // read may wrap around
                        lemma_mod_not_between(self.log_size as int, self.head as int, self.tail as int, pos as int);
                        if physical_pos <= physical_tail {
                            lemma_mod_wrapped_len(self.head as int, pos as int, self.log_size as int);
                        } else {
                            lemma_mod_difference_equal(self.head as int, pos as int, self.log_size as int);
                        }
                    }
                }

                let physical_head = Self::addr_logical_to_physical(self.head, self.log_size);
                let physical_tail = Self::addr_logical_to_physical(self.tail, self.log_size);

                let ghost log = Self::recover(pm@).unwrap();
                let ghost true_bytes = log.log.subrange(pos - log.head, pos + len - log.head);
                if physical_head == physical_tail {
                    assert (Seq::<u8>::empty() =~= log.log.subrange(pos - log.head, pos + len - log.head));
                    let buf = Vec::new();
                    let ghost addrs = Seq::<int>::empty();
                    assert (if wrpm.constants().impervious_to_corruption { buf@ == true_bytes }
                            else { maybe_corrupted(buf@, true_bytes, addrs) });
                    Ok(buf)
                } else if physical_pos >= physical_head && physical_pos >= contents_end - len {
                    let r1_len: u64 = contents_end - physical_pos;
                    let r2_len: u64 = len - r1_len;

                    let mut r1 = pm.read(physical_pos, r1_len);
                    let mut r2 = pm.read(contents_offset, r2_len);
                    let ghost r1_addrs = Seq::<int>::new(r1_len as nat, |i: int| i + physical_pos as int);
                    let ghost r2_addrs = Seq::<int>::new(r2_len as nat, |i: int| i + contents_offset as int);
                    let ghost addrs: Seq<int> = r1_addrs.add(r2_addrs);

                    r1.append(&mut r2);
                    assert (pm@.subrange(physical_pos as int, physical_pos + r1_len)
                                + pm@.subrange(contents_offset as int, contents_offset + r2_len)
                                =~= log.log.subrange(pos - log.head, pos + len - log.head));
                    assert (if wrpm.constants().impervious_to_corruption { r1@ == true_bytes }
                            else { maybe_corrupted(r1@, true_bytes, addrs) });
                    Ok(r1)
                } else {
                    assert (pm@.subrange(physical_pos as int, physical_pos + len) =~=
                                log.log.subrange(pos - log.head, pos + len - log.head));
                    let ghost addrs = Seq::<int>::new(len as nat, |i: int| i + physical_pos);
                    let buf = pm.read(physical_pos, len);
                    assert (if wrpm.constants().impervious_to_corruption { buf@ == true_bytes }
                            else { maybe_corrupted(buf@, true_bytes, addrs) });
                    Ok(buf)
                }
            }
        }

        pub exec fn untrusted_get_head_and_tail<Perm, PM>(
            &self,
            wrpm: &WriteRestrictedPersistentMemory<Perm, PM>
        ) -> (result: Result<(u64, u64, u64), InfiniteLogErr>)
            where
                Perm: CheckPermission<Seq<u8>>,
                PM: PersistentMemory
            requires
                self.inv(wrpm),
                Self::recover(wrpm@).is_Some()
            ensures
                match result {
                    Ok((result_head, result_tail, result_capacity)) =>
                        match Self::recover(wrpm@).unwrap() {
                            AbstractInfiniteLogState{ head: head, log: log, capacity: capacity } => {
                                &&& result_head == head
                                &&& result_tail == head + log.len()
                                &&& result_capacity == capacity
                            }
                        },
                    Err(_) => false,
                }
        {
            let pm = wrpm.get_pm_ref();
            proof { lemma_pm_state_header(pm@); }
            Ok((self.head, self.tail, self.log_size - 1))
        }
    }
}

================
File: ./pmemlog/src/lib.rs
================

#![allow(unused_imports)]
#![allow(dead_code)]
#![allow(non_upper_case_globals)]
#![allow(unused_variables)]

pub mod infinitelog_t;
pub mod logimpl_v;
pub mod main_t;
pub mod math;
pub mod pmemmock_t;
pub mod pmemspec_t;
pub mod sccf;

================
File: ./pmemlog/src/sccf.rs
================

/*
  Simple crash-consistency framework (open source)
*/

use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {

    pub open spec fn is_state_allowable<AbstractStorage, AbstractService>(
        pre_operation_state: AbstractStorage,
        crash_state: AbstractStorage,
        recovery_view: FnSpec(AbstractStorage) -> Option<AbstractService>,
        abstract_next: FnSpec(AbstractService, AbstractService) -> bool
        ) -> bool
    {
        let pre_operation_abstract_state = recovery_view(pre_operation_state);
        let crash_abstract_state = recovery_view(crash_state);
        ||| crash_abstract_state == pre_operation_abstract_state
        ||| {
                &&& pre_operation_abstract_state.is_Some()
                &&& crash_abstract_state.is_Some()
                &&& abstract_next(pre_operation_abstract_state.unwrap(), crash_abstract_state.unwrap())
            }
    }

    pub trait CheckPermission<AbstractStorage>
    {
        spec fn check_permission(&self, state: AbstractStorage) -> bool;
    }

}

================
File: ./pmemlog/src/pmemspec_t.rs
================

/*

  This file specifies, with the `PersistentMemory` type, the behavior
  of a persistent memory region. One of the things it models is what
  can happen to the persistent memory region if the system crashes in
  the middle of a write.

*/

use crate::sccf::CheckPermission;
use builtin::*;
use builtin_macros::*;
use vstd::bytes::*;
use vstd::prelude::*;
use vstd::set::*;
use vstd::slice::*;

#[cfg(not(verus_keep_ghost))]
use crc64fast::Digest;

verus! {

    pub open spec fn all_elements_unique(seq: Seq<int>) -> bool {
        forall |i: int, j: int| 0 <= i < j < seq.len() ==> seq[i] != seq[j]
    }

    pub closed spec fn maybe_corrupted_byte(byte: u8, true_byte: u8, addr: int) -> bool;

    pub open spec fn maybe_corrupted(bytes: Seq<u8>, true_bytes: Seq<u8>, addrs: Seq<int>) -> bool {
        &&& bytes.len() == true_bytes.len() == addrs.len()
        &&& forall |i: int| #![auto] 0 <= i < bytes.len() ==> maybe_corrupted_byte(bytes[i], true_bytes[i], addrs[i])
    }

    pub const crc_size: u64 = 8;

    pub closed spec fn spec_crc_bytes(header_bytes: Seq<u8>) -> Seq<u8>;

    #[verifier::external_body]
    pub exec fn bytes_crc(header_bytes: &Vec<u8>) -> (out: Vec<u8>)
        ensures
            spec_crc_bytes(header_bytes@) == out@,
            out@.len() == crc_size
    {
        #[cfg(not(verus_keep_ghost))]
        {
            let mut c = Digest::new();
            c.write(header_bytes.as_slice());
            u64_to_le_bytes(c.sum64())
        }
        #[cfg(verus_keep_ghost)]
        unimplemented!()
    }

    // We make two assumptions about how CRCs can be used to detect
    // corruption.

    // The first assumption, encapsulated in
    // `axiom_bytes_uncorrupted`, is that if we store byte sequences
    // `x` and `y` to persistent memory where `y` is the CRC of `x`,
    // then we can detect an absence of corruption by reading both of
    // them. Specifically, if we read from those locations and get
    // `x_c` and `y_c` (corruptions of `x` and `y` respectively), and
    // `y_c` is the CRC of `x_c`, then we can conclude that `x` wasn't
    // corrupted, i.e., that `x_c == x`.

    #[verifier(external_body)]
    pub proof fn axiom_bytes_uncorrupted(x_c: Seq<u8>, x: Seq<u8>, x_addrs: Seq<int>,
                                         y_c: Seq<u8>, y: Seq<u8>, y_addrs: Seq<int>)
        requires
            maybe_corrupted(x_c, x, x_addrs),
            maybe_corrupted(y_c, y, y_addrs),
            y == spec_crc_bytes(x),
            y_c == spec_crc_bytes(x_c),
            all_elements_unique(x_addrs),
            all_elements_unique(y_addrs)
        ensures
            x == x_c
    {}

    // The second assumption, encapsulated in
    // `axiom_corruption_detecting_boolean`, is that the values
    // `cdb0_val` and `cdb1_val` are so randomly different from each
    // other that corruption can't make one appear to be the other.
    // That is, if we know we wrote either `cdb0_val` or `cdb1_val` to
    // a certain part of persistent memory, and when we read that same
    // part we get `cdb0_val` or `cdb1_val`, we can assume it matches
    // what we last wrote to it. To justify the assumption that
    // `cdb0_val` and `cdb1_val` are different from each other, we set
    // them to CRC(b"0") and CRC(b"1"), respectively.

    pub const cdb0_val: u64 = 0xa32842d19001605e; // CRC(b"0")
    pub const cdb1_val: u64 = 0xab21aa73069531b7; // CRC(b"1")

    #[verifier(external_body)]
    pub proof fn axiom_corruption_detecting_boolean(cdb_c: u64, cdb: u64, addrs: Seq<int>)
        requires
            maybe_corrupted(spec_u64_to_le_bytes(cdb_c), spec_u64_to_le_bytes(cdb), addrs),
            all_elements_unique(addrs),
            cdb == cdb0_val || cdb == cdb1_val,
            cdb_c == cdb0_val || cdb_c == cdb1_val,
        ensures
            cdb_c == cdb
    {}

    pub struct PersistentMemoryConstants {
        pub impervious_to_corruption: bool
    }

    // We mark this as `external_body` so that the verifier can't see
    // that there's nothing important in it and thereby shortcut some
    // checks.

    pub trait PersistentMemory : Sized {
        spec fn view(self) -> Seq<u8>;

        spec fn inv(self) -> bool;

        spec fn constants(self) -> PersistentMemoryConstants;

        /// This is the model of some routine that reads the
        /// `num_bytes` bytes at address `addr`.
        fn read(&self, addr: u64, num_bytes: u64) -> (bytes: Vec<u8>)
            requires
                self.inv(),
                addr + num_bytes <= self@.len()
            ensures
                ({
                    let true_bytes = self@.subrange(addr as int, addr + num_bytes);
                    let addrs = Seq::<int>::new(num_bytes as nat, |i: int| i + addr);
                    if self.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    }
                    else {
                        maybe_corrupted(bytes@, true_bytes, addrs)
                    }
                });

        /// This is the model of some routine that writes `bytes`
        /// starting at address `addr`.
        fn write(&mut self, addr: u64, bytes: &[u8])
            requires
                old(self).inv(),
                addr + bytes@.len() <= (old(self))@.len(),
                addr + bytes@.len() <= u64::MAX
            ensures
                self.inv(),
                self.constants() == old(self).constants(),
                self@ == update_contents_to_reflect_write(old(self)@, addr as int, bytes@);
    }

    /// We model the persistent memory as getting flushed in chunks,
    /// where each chunk has `persistence_chunk_size` bytes. We refer
    /// to chunk number `id` as the set of addresses `addr` such that
    /// `addr / persistence_chunk_size == id`.
    pub spec const persistence_chunk_size: int = 8;

    /// Return the byte at address `addr` after writing
    /// `write_bytes` to address `write_addr`, if the byte at
    /// `addr` before the write was `prewrite_byte`.
    pub open spec fn update_byte_to_reflect_write(addr: int, prewrite_byte: u8, write_addr: int,
                                                  write_bytes: Seq<u8>) -> u8
    {
        if write_addr <= addr && addr < write_addr + write_bytes.len() {
            write_bytes[addr - write_addr]
        }
        else {
            prewrite_byte
        }
    }

    /// Return the contents of persistent memory after writing
    /// `write_bytes` to address `write_addr`, if the contents
    /// before the write was `prewrite_contents`.
    pub open spec(checked) fn update_contents_to_reflect_write(prewrite_contents: Seq<u8>, write_addr: int,
                                                               write_bytes: Seq<u8>) -> Seq<u8>
        recommends
            0 <= write_addr,
            write_addr + write_bytes.len() <= prewrite_contents.len(),
    {
        Seq::<u8>::new(prewrite_contents.len(),
                       |addr| update_byte_to_reflect_write(addr, prewrite_contents[addr],
                                                           write_addr, write_bytes))
    }

    /// Return the byte at address `addr` after initiating (but
    /// not necessarily completing) a write of `write_bytes` to
    /// address `write_addr`, given that the byte at `addr` before
    /// the write was `prewrite_byte` and given that the set of
    /// chunk IDs that have been flushed since the initiation of
    /// the write is `chunks_flushed`.
    pub open spec fn update_byte_to_reflect_partially_flushed_write(addr: int, prewrite_byte: u8, write_addr: int,
                                                                    write_bytes: Seq<u8>,
                                                                    chunks_flushed: Set<int>) -> u8
    {
        if chunks_flushed.contains(addr / persistence_chunk_size) {
            update_byte_to_reflect_write(addr, prewrite_byte, write_addr, write_bytes)
        }
        else {
            prewrite_byte
        }
    }

    /// Return the contents of persistent memory after initiating
    /// (but not necessarily completing) a write of `write_bytes`
    /// to address `write_addr`, given that the contents before
    /// the write were `prewrite_contents` and given that the set of
    /// chunk IDs that have been flushed since the initiation of
    /// the write is `chunks_flushed`.
    pub open spec(checked) fn update_contents_to_reflect_partially_flushed_write(contents: Seq<u8>, write_addr: int,
                                                                                 write_bytes: Seq<u8>,
                                                                                 chunks_flushed: Set<int>) -> Seq<u8>
        recommends
            0 <= write_addr,
            write_addr + write_bytes.len() <= contents.len(),
    {
        Seq::<u8>::new(contents.len(),
                       |addr| update_byte_to_reflect_partially_flushed_write(addr, contents[addr], write_addr,
                                                                             write_bytes, chunks_flushed))
    }

    /// A `WriteRestrictedPersistentMemory<P>` object wraps a
    /// `PersistentMemory` object to restrict how it's written.
    /// Untrusted code passed one of these can only write to the
    /// encapsulated persistent memory by providing a permission of
    /// type `P`. That permission must allow all possible states `s`
    /// such that crashing in the middle of the write might leave the
    /// persistent memory in state `s`.
    pub struct WriteRestrictedPersistentMemory<Perm, PM>
        where
            Perm: CheckPermission<Seq<u8>>,
            PM: PersistentMemory
    {
        pm: PM,
        ghost perm: Option<Perm> // unused, but Rust demands some reference to Perm
    }

    impl <Perm, PM> WriteRestrictedPersistentMemory<Perm, PM>
        where
            Perm: CheckPermission<Seq<u8>>,
            PM: PersistentMemory
    {
        pub closed spec fn view(self) -> Seq<u8> {
            self.pm@
        }

        pub closed spec fn inv(self) -> bool {
            self.pm.inv()
        }

        pub closed spec fn constants(self) -> PersistentMemoryConstants {
            self.pm.constants()
        }

        pub exec fn new(pm: PM) -> (wrpm: Self)
            requires
                pm.inv()
            ensures
                wrpm@ == pm@,
                wrpm.inv(),
                wrpm.constants() == pm.constants()
        {
            Self { pm: pm, perm: None }
        }

        pub exec fn get_pm_ref(&self) -> (pm: &PM)
            requires
                self.inv()
            ensures
                pm.inv(),
                pm@ == self@,
                pm.constants() == self.constants()
        {
            &self.pm
        }

        /// This `write` function can only be called if a crash in the
        /// middle of the requested write will leave the persistent
        /// memory in a state allowed by `perm`. The state must be
        /// allowed no matter what subset of the persistence chunks
        /// have been flushed.
        pub exec fn write(&mut self, addr: u64, bytes: &[u8], perm: Tracked<&Perm>)
            requires
                old(self).inv(),
                addr + bytes@.len() <= old(self)@.len(),
                addr + bytes@.len() <= u64::MAX,
                forall |chunks_flushed| {
                    let new_contents: Seq<u8> =
                        #[trigger] update_contents_to_reflect_partially_flushed_write(
                            old(self)@, addr as int, bytes@, chunks_flushed
                        );
                    perm@.check_permission(new_contents)
                },
            ensures
                self.inv(),
                self.constants() == old(self).constants(),
                self@ == update_contents_to_reflect_write(old(self)@, addr as int, bytes@),
        {
            self.pm.write(addr, bytes)
        }
    }

}

================
File: ./pmemlog/src/math.rs
================

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {

    /*
      From Ironfleet's math library's mul_nonlinear.i.dfy
    */

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_strictly_positive(x: int, y: int)
        ensures
            (0 < x && 0 < y) ==> (0 < x*y)
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_nonzero(x: int, y: int)
        ensures
            x*y != 0 <==> x != 0 && y != 0
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_is_associative(x: int, y: int, z: int)
        ensures
            x * (y * z) == (x * y) * z
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_is_distributive_add(x: int, y: int, z: int)
        ensures
            x*(y + z) == x*y + x*z
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_ordering(x: int, y: int)
        requires
            0 < x,
            0 < y,
            0 <= x*y
        ensures
            x <= x*y && y <= x*y
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_strict_inequality(x: int, y: int, z: int)
        requires
            x < y,
            z > 0
        ensures
            x*z < y*z
    {
    }

    pub proof fn lemma_mul_by_zero_is_zero(x: int)
        ensures
            0*x == 0,
            x*0 == 0
    {
    }

    /*
      From Ironfleet's math library's mul.i.dfy
    */

    #[verifier(opaque)]
    pub open spec fn mul_pos(x: int, y: int) -> int
        recommends
            x >= 0
        decreases
            x
    {
        if x <= 0 {
            0
        }
        else {
            y + mul_pos(x - 1, y)
        }
    }

    pub open spec fn mul_recursive(x: int, y: int) -> int
    {
        if x >= 0 {
            mul_pos(x, y)
        }
        else {
            -1 * mul_pos(-1 * x, y)
        }
    }

    pub proof fn lemma_mul_is_mul_pos(x: int, y: int)
        requires
            x >= 0
        ensures
            x * y == mul_pos(x, y)
        decreases
            x
    {
        reveal(mul_pos);
        if x > 0 {
            lemma_mul_is_mul_pos(x - 1, y);
            lemma_mul_is_distributive_add_other_way(y, x - 1, 1);
            assert (x * y == (x - 1) * y + y);
        }
    }

    pub proof fn lemma_mul_is_mul_recursive(x: int, y: int)
        ensures
            x * y == mul_recursive(x, y)
    {
        if (x >= 0) {
            lemma_mul_is_mul_pos(x, y);
        }
        else if (x <= 0) {
            lemma_mul_is_mul_pos(-x, y);
            lemma_mul_is_associative(-1, -x, y);
        }
    }

    pub proof fn lemma_mul_basics(x: int)
        ensures
            0 * x == 0,
            x * 0 == 0,
            1 * x == x,
            x * 1 == x
    {
    }

    pub proof fn lemma_mul_is_commutative(x: int, y: int)
        ensures
            x * y == y * x
    {
    }

    pub proof fn lemma_mul_inequality(x: int, y: int, z: int)
        requires
            x <= y,
            z >= 0,
        ensures
            x * z <= y * z
        decreases
            z
    {
        if z > 0 {
            lemma_mul_inequality(x, y, z - 1);
            lemma_mul_is_distributive_add(x, z - 1, 1);
            lemma_mul_basics(x);
            assert (x * z == x * (z - 1) + x);
            lemma_mul_is_distributive_add(y, z - 1, 1);
            lemma_mul_basics(y);
            assert (y * z == y * (z - 1) + y);
        }
    }

    pub proof fn lemma_mul_upper_bound(x: int, x_bound: int, y: int, y_bound: int)
        requires
            x <= x_bound,
            y <= y_bound,
            0 <= x,
            0 <= y
        ensures
            x * y <= x_bound * y_bound
    {
        lemma_mul_inequality(x, x_bound, y);
        lemma_mul_inequality(y, y_bound, x_bound);
    }

    /// This lemma is less precise than the non-strict version, since
    /// it uses two < facts to achieve only one < result. Thus, use it with
    /// caution -- it may be throwing away precision you'll require later.
    #[verifier(nonlinear)]
    pub proof fn lemma_mul_strict_upper_bound(x: int, x_bound: int, y: int, y_bound: int)
        requires
            x < x_bound,
            y < y_bound,
            0 <= x,
            0 <= y
        ensures
            x * y < x_bound * y_bound
        decreases
            y
    {
        lemma_mul_upper_bound(x, x_bound - 1, y, y_bound - 1);
        assert ((x_bound - 1) * (y_bound - 1) == x_bound * y_bound - y_bound - x_bound + 1);
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_left_inequality(x: int, y: int, z: int)
        requires
            x > 0
        ensures
            y <= z ==> x * y <= x * z,
            y < z ==> x * y < x * z
        decreases
            x
    {
        if x > 1 {
            lemma_mul_left_inequality(x - 1, y, z);
            assert (x * y == (x - 1) * y + y);
            assert (x * z == (x - 1) * z + z);
        }
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_inequality_converse(x: int, y: int, z: int)
        requires
            x*z <= y*z,
            z > 0
        ensures
            x <= y
        decreases
            z
    {
        if z > 1 {
            if (x * (z - 1) <= y * (z - 1)) {
                lemma_mul_inequality_converse(x, y, z - 1);
            }
            else {
                lemma_mul_inequality_converse(y, x, z - 1);
                assert (false);
            }
        }
    }

    pub proof fn lemma_mul_equality_converse(x: int, y: int, z: int)
        requires
            x*z == y*z,
            0<z
        ensures
            x==y
    {
        lemma_mul_inequality_converse(x, y, z);
        lemma_mul_inequality_converse(y, x, z);
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_is_distributive_add_other_way(x: int, y: int, z: int)
        ensures
            (y + z)*x == y*x + z*x
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_is_distributive_sub(x: int, y: int, z: int)
        ensures
            x*(y - z) == x*y - x*z
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_is_distributive_sub_other_way(x: int, y: int, z: int)
        ensures
            (y - z)*x == y*x - z*x
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_is_distributive(x: int, y: int, z: int)
        ensures
            x*(y + z) == x*y + x*z,
            x*(y - z) == x*y - x*z,
            (y + z)*x == y*x + z*x,
            (y - z)*x == y*x - z*x,
            x*(y + z) == (y + z)*x,
            x*(y - z) == (y - z)*x,
            x*y == y*x,
            x*z == z*x
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_strictly_increases(x: int, y: int)
        requires
            1 < x,
            0 < y
        ensures
            y < x*y
    {
        assert (x * y == (x - 1) * y + y);
        lemma_mul_strictly_positive(x - 1, y);
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_increases(x: int, y: int)
        requires
            0<x,
            0<y
        ensures
            y <= x*y
        decreases
            x
    {
        if x > 1 {
            lemma_mul_increases(x - 1, y);
            assert (x * y == (x - 1) * y + y);
        }
    }

    pub proof fn lemma_mul_nonnegative(x: int, y: int)
        requires
            0 <= x,
            0 <= y
        ensures
            0 <= x*y
    {
        if x != 0 && y != 0 {
            lemma_mul_strictly_positive(x, y);
        }
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_unary_negation(x: int, y: int)
        ensures
            (-x)*y == -(x*y),
            -(x*y) == x*(-y)
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_one_to_one(m: int, x: int, y: int)
        requires
            m!=0,
            m*x == m*y
        ensures
            x == y
    {
        if m > 0 {
            if x < y {
                lemma_mul_strict_inequality(x, y, m);
            }
            if x > y {
                lemma_mul_strict_inequality(y, x, m);
            }
        }
        else {
            assert (x * m == -(x * -m));
            assert (y * m == -(y * -m));
            if x < y {
                lemma_mul_strict_inequality(x, y, -m);
            }
            if x > y {
                lemma_mul_strict_inequality(y, x, -m);
            }
        }
    }

    /*
      From Ironfleet's math library's div_nonlinear.i.dfy
    */

    pub proof fn lemma_div_of_0(d: int)
        requires
            d != 0
        ensures
            0int / d == 0
    {
    }

    pub proof fn lemma_div_by_self(d: int)
        requires
              d != 0
        ensures
            d / d == 1
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_small_div(x: int, d: int)
        requires
            0 <= x < d,
            d > 0
        ensures
            x / d == 0
    {
    }

    pub proof fn lemma_mod_of_zero_is_zero(m: int)
        requires
            0 < m
        ensures
            0int % m == 0
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_fundamental_div_mod(x: int, d: int)
        requires
            d != 0
        ensures
            x == d * (x/d) + (x%d)
    {
    }

    #[verifier(nonlinear)]
    pub proof fn lemma_small_mod(x: int, m: int)
        requires
            0 <= x < m,
            0 < m
        ensures
            x % m == x
    {
    }

    pub proof fn lemma_mod_range(x: int, m: int)
        requires
            m > 0
        ensures
            0 <= x % m < m
    {
    }

    /*
      From Ironfleet's math library's mod_auto_proofs.i.dfy
    */

    pub proof fn lemma_mod_auto_basics(n: int, x: int)
        requires
            n > 0
        ensures
            (x + n) % n == x % n,
            (x - n) % n == x % n,
            (x + n) / n == x / n + 1,
            (x - n) / n == x / n - 1,
            0 <= x < n <==> x % n == x,
    {
        lemma_mod_range(x, n);
        lemma_fundamental_div_mod(x, n);
        lemma_fundamental_div_mod(x + n, n);
        lemma_fundamental_div_mod(x - n, n);
        lemma_mod_range(x, n);
        lemma_mod_range(x + n, n);
        lemma_mod_range(x - n, n);
        let zp = (x + n) / n - x / n - 1;
        let zm = (x - n) / n - x / n + 1;
        lemma_mul_is_distributive_sub(n, (x + n) / n, x / n + 1);
        lemma_mul_is_distributive_add(n, x / n, 1);
        assert (n * zp == n * ((x + n) / n) - n * (x / n) - n * 1);
        assert (0 == n * zp + ((x + n) % n) - (x % n));
        lemma_mul_is_distributive_sub(n, (x - n) / n, x / n - 1);
        lemma_mul_is_distributive_sub(n, x / n, 1);
        assert (n * zm == n * ((x - n) / n) - n * (x / n) + n * 1);
        assert (0 == n * zm + ((x - n) % n) - (x % n));
        if (zp > 0) { lemma_mul_inequality(1, zp, n); }
        if (zp < 0) { lemma_mul_inequality(zp, -1, n); }
        if (zp == 0) { lemma_mul_by_zero_is_zero(n); }
        if (zm > 0) { lemma_mul_inequality(1, zm, n); }
        if (zm < 0) { lemma_mul_inequality(zm, -1, n); }
        if 0 <= x < n {
            lemma_small_div(x, n);
        }
    }

    /*
      From Ironfleet's div.i.dfy
    */

    proof fn lemma_fundamental_div_mod_converse_helper_1(u: int, d: int, r: int)
        requires
            d != 0,
            0 <= r < d
        ensures
            u == (u * d + r) / d
        decreases
            if u >= 0 { u } else { -u }
    {
        if u < 0 {
            lemma_fundamental_div_mod_converse_helper_1(u + 1, d, r);
            lemma_mod_auto_basics(d, u * d + r);
            lemma_mul_is_distributive_add_other_way(d, u + 1, -1);
            assert (u * d + r == (u + 1) * d + r - d);
            assert (u == (u * d + r) / d);
        }
        else if u == 0 {
            lemma_small_div(r, d);
            assert (u == 0 ==> u * d == 0) by (nonlinear_arith);
            assert (u == (u * d + r) / d);
        }
        else {
            lemma_fundamental_div_mod_converse_helper_1(u - 1, d, r);
            lemma_mod_auto_basics(d, (u - 1) * d + r);
            lemma_mul_is_distributive_add_other_way(d, u - 1, 1);
            assert (u * d + r == (u - 1) * d + r + d);
            assert (u == (u * d + r) / d);
        }
    }

    proof fn lemma_fundamental_div_mod_converse_helper_2(u: int, d: int, r: int)
        requires
            d != 0,
            0 <= r < d
        ensures
            r == (u * d + r) % d
        decreases
            if u >= 0 { u } else { -u }
    {
        if u < 0 {
            lemma_fundamental_div_mod_converse_helper_2(u + 1, d, r);
            lemma_mod_auto_basics(d, u * d + r);
            lemma_mul_is_distributive_add_other_way(d, u + 1, -1);
            assert (u * d == (u + 1) * d + (-1) * d);
            assert (u * d + r == (u + 1) * d + r - d);
            assert (r == (u * d + r) % d);
        }
        else if u == 0 {
            assert (u == 0 ==> u * d == 0) by (nonlinear_arith);
            if d > 0 {
                lemma_small_mod(r, d);
            }
            else {
                lemma_small_mod(r, -d);
            }
            assert (r == (u * d + r) % d);
        }
        else {
            lemma_fundamental_div_mod_converse_helper_2(u - 1, d, r);
            lemma_mod_auto_basics(d, (u - 1) * d + r);
            lemma_mul_is_distributive_add_other_way(d, u - 1, 1);
            assert (u * d + r == (u - 1) * d + r + d);
            assert (r == (u * d + r) % d);
        }
    }

    pub proof fn lemma_fundamental_div_mod_converse(x: int, d: int, q: int, r: int)
        requires
            d != 0,
            0 <= r < d,
            x == q * d + r
        ensures
            q == x / d,
            r == x % d
    {
        lemma_fundamental_div_mod_converse_helper_1(q, d, r);
        assert (q == (q * d + r) / d);
        lemma_fundamental_div_mod_converse_helper_2(q, d, r);
    }

    /*
      Lemmas we need for this project
    */

    pub proof fn lemma_div_relation_when_mods_have_same_order(d: int, x: int, y: int)
        requires
            d > 0,
            x < y,
            y - x <= d,
            x % d < y % d
        ensures
            y / d == x / d
    {
        lemma_fundamental_div_mod(x, d);
        lemma_fundamental_div_mod(y, d);
        lemma_mod_range(x, d);
        lemma_mod_range(y, d);

        lemma_mul_is_distributive_sub_other_way(d, y / d, x / d);
        lemma_mul_is_commutative(y / d, d);
        lemma_mul_is_commutative(x / d, d);

        if (y / d) > (x / d) {
            lemma_mul_inequality(1, (y / d) - (x / d), d);
            assert (((y / d) - (x / d)) * d >= 1 * d);
            assert ((y / d) * d - (x / d) * d >= d);
            assert (false);
        }
        if (y / d) < (x / d) {
            lemma_mul_inequality((y / d) - (x / d), -1, d);
            assert (((y / d) - (x / d)) * d <= (-1) * d);
            lemma_mul_is_distributive_sub_other_way(d, y / d, x / d);
            assert (false);
        }
    }

    pub proof fn lemma_div_relation_when_mods_have_same_order_alt(d: int, x: int, y: int)
        requires
            d > 0,
            x <= y,
            y - x < d,
            x % d <= y % d
        ensures
            y / d == x / d
    {
        lemma_fundamental_div_mod(x, d);
        lemma_fundamental_div_mod(y, d);
        lemma_mod_range(x, d);
        lemma_mod_range(y, d);

        lemma_mul_is_distributive_sub_other_way(d, y / d, x / d);
        lemma_mul_is_commutative(y / d, d);
        lemma_mul_is_commutative(x / d, d);

        if (y / d) > (x / d) {
            lemma_mul_inequality(1, (y / d) - (x / d), d);
            assert (((y / d) - (x / d)) * d >= 1 * d);
            assert (false);
        }
        if (y / d) < (x / d) {
            lemma_mul_inequality((y / d) - (x / d), -1, d);
            assert (((y / d) - (x / d)) * d <= (-1) * d);
            assert (false);
        }
    }

    pub proof fn lemma_div_relation_when_mods_have_different_order(d: int, x: int, y: int)
        requires
            d > 0,
            x < y,
            y - x <= d,
            y % d <= x % d
        ensures
            y / d == x / d + 1
    {
        lemma_fundamental_div_mod(x, d);
        lemma_fundamental_div_mod(y, d);
        lemma_mod_range(x, d);
        lemma_mod_range(y, d);

        lemma_mul_is_distributive_sub_other_way(d, y / d, x / d);
        lemma_mul_is_commutative(y / d, d);
        lemma_mul_is_commutative(x / d, d);

        if (y / d) > (x / d) + 1 {
            lemma_mul_inequality(2, (y / d) - (x / d), d);
            assert (((y / d) - (x / d)) * d >= 2 * d);
            assert (false);
        }
        if (y / d) <= (x / d) {
            lemma_mul_inequality(0, (x / d) - (y / d), d);
            assert (0 * d <= ((x / d) - (y / d)) * d);
            lemma_mul_is_commutative((x / d) - (y / d), d);
            lemma_mul_is_distributive_sub(d, x / d, y / d);
            assert (d * ((x / d) - (y / d)) == d * (x / d) - d * (y / d));
            assert (0 * d <= x - y - x % d + y % d);
            assert (false);
        }
    }

    pub proof fn lemma_div_relation_when_mods_have_different_order_alt(d: int, x: int, y: int)
        requires
            d > 0,
            x <= y,
            y - x < d,
            y % d < x % d
        ensures
            y / d == x / d + 1
    {
        lemma_fundamental_div_mod(x, d);
        lemma_fundamental_div_mod(y, d);
        lemma_mod_range(x, d);
        lemma_mod_range(y, d);

        lemma_mul_is_commutative(y / d, d);
        lemma_mul_is_commutative(x / d, d);

        if (y / d) > (x / d) + 1 {
            lemma_mul_inequality(2, (y / d) - (x / d), d);
            lemma_mul_is_distributive_sub_other_way(d, y / d, x / d);
            assert (((y / d) - (x / d)) * d >= 2 * d);
            assert (false);
        }
        if (y / d) <= (x / d) {
            lemma_mul_inequality(0, (x / d) - (y / d), d);
            assert (0 * d <= ((x / d) - (y / d)) * d);
            lemma_mul_is_commutative((x / d) - (y / d), d);
            lemma_mul_is_distributive_sub(d, x / d, y / d);
            assert (d * ((x / d) - (y / d)) == d * (x / d) - d * (y / d));
            assert (0 * d <= x - y - x % d + y % d);
            assert (false);
        }
    }

    pub proof fn lemma_mod_between(d: int, x: int, y: int, z: int)
        requires
            d > 0,
            x % d < y % d,
            y - x <= d,
            x <= z <= y
        ensures
            x % d <= z % d <= y % d
    {
        if y - x == d {
            lemma_mod_auto_basics(d, x);
            assert (y % d == x % d);
            assert (false);
        }
        else {
            lemma_fundamental_div_mod(x, d);
            lemma_fundamental_div_mod(y, d);
            lemma_fundamental_div_mod(z, d);
            lemma_mod_range(x, d);
            lemma_mod_range(y, d);
            lemma_mod_range(z, d);
            assert (d * (y / d) - d * (x / d) + y % d - x % d < d);
            assert (d * (y / d) - d * (x / d) < d);
            lemma_mul_is_distributive_sub(d, (y / d), (x / d));
            assert (d * ((y / d) - (x / d)) < d);

            lemma_div_relation_when_mods_have_same_order(d, x, y);

            let z_mod_d = x % d + (z - x);
            assert (z == (x / d) * d + z_mod_d) by {
                assert (z == d * (x / d) + z_mod_d);
                lemma_mul_is_commutative(d, (x / d));
            }
            lemma_fundamental_div_mod_converse(z, d, (x / d), z_mod_d);
        }
    }

    pub proof fn lemma_mod_not_between(d: int, x: int, y: int, z: int)
        requires
            d > 0,
            y % d < x % d,
            y - x <= d,
            x <= z <= y
        ensures
            z % d <= y % d || z % d >= x % d
    {
        if y - x == d {
            lemma_mod_auto_basics(d, x);
            assert (y % d == x % d);
            assert (false);
        }
        else {
            lemma_fundamental_div_mod(x, d);
            lemma_fundamental_div_mod(y, d);
            lemma_fundamental_div_mod(z, d);
            lemma_mod_range(x, d);
            lemma_mod_range(y, d);
            lemma_mod_range(z, d);
            assert (d * (y / d) - d * (x / d) + y % d - x % d >= 0);
            assert (d * (y / d) - d * (x / d) >= 0);
            lemma_mul_is_distributive_sub(d, (y / d), (x / d));
            assert (d * ((y / d) - (x / d)) >= 0);

            lemma_div_relation_when_mods_have_different_order(d, x, y);

            if y % d < z % d < x % d {
                lemma_div_relation_when_mods_have_different_order(d, z, y);
                lemma_div_relation_when_mods_have_same_order(d, z, x);
                assert (false);
            }
        }
    }

    pub proof fn lemma_mod_addition_when_bounded(x: int, y: int, d: int)
        requires
            d > 0,
            y >= 0,
            (x % d) + y < d,
        ensures
            (x + y) % d == (x % d) + y
    {
        lemma_fundamental_div_mod(x, d);
        lemma_mul_is_commutative(x / d, d);
        lemma_fundamental_div_mod_converse(x + y, d, x / d, x % d + y);
    }

    pub proof fn lemma_mod_difference_equal(x: int, y: int, d: int)
        requires
            d > 0,
            x <= y,
            x % d <= y % d,
            y - x < d
        ensures
            y % d - x % d == y - x
    {
        lemma_fundamental_div_mod(x, d);
        lemma_fundamental_div_mod(y, d);
        lemma_mod_range(x, d);
        lemma_mod_range(y, d);

        assert (d * (y / d) - d * (x / d) + y % d - x % d == y - x);
        lemma_mul_is_distributive_sub(d, y / d, x / d);
        assert (d * (y / d - x / d) + y % d - x % d == y - x);
        assert (0 <= d * (y / d - x / d) + y % d - x % d < d);
        lemma_div_relation_when_mods_have_same_order_alt(d, x, y);
        assert (y / d == x / d);
    }

    pub proof fn lemma_mod_wrapped_len(x: int, y: int, d: int)
        requires
            d > 0,
            x <= y,
            x % d > y % d,
            y - x < d
        ensures
            d - (x % d) + (y % d) == y - x
    {
        lemma_fundamental_div_mod(x, d);
        lemma_fundamental_div_mod(y, d);
        lemma_mod_range(x, d);
        lemma_mod_range(y, d);
        assert (d * (y / d) - d * (x / d) + y % d - x % d == y - x);
        lemma_mul_is_distributive_sub(d, y / d, x / d);
        assert (d * (y / d - x / d) + y % d - x % d == y - x);
        assert (0 <= d * (y / d - x / d) + y % d - x % d < d);
        lemma_div_relation_when_mods_have_different_order_alt(d, x, y);
        assert (y / d == x / d + 1);
        assert (y / d - x / d == 1 ==> d * (y / d - x / d) == d) by (nonlinear_arith);
    }

    pub proof fn lemma_mod_equal(x: int, y: int, d: int)
        requires
            d > 0,
            x <= y,
            x % d == y % d,
            y - x < d
        ensures
            x == y
    {
        lemma_mod_difference_equal(x, y, d);
    }

    pub proof fn lemma_mod_equal_converse(x: int, y: int, d: int)
        requires 
            d > 0,
            x == y,
        ensures 
            x % d == y % d
    {}

    pub proof fn lemma_mod_not_equal(x: int, y: int, d: int) 
        requires 
            d > 0,
            y - x < d,
            y - x >= 0,
            x != y,
        ensures 
            x % d != y % d
    {
        if x % d == y % d {
            if x < y {
                lemma_mod_equal(x, y, d);
                assert(false);
            } else {
                assert(y - x < 0);
                assert(false);
            }
        }

    }

    #[verifier(nonlinear)]
    pub proof fn lemma_mul_div_equal(x: int, q: int, d: int)
        requires
            q * d <= x < (q + 1) * d
        ensures
            (x / d) == q
    {}

    pub proof fn lemma_mod_subtract(x: int, y: int, d: int)
        requires
            d > 0,
            (x % d) + y >= d,
            0 <= y < d
        ensures
            (x % d) + y - d == (x + y) % d
    {
        assert(d <= (x % d) + y < 2 * d);
        assert((x / d) * d + d <= (x / d) * d + (x % d) + y < (x / d) * d + 2 * d);
        lemma_fundamental_div_mod(x, d);
        lemma_mul_is_commutative(x / d, d);
        lemma_mul_is_distributive_add_other_way(d, x / d, 1);
        lemma_mul_is_distributive_add_other_way(d, x / d, 2);
        assert((x / d + 1) * d <= x + y < (x / d + 2) * d);
        lemma_mul_div_equal(x + y, (x / d + 1), d);
        assert(x / d + 1 == (x + y) / d);
        lemma_fundamental_div_mod(x + y, d);
        assert(x + y == d * ((x + y) / d) + (x + y) % d);
    }
}

================
File: ./pmemlog/src/pmemmock_t.rs
================

use builtin::*;
use builtin_macros::*;
use crate::pmemspec_t::*;
use std::convert::*;
use vstd::prelude::*;

verus! {

    pub struct VolatileMemoryMockingPersistentMemory
    {
        contents: Vec<u8>
    }

    impl VolatileMemoryMockingPersistentMemory {
        #[verifier::external_body]
        pub fn new(device_size: u64) -> (result: Result<Self, ()>)
            ensures
                match result {
                    Ok(pm) => pm@.len() == device_size && pm.inv(),
                    Err(_) => true
                }
        {
            Ok(Self {contents: vec![0; device_size as usize]})
        }
    }

    impl PersistentMemory for VolatileMemoryMockingPersistentMemory {
        closed spec fn view(self) -> Seq<u8>
        {
            self.contents@
        }

        closed spec fn inv(self) -> bool
        {
            self.contents.len() <= u64::MAX
        }

        closed spec fn constants(self) -> PersistentMemoryConstants
        {
            PersistentMemoryConstants { impervious_to_corruption: true }
        }

        #[verifier::external_body]
        fn read(&self, addr: u64, num_bytes: u64) -> Vec<u8>
        {
            let addr_usize: usize = addr.try_into().unwrap();
            let num_bytes_usize: usize = num_bytes.try_into().unwrap();
            self.contents[addr_usize..addr_usize+num_bytes_usize].to_vec()
        }

        #[verifier::external_body]
        fn write(&mut self, addr: u64, bytes: &[u8])
        {
            let addr_usize: usize = addr.try_into().unwrap();
            self.contents.splice(addr_usize..addr_usize+bytes.len(), bytes.iter().cloned());
        }
    }

}

================
File: ./pmemlog/src/main.rs
================

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;
use vstd::slice::*;

pub mod infinitelog_t;
pub mod logimpl_v;
pub mod main_t;
pub mod math;
pub mod pmemmock_t;
pub mod pmemspec_t;
pub mod sccf;

use crate::main_t::*;
use crate::pmemspec_t::*;
use crate::pmemmock_t::*;

verus! {

    fn main() {
        let device_size: u64 = 4096;
        if let Ok(mut pm) = VolatileMemoryMockingPersistentMemory::new(device_size) {
            if let Ok(_) = InfiniteLogImpl::setup(&mut pm, device_size) {
                if let Ok(mut log) = InfiniteLogImpl::start(pm, device_size) {
                    let mut v: Vec<u8> = Vec::<u8>::new();
                    v.push(30); v.push(42); v.push(100);
                    if let Ok(pos) = log.append(&v) {
                        if let Ok((head, tail, capacity)) = log.get_head_and_tail() {
                            assert (head == 0);
                            assert (tail == 3);
                            // TODO: add an assertion using maybe_corrupted here
                            // if let Ok(bytes) = log.read(1, 2) {
                            //     assert (bytes@[0] == 42);
                            // }
                        }
                    }
                }
            }
        }
    }

}

================
File: ./deps_hack/Cargo.toml
================

[package]
name = "deps_hack"
version = "0.1.0"
edition = "2021"

[features]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
crc64fast = "1.0.0"
pmsafe = { path = "../pmsafe" }
rand = "0.8.5"

[target.'cfg(target_os = "windows")'.dependencies]
winapi = { version = "0.3.9", features = ["errhandlingapi", "fileapi", "handleapi", "memoryapi", "winbase", "winerror", "winnt"] }

[target.'cfg(target_os = "linux")'.dependencies]
nix = "0.27.1"

[target.'cfg(target_os = "linux")'.build-dependencies]
bindgen = "0.69.1"
================
File: ./deps_hack/wrapper.h
================

#include <libpmemlog.h>
#include <libpmem.h>
#include <fcntl.h>
================
File: ./deps_hack/build.rs
================

#[cfg(target_os = "linux")]
extern crate bindgen;

#[cfg(target_os = "linux")]
use std::path::PathBuf;

#[cfg(target_os = "linux")]
fn setup_linux()
{
    println!("cargo:rustc-link-search=libpmemlog1");
    println!("cargo:rustc-link-lib=pmemlog");
    println!("cargo:rustc-link-lib=pmem");

    let bindings = bindgen::Builder::default()
        .header("wrapper.h")
        .generate()
        .expect("Unable to generate bindings");

    let out_path = PathBuf::from("./src/");
    bindings
        .write_to_file(out_path.join("bindings.rs"))
        .expect("Couldn't write bindings!");
}

fn main() {
    #[cfg(target_os = "linux")]
    setup_linux();
}

================
File: ./deps_hack/rust-toolchain.toml
================

[toolchain]
channel = "1.76"
================
File: ./deps_hack/src/lib.rs
================

#![allow(non_upper_case_globals)]
#![allow(non_camel_case_types)]
#![allow(non_snake_case)]

pub use core;
pub use crc64fast;
#[cfg(target_os = "linux")]
pub use nix;
pub use rand;
#[cfg(target_os = "windows")]
pub use winapi;
#[cfg(target_os = "windows")]
pub use winapi::shared::winerror::SUCCEEDED;
#[cfg(target_os = "windows")]
pub use winapi::um::errhandlingapi::GetLastError;
#[cfg(target_os = "windows")]
pub use winapi::um::fileapi::{CreateFileA, OPEN_ALWAYS};
#[cfg(target_os = "windows")]
pub use winapi::um::handleapi::INVALID_HANDLE_VALUE;
#[cfg(target_os = "windows")]
pub use winapi::um::memoryapi::{MapViewOfFile, FILE_MAP_ALL_ACCESS};
#[cfg(target_os = "windows")]
pub use winapi::um::winbase::CreateFileMappingA;
#[cfg(target_os = "windows")]
pub use winapi::um::winnt::{
    FILE_ATTRIBUTE_NORMAL, FILE_SHARE_DELETE, FILE_SHARE_READ, FILE_SHARE_WRITE, GENERIC_READ,
    GENERIC_WRITE, PAGE_READWRITE, ULARGE_INTEGER,
};

pub use pmsafe::{PmSafe, PmSized, pmsized_primitive};

#[cfg(target_os = "linux")]
pub mod pmem;

#[cfg(target_os = "linux")]
include!("./bindings.rs");

================
File: ./deps_hack/src/pmem.rs
================

use crate::pmem_memcpy_nodrain;
use core::ffi::c_void;

pub unsafe fn pmem_memcpy_nodrain_helper(
    pm_ptr: *mut c_void,
    bytes_ptr: *const c_void,
    len: usize,
) {
    unsafe {
        pmem_memcpy_nodrain(pm_ptr, bytes_ptr, len);
    }
}

================
File: ./README.md
================

# Project

This repository contains code for verified storage systems. The code is
written in, and its correctness properties are verified with,
[Verus](https://github.com/verus-lang/verus).

This project contains the following crates: 

* `pmemlog` implements an append-only log on persistent memory. The
  implementation handles crash consistency, ensuring that even if the process
  or machine crashes, it acts like an append-only log across the crashes. It
  also handles bit corruption, detecting if metadata read from persistent
  memory is corrupted.
* `storage_node` is an in-progress persistent memory key-value store. Its structure 
  is further described in its [README](storage_node/README.md).
* `unverified` contains unverified mocks and tests related to the `storage_node` key value store. 
* `deps_hack` contains unverified dependencies that are imported by `storage_node`.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.

================
File: ./SUPPORT.md
================

# TODO: The maintainer of this repo has not yet edited this file

**REPO OWNER**: Do you want Customer Service & Support (CSS) support for this product/project?

- **No CSS support:** Fill out this template with information about how to file issues and get help.
- **Yes CSS support:** Fill out an intake form at [aka.ms/onboardsupport](https://aka.ms/onboardsupport). CSS will work with/help you to determine next steps.
- **Not sure?** Fill out an intake as though the answer were "Yes". CSS will help you decide.

*Then remove this first heading from this SUPPORT.MD file before publishing your repo.*

# Support

## How to file issues and get help  

This project uses GitHub Issues to track bugs and feature requests. Please search the existing 
issues before filing new issues to avoid duplicates.  For new issues, file your bug or 
feature request as a new Issue.

For help and questions about using this project, please **REPO MAINTAINER: INSERT INSTRUCTIONS HERE 
FOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER
CHANNEL. WHERE WILL YOU HELP PEOPLE?**.

## Microsoft Support Policy  

Support for this **PROJECT or PRODUCT** is limited to the resources listed above.

================
File: ./unverified/multilog_test/Cargo.toml
================

[package]
name = "multilog_test"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
multilog = { path = "../../multilog" }

================
File: ./unverified/multilog_test/src/main.rs
================

use multilog::test_multilog;

fn main() {
    println!("test_multilog returned {}", test_multilog());
}

================
File: ./unverified/metadata_kv/Cargo.toml
================

[package]
name = "metadata_kv"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
rand = "0.8"
vstd = { git = "https://github.com/verus-lang/verus.git" }
proptest = "1.4"

================
File: ./unverified/metadata_kv/Cargo.lock
================

# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 3

[[package]]
name = "autocfg"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d468802bab17cbc0cc575e9b053f41e72aa36bfa6b7f55e3529ffa43161b97fa"

[[package]]
name = "bit-set"
version = "0.5.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0700ddab506f33b20a03b13996eccd309a48e5ff77d0d95926aa0210fb4e95f1"
dependencies = [
 "bit-vec",
]

[[package]]
name = "bit-vec"
version = "0.6.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "349f9b6a179ed607305526ca489b34ad0a41aed5f7980fa90eb03160b69598fb"

[[package]]
name = "bitflags"
version = "1.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"

[[package]]
name = "bitflags"
version = "2.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "327762f6e5a765692301e5bb513e0d9fef63be86bbc14528052b1cd3e6f03e07"

[[package]]
name = "builtin"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git#2b607f74e57f0010a26c3e8d9f3b047a752f6c10"

[[package]]
name = "builtin_macros"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git#2b607f74e57f0010a26c3e8d9f3b047a752f6c10"
dependencies = [
 "prettyplease_verus",
 "proc-macro2",
 "quote",
 "syn",
 "syn_verus",
 "synstructure",
]

[[package]]
name = "cfg-if"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"

[[package]]
name = "errno"
version = "0.3.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a258e46cdc063eb8519c00b9fc845fc47bcfca4130e2f08e88665ceda8474245"
dependencies = [
 "libc",
 "windows-sys 0.52.0",
]

[[package]]
name = "fastrand"
version = "2.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "25cbce373ec4653f1a01a31e8a5e5ec0c622dc27ff9c4e6606eefef5cbbed4a5"

[[package]]
name = "fnv"
version = "1.0.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3f9eec918d3f24069decb9af1554cad7c880e2da24a9afd88aca000531ab82c1"

[[package]]
name = "getrandom"
version = "0.2.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fe9006bed769170c11f845cf00c7c1e9092aeb3f268e007c3e760ac68008070f"
dependencies = [
 "cfg-if",
 "libc",
 "wasi",
]

[[package]]
name = "lazy_static"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e2abad23fbc42b3700f2f279844dc832adb2b2eb069b2df918f455c4e18cc646"

[[package]]
name = "libc"
version = "0.2.151"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "302d7ab3130588088d277783b1e2d2e10c9e9e4a16dd9050e6ec93fb3e7048f4"

[[package]]
name = "libm"
version = "0.2.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4ec2a862134d2a7d32d7983ddcdd1c4923530833c9f2ea1a44fc5fa473989058"

[[package]]
name = "linux-raw-sys"
version = "0.4.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c4cd1a83af159aa67994778be9070f0ae1bd732942279cabb14f86f986a21456"

[[package]]
name = "metadata_kv"
version = "0.1.0"
dependencies = [
 "proptest",
 "rand",
 "vstd",
]

[[package]]
name = "num-traits"
version = "0.2.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "39e3200413f237f41ab11ad6d161bc7239c84dcb631773ccd7de3dfe4b5c267c"
dependencies = [
 "autocfg",
 "libm",
]

[[package]]
name = "ppv-lite86"
version = "0.2.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b40af805b3121feab8a3c29f04d8ad262fa8e0561883e7653e024ae4479e6de"

[[package]]
name = "prettyplease_verus"
version = "0.1.15"
source = "git+https://github.com/verus-lang/verus.git#2b607f74e57f0010a26c3e8d9f3b047a752f6c10"
dependencies = [
 "proc-macro2",
 "syn_verus",
]

[[package]]
name = "proc-macro2"
version = "1.0.70"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "39278fbbf5fb4f646ce651690877f89d1c5811a3d4acb27700c1cb3cdb78fd3b"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "proptest"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "31b476131c3c86cb68032fdc5cb6d5a1045e3e42d96b69fa599fd77701e1f5bf"
dependencies = [
 "bit-set",
 "bit-vec",
 "bitflags 2.4.1",
 "lazy_static",
 "num-traits",
 "rand",
 "rand_chacha",
 "rand_xorshift",
 "regex-syntax",
 "rusty-fork",
 "tempfile",
 "unarray",
]

[[package]]
name = "quick-error"
version = "1.2.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a1d01941d82fa2ab50be1e79e6714289dd7cde78eba4c074bc5a4374f650dfe0"

[[package]]
name = "quote"
version = "1.0.33"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5267fca4496028628a95160fc423a33e8b2e6af8a5302579e322e4b520293cae"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "rand"
version = "0.8.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "34af8d1a0e25924bc5b7c43c079c942339d8f0a8b57c39049bef581b46327404"
dependencies = [
 "libc",
 "rand_chacha",
 "rand_core",
]

[[package]]
name = "rand_chacha"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6c10a63a0fa32252be49d21e7709d4d4baf8d231c2dbce1eaa8141b9b127d88"
dependencies = [
 "ppv-lite86",
 "rand_core",
]

[[package]]
name = "rand_core"
version = "0.6.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec0be4795e2f6a28069bec0b5ff3e2ac9bafc99e6a9a7dc3547996c5c816922c"
dependencies = [
 "getrandom",
]

[[package]]
name = "rand_xorshift"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d25bf25ec5ae4a3f1b92f929810509a2f53d7dca2f50b794ff57e3face536c8f"
dependencies = [
 "rand_core",
]

[[package]]
name = "redox_syscall"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4722d768eff46b75989dd134e5c353f0d6296e5aaa3132e776cbdb56be7731aa"
dependencies = [
 "bitflags 1.3.2",
]

[[package]]
name = "regex-syntax"
version = "0.8.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c08c74e62047bb2de4ff487b251e4a92e24f48745648451635cec7d591162d9f"

[[package]]
name = "rustix"
version = "0.38.28"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72e572a5e8ca657d7366229cdde4bd14c4eb5499a9573d4d366fe1b599daa316"
dependencies = [
 "bitflags 2.4.1",
 "errno",
 "libc",
 "linux-raw-sys",
 "windows-sys 0.52.0",
]

[[package]]
name = "rusty-fork"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cb3dcc6e454c328bb824492db107ab7c0ae8fcffe4ad210136ef014458c1bc4f"
dependencies = [
 "fnv",
 "quick-error",
 "tempfile",
 "wait-timeout",
]

[[package]]
name = "syn"
version = "1.0.109"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72b64191b275b66ffe2469e8af2c1cfe3bafa67b529ead792a6d0160888b4237"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "syn_verus"
version = "1.0.95"
source = "git+https://github.com/verus-lang/verus.git#2b607f74e57f0010a26c3e8d9f3b047a752f6c10"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "synstructure"
version = "0.12.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f36bdaa60a83aca3921b5259d5400cbf5e90fc51931376a9bd4a0eb79aa7210f"
dependencies = [
 "proc-macro2",
 "quote",
 "syn",
 "unicode-xid",
]

[[package]]
name = "tempfile"
version = "3.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7ef1adac450ad7f4b3c28589471ade84f25f731a7a0fe30d71dfa9f60fd808e5"
dependencies = [
 "cfg-if",
 "fastrand",
 "redox_syscall",
 "rustix",
 "windows-sys 0.48.0",
]

[[package]]
name = "unarray"
version = "0.1.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "eaea85b334db583fe3274d12b4cd1880032beab409c0d774be044d4480ab9a94"

[[package]]
name = "unicode-ident"
version = "1.0.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3354b9ac3fae1ff6755cb6db53683adb661634f67557942dea4facebec0fee4b"

[[package]]
name = "unicode-xid"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f962df74c8c05a667b5ee8bcf162993134c104e96440b663c8daa176dc772d8c"

[[package]]
name = "vstd"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git#2b607f74e57f0010a26c3e8d9f3b047a752f6c10"
dependencies = [
 "builtin",
 "builtin_macros",
]

[[package]]
name = "wait-timeout"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9f200f5b12eb75f8c1ed65abd4b2db8a6e1b138a20de009dacee265a2498f3f6"
dependencies = [
 "libc",
]

[[package]]
name = "wasi"
version = "0.11.0+wasi-snapshot-preview1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9c8d87e72b64a3b4db28d11ce29237c246188f4f51057d65a7eab63b7987e423"

[[package]]
name = "windows-sys"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "677d2418bec65e3338edb076e806bc1ec15693c5d0104683f2efe857f61056a9"
dependencies = [
 "windows-targets 0.48.5",
]

[[package]]
name = "windows-sys"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "282be5f36a8ce781fad8c8ae18fa3f9beff57ec1b52cb3de0789201425d9a33d"
dependencies = [
 "windows-targets 0.52.0",
]

[[package]]
name = "windows-targets"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9a2fa6e2155d7247be68c096456083145c183cbbbc2764150dda45a87197940c"
dependencies = [
 "windows_aarch64_gnullvm 0.48.5",
 "windows_aarch64_msvc 0.48.5",
 "windows_i686_gnu 0.48.5",
 "windows_i686_msvc 0.48.5",
 "windows_x86_64_gnu 0.48.5",
 "windows_x86_64_gnullvm 0.48.5",
 "windows_x86_64_msvc 0.48.5",
]

[[package]]
name = "windows-targets"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8a18201040b24831fbb9e4eb208f8892e1f50a37feb53cc7ff887feb8f50e7cd"
dependencies = [
 "windows_aarch64_gnullvm 0.52.0",
 "windows_aarch64_msvc 0.52.0",
 "windows_i686_gnu 0.52.0",
 "windows_i686_msvc 0.52.0",
 "windows_x86_64_gnu 0.52.0",
 "windows_x86_64_gnullvm 0.52.0",
 "windows_x86_64_msvc 0.52.0",
]

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2b38e32f0abccf9987a4e3079dfb67dcd799fb61361e53e2882c3cbaf0d905d8"

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cb7764e35d4db8a7921e09562a0304bf2f93e0a51bfccee0bd0bb0b666b015ea"

[[package]]
name = "windows_aarch64_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc35310971f3b2dbbf3f0690a219f40e2d9afcf64f9ab7cc1be722937c26b4bc"

[[package]]
name = "windows_aarch64_msvc"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bbaa0368d4f1d2aaefc55b6fcfee13f41544ddf36801e793edbbfd7d7df075ef"

[[package]]
name = "windows_i686_gnu"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a75915e7def60c94dcef72200b9a8e58e5091744960da64ec734a6c6e9b3743e"

[[package]]
name = "windows_i686_gnu"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a28637cb1fa3560a16915793afb20081aba2c92ee8af57b4d5f28e4b3e7df313"

[[package]]
name = "windows_i686_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8f55c233f70c4b27f66c523580f78f1004e8b5a8b659e05a4eb49d4166cca406"

[[package]]
name = "windows_i686_msvc"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ffe5e8e31046ce6230cc7215707b816e339ff4d4d67c65dffa206fd0f7aa7b9a"

[[package]]
name = "windows_x86_64_gnu"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "53d40abd2583d23e4718fddf1ebec84dbff8381c07cae67ff7768bbf19c6718e"

[[package]]
name = "windows_x86_64_gnu"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3d6fa32db2bc4a2f5abeacf2b69f7992cd09dca97498da74a151a3132c26befd"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0b7b52767868a23d5bab768e390dc5f5c55825b6d30b86c844ff2dc7414044cc"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1a657e1e9d3f514745a572a6846d3c7aa7dbe1658c056ed9c3344c4109a6949e"

[[package]]
name = "windows_x86_64_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ed94fce61571a4006852b7389a063ab983c02eb1bb37b47f8272ce92d06d9538"

[[package]]
name = "windows_x86_64_msvc"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dff9641d1cd4be8d1a070daf9e3773c5f67e78b4d9d42263020c057706765c04"

================
File: ./unverified/metadata_kv/src/metadata_kv.rs
================

use std::collections::HashMap;

/// The `MetadataStore` module provides an in-memory key-value store, specialized
/// for managing extent metadata (`ExtentMetadata`) and index data (`IndexData`).
/// This module is designed to facilitate operations on two different types of data structures:
/// fixed-length data (metadata) and variable-length data (index).
///
/// `Metadata` is a fixed-length data structure used to store metadata about extents.
/// It includes fields like `index_tiered_length` and `index_inflight_tiering_length`,
/// which are crucial for the tiering process.
///
/// `Index` is a variable-length data structure represented as a `Vec<u8>`, used
/// to store index data associated with extents. It allows operations like appending
/// and overwriting data.
///
/// The `MetadataStore` struct serves as the core of this module, providing
/// a unified interface to manage both metadata and index data associated with
/// various extents. It supports operations like creating extents, reading, updating,
/// and deleting metadata, as well as appending and overwriting index data.
///
/// This module is designed with a focus on simplicity and efficiency, suitable for
/// scenarios where an in-memory, non-concurrent data store is sufficient.

pub trait Metadata {
    fn new() -> Self;
    fn update(&mut self, other: Self);
}

// Generic struct for Index data
pub struct Index<I> {
    data: Vec<I>,
}

impl<I> Index<I> {
    pub fn new() -> Self {
        Index { data: Vec::new() }
    }

    // pub fn append(&mut self, item: I) {
    //     self.data.push(item);
    // }

    // pub fn trim_start(&mut self, amount: usize) {
    //     if amount <= self.data.len() {
    //         self.data.drain(0..amount);
    //     }
    // }

    // pub fn overwrite(&mut self, new_data: Vec<I>) {
    //     self.data = new_data;
    // }
}

// Define the KV store with generic Metadata and Index
pub struct MetadataStore<M, I> where M: Metadata {
    pub metadata_store: HashMap<String, M>,
    index_store: HashMap<String, Index<I>>,
}

impl<M, I> MetadataStore<M, I> where M: Metadata, I: Clone {
    pub fn new() -> MetadataStore<M, I> {
        MetadataStore {
            metadata_store: HashMap::new(),
            index_store: HashMap::new(),
        }
    }

    pub fn create_metadata(&mut self, key: String) {
        self.metadata_store.insert(key.clone(), M::new());
        self.index_store.insert(key, Index::new());
    }

    pub fn read_metadata(&self, key: &str) -> Option<&M> {
        self.metadata_store.get(key)
    }

    pub fn update_metadata(&mut self, key: &str, data: M) {
        if let Some(metadata) = self.metadata_store.get_mut(key) {
            metadata.update(data);
        }
    }

    pub fn delete_metadata(&mut self, key: &str) {
        self.metadata_store.remove(key);
        self.index_store.remove(key);
    }

    pub fn read_index(&self, key: &str) -> Option<&Vec<I>> {
        if let Some(data) = self.index_store.get(key) {
            Some(&data.data)
        } else {
            None
        }
    }

    pub fn append_index(&mut self, key: &str, additional_data: &[I]) {
        if let Some(index) = self.index_store.get_mut(key) {
            index.data.extend_from_slice(additional_data);
        }
    }

    pub fn overwrite_index(&mut self, key: String, new_data: Index<I>) {
        self.index_store.insert(key, new_data);
    }

    pub fn trim_index_data(&mut self, key: &str, trim_length: usize) {
        if let Some(index_data) = self.index_store.get_mut(key) {
            if trim_length <= index_data.data.len() {
                index_data.data.drain(0..trim_length);
            }
        }
    }        
}

#[cfg(test)]
mod tests {
    use super::*;

    #[derive(Clone, Debug)]
    pub struct ToyMetadata {
        pub length: usize,
    }
    
    impl Metadata for ToyMetadata {
        fn new() -> Self {
            ToyMetadata {
                length: 0,
            }
        }
    
        fn update(&mut self, other: Self) {
            self.length = other.length;
        }
    }

    #[test]
    fn test_create_metadata_and_read() {
        let mut store = MetadataStore::<ToyMetadata, u8>::new();
        let key = "test_extent".to_string();

        store.create_metadata(key.clone());
        let metadata = store.read_metadata(&key);

        assert!(metadata.is_some());
        assert_eq!(metadata.unwrap().length, 0);
    }

    #[test]
    fn test_update_metadata() {
        let mut store = MetadataStore::<ToyMetadata, u8>::new();
        let key = "test_extent".to_string();

        store.create_metadata(key.clone());
        let new_metadata = ToyMetadata {
            length: 10,
        };

        store.update_metadata(&key, new_metadata);

        let updated_metadata = store.read_metadata(&key).unwrap();
        assert_eq!(updated_metadata.length, 10);
    }

    #[test]
    fn test_append_and_overwrite_index() {
        let mut store = MetadataStore::<ToyMetadata, u8>::new();
        let key = "test_extent".to_string();

        store.create_metadata(key.clone());
        store.append_index(&key, &[1, 2, 3]);

        let index_data = store.read_index(&key).unwrap();
        assert_eq!(index_data, &vec![1, 2, 3]);

        store.overwrite_index(key.clone(), Index::<u8> { data: vec![4, 5, 6] });
        let new_index_data = store.read_index(&key).unwrap();
        assert_eq!(new_index_data, &vec![4, 5, 6]);
    }

    #[test]
    fn test_trim_index_data() {
        let mut store = MetadataStore::<ToyMetadata, u8>::new();
        
        // Create an extent with initial index data
        let key = "test_extent".to_string();
        store.create_metadata(key.clone());
        let initial_data = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
        store.append_index(&key, &initial_data);

        // Trim the index data
        let trim_length = 5; // Length to trim from the beginning
        store.trim_index_data(&key, trim_length);

        // Retrieve the trimmed data and verify its length and contents
        let trimmed_data = store.read_index(&key).unwrap();
        assert_eq!(trimmed_data.len(), initial_data.len() - trim_length);
        assert_eq!(*trimmed_data, initial_data[trim_length..]);
    }    

    #[test]
    fn test_delete_metadata() {
        let mut store = MetadataStore::<ToyMetadata, u8>::new();
        let key = "test_extent".to_string();

        store.create_metadata(key.clone());
        store.delete_metadata(&key);

        assert!(store.read_metadata(&key).is_none());
        assert!(store.read_index(&key).is_none());
    }
}

================
File: ./unverified/metadata_kv/src/tiering_engine_proptest.rs
================

#[cfg(test)]
extern crate proptest;
use proptest::prelude::*;
use rand::Rng;
use rand::seq::SliceRandom;
use crate::metadata_kv::MetadataStore;
use crate::tiering_engine::{TieringEngine, ExtentMetadataWithTiering};
use std::collections::HashMap;

fn divide_into_random_segments(vec: &Vec<u8>, num_segments: usize) -> Vec<Vec<u8>> {
    let mut rng = rand::thread_rng();
    let mut segments = Vec::new();
    let mut start = 0;

    // Ensure we have at least one segment and not more segments than elements
    let num_segments = num_segments.min(vec.len()).max(1);
    let mut split_points: Vec<usize> = (1..vec.len()).collect();

    // Shuffle and pick the first `num_segments - 1` split points
    split_points.shuffle(&mut rng);
    split_points.truncate(num_segments - 1);
    split_points.sort_unstable();

    // Create each segment based on the split points
    for end in split_points.into_iter() {
        segments.push(vec[start..end].to_vec());
        start = end;
    }

    // Add the last segment
    segments.push(vec[start..].to_vec());

    // Validation: Combine the segments and compare with the original vector
    let combined: Vec<u8> = segments.iter().flatten().copied().collect();
    assert_eq!(combined, *vec);

    segments
}

proptest! {
    #![proptest_config(ProptestConfig::with_cases(10000))]

    /// Property test for `TieringEngine` using randomized data and operations.
    /// 
    /// The test creates a simulated environment with two extents and a series of randomized operations
    /// to simulate a real-world use case of the tiering engine. It tests the tiering engine's ability to handle
    /// various scenarios, including appending index data to extents, starting and ending tiering processes,
    /// and ensuring the data integrity and metadata states are consistent throughout the process.
    ///
    /// # Test Steps:
    /// 1. Initialize an `MetadataStore` with two extents.
    /// 2. Create a temporary `tiered_store` to track tiered data for validation.
    /// 3. Generate random data segments and divide them into random segments for each extent.
    /// 4. Perform a series of randomized operations:
    ///    - Append segments to extents.
    ///    - Start or end tiering processes.
    /// 5. After the randomized operations, ensure any in-progress tiering is completed.
    /// 6. Validate that the tiering process was correctly handled:
    ///    - Check if the `index_inflight_tiering_length` is zero, indicating no ongoing tiering.
    ///    - Verify that the `index_tiered_length` matches the expected length.
    ///    - Confirm that the index data is empty and the `tiered_store` contains all original data.
    ///
    /// # Parameters:
    /// - `data_segments`: Randomly generated vector of bytes, representing index data.
    /// - `num_segments`: Random number of segments to split the data into.
    /// - `num_operations`: Random number of operations to perform on the data.
    ///
    /// The test uses a mixture of deterministic logic and randomized operations to thoroughly
    /// exercise the functionality of the `TieringEngine`. It includes comprehensive assertions to
    /// validate the correctness of the engine under various conditions.
    #[test]
    fn tiering_engine_property_test(data_segments in prop::collection::vec(any::<u8>(), 16..=1024),
                                    num_segments in 1usize..=16,
                                    num_operations in 1usize..=128) {    
        let mut store = MetadataStore::<ExtentMetadataWithTiering, u8>::new();
        store.create_metadata("extent1".to_string());
        store.create_metadata("extent2".to_string());

        let mut tiered_store: HashMap<String, Vec<u8>> = HashMap::new();
        tiered_store.insert("extent1".to_string(), vec![]);
        tiered_store.insert("extent2".to_string(), vec![]);

        let engine = TieringEngine::<ExtentMetadataWithTiering>::new();
        let mut rng = rand::thread_rng();

        // Divide data_segments into random segments
        let segments_extent1 = divide_into_random_segments(&data_segments, num_segments);
        let mut segments_extent1_iter = segments_extent1.into_iter();
        let segments_extent2 = divide_into_random_segments(&data_segments, num_segments);
        let mut segments_extent2_iter = segments_extent2.into_iter();

        let mut ops = Vec::new();
        for _ in 0..rng.gen_range(1..=num_operations) {
            match rng.gen_range(0..4) {
                0 => {
                    if let Some(segment) = segments_extent1_iter.next() {
                        store.append_index("extent1", &segment);
                        ops.push(format!("[0] append_index(extent1, {:?})", segment));
                    }
                },
                1 => {
                    if let Some(segment) = segments_extent2_iter.next() {
                        store.append_index("extent2", &segment);
                        ops.push(format!("[1] append_index(extent2, {:?})", segment));
                    }
                },
                2 => {
                    let extent_key = if rng.gen() { "extent1" } else { "extent2" };
                    let is_tiering_in_progress = store.read_metadata(extent_key).unwrap().index_inflight_tiering_length > 0;
                    engine.tier_extent_start(&mut store, extent_key);
                    if !is_tiering_in_progress {
                        let data = store.read_index(extent_key).unwrap();
                        tiered_store.get_mut(extent_key).unwrap().extend(data);
                        ops.push(format!("[2] tier_extent_start({} {:?})", extent_key, data));
                    }
                },
                _ => {
                    let extent_key = if rng.gen() { "extent1" } else { "extent2" };
                    engine.tier_extent_end(&mut store, extent_key);
                    ops.push(format!("[4] tier_extent_end({})", extent_key));
                }
            }
        }

        for extent_key in ["extent1".to_string(), "extent2".to_string()].iter() {
            // in case there is a tiering in progress, complete it
            engine.tier_extent_end(&mut store, extent_key);
            assert_eq!(store.read_metadata(extent_key).unwrap().index_inflight_tiering_length, 0);

            // tier index data if any
            engine.tier_extent_start(&mut store, extent_key);
            let data = store.read_index(extent_key).unwrap();
            tiered_store.get_mut(extent_key).unwrap().extend(data);
            engine.tier_extent_end(&mut store, extent_key);

            assert_eq!(store.read_metadata(extent_key).unwrap().index_inflight_tiering_length, 0);
            assert_eq!(store.read_index(extent_key).unwrap(), &vec![]);
        }

        // copmlete the remaining segments if any
        for _ in 0..num_segments {
            if let Some(segment) = segments_extent1_iter.next() {
                let extent_key = "extent1";
                store.append_index(extent_key, &segment);
                engine.tier_extent_start(&mut store, extent_key);
                let data = store.read_index(extent_key).unwrap();
                tiered_store.get_mut(extent_key).unwrap().extend(data);
                engine.tier_extent_end(&mut store, extent_key);
            }
            if let Some(segment) = segments_extent2_iter.next() {
                let extent_key = "extent2";
                store.append_index(extent_key, &segment);
                engine.tier_extent_start(&mut store, extent_key);
                let data = store.read_index(extent_key).unwrap();
                tiered_store.get_mut(extent_key).unwrap().extend(data);
                engine.tier_extent_end(&mut store, extent_key);
            }            
        }

        for extent_key in ["extent1".to_string(), "extent2".to_string()].iter() {
            assert_eq!(store.read_metadata(extent_key).unwrap().index_inflight_tiering_length, 0, "ops: {:?}", ops);
            assert_eq!(store.read_metadata(extent_key).unwrap().index_tiered_length, data_segments.len(), "ops: {:?}", ops);
            assert_eq!(store.read_index(extent_key).unwrap(), &vec![], "ops: {:?}", ops);
            assert_eq!(tiered_store.get(extent_key).unwrap(), &data_segments, "ops: {:?}", ops);
        }
    }
}

================
File: ./unverified/metadata_kv/src/main.rs
================

mod metadata_kv;
mod tiering_engine;
mod tiering_engine_proptest;
use crate::metadata_kv::MetadataStore;
use crate::tiering_engine::{TieringEngine, ExtentMetadataWithTiering};

fn main() {
    let mut store = MetadataStore::<ExtentMetadataWithTiering, u8>::new();

    store.create_metadata("test_extent".to_string());
    store.append_index("test_extent", &[1, 2, 3]);

    let tiering_engine = TieringEngine::new();
    tiering_engine.tier_extent_start(&mut store, "test_extent");
    tiering_engine.tier_extent_end(&mut store, "test_extent");
}
================
File: ./unverified/metadata_kv/src/tiering_engine.rs
================

use crate::metadata_kv::{Metadata, MetadataStore};

// metadata with tiering
pub trait MetadataWithTiering: Metadata {
    fn get_inflight_tiering_length(&self) -> usize;
    fn start_tiering(&mut self, length: usize);
    fn end_tiering(&mut self);
}

#[derive(Clone, Debug)]
pub struct ExtentMetadataWithTiering {
    pub index_tiered_length: usize,
    pub index_inflight_tiering_length: usize,
}

impl Metadata for ExtentMetadataWithTiering {
    fn new() -> Self {
        ExtentMetadataWithTiering {
            index_tiered_length: 0,
            index_inflight_tiering_length: 0,
        }
        }

    fn update(&mut self, other: Self) {
        self.index_tiered_length = other.index_tiered_length;
        self.index_inflight_tiering_length = other.index_inflight_tiering_length;
    }
}

impl MetadataWithTiering for ExtentMetadataWithTiering {
    fn get_inflight_tiering_length(&self) -> usize {
        self.index_inflight_tiering_length
    }

    fn start_tiering(&mut self, length: usize) {
        self.index_inflight_tiering_length = length;
    }

    fn end_tiering(&mut self) {
        if self.index_inflight_tiering_length > 0 {
            self.index_tiered_length += self.index_inflight_tiering_length;
            self.index_inflight_tiering_length = 0;
        }
    }
}

/// `TieringEngine` is responsible for managing the tiering process of extents
/// within an `ExtentMetadataStore`. It provides functionalities to start and 
/// end the tiering process for a given extent.
///
/// The tiering process involves updating metadata and manipulating index data
/// associated with an extent. This engine ensures that tiering operations are
/// performed safely and correctly, based on the current state of the metadata.

pub struct TieringEngine<M> where M: MetadataWithTiering {
    _marker: std::marker::PhantomData<M>,
}

impl<M> TieringEngine::<M> where M: MetadataWithTiering {
    pub fn new() -> TieringEngine<M> {
        TieringEngine {
            _marker: std::marker::PhantomData,
        }
    }

    /// Starts the tiering process for a given extent.
    ///
    /// This function takes a mutable reference to an `ExtentMetadataStore` and the
    /// key of the extent. If the `index_inflight_tiering_length` of the extent's metadata
    /// is zero, it updates this value to the current length of the index data,
    /// indicating that the tiering process has started. If tiering is already in progress,
    /// this function acts as a no-op.    
    pub fn tier_extent_start<I>(&self, store: &mut MetadataStore<M, I>, key: &str) where I: Clone {
        // get the length of the index data (immutable borrow)
        let index_data_length = if let Some(index_data) = store.read_index(&key) {
            Some(index_data.len())
        } else {
            None
        };
    
        // perform the mutation (mutable borrow)
        if let Some(metadata) = store.metadata_store.get_mut(key) {
            if metadata.get_inflight_tiering_length() == 0 {
                if let Some(length) = index_data_length {
                    metadata.start_tiering(length);
                }
            }
        }
    }
    
    /// Ends the tiering process for a given extent.
    ///
    /// This function also takes a mutable reference to an `ExtentMetadataStore` and the
    /// key of the extent. If tiering is in progress (indicated by a non-zero 
    /// `index_inflight_tiering_length`), it trims the index data by removing the 
    /// first `index_inflight_tiering_length` bytes and updates the metadata accordingly.
    /// If no tiering is in progress, it is a no-op.    
    pub fn tier_extent_end<I>(&self, store: &mut MetadataStore<M, I>, key: &str) where I: Clone {
        // perform the trimming operation, which requires a mutable borrow of `store`
        if let Some(metadata) = store.metadata_store.get(key) {
            if metadata.get_inflight_tiering_length() > 0 {
                store.trim_index_data(key, metadata.get_inflight_tiering_length());
            }
        }
    
        // update the metadata, which again requires a mutable borrow of `store`
        if let Some(metadata) = store.metadata_store.get_mut(key) {
            metadata.end_tiering();
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn setup_test_store() -> MetadataStore::<ExtentMetadataWithTiering, u8> {
        let mut store = MetadataStore::<ExtentMetadataWithTiering, u8>::new();
        store.create_metadata("test_extent".to_string());
        store.append_index("test_extent", &[1, 2, 3]);
        store
    }

    #[test]
    fn test_tier_extent_start() {
        let mut store = setup_test_store();
        let engine = TieringEngine::<ExtentMetadataWithTiering>::new();

        engine.tier_extent_start(&mut store, "test_extent");
        let metadata = store.read_metadata("test_extent").unwrap();
        assert_eq!(metadata.index_inflight_tiering_length, 3);
    }

    #[test]
    fn test_tier_extent_start_no_op_if_already_in_progress() {
        let mut store = setup_test_store();
        let engine = TieringEngine::<ExtentMetadataWithTiering>::new();

        engine.tier_extent_start(&mut store, "test_extent");
        let initial_metadata = store.read_metadata("test_extent").unwrap().clone();
        store.append_index("test_extent", &[4, 5, 6, 7, 8]);
        engine.tier_extent_start(&mut store, "test_extent");
        let metadata = store.read_metadata("test_extent").unwrap();

        assert_eq!(metadata.index_inflight_tiering_length, initial_metadata.index_inflight_tiering_length);
    }

    #[test]
    fn test_tier_extent_end() {
        let mut store = setup_test_store();
        let engine = TieringEngine::<ExtentMetadataWithTiering>::new();

        engine.tier_extent_start(&mut store, "test_extent");
        engine.tier_extent_end(&mut store, "test_extent");
        let metadata = store.read_metadata("test_extent").unwrap();

        assert_eq!(metadata.index_inflight_tiering_length, 0);
        assert_eq!(metadata.index_tiered_length, 3);
    }

    #[test]
    fn test_tier_extent_start_append_end() {
        let mut store = setup_test_store();
        let engine = TieringEngine::<ExtentMetadataWithTiering>::new();

        engine.tier_extent_start(&mut store, "test_extent");
        store.append_index("test_extent", &[4, 5, 6, 7, 8]);
        engine.tier_extent_end(&mut store, "test_extent");
        let metadata = store.read_metadata("test_extent").unwrap();

        assert_eq!(metadata.index_inflight_tiering_length, 0);
        assert_eq!(metadata.index_tiered_length, 3);
    }

    #[test]
    fn test_tier_extent_end_no_op_if_not_in_progress() {
        let mut store = setup_test_store();
        let engine = TieringEngine::<ExtentMetadataWithTiering>::new();

        let initial_metadata = store.read_metadata("test_extent").unwrap().clone();
        engine.tier_extent_end(&mut store, "test_extent");
        let metadata = store.read_metadata("test_extent").unwrap();

        assert_eq!(metadata.index_inflight_tiering_length, initial_metadata.index_inflight_tiering_length);
        assert_eq!(metadata.index_tiered_length, initial_metadata.index_tiered_length);
        assert_eq!(metadata.index_tiered_length, 0);
    }
}

================
File: ./storage_node/Cargo.toml
================

[package]
name = "storage_node"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
builtin_macros = { git = "https://github.com/verus-lang/verus.git", rev="a53f39271666ac7dc9f455b6267da4c49a5f75c6" }
builtin = { git = "https://github.com/verus-lang/verus.git", rev="a53f39271666ac7dc9f455b6267da4c49a5f75c6" }
vstd = { git = "https://github.com/verus-lang/verus.git", rev="a53f39271666ac7dc9f455b6267da4c49a5f75c6" }
deps_hack = { path = "../deps_hack" }
pmsafe = { path = "../pmsafe" }

[package.metadata.verus.ide]
extra_args = "--crate-type=lib --expand-errors -L dependency=../deps_hack/target/release/deps --extern=deps_hack=../deps_hack/target/release/libdeps_hack.rlib"
================
File: ./storage_node/Cargo.lock
================

# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 3

[[package]]
name = "aho-corasick"
version = "1.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916"
dependencies = [
 "memchr",
]

[[package]]
name = "bindgen"
version = "0.69.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a00dc851838a2120612785d195287475a3ac45514741da670b735818822129a0"
dependencies = [
 "bitflags",
 "cexpr",
 "clang-sys",
 "itertools",
 "lazy_static",
 "lazycell",
 "log",
 "prettyplease",
 "proc-macro2",
 "quote",
 "regex",
 "rustc-hash",
 "shlex",
 "syn 2.0.66",
 "which",
]

[[package]]
name = "bitflags"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cf4b9d6a944f767f8e5e0db018570623c85f3d925ac718db4e06d0187adb21c1"

[[package]]
name = "builtin"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git?rev=a53f39271666ac7dc9f455b6267da4c49a5f75c6#a53f39271666ac7dc9f455b6267da4c49a5f75c6"

[[package]]
name = "builtin_macros"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git?rev=a53f39271666ac7dc9f455b6267da4c49a5f75c6#a53f39271666ac7dc9f455b6267da4c49a5f75c6"
dependencies = [
 "prettyplease_verus",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
 "syn_verus",
 "synstructure",
]

[[package]]
name = "cexpr"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6fac387a98bb7c37292057cffc56d62ecb629900026402633ae9160df93a8766"
dependencies = [
 "nom",
]

[[package]]
name = "cfg-if"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"

[[package]]
name = "clang-sys"
version = "1.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "67523a3b4be3ce1989d607a828d036249522dd9c1c8de7f4dd2dae43a37369d1"
dependencies = [
 "glob",
 "libc",
 "libloading",
]

[[package]]
name = "crc64fast"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "26bb92ecea20291efcf0009e2713d64b7e327dedb8ce780545250f24075429e2"

[[package]]
name = "deps_hack"
version = "0.1.0"
dependencies = [
 "bindgen",
 "crc64fast",
 "nix",
 "pmsafe",
 "rand",
 "winapi",
]

[[package]]
name = "either"
version = "1.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "11157ac094ffbdde99aa67b23417ebdd801842852b500e395a45a9c0aac03e4a"

[[package]]
name = "errno"
version = "0.3.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a258e46cdc063eb8519c00b9fc845fc47bcfca4130e2f08e88665ceda8474245"
dependencies = [
 "libc",
 "windows-sys",
]

[[package]]
name = "getrandom"
version = "0.2.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "190092ea657667030ac6a35e305e62fc4dd69fd98ac98631e5d3a2b1575a12b5"
dependencies = [
 "cfg-if",
 "libc",
 "wasi",
]

[[package]]
name = "glob"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d2fabcfbdc87f4758337ca535fb41a6d701b65693ce38287d856d1674551ec9b"

[[package]]
name = "home"
version = "0.5.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e3d1354bf6b7235cb4a0576c2619fd4ed18183f689b12b006a0ee7329eeff9a5"
dependencies = [
 "windows-sys",
]

[[package]]
name = "itertools"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ba291022dbbd398a455acf126c1e341954079855bc60dfdda641363bd6922569"
dependencies = [
 "either",
]

[[package]]
name = "lazy_static"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e2abad23fbc42b3700f2f279844dc832adb2b2eb069b2df918f455c4e18cc646"

[[package]]
name = "lazycell"
version = "1.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "830d08ce1d1d941e6b30645f1a0eb5643013d835ce3779a5fc208261dbe10f55"

[[package]]
name = "libc"
version = "0.2.153"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9c198f91728a82281a64e1f4f9eeb25d82cb32a5de251c6bd1b5154d63a8e7bd"

[[package]]
name = "libloading"
version = "0.8.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0c2a198fb6b0eada2a8df47933734e6d35d350665a33a3593d7164fa52c75c19"
dependencies = [
 "cfg-if",
 "windows-targets",
]

[[package]]
name = "linux-raw-sys"
version = "0.4.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "01cda141df6706de531b6c46c3a33ecca755538219bd484262fa09410c13539c"

[[package]]
name = "log"
version = "0.4.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "90ed8c1e510134f979dbc4f070f87d4313098b704861a105fe34231c70a3901c"

[[package]]
name = "memchr"
version = "2.7.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6c8640c5d730cb13ebd907d8d04b52f55ac9a2eec55b440c8892f40d56c76c1d"

[[package]]
name = "minimal-lexical"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "68354c5c6bd36d73ff3feceb05efa59b6acb7626617f4962be322a825e61f79a"

[[package]]
name = "nix"
version = "0.27.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2eb04e9c688eff1c89d72b407f168cf79bb9e867a9d3323ed6c01519eb9cc053"
dependencies = [
 "bitflags",
 "cfg-if",
 "libc",
]

[[package]]
name = "nom"
version = "7.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d273983c5a657a70a3e8f2a01329822f3b8c8172b73826411a55751e404a0a4a"
dependencies = [
 "memchr",
 "minimal-lexical",
]

[[package]]
name = "once_cell"
version = "1.19.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3fdb12b2476b595f9358c5161aa467c2438859caa136dec86c26fdd2efe17b92"

[[package]]
name = "pmsafe"
version = "0.1.0"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.66",
]

[[package]]
name = "ppv-lite86"
version = "0.2.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b40af805b3121feab8a3c29f04d8ad262fa8e0561883e7653e024ae4479e6de"

[[package]]
name = "prettyplease"
version = "0.2.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8d3928fb5db768cb86f891ff014f0144589297e3c6a1aba6ed7cecfdace270c7"
dependencies = [
 "proc-macro2",
 "syn 2.0.66",
]

[[package]]
name = "prettyplease_verus"
version = "0.1.15"
source = "git+https://github.com/verus-lang/verus.git?rev=a53f39271666ac7dc9f455b6267da4c49a5f75c6#a53f39271666ac7dc9f455b6267da4c49a5f75c6"
dependencies = [
 "proc-macro2",
 "syn_verus",
]

[[package]]
name = "proc-macro2"
version = "1.0.84"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec96c6a92621310b51366f1e28d05ef11489516e93be030060e5fc12024a49d6"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "quote"
version = "1.0.35"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "291ec9ab5efd934aaf503a6466c5d5251535d108ee747472c3977cc5acc868ef"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "rand"
version = "0.8.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "34af8d1a0e25924bc5b7c43c079c942339d8f0a8b57c39049bef581b46327404"
dependencies = [
 "libc",
 "rand_chacha",
 "rand_core",
]

[[package]]
name = "rand_chacha"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6c10a63a0fa32252be49d21e7709d4d4baf8d231c2dbce1eaa8141b9b127d88"
dependencies = [
 "ppv-lite86",
 "rand_core",
]

[[package]]
name = "rand_core"
version = "0.6.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec0be4795e2f6a28069bec0b5ff3e2ac9bafc99e6a9a7dc3547996c5c816922c"
dependencies = [
 "getrandom",
]

[[package]]
name = "regex"
version = "1.10.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c117dbdfde9c8308975b6a18d71f3f385c89461f7b3fb054288ecf2a2058ba4c"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-automata",
 "regex-syntax",
]

[[package]]
name = "regex-automata"
version = "0.4.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "86b83b8b9847f9bf95ef68afb0b8e6cdb80f498442f5179a29fad448fcc1eaea"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-syntax",
]

[[package]]
name = "regex-syntax"
version = "0.8.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "adad44e29e4c806119491a7f06f03de4d1af22c3a680dd47f1e6e179439d1f56"

[[package]]
name = "rustc-hash"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "08d43f7aa6b08d49f382cde6a7982047c3426db949b1424bc4b7ec9ae12c6ce2"

[[package]]
name = "rustix"
version = "0.38.32"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "65e04861e65f21776e67888bfbea442b3642beaa0138fdb1dd7a84a52dffdb89"
dependencies = [
 "bitflags",
 "errno",
 "libc",
 "linux-raw-sys",
 "windows-sys",
]

[[package]]
name = "shlex"
version = "1.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0fda2ff0d084019ba4d7c6f371c95d8fd75ce3524c3cb8fb653a3023f6323e64"

[[package]]
name = "storage_node"
version = "0.1.0"
dependencies = [
 "builtin",
 "builtin_macros",
 "deps_hack",
 "vstd",
]

[[package]]
name = "syn"
version = "1.0.109"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72b64191b275b66ffe2469e8af2c1cfe3bafa67b529ead792a6d0160888b4237"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "syn"
version = "2.0.66"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c42f3f41a2de00b01c0aaad383c5a45241efc8b2d1eda5661812fda5f3cdcff5"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "syn_verus"
version = "1.0.95"
source = "git+https://github.com/verus-lang/verus.git?rev=a53f39271666ac7dc9f455b6267da4c49a5f75c6#a53f39271666ac7dc9f455b6267da4c49a5f75c6"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "synstructure"
version = "0.12.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f36bdaa60a83aca3921b5259d5400cbf5e90fc51931376a9bd4a0eb79aa7210f"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
 "unicode-xid",
]

[[package]]
name = "unicode-ident"
version = "1.0.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3354b9ac3fae1ff6755cb6db53683adb661634f67557942dea4facebec0fee4b"

[[package]]
name = "unicode-xid"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f962df74c8c05a667b5ee8bcf162993134c104e96440b663c8daa176dc772d8c"

[[package]]
name = "vstd"
version = "0.1.0"
source = "git+https://github.com/verus-lang/verus.git?rev=a53f39271666ac7dc9f455b6267da4c49a5f75c6#a53f39271666ac7dc9f455b6267da4c49a5f75c6"
dependencies = [
 "builtin",
 "builtin_macros",
]

[[package]]
name = "wasi"
version = "0.11.0+wasi-snapshot-preview1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9c8d87e72b64a3b4db28d11ce29237c246188f4f51057d65a7eab63b7987e423"

[[package]]
name = "which"
version = "4.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "87ba24419a2078cd2b0f2ede2691b6c66d8e47836da3b6db8265ebad47afbfc7"
dependencies = [
 "either",
 "home",
 "once_cell",
 "rustix",
]

[[package]]
name = "winapi"
version = "0.3.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
dependencies = [
 "winapi-i686-pc-windows-gnu",
 "winapi-x86_64-pc-windows-gnu",
]

[[package]]
name = "winapi-i686-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"

[[package]]
name = "winapi-x86_64-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"

[[package]]
name = "windows-sys"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "282be5f36a8ce781fad8c8ae18fa3f9beff57ec1b52cb3de0789201425d9a33d"
dependencies = [
 "windows-targets",
]

[[package]]
name = "windows-targets"
version = "0.52.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7dd37b7e5ab9018759f893a1952c9420d060016fc19a472b4bb20d1bdd694d1b"
dependencies = [
 "windows_aarch64_gnullvm",
 "windows_aarch64_msvc",
 "windows_i686_gnu",
 "windows_i686_msvc",
 "windows_x86_64_gnu",
 "windows_x86_64_gnullvm",
 "windows_x86_64_msvc",
]

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.52.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bcf46cf4c365c6f2d1cc93ce535f2c8b244591df96ceee75d8e83deb70a9cac9"

[[package]]
name = "windows_aarch64_msvc"
version = "0.52.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "da9f259dd3bcf6990b55bffd094c4f7235817ba4ceebde8e6d11cd0c5633b675"

[[package]]
name = "windows_i686_gnu"
version = "0.52.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b474d8268f99e0995f25b9f095bc7434632601028cf86590aea5c8a5cb7801d3"

[[package]]
name = "windows_i686_msvc"
version = "0.52.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1515e9a29e5bed743cb4415a9ecf5dfca648ce85ee42e15873c3cd8610ff8e02"

[[package]]
name = "windows_x86_64_gnu"
version = "0.52.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5eee091590e89cc02ad514ffe3ead9eb6b660aedca2183455434b93546371a03"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.52.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "77ca79f2451b49fa9e2af39f0747fe999fcda4f5e241b2898624dca97a1f2177"

[[package]]
name = "windows_x86_64_msvc"
version = "0.52.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32b752e52a2da0ddfbdbcc6fceadfeede4c939ed16d13e648833a61dfb611ed8"

================
File: ./storage_node/README.md
================

# Verified key-value store

## Overview 

The `storage_node` crate contains a verified persistent-memory key value store. It contains the following submodules:

* `pmem`: a specification of how persistent memory is assumed to behave, including both normal operation and exceptional cases like crashes and bit corruiption. This submodule also contains a mock of persistent memory using volatile memory and implementations of the PM interface for use on Linux with a DAX-supported file system and on Winows.
* `multilog`: a verified implementation of a "multilog", a structure that stores multiple circular logs on persistent memory and provides crash-atomic appends to multiple logs at a time. More documentation on the multilog can be found in its [README](multilog/README.md).
* `kv`: a verified implementation of a crash-consistent, byte-corruption-resistent persistent memory key-value store. 

## Verifying, building, and running the code

To verify, build, and run the code, follow the following steps.

1. Install [Verus](https://github.com/verus-lang/verus) if you don't already have it.

2. Build the `pmsafe` crate if you haven't yet done so, with:
```
cd pmsafe
cargo build
```

3. Build the `deps_hack` crate if you haven't yet done so. 
   - If you are running the KV store on Linux, `deps_hack` depends on `bindgen` and several libraries from [PMDK](https://pmem.io/pmdk/). Install the following packages to meet these dependencies: `llvm-dev libclang-dev clang libpmem1 libpmemlog1 libpmem-dev libpmemlog-dev`
   - If you are using Windows, `cargo` will take care of all dependencies.
  
Then, to build `deps_hack`, run:
```
cd deps_hack
cargo build
```

4. Verify the code with:
```
cd storage_node
verus --crate-type=lib --compile -L dependency=../deps_hack/target/debug/deps --extern=deps_hack=../deps_hack/target/debug/libdeps_hack.rlib src/lib.rs
```
Alternatively, set the `VERUS_PATH` variable in `verify.sh` to point to your local Verus installation, and run `./verify.sh`. 
The `--compile` flag is necessary to perform some non-Verus compile time checks that are part of the verification process. 
Specifically, compile-time assertions, which help check that we use the correct size for structures in proofs, are run by the Rust compiler, not by Verus.

It should report 0 verification errors.

================
File: ./storage_node/rust-toolchain.toml
================

[toolchain]
channel = "1.76"
================
File: ./storage_node/src/kv/volatile/volatileimpl_v.rs
================

//! This file contains a trait that defines the interface for the high-level
//! volatile component of the KV store.

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use crate::kv::kvimpl_t::*;
use crate::kv::volatile::volatilespec_t::*;
use crate::pmem::pmcopy_t::*;
use std::hash::Hash;

verus! {
    pub trait VolatileKvIndex<K, E> : Sized
    where
        K: Hash + Eq + Clone + PmCopy + Sized + std::fmt::Debug,
        E: std::fmt::Debug,
    {
        spec fn view(&self) -> VolatileKvIndexView<K>;

        spec fn valid(&self) -> bool;

        fn new(
            kvstore_id: u128,
            max_keys: usize,
        ) -> (result: Result<Self, KvError<K, E>>)
            ensures
                match result {
                    Ok(volatile_index) => {
                        &&& volatile_index@.empty()
                        &&& volatile_index.valid()
                    }
                    Err(_) => true // TODO
                }
        ;

        fn insert_item_offset(
            &mut self,
            key: &K,
            offset: u64,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid(),
                old(self)@[*key] is None,
            ensures
                self.valid(),
                match result {
                    Ok(()) => {
                        &&& self@ == old(self)@.insert_item_offset(*key, offset as int)
                        &&& self@.len() == old(self)@.len() + 1
                    }
                    Err(_) => false // TODO
                }
        ;

        fn append_to_list(
            &mut self,
            key: &K,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid(),
                // The caller has to prove that 1) the key exists and 2) the node we will add to has free
                // space. This function should be called only after a successful durable append,
                // which they can use to prove this.
                old(self)@.contains_key(*key),
                ({
                    let (_, node_view) = old(self)@.get_node_view::<E>(*key, old(self)@.list_len(*key) - 1).unwrap();
                    node_view.has_free_space()
                })
            ensures
                self.valid(),
                ({
                    let spec_result = old(self)@.append_to_list::<E>(*key);
                    match (result, spec_result) {
                        (Ok(()), Ok(new_state)) => self@ == new_state,
                        (Ok(()), Err(_)) => false,
                        (Err(KvError::KeyNotFound), Err(KvError::KeyNotFound)) => {
                            &&& !old(self)@.contains_key(*key)
                            &&& self@ == old(self)@
                        }
                        _ => false
                    }
                })
        ;

        fn get(
            &self,
            key: &K
        ) -> (result: Option<u64>)
            requires
                self.valid(),
            ensures
                match result {
                    Some(offset) => match self@[*key] {
                            Some(val) => offset == val.item_offset,
                            None => false
                        }
                    None => self@[*key].is_None()
                }
        ;

        // Returns the physical location of the list entry at the specified index.
        // Returns the address of the entry, not the address of the node that contains it
        fn get_entry_location_by_index(
            &self,
            key: &K,
            idx: usize,
        ) -> (result: Result<u64, KvError<K, E>>)
            requires
                self.valid(),
            ensures
                match result {
                    Ok(offset) => match self@[*key] {
                        Some(entry) => true, // TODO
                        // Some(entry) => entry.list_node_offsets[idx as int] == offset as int,
                        None => false
                    },
                    Err(KvError::KeyNotFound) => !self@.contains_key(*key),
                    Err(KvError::IndexOutOfRange) => match self@[*key] {
                        Some(entry) => idx >= entry.list_node_offsets.len(),
                        None => false
                    }
                    Err(_) => false,
                }
        ;

        // returns a pointer to the list node that contains the specified index
        fn get_node_offset(
            &self,
            key: &K,
            idx: usize
        ) -> (result: Result<u64, KvError<K, E>>)
            requires
                self.valid(),
            ensures
                ({
                    let spec_result = self@.get_node_offset::<E>(*key, idx as int);
                    match (result, spec_result) {
                        (Ok(node_offset), Ok(spec_offset)) => node_offset as int == spec_offset,
                        (Err(KvError::KeyNotFound), Err(KvError::KeyNotFound)) => !self@.contains_key(*key),
                        (Err(KvError::IndexOutOfRange), Err(KvError::IndexOutOfRange)) => idx >= self@[*key].unwrap().list_len,
                        _ => false
                    }
                })
        ;

        fn remove(
            &mut self,
            key: &K
        ) -> (result: Result<u64, KvError<K, E>>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                match result {
                    Ok(offset) => {
                        match old(self)@[*key] {
                            Some(entry) => {
                                &&& entry.item_offset == offset as int
                                &&& self@[*key].is_None()
                            }
                            None => false
                        }
                    }
                    Err(_) => true // TODO
                }
        ;

        // trims the volatile index for the list associated with the key
        fn trim_list(
            &mut self,
            key: &K,
            trim_length: usize
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                ({
                    let spec_result = old(self)@.trim_list::<E>(*key, trim_length as int);
                    match (result, spec_result) {
                        (Ok(()), Ok(spec_self)) => self@ == spec_self,
                        (Err(KvError::KeyNotFound), Err(KvError::KeyNotFound)) => {
                            &&& !old(self)@.contains_key(*key)
                            &&& self@ == old(self)@
                        }
                        (Err(KvError::IndexOutOfRange), Err(KvError::IndexOutOfRange)) => {
                            &&& old(self)@.contains_key(*key)
                            &&& old(self)@[*key].unwrap().list_len <= trim_length
                            &&& self@ == old(self)@
                        }
                        _ => false
                    }
                })
        ;

        fn get_keys(
            &self
        ) -> (result: Vec<K>)
            requires
                self.valid(),
            ensures
                self@.keys() == result@.to_set()
        ;

    }
}

================
File: ./storage_node/src/kv/volatile/mod.rs
================

pub mod volatileimpl_v;
pub mod volatilespec_t;

================
File: ./storage_node/src/kv/volatile/volatilespec_t.rs
================

//! A `VolatileKvIndex` represents the volatile component of a `KvStore`.
//! Currently, it maps each key to 1) the physical offset of the metadata header associated
//! with that key in the header store, and 2) a list of physical offsets of list entries
//! associated with that key.

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use crate::kv::kvimpl_t::*;
use std::hash::Hash;

verus! {
    pub struct ListNodeIndexEntry {
        pub start_index: int, // first logical list index stored in this node
        pub live_index: int, // first physical slot occupied by a valid list entry
        pub physical_offset: int, // TODO: this can probably be removed?
        pub free_entries: int,
    }

    impl ListNodeIndexEntry {
        pub open spec fn has_free_space(self) -> bool
        {
            self.free_entries > 0
        }

        // Reflects an entry being appended to the corresponding durable list node
        // by updating the number of entries for the node in the index
        pub open spec fn append_entry<K, E>(self) -> Result<Self, KvError<K, E>>
            where
                K: std::fmt::Debug,
                E: std::fmt::Debug,
        {
            if self.free_entries <= 0 {
                Err(KvError::OutOfSpace)
            } else {
                Ok(Self {
                    start_index: self.start_index,
                    live_index: self.live_index,
                    physical_offset: self.physical_offset,
                    free_entries: self.free_entries - 1
                })
            }
        }
    }

    pub struct VolatileKvIndexEntry
    {
        pub item_offset: int, // the physical offset of the metadata header associated with this key
        pub list_node_offsets: Map<(int, int), ListNodeIndexEntry>, // maps a range of indexes to the corresponding entry
        pub list_len: int,
    }

    #[verifier::reject_recursive_types(K)]
    pub struct VolatileKvIndexView<K>
    where
        K: Hash + Eq,
    {
        pub contents: Map<K, VolatileKvIndexEntry>,
        pub list_entries_per_node: int
    }

    impl<K> VolatileKvIndexView<K>
    where
        K: Hash + Eq + std::fmt::Debug,
    {
        pub open spec fn spec_index(&self, key: K) -> Option<VolatileKvIndexEntry>
        {
            if self.contents.contains_key(key) {
                Some(self.contents.index(key))
            } else {
                None
            }
        }

        pub open spec fn contains_key(&self, key: K) -> bool
        {
            self[key] is Some
        }

        pub open spec fn len(&self) -> int
        {
            self.contents.len() as int
        }

        pub open spec fn insert_item_offset(&self, key: K, item_offset: int) -> Self
        {
            Self {
                contents: self.contents.insert(
                        key,
                        VolatileKvIndexEntry {
                            item_offset,
                            list_node_offsets: Map::empty(),
                            list_len: 0
                        }
                    ),
                list_entries_per_node:self.list_entries_per_node
            }
        }

        // adds a new list node's offset to the volatile index. In order to call this, we must have first
        // allocated a new node and inserted an entry into it in the durable store, so we insert
        // the node into the index with `num_entries` set to 1.
        pub open spec fn append_node_offset(&self, key: K, node_offset: int, start_index: int) -> Self
        {
            let current_entry = self.contents[key];
            Self {
                contents: self.contents.insert(
                    key,
                    VolatileKvIndexEntry {
                        item_offset: current_entry.item_offset,
                        list_node_offsets: current_entry.list_node_offsets.insert(
                            (start_index, start_index + 1),
                            ListNodeIndexEntry {
                                start_index,
                                live_index: 0,
                                physical_offset: node_offset,
                                free_entries: self.list_entries_per_node
                            }),
                        list_len: current_entry.list_len + 1
                    }),
                list_entries_per_node: self.list_entries_per_node,
            }
        }


        // Returns the index key and the view of the list node that contains the specified
        // logical list index.
        pub open spec fn get_node_view<E>(&self, key: K, index: int) -> Result<((int, int), ListNodeIndexEntry), KvError<K,E>>
            where
                E: std::fmt::Debug
        {
            if !self.contains_key(key) {
                Err(KvError::KeyNotFound)
            } else {
                let index_entry = self.contents[key];
                if exists |k| {
                    let (i, j) = k;
                    &&& i <= index < j
                    &&& #[trigger] index_entry.list_node_offsets.contains_key(k)
                } {
                    let range = choose |k| {
                        let (i, j) = k;
                        &&& i <= index < j
                        &&& #[trigger] index_entry.list_node_offsets.contains_key(k)
                    };
                    Ok((range, index_entry.list_node_offsets[range]))
                } else {
                    Err(KvError::IndexOutOfRange)
                }
            }
        }

        // returns the offset of the node that contains the specified logical list index
        pub open spec fn get_node_offset<E>(&self, key: K, index: int) -> Result<int, KvError<K, E>>
            where
                E: std::fmt::Debug
        {
            match self.get_node_view(key, index) {
                Ok((_, node_view)) => Ok(node_view.physical_offset),
                Err(e) => Err(e)
            }
        }

        // returns the length of the list associated with this key
        // TODO: should maintain as an invariant that this actually matches the
        // number of entries in all associated nodes
        pub open spec(checked) fn list_len(&self, key: K) -> int
            recommends
                self.contains_key(key)
        {
            self[key].unwrap().list_len
        }

        // Updates the index to reflect that an entry has been appended to the end of the list.
        // It doesn't actually matter what the entry is -- we just need to update the index
        // to reflect that something new has been added
        pub open spec fn append_to_list<E>(self, key: K) -> Result<Self, KvError<K, E>>
            where
                E: std::fmt::Debug
        {
            if !self.contents.contains_key(key) {
                Err(KvError::KeyNotFound)
            } else {
                let old_index_entry = self.contents[key];
                match self.get_node_view(key, old_index_entry.list_len - 1) {
                    Ok((range, old_node_view)) => {
                        let new_node_view = old_node_view.append_entry();
                        match new_node_view {
                            Ok(new_node_view) => {
                                let new_index_entry = VolatileKvIndexEntry {
                                    item_offset: old_index_entry.item_offset,
                                    list_node_offsets: old_index_entry.list_node_offsets.insert(range, new_node_view),
                                    list_len: old_index_entry.list_len + 1
                                };

                                Ok(Self {
                                    contents: self.contents.insert(key, new_index_entry),
                                    list_entries_per_node: self.list_entries_per_node
                                })
                            }
                            Err(e) => Err(e)
                        }
                    }
                    Err(e) => Err(e)
                }
            }

        }

        // TODO: clean this up/split into multiple spec functions
        pub open spec fn trim_list<E>(self, key: K, trim_length: int) -> Result<Self, KvError<K, E>>
            where
                E: std::fmt::Debug
        {
            if !self.contents.contains_key(key) {
                Err(KvError::KeyNotFound)
            } else {
                let entry = self.contents[key];
                // First, determine which (if any) nodes will be completely removed
                let nodes_to_remove = Set::new(|k| {
                    let (i, j) = k;
                    &&& i <= j < trim_length
                    &&& entry.list_node_offsets.contains_key((i, j))
                });
                // There may also be a node that needs some internal trimming
                // let (range_key, node_to_trim_internally) = self.get_node_view(key, trim_length);
                match self.get_node_view(key, trim_length) {
                    Ok((range_key, node_to_trim_internally)) => {
                        let internal_trim_size = trim_length - node_to_trim_internally.start_index;
                        let trimmed_entry = ListNodeIndexEntry {
                            start_index: 0, // this is the new head node
                            live_index: node_to_trim_internally.live_index + internal_trim_size,
                            physical_offset: node_to_trim_internally.physical_offset,
                            free_entries: node_to_trim_internally.free_entries + internal_trim_size,
                        };

                        // Since we have trimmed from the front, we have to rekey all of the remaining
                        // nodes, since their indexes have changed.
                        // TODO: this is fine here, but this could have significant performance issues
                        // if the in-memory index has to do this... so maybe we need to index on something
                        // different? Or use a structure that doesn't depend on keys (e.g. put them in vector)
                        let new_node_map = entry.list_node_offsets
                            .remove_keys(nodes_to_remove) // remove nodes that will be deleted entirely
                            .remove(range_key); // remove the node to trim so that we can update other nodes without worrying about this one

                        // shift all indexes in the map over by the trim length
                        let shifted_node_map = Map::new(
                            |k: (int, int)| {
                                let (i, j) = k;
                                new_node_map.contains_key((i + trim_length, j + trim_length))
                            },
                            |k: (int, int)| {
                                let (i, j) = k;
                                let entry = new_node_map[(i, j)];
                                ListNodeIndexEntry {
                                    start_index: i,
                                    live_index: entry.live_index,
                                    physical_offset: entry.physical_offset,
                                    free_entries: entry.free_entries
                                }
                            }
                        );

                        // add the trimmed node entry back in
                        let final_node_map = shifted_node_map.insert(range_key, trimmed_entry);

                        Ok(Self {
                            contents: self.contents.insert(
                                key,
                                VolatileKvIndexEntry {
                                    item_offset: entry.item_offset,
                                    list_node_offsets: final_node_map,
                                    list_len: entry.list_len - trim_length
                                }
                            ),
                            list_entries_per_node: self.list_entries_per_node
                        })
                    }
                    Err(e) => Err(e)
                }
            }
        }

        pub closed spec fn remove(&self, key: K) -> Self
        {
            Self {
                contents: self.contents.remove(key),
                list_entries_per_node: self.list_entries_per_node
            }
        }

        pub open spec fn empty(self) -> bool {
            &&& self.contents.is_empty()
            &&& self.contents.dom().finite()
        }

        pub open spec fn keys(self) -> Set<K> {
            self.contents.dom()
        }
    }


}

================
File: ./storage_node/src/kv/kvimpl_v.rs
================

//! This file contains the verified implementation of the KV store. The methods
//! defined here are meant to be called by methods in kvimpl_t.rs with `Perm`
//! arguments that have been audited along with the rest of that file.
//!
//! The UntrustedKvStoreImpl is split into two key components: a volatile index
//! and a durable store. Each of these may be further split into separate components,
//! but having a high-level split between volatile and durable should make the
//! distinction between updates that require crash-consistency proofs, and updates
//! that don't, clear. The view of an UntrustedKvStoreImpl is obtained using the contents
//! of its current volatile and durable components.

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;
use vstd::seq::*;

use super::durable::durableimpl_v::*;
use super::durable::durablespec_t::*;
use super::inv_v::*;
use super::kvspec_t::*;
use super::volatile::volatileimpl_v::*;
use super::volatile::volatilespec_t::*;
use crate::kv::kvimpl_t::*;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;

use std::hash::Hash;

verus! {

pub struct UntrustedKvStoreImpl<PM, K, I, L, D, V, E>
where
    PM: PersistentMemoryRegions,
    K: Hash + Eq + Clone + PmCopy + std::fmt::Debug,
    I: PmCopy + Item<K> + std::fmt::Debug,
    L: PmCopy + std::fmt::Debug,
    D: DurableKvStore<PM, K, I, L, E>,
    V: VolatileKvIndex<K, E>,
    E: std::fmt::Debug,
{
    id: u128,
    durable_store: D,
    volatile_index: V,
    entries_per_list_node: usize,
    _phantom: Ghost<core::marker::PhantomData<(PM, K, I, L, E)>>,
}

impl<PM, K, I, L, D, V, E> UntrustedKvStoreImpl<PM, K, I, L, D, V, E>
where
    PM: PersistentMemoryRegions,
    K: Hash + Eq + Clone + PmCopy + Sized + std::fmt::Debug,
    I: PmCopy + Item<K> + Sized + std::fmt::Debug,
    L: PmCopy + std::fmt::Debug,
    D: DurableKvStore<PM, K, I, L, E>,
    V: VolatileKvIndex<K, E>,
    E: std::fmt::Debug,
{

    // This function specifies how all durable contents of the KV
    // should be viewed upon recovery as an abstract paged KV state.
    // TODO: write this
    pub closed spec fn recover(mems: Seq<Seq<u8>>, kv_id: u128) -> Option<AbstractKvStoreState<K, I, L, E>>
    {
        None
    }

    pub closed spec fn view(&self) -> AbstractKvStoreState<K, I, L, E>
    {
        AbstractKvStoreState {
            id: self.id,
            contents: AbstractKvStoreState::construct_view_contents(
                self.volatile_index@, self.durable_store@),
            _phantom: None,
        }
    }

    // Proves that if the durable store and volatile index comprising a KV are both empty,
    // then the view of the KV is also empty.
    proof fn lemma_empty_kv(self)
        requires
            self.durable_store@.empty(),
            self.volatile_index@.empty(),
        ensures
            self@.empty()
    {
        lemma_empty_map_contains_no_keys(self.volatile_index@.contents);
        assert(Set::new(|k| self.volatile_index@.contains_key(k)) =~= Set::<K>::empty());
    }

    pub closed spec fn valid(self) -> bool
    {
        &&& self.durable_store@.matches_volatile_index(self.volatile_index@)
        &&& self.durable_store.valid()
        &&& self.volatile_index.valid()
    }

    pub fn untrusted_new(
        pmem: PM,
        kvstore_id: u128,
        max_keys: usize,
        list_node_size: usize,
    ) -> (result: Result<Self, KvError<K, E>>)
        ensures
            match result {
                Ok(new_kv) => {
                    &&& new_kv.valid()
                }
                Err(_) => true
            }
    {
        let durable_store = D::new(pmem, kvstore_id, max_keys, list_node_size)?;
        let volatile_index = V::new(kvstore_id, max_keys)?;
        let kv = Self {
            id: kvstore_id,
            durable_store,
            volatile_index,
            entries_per_list_node: list_node_size,
            _phantom: Ghost(spec_phantom_data()),
        };
        proof {
            lemma_empty_index_matches_empty_store(durable_store@, volatile_index@);
            kv.lemma_empty_kv();
        }
        Ok(kv)
    }

    pub fn untrusted_create(
        &mut self,
        key: &K,
        item: I,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid(),
            key == item.spec_key(),
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.create(*key, item).unwrap()
                }
                Err(KvError::KeyAlreadyExists) => {
                    &&& old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        // check whether the key already exists
        if self.volatile_index.get(key).is_some() {
            return Err(KvError::KeyAlreadyExists);
        }

        let ghost old_durable_state = self.durable_store@;
        let ghost old_volatile_state = self.volatile_index@;
        let ghost old_kv_state = self@;

        // `item` stores its own key, so we don't have to pass its key to the durable
        // store separately.
        let offset = self.durable_store.create(item, perm)?;
        self.volatile_index.insert_item_offset(key, offset)?;

        proof {
            // the volatile index and durable store match after creating the new entry in both
            lemma_volatile_matches_durable_after_create(old_durable_state, old_volatile_state, offset as int, *key, item);
            let new_kv_state = old_kv_state.create(*key, item).unwrap();
            // the kv state reflects the new volatile and durable store states
            assert(new_kv_state.contents =~= AbstractKvStoreState::construct_view_contents(
                    self.volatile_index@, self.durable_store@));
        }

        Ok(())
    }

    pub fn untrusted_read_item(&self, key: &K) -> (result: Option<&I>)
        requires
            self.valid()
        ensures
        ({
            let spec_result = self@.read_item_and_list(*key);
            match (result, spec_result) {
                (Some(output_item), Some((spec_item, pages))) => {
                    &&& spec_item == output_item
                }
                (Some(output_item), None) => false,
                (None, Some((spec_item, pages))) => false,
                (None, None) => true,
            }
        })
    {
        assume(false); // TODO

        // First, get the offset of the header in the durable store using the volatile index
        let offset = self.volatile_index.get(key);
        match offset {
            Some(offset) => self.durable_store.read_item(offset),
            None => None
        }
    }

    // // TODO: return a Vec<&L> to save space/reduce copies
    // pub fn untrusted_read_item_and_list(&self, key: &K) -> (result: Option<(&I, Vec<&L>)>)
    //     requires
    //         self.valid(),
    //     ensures
    //     ({
    //         let spec_result = self@.read_item_and_list(*key);
    //         match (result, spec_result) {
    //             (Some((output_item, output_pages)), Some((spec_item, spec_pages))) => {
    //                 &&& spec_item == output_item
    //                 &&& spec_pages == output_pages@
    //             }
    //             (Some((output_item, output_pages)), None) => false,
    //             (None, Some((spec_item, spec_pages))) => false,
    //             (None, None) => true,
    //         }
    //     })
    // {
    //     assume(false);
    //     // First, get the offset of the header in the durable store using the volatile index
    //     let offset = self.volatile_index.get(key);
    //     match offset {
    //         Some(offset) => self.durable_store.read_item_and_list(offset),
    //         None => None
    //     }
    // }

    pub fn untrusted_read_list_entry_at_index(&self, key: &K, idx: u64) -> (result: Result<&L, KvError<K, E>>)
        requires
            self.valid()
        ensures
            ({
                let spec_result = self@.read_list_entry_at_index(*key, idx as int);
                match (result, spec_result) {
                    (Ok(output_entry), Ok(spec_entry)) => {
                        &&& output_entry == spec_entry
                    }
                    (Err(KvError::IndexOutOfRange), Err(KvError::IndexOutOfRange)) => {
                        &&& self@.contents.contains_key(*key)
                        &&& self@.contents[*key].1.len() <= idx
                    }
                    (Err(KvError::KeyNotFound), Err(KvError::KeyNotFound)) => {
                        &&& !self@.contents.contains_key(*key)
                    }
                    (_, _) => false
                }
            })
    {
        assume(false);
        Err(KvError::NotImplemented)
    }

    // pub fn untrusted_read_list(&self, key: &K) -> (result: Option<&Vec<L>>)
    //     requires
    //         self.valid(),
    //     ensures
    //     ({
    //         let spec_result = self@.read_item_and_list(*key);
    //         match (result, spec_result) {
    //             (Some(output_pages), Some((spec_item, spec_pages))) => {
    //                 &&& spec_pages == output_pages@
    //             }
    //             (Some(output_pages), None) => false,
    //             (None, Some((spec_item, spec_pages))) => false,
    //             (None, None) => true,
    //         }
    //     })
    // {
    //     assume(false);
    //     let offset = self.volatile_index.get(key);
    //     match offset {
    //         Some(offset) => self.durable_store.read_list(offset),
    //         None => None
    //     }
    // }

    pub fn untrusted_update_item(
        &mut self,
        key: &K,
        new_item: I,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid(),
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.update_item(*key, new_item).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        assume(false);
        let offset = self.volatile_index.get(key);
        match offset {
            Some(offset) => self.durable_store.update_item(offset, new_item),
            None => Err(KvError::KeyNotFound)
        }
    }

    pub fn untrusted_delete(
        &mut self,
        key: &K,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.delete(*key).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        assume(false);
        // Remove the entry from the volatile index, obtaining the physical offset as the return value
        let offset = self.volatile_index.remove(key)?;
        self.durable_store.delete(offset, perm)
    }

    pub fn untrusted_append_to_list(
        &mut self,
        key: &K,
        new_list_entry: L,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.append_to_list(*key, new_list_entry).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                // TODO: case for if we run out of space to append to the list
                Err(_) => false
            }
    {
        assume(false);
        return Err(KvError::InternalError);
        // let offset = self.volatile_index.get(key);
        // // append a page to the list rooted at this offset
        // let page_offset = match offset {
        //     Some(offset) => self.durable_store.append(offset, new_list_entry, perm)?,
        //     None => return Err(KvError::KeyNotFound)
        // };
        // // add the durable location of the page to the in-memory list
        // self.volatile_index.append_offset_to_list(key, page_offset)
    }

    pub fn untrusted_append_to_list_and_update_item(
        &mut self,
        key: &K,
        new_list_entry: L,
        new_item: I,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.append_to_list_and_update_item(*key, new_list_entry, new_item).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                // TODO: case for if we run out of space to append to the list
                Err(_) => false
            }
    {
        assume(false);
        let offset = self.volatile_index.get(key);
        // update the header at this offset append a page to the list rooted there
        let page_offset = match offset {
            Some(offset) => self.durable_store.update_item_and_append(offset, new_list_entry, new_item, perm)?,
            None => return Err(KvError::KeyNotFound)
        };

        // TODO: use append_node_offset or append_to_list depending on whether you need to allocate or not?
        self.volatile_index.append_to_list(key)
    }

    pub fn untrusted_update_list_entry_at_index(
        &mut self,
        key: &K,
        idx: usize,
        new_list_entry: L,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.update_list_entry_at_index(*key, idx, new_list_entry).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        assume(false);
        let header_offset = self.volatile_index.get(key);
        let entry_offset = self.volatile_index.get_entry_location_by_index(key, idx);
        match (header_offset, entry_offset) {
            (Some(header_offset), Ok(entry_offset)) => self.durable_store.update_list_entry_at_index(header_offset, entry_offset, new_list_entry, perm),
            (None, _) => Err(KvError::KeyNotFound),
            (_, Err(KvError::IndexOutOfRange)) => Err(KvError::IndexOutOfRange),
            (_, Err(_)) => Err(KvError::InternalError), // TODO: better error handling for all cases
        }
    }

    pub fn untrusted_update_entry_at_index_and_item(
        &mut self,
        key: &K,
        idx: usize,
        new_list_entry: L,
        new_item: I,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            match result {
                Ok(()) => {
                    &&& self.valid()
                    &&& self@ == old(self)@.update_entry_at_index_and_item(*key, idx, new_list_entry, new_item).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        assume(false);
        let header_offset = self.volatile_index.get(key);
        let entry_offset = self.volatile_index.get_entry_location_by_index(key, idx);
        match (header_offset, entry_offset) {
            (Some(header_offset), Ok(entry_offset)) => self.durable_store.update_entry_at_index_and_item(header_offset, entry_offset, new_item, new_list_entry,  perm),
            (None, _) => Err(KvError::KeyNotFound),
            (_, Err(KvError::IndexOutOfRange)) => Err(KvError::IndexOutOfRange),
            (_, Err(_)) => Err(KvError::InternalError), // TODO: better error handling for all cases
        }
    }

    pub fn untrusted_trim_list(
        &mut self,
        key: &K,
        trim_length: usize,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            match result {
                Ok(()) => {
                    &&& self.valid()
                    &&& self@ == old(self)@.trim_list(*key, trim_length as int).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
        {
        // use the volatile index to figure out which physical offsets should be removed
        // from the list, then use that information to trim the list on the durable side
        // TODO: trim_length is in terms of list entries, not bytes, right? Check Jay's impl
        // note: we trim from the beginning of the list, not the end
        assume(false);
        let item_offset = self.volatile_index.get(key);
        let old_list_head_offset = self.volatile_index.get_node_offset(key, 0);
        let new_list_head_offset = self.volatile_index.get_node_offset(key, trim_length);
        self.volatile_index.trim_list(key, trim_length)?;
        match (item_offset, old_list_head_offset, new_list_head_offset) {
            (Some(item_offset), Ok(old_list_head_offset), Ok(new_list_head_offset)) =>
                self.durable_store.trim_list(item_offset, old_list_head_offset, new_list_head_offset, trim_length, perm),
            (None, _, _) => Err(KvError::KeyNotFound),
            (_, _, Err(KvError::IndexOutOfRange)) | (_, Err(KvError::IndexOutOfRange), _) => Err(KvError::IndexOutOfRange),
            (_, _, Err(_)) | (_, Err(_), _) => Err(KvError::InternalError), // TODO: better error handling for all cases
        }
    }

    pub fn untrusted_trim_list_and_update_item(
        &mut self,
        key: &K,
        trim_length: usize,
        new_item: I,
        perm: Tracked<&TrustedKvPermission<PM, K, I, L, D, E>>
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            match result {
                Ok(()) => {
                    &&& self.valid()
                    &&& self@ == old(self)@.trim_list_and_update_item(*key, trim_length as int, new_item).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        assume(false);
        let item_offset = self.volatile_index.get(key);
        let old_list_head_offset = self.volatile_index.get_node_offset(key, 0);
        let new_list_head_offset = self.volatile_index.get_node_offset(key, trim_length);
        self.volatile_index.trim_list(key, trim_length)?;
        match (item_offset, old_list_head_offset, new_list_head_offset) {
            (Some(item_offset), Ok(old_list_head_offset), Ok(new_list_head_offset)) =>
                self.durable_store.trim_list_and_update_item(item_offset, old_list_head_offset, new_list_head_offset, trim_length, new_item, perm),
            (None, _, _) => Err(KvError::KeyNotFound),
            (_, _, Err(KvError::IndexOutOfRange)) | (_, Err(KvError::IndexOutOfRange), _,) => Err(KvError::IndexOutOfRange),
            (_, _, Err(_)) | (_, Err(_), _)=> Err(KvError::InternalError), // TODO: better error handling for all cases
        }
    }

    pub fn untrusted_get_keys(&self) -> (result: Vec<K>)
        requires
            self.valid()
        ensures
            result@.to_set() == self@.get_keys()
    {
        assume(false);
        self.volatile_index.get_keys()
    }

    pub fn untrusted_contains_key(&self, key: &K) -> (result: bool)
        requires
            self.valid(),
        ensures
            match result {
                true => self@[*key] is Some,
                false => self@[*key] is None
            }
    {
        assume(false);
        self.volatile_index.get(key).is_some()
    }

}

}

================
File: ./storage_node/src/kv/mod.rs
================

//! This module represents key-value store that maps
//! keys to 1) a fixed-size header and 2) a list of fixed-size index
//! pages.
//!
//! This module is in charge of managing both volatile indexes, which
//! actually map keys to values, and the durable structures that store
//! the values themselves.

pub mod durable;
pub mod inv_v;
pub mod kvimpl_t;
pub mod kvimpl_v;
pub mod kvspec_t;
pub mod volatile;

================
File: ./storage_node/src/kv/durable/durablespec_t.rs
================

//! A `DurableKvStore` represents the durable components of a `KvStore`. It is generic
//! to allow for different PM abstractions, persistent layouts, etc.
//! It should refine an array where each element optionally contains a key, a item,
//! and a list of pages. This structure encompasses all of the durable KV entries,
//! so we aren't distinguishing between separate physical memory regions. I think
//! we want to stay at a higher level of abstraction here to make it easier to jump
//! up to the overall KV store

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use crate::kv::kvimpl_t::*;
use crate::kv::volatile::volatilespec_t::*;
use crate::pmem::pmemspec_t::*;
use std::hash::Hash;

// TODO: is it safe for the fields of the structs in this file to be pub?

verus! {
    pub struct DurableKvStoreList<L>
    {
        pub list: Seq<L>,
        pub node_offset_map: Map<int, int> // maps nodes to the first logical list index they contain
    }

    impl<L> DurableKvStoreList<L>
    {
        pub open spec fn spec_index(self, idx: int) -> Option<L>
        {
            if idx < self.list.len() {
                Some(self.list[idx])
            } else {
                None
            }
        }

        pub open spec fn offset_index(self, offset: int) -> Option<int>
        {
            if self.node_offset_map.contains_key(offset) {
                Some(self.node_offset_map[offset])
            } else {
                None
            }
        }

        pub open spec fn len(self) -> int
        {
            self.list.len() as int
        }

        pub open spec fn empty() -> Self
        {
            DurableKvStoreList {
                list: Seq::empty(),
                node_offset_map: Map::empty(),
            }
        }
    }

    pub struct DurableKvStoreViewEntry<K, I, L>
    where
        K: Hash + Eq,
    {
        pub key: K,
        pub item: I,
        pub list: DurableKvStoreList<L>,

    }

    // TODO: remove since the fields are public
    impl<K, I, L> DurableKvStoreViewEntry<K, I, L>
    where
        K: Hash + Eq,
    {
        pub open spec fn key(self) -> K
        {
            self.key
        }

        pub open spec fn item(self) -> I
        {
            self.item
        }

        pub open spec fn list(self) -> DurableKvStoreList<L>
        {
            self.list
        }
    }

    pub struct DurableKvStoreView<K, I, L, E>
    where
        K: Hash + Eq + std::fmt::Debug,
        I: Item<K>,
        E: std::fmt::Debug
    {
        pub contents: Map<int, DurableKvStoreViewEntry<K, I, L>>,
        pub index_to_key_map: Map<int, K>,
        pub _phantom: Option<E>
    }

    impl<K, I, L, E> DurableKvStoreView<K, I, L, E>
    where
        K: Hash + Eq + std::fmt::Debug,
        I: Item<K>,
        E: std::fmt::Debug
    {
        pub open spec fn spec_index(self, idx: int) -> Option<DurableKvStoreViewEntry<K, I, L>>
        {
            if self.contents.contains_key(idx) {
                Some(self.contents[idx])
            } else {
                None
            }
        }

        pub open spec fn contains_key(self, idx: int) -> bool
        {
            self[idx] is Some
        }

        pub open spec fn empty(self) -> bool
        {
            self.contents.is_empty()
        }

        pub open spec fn len(self) -> nat
        {
            self.contents.len()
        }

        pub open spec fn create(self, offset: int, item: I) -> Result<Self, KvError<K, E>>
        {
            if self.contents.contains_key(offset) {
                Err(KvError::KeyAlreadyExists)
            } else {
                Ok(
                    Self {
                        contents: self.contents.insert(
                            offset,
                            DurableKvStoreViewEntry {
                                key: item.spec_key(),
                                item,
                                list: DurableKvStoreList::empty()
                            }
                        ),
                        index_to_key_map: self.index_to_key_map.insert(offset, item.spec_key()),
                        _phantom: None
                    }
                )
            }
        }

        // Returns true if the keys in the durable store match the keys in the ghost index_to_key_map
        pub open spec fn valid(self) -> bool
        {
            &&& forall |i: int| #![auto] self.contains_key(i) <==> self.index_to_key_map.contains_key(i)
            &&& forall |i: int| #![auto] self.index_to_key_map.contains_key(i) ==> {
                    &&& self[i] is Some
                    &&& self[i].unwrap().key() == self.index_to_key_map[i]
                }
        }

        // TODO: might be cleaner to define this elsewhere (like in the interface)
        pub open spec fn matches_volatile_index(&self, volatile_index: VolatileKvIndexView<K>) -> bool
        {
            &&& self.len() == volatile_index.len()
            &&& self.contents.dom().finite()
            &&& volatile_index.contents.dom().finite()
            &&& self.valid()
            // all keys in the volatile index are stored at the indexed offset in the durable store
            &&& forall |k: K| #![auto] volatile_index.contains_key(k) ==> {
                    let indexed_offset = volatile_index[k].unwrap().item_offset;
                    &&& self.index_to_key_map.contains_key(indexed_offset)
                    &&& self.index_to_key_map[indexed_offset] == k
                }
            // all offsets in the durable store have a corresponding entry in the volatile index
            &&& forall |i: int| #![auto] self.contains_key(i) ==> {
                &&& self.index_to_key_map.contains_key(i)
                &&& volatile_index.contains_key(self.index_to_key_map[i])
                &&& volatile_index[self.index_to_key_map[i]].unwrap().item_offset == i
            }
        }
    }
}

================
File: ./storage_node/src/kv/durable/mod.rs
================

pub mod durableimpl_v;
pub mod durablespec_t;

================
File: ./storage_node/src/kv/durable/durableimpl_v.rs
================

//! This file contains a trait that defines the interface for the high-level
//! durable component of the KV store.

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use crate::kv::durable::durablespec_t::*;
use crate::kv::kvimpl_t::*;
use crate::kv::kvspec_t::*;
use crate::kv::volatile::volatilespec_t::*;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use std::hash::Hash;

verus! {
    // TODO: this should just be a struct, the interface is going to be very
    // specific to the structure
    pub trait DurableKvStore<PM, K, I, L, E> : Sized
    where
        PM: PersistentMemoryRegions,
        K: Hash + Eq + Clone + PmCopy + Sized + std::fmt::Debug,
        I: PmCopy + Item<K> + Sized + std::fmt::Debug,
        L: PmCopy + std::fmt::Debug,
        E: std::fmt::Debug,
    {
        spec fn view(&self) -> DurableKvStoreView<K, I, L, E>;

        spec fn recover_to_kv_state(bytes: Seq<Seq<u8>>, id: u128) -> Option<AbstractKvStoreState<K, I, L, E>>;

        spec fn valid(self) -> bool;

        fn new(pmem: PM,
            kvstore_id: u128,
            max_keys: usize,
            lower_bound_on_max_pages: usize,
        ) -> (result: Result<Self, KvError<K, E>>)
            ensures
                match(result) {
                    Ok(durable_store) => {
                        &&& durable_store@.empty()
                        &&& durable_store.valid()
                        &&& durable_store@.valid()
                        &&& durable_store@.contents.dom().finite()
                    }
                    Err(_) => true // TODO
                };

        fn create(
            &mut self,
            item: I,
            perm: Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>
        ) -> (result: Result<u64, KvError<K, E>>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                ({
                    match result {
                        Ok(offset) => {
                            let spec_result = old(self)@.create(offset as int, item);
                            match spec_result {
                                Ok(spec_result) => {
                                    &&& self@.len() == old(self)@.len() + 1
                                    &&& self@ == spec_result
                                    &&& 0 <= offset < self@.len()
                                    &&& self@[offset as int].is_Some()
                                }
                                Err(_) => false
                            }
                        }
                        Err(_) => false
                    }
                })
        ;

        fn read_item(
            &self,
            offset: u64
        ) -> (result: Option<&I>)
            requires
                self.valid(),
            ensures
                match result {
                    Some(item) => {
                        match self@[offset as int] {
                            Some(entry) => entry.item() == item,
                            None => false
                        }
                    }
                    None => self@[offset as int].is_None()
                }
        ;

        fn read_list_entry_at_index(
            &self,
            offset: u64,
            idx: u64
        ) -> (result: Result<&L, KvError<K, E>>)
            requires
                self.valid(),
            ensures
                match (result, self@[offset as int]) {
                    (Ok(output_list_entry), Some(spec_entry)) => {
                        let spec_list_entry = spec_entry.list()[idx as int];
                        &&& spec_list_entry is Some
                        &&& spec_list_entry.unwrap() == output_list_entry
                    }
                    (Err(KvError::IndexOutOfRange), _) => {
                        &&& self@[offset as int] is Some
                        &&& self@[offset as int].unwrap().list()[idx as int] is None
                    }
                    (Err(_), Some(spec_entry)) => false,
                    (Ok(output_list_entry), None) => false,
                    (_, _) => false
                }
        ;

        fn update_item(
            &mut self,
            offset: u64,
            new_item: I,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid()
            ensures
                self.valid(),
                match result {
                    Ok(()) => {
                        match (old(self)@[offset as int], self@[offset as int]) {
                            (Some(old_entry), Some(entry)) => {
                                &&& entry.key() == old_entry.key()
                                &&& entry.item() == new_item
                                &&& entry.list() == old_entry.list()
                            }
                            (_, _) => false
                        }
                    }
                    Err(KvError::KeyNotFound) => self@[offset as int].is_None(),
                    Err(_) => true // TODO
                }
        ;

        fn delete(
            &mut self,
            offset: u64,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid()
            ensures
                self.valid(),
                match result {
                    Ok(()) => {
                        self@[offset as int].is_None()
                    }
                    Err(_) => true // TODO
                }
        ;

        fn append(
            &mut self,
            offset: u64,
            new_entry: L,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid(),
                // should require that there is enough space in the tail node
            ensures
                self.valid(),
                match result {
                    Ok(()) => {
                        let old_record = old(self)@.contents[offset as int];
                        let new_record = self@.contents[offset as int];
                        &&& new_record.list().list == old_record.list().list.push(new_entry)
                    }
                    Err(_) => false // TODO
                }
        ;

        fn alloc_list_node_and_append(
            &mut self,
            offset: u64,
            new_entry: L,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>,
        ) -> (result: Result<u64, KvError<K, E>>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                match result {
                    Ok(node_phys_offset) => {
                        let old_record = old(self)@.contents[offset as int];
                        let new_record = self@.contents[offset as int];
                        &&& new_record.list().list == old_record.list().list.push(new_entry)
                        &&& new_record.list().node_offset_map ==
                                old_record.list().node_offset_map.insert(node_phys_offset as int, old(self)@.len() as int)
                    }
                    Err(_) => false // TODO
                }
        ;

        fn update_item_and_append(
            &mut self,
            offset: u64,
            new_entry: L,
            new_item: I,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>
        ) -> (result: Result<u64, KvError<K, E>>)
            requires
                old(self).valid()
                // should require that there is enough space in the tail node
            ensures
                self.valid(),
                match result {
                    Ok(phys_offset) => {
                        let old_record = old(self)@.contents[offset as int];
                        let new_record = self@.contents[offset as int];
                        &&& new_record.item() == new_item
                        &&& new_record.list().list == old_record.list().list.push(new_entry)
                    }
                    Err(_) => false // TODO
                }
        ;

        fn alloc_list_node_update_item_and_append(
            &mut self,
            offset: u64,
            new_entry: L,
            new_item: I,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>
        ) -> (result: Result<u64, KvError<K, E>>)
            requires
                old(self).valid()
            ensures
                self.valid(),
                match result {
                    Ok(phys_offset) => {
                        let old_record = old(self)@.contents[offset as int];
                        let new_record = self@.contents[offset as int];
                        &&& new_record.item() == new_item
                        &&& new_record.list().list == old_record.list().list.push(new_entry)
                        &&& new_record.list().node_offset_map ==
                                old_record.list().node_offset_map.insert(phys_offset as int, old(self)@.len() as int)
                    }
                    Err(_) => false // TODO
                }
        ;

        fn update_list_entry_at_index(
            &mut self,
            item_offset: u64, // TODO: is this necessary? maybe just as ghost state
            entry_offset: u64,
            new_entry: L,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                match result {
                    Ok(()) => {
                        let old_record = old(self)@.contents[item_offset as int];
                        let new_record = self@.contents[item_offset as int];
                        let list_index = new_record.list().node_offset_map[entry_offset as int];
                        &&& list_index == old_record.list().node_offset_map[entry_offset as int]
                        &&& new_record.list()[list_index as int] is Some
                        &&& new_record.list()[list_index as int].unwrap() == new_entry
                    }
                    Err(_) => false // TODO
                }
        ;

        fn update_entry_at_index_and_item(
            &mut self,
            item_offset: u64,
            entry_offset: u64,
            new_item: I,
            new_entry: L,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                match result {
                    Ok(()) => {
                        let old_record = old(self)@.contents[item_offset as int];
                        let new_record = self@.contents[item_offset as int];
                        let list_index = new_record.list().node_offset_map[entry_offset as int];
                        &&& list_index == old_record.list().node_offset_map[entry_offset as int]
                        &&& new_record.list()[list_index as int] is Some
                        &&& new_record.list()[list_index as int].unwrap() == new_entry
                        &&& new_record.item() == new_item
                    }
                    Err(_) => false // TODO
                }
        ;

        fn trim_list(
            &mut self,
            item_offset: u64,
            old_head_node_offset: u64,
            new_head_node_offset: u64,
            trim_length: usize,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                match result {
                    Ok(()) => {
                        let old_record = old(self)@.contents[item_offset as int];
                        let new_record = self@.contents[item_offset as int];
                        &&& new_record.list().list == old_record.list().list.subrange(trim_length as int, old_record.list().len() as int)
                        // offset map entries pointing to trimmed indices should have been removed from the view
                        &&& forall |i: int| 0 <= old_record.list().node_offset_map[i] < trim_length ==> {
                            new_record.list().offset_index(i) is None
                        }
                    }
                    Err(_) => false // TODO
                }
        ;

        fn trim_list_and_update_item(
            &mut self,
            item_offset: u64,
            old_head_node_offset: u64,
            new_head_node_offset: u64,
            trim_length: usize,
            new_item: I,
            Tracked(perm): Tracked<&TrustedKvPermission<PM, K, I, L, Self, E>>,
        ) -> (result: Result<(), KvError<K, E>>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                match result {
                    Ok(()) => {
                        let old_record = old(self)@.contents[item_offset as int];
                        let new_record = self@.contents[item_offset as int];
                        &&& new_record.item() == new_item
                        &&& new_record.list().list == old_record.list().list.subrange(trim_length as int, old_record.list().len() as int)
                        // offset map entries pointing to trimmed indices should have been removed from the view
                        &&& forall |i: int| 0 <= old_record.list().node_offset_map[i] < trim_length ==>
                                new_record.list().offset_index(i) is None
                    }
                    Err(_) => false // TODO
                }
        ;
    }
}

================
File: ./storage_node/src/kv/kvimpl_t.rs
================

//! This file contains the public interface of the paged key-value store.
//! The methods offered by this file should match the mocks.
//! The key-value store itself should be as generic as possible, not
//! restricted to particular data structures.
//! We define legal crash states at this level and pass them
//! to the untrusted implementation, which passes them along
//! to untrusted components.
//!
//! Note that the design of this component is different from the original
//! verified log in that the untrusted implementation, rather than
//! the trusted implementation in this file, owns the
//! WriteRestrictedPersistentMemoryRegions backing the structures.
//! This makes the interface to the untrusted component simpler and
//! will make it easier to distinguish between regions owned by
//! different components.
//!
//! This file is unverified and should be tested/audited for correctness.
//!
//! TODO: handle errors properly in postconditions

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use super::durable::durableimpl_v::*;
use super::durable::durablespec_t::*;
use super::kvimpl_v::*;
use super::kvspec_t::*;
use super::volatile::volatileimpl_v::*;
use super::volatile::volatilespec_t::*;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use std::hash::Hash;

verus! {

#[derive(Debug, PartialEq, Clone)]
pub enum KvError<K, E>
where
    K: std::fmt::Debug,
    E: std::fmt::Debug,
{
    NotImplemented,
    InvalidParameter,
    InternalError, // TODO: reason
    KeyNotFound,
    KeyAlreadyExists,
    InvalidKey{ key: K },
    IndexOutOfRange,
    RegionTooSmall { required: usize, actual: usize },
    OutOfSpace,
    InvalidPersistentMemoryRegionProvided, // TODO: reason
    SerializationError { error: E },
    DeserializationError { error: E },
}

pub trait Item<K> : Sized {
    spec fn spec_key(self) -> K;

    fn key(&self) -> (out: K)
        ensures
            out == self.spec_key()
    ;
}

// TODO: should the constructor take one PM region and break it up into the required sub-regions,
// or should the caller provide it split up in the way that they want?
pub struct KvStore<PM, K, I, L, D, V, E>
where
    PM: PersistentMemoryRegions,
    K: Hash + Eq + Clone + PmCopy + Sized + std::fmt::Debug,
    I: PmCopy + Item<K> + Sized + std::fmt::Debug,
    L: PmCopy + std::fmt::Debug,
    D: DurableKvStore<PM, K, I, L, E>,
    V: VolatileKvIndex<K, E>,
    E: std::fmt::Debug,
{
    id: u128,
    untrusted_kv_impl: UntrustedKvStoreImpl<PM, K, I, L, D, V, E>,
}

// TODO: is there a better way to handle PhantomData?
#[verifier::external_body]
pub closed spec fn spec_phantom_data<V: ?Sized>() -> core::marker::PhantomData<V> {
    core::marker::PhantomData::default()
}

impl<PM, K, I, L, D, V, E> KvStore<PM, K, I, L, D, V, E>
where
    PM: PersistentMemoryRegions,
    K: Hash + Eq + Clone + PmCopy + Sized + std::fmt::Debug,
    I: PmCopy + Item<K> + Sized + std::fmt::Debug,
    L: PmCopy + std::fmt::Debug,
    D: DurableKvStore<PM, K, I, L, E>,
    V: VolatileKvIndex<K, E>,
    E: std::fmt::Debug,
{
    pub closed spec fn view(&self) -> AbstractKvStoreState<K, I, L, E>
    {
        self.untrusted_kv_impl@
    }

    pub closed spec fn valid(self) -> bool
    {
        self.untrusted_kv_impl.valid()
    }

    /// The `KvStore` constructor calls the constructors for the durable and
    /// volatile components of the key-value store.
    /// `list_node_size` is the number of list entries in each node (not the number
    /// of bytes used by each node)
    fn new(
        pmem: PM,
        kvstore_id: u128,
        max_keys: usize,
        list_node_size: usize
    ) -> (result: Result<Self, KvError<K, E>>)
        requires
            pmem.inv(),
        ensures
            match result {
                Ok(new_kv) => {
                    &&& new_kv.valid()
                }
                Err(_) => true
            }
    {
        Ok(
            Self {
                id: kvstore_id,
                untrusted_kv_impl: UntrustedKvStoreImpl::untrusted_new(
                    pmem,
                    kvstore_id,
                    max_keys,
                    list_node_size
                )?
            }
        )
    }

    fn restore(pmem: PM, region_size: usize, kvstore_id: u128) -> (result: Result<Self, KvError<K, E>>)
        requires
            pmem.inv(),
        ensures
            match result {
                Ok(restored_kv) => {
                    let restored_state = UntrustedKvStoreImpl::<PM, K, I, L, D, V, E>::recover(pmem@.committed(), kvstore_id);
                    match restored_state {
                        Some(restored_state) => restored_kv@ == restored_state,
                        None => false
                    }
                }
                Err(_) => true // TODO
            }
    {
        Err(KvError::NotImplemented)
    }

    fn create(&mut self, key: &K, item: I) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid(),
            key == item.spec_key(),
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.create(*key, item).unwrap()
                }
                Err(KvError::KeyAlreadyExists) => {
                    &&& old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            Err(KvError::KeyAlreadyExists)
        } else {
            let tracked perm =
            TrustedKvPermission::new_two_possibilities(self.id, self@, self@.create(*key, item).unwrap());
            self.untrusted_kv_impl.untrusted_create(key, item, Tracked(&perm))
        }
    }

    fn read_item(&self, key: &K) -> (result: Option<&I>)
        requires
            self.valid()
        ensures
        ({
            let spec_result = self@.read_item_and_list(*key);
            match (result, spec_result) {
                (Some(output_item), Some((spec_item, pages))) => {
                    &&& spec_item == output_item
                }
                (Some(output_item), None) => false,
                (None, Some((spec_item, pages))) => false,
                (None, None) => true,
            }
        })
    {

        self.untrusted_kv_impl.untrusted_read_item(key)
    }

    // fn read_item_and_list(&self, key: &K) -> (result: Option<(&I, Vec<&L>)>)
    //     requires
    //         self.valid(),
    //     ensures
    //     ({
    //         let spec_result = self@.read_item_and_list(*key);
    //         match (result, spec_result) {
    //             (Some((output_item, output_pages)), Some((spec_item, spec_pages))) => {
    //                 &&& spec_item == output_item
    //                 &&& spec_pages == output_pages@
    //             }
    //             (Some((output_item, output_pages)), None) => false,
    //             (None, Some((spec_item, spec_pages))) => false,
    //             (None, None) => true,
    //         }
    //     })
    // {
    //     self.untrusted_kv_impl.untrusted_read_item_and_list(key)
    // }

    fn read_list_entry_at_index(&self, key: &K, idx: u64) -> (result: Result<&L, KvError<K, E>>)
        requires
            self.valid()
        ensures
            ({
                let spec_result = self@.read_list_entry_at_index(*key, idx as int);
                match (result, spec_result) {
                    (Ok(output_entry), Ok(spec_entry)) => {
                        &&& output_entry == spec_entry
                    }
                    (Err(KvError::IndexOutOfRange), Err(KvError::IndexOutOfRange)) => {
                        &&& self@.contents.contains_key(*key)
                        &&& self@.contents[*key].1.len() <= idx
                    }
                    (Err(KvError::KeyNotFound), Err(KvError::KeyNotFound)) => {
                        &&& !self@.contents.contains_key(*key)
                    }
                    (_, _) => false
                }
            })
    {
        self.untrusted_kv_impl.untrusted_read_list_entry_at_index(key, idx)
    }

    // fn read_list(&self, key: &K) -> (result: Option<&Vec<L>>)
    //     requires
    //         self.valid(),
    //     ensures
    //     ({
    //         let spec_result = self@.read_item_and_list(*key);
    //         match (result, spec_result) {
    //             (Some(output_pages), Some((spec_item, spec_pages))) => {
    //                 &&& spec_pages == output_pages@
    //             }
    //             (Some(output_pages), None) => false,
    //             (None, Some((spec_item, spec_pages))) => false,
    //             (None, None) => true,
    //         }
    //     })
    // {
    //     self.untrusted_kv_impl.untrusted_read_list(key)
    // }

    fn update_item(&mut self, key: &K, new_item: I) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid(),
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.update_item(*key, new_item).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            let tracked perm = TrustedKvPermission::new_two_possibilities(self.id, self@, self@.update_item(*key, new_item).unwrap());
            self.untrusted_kv_impl.untrusted_update_item(key, new_item, Tracked(&perm))
        } else {
            Err(KvError::KeyNotFound)
        }

    }

    fn delete(&mut self, key: &K) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.delete(*key).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            let tracked perm = TrustedKvPermission::new_two_possibilities(self.id, self@, self@.delete(*key).unwrap());
            self.untrusted_kv_impl.untrusted_delete(key, Tracked(&perm))
        } else {
            Err(KvError::KeyNotFound)
        }
    }

    // TODO: remove?
    fn append_to_list(
        &mut self,
        key: &K,
        new_list_entry: L
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.append_to_list(*key, new_list_entry).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                // TODO: case for if we run out of space to append to the list
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            let tracked perm = TrustedKvPermission::new_two_possibilities(self.id, self@, self@.append_to_list(*key, new_list_entry).unwrap());
            self.untrusted_kv_impl.untrusted_append_to_list(key, new_list_entry, Tracked(&perm))
        } else {
            Err(KvError::KeyNotFound)
        }
    }

    fn append_to_list_and_update_item(
        &mut self,
        key: &K,
        new_list_entry: L,
        new_item: I,
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.append_to_list_and_update_item(*key, new_list_entry, new_item).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                // TODO: case for if we run out of space to append to the list
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            let tracked perm = TrustedKvPermission::new_two_possibilities(self.id, self@, self@.append_to_list_and_update_item(*key, new_list_entry, new_item).unwrap());
            self.untrusted_kv_impl.untrusted_append_to_list_and_update_item(key,  new_list_entry, new_item, Tracked(&perm))
        } else {
            Err(KvError::KeyNotFound)
        }
    }

    fn update_list_entry_at_index(&mut self, key: &K, idx: usize, new_list_entry: L) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            self.valid(),
            match result {
                Ok(()) => {
                    &&& self@ == old(self)@.update_list_entry_at_index(*key, idx, new_list_entry).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            let tracked perm = TrustedKvPermission::new_two_possibilities(self.id, self@, self@.update_list_entry_at_index(*key, idx, new_list_entry).unwrap());
            self.untrusted_kv_impl.untrusted_update_list_entry_at_index(key, idx, new_list_entry, Tracked(&perm))
        } else {
            Err(KvError::KeyNotFound)
        }
    }

    fn update_entry_at_index_and_item(
        &mut self,
        key: &K,
        idx: usize,
        new_list_entry: L,
        new_item: I,
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            match result {
                Ok(()) => {
                    &&& self.valid()
                    &&& self@ == old(self)@.update_entry_at_index_and_item(*key, idx, new_list_entry, new_item).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            let tracked perm = TrustedKvPermission::new_two_possibilities(self.id, self@, self@.update_entry_at_index_and_item(*key, idx, new_list_entry, new_item).unwrap());
            self.untrusted_kv_impl.untrusted_update_entry_at_index_and_item(key,  idx, new_list_entry, new_item, Tracked(&perm))
        } else {
            Err(KvError::KeyNotFound)
        }
    }

    fn trim_list(
        &mut self,
        key: &K,
        trim_length: usize,
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            match result {
                Ok(()) => {
                    &&& self.valid()
                    &&& self@ == old(self)@.trim_list(*key, trim_length as int).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            let tracked perm = TrustedKvPermission::new_two_possibilities(self.id, self@, self@.trim_list(*key, trim_length as int).unwrap());
            self.untrusted_kv_impl.untrusted_trim_list(key, trim_length, Tracked(&perm))
        } else {
            Err(KvError::KeyNotFound)
        }
    }

    fn trim_list_and_update_item(
        &mut self,
        key: &K,
        trim_length: usize,
        new_item: I,
    ) -> (result: Result<(), KvError<K, E>>)
        requires
            old(self).valid()
        ensures
            match result {
                Ok(()) => {
                    &&& self.valid()
                    &&& self@ == old(self)@.trim_list_and_update_item(*key, trim_length as int, new_item).unwrap()
                }
                Err(KvError::KeyNotFound) => {
                    &&& !old(self)@.contents.contains_key(*key)
                    &&& old(self)@ == self@
                }
                Err(_) => false
            }
    {
        if self.untrusted_kv_impl.untrusted_contains_key(key) {
            let tracked perm = TrustedKvPermission::new_two_possibilities(self.id, self@, self@.trim_list_and_update_item(*key, trim_length as int, new_item).unwrap());
            self.untrusted_kv_impl.untrusted_trim_list_and_update_item(key, trim_length, new_item, Tracked(&perm))
        } else {
            Err(KvError::KeyNotFound)
        }
    }

    fn get_keys(&self) -> (result: Vec<K>)
        requires
            self.valid()
        ensures
            result@.to_set() == self@.get_keys()
    {
        self.untrusted_kv_impl.untrusted_get_keys()
    }
}

}

================
File: ./storage_node/src/kv/inv_v.rs
================

//! This file contains helper proofs and invariants about the state
//! of the high-level KV store.

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;
use vstd::set::*;
use vstd::set_lib::*;

use crate::kv::durable::durableimpl_v::*;
use crate::kv::durable::durablespec_t::*;
use crate::kv::kvimpl_t::*;
use crate::kv::kvimpl_v::*;
use crate::kv::kvspec_t::*;
use crate::kv::volatile::volatileimpl_v::*;
use crate::kv::volatile::volatilespec_t::*;
use crate::pmem::pmemspec_t::*;
use std::hash::Hash;

verus! {
    pub proof fn lemma_empty_index_matches_empty_store<K, I, L, E>(durable_store: DurableKvStoreView<K, I, L, E>, volatile_index: VolatileKvIndexView<K>)
        where
            K: Hash + Eq + std::fmt::Debug,
            I: Item<K>,
            E: std::fmt::Debug
        requires
            durable_store.empty(),
            durable_store.valid(),
            durable_store.contents.dom().finite(),
            volatile_index.empty(),
        ensures
            durable_store.matches_volatile_index(volatile_index)
    {
        assert(durable_store.contents.is_empty());
        assert(durable_store.contents.dom().is_empty());
        lemma_empty_map_contains_no_keys(durable_store.contents);
    }

    pub proof fn lemma_empty_map_contains_no_keys<K, V>(map: Map<K, V>)
        requires
            map.is_empty(),
            map.dom().finite(),
        ensures
            forall |k: K| !map.contains_key(k)
    {}

    /// This lemma proves that, given a durable state and a volatile state that matches it
    /// (i.e., the durable and volatile states are the same size, the volatile index maps
    /// all keys to an offset with the corresponding durable entry and the durable store's
    /// entries correspond to the volatile index), then after creating a new entry in each
    /// using the same offset, key, and item, the durable and volatile states still match.
    pub proof fn lemma_volatile_matches_durable_after_create<K, I, L, E>(
        old_durable_state: DurableKvStoreView<K, I, L, E>,
        old_volatile_state: VolatileKvIndexView<K>,
        offset: int,
        key: K,
        item: I
    )
        where
            K: Hash + Eq + std::fmt::Debug,
            I: Item<K>,
            E: std::fmt::Debug
        requires
            old_durable_state.matches_volatile_index(old_volatile_state),
            old_durable_state[offset] is None,
            old_volatile_state[key] is None,
            item.spec_key() == key,
        ensures
            ({
                let new_durable_state = old_durable_state.create(offset, item).unwrap();
                let new_volatile_state = old_volatile_state.insert_item_offset(key, offset);
                new_durable_state.matches_volatile_index(new_volatile_state)
            })
    {
        let new_durable_state = old_durable_state.create(offset, item).unwrap();
        let new_volatile_state = old_volatile_state.insert_item_offset(key, offset);

        assert forall |k: K| #![auto] new_volatile_state.contains_key(k) implies {
            let indexed_offset = new_volatile_state[k].unwrap().item_offset;
            &&& new_durable_state.index_to_key_map.contains_key(indexed_offset)
            &&& new_durable_state.index_to_key_map[indexed_offset] == k
        } by {
            if k != key {
                assert(old_volatile_state.contains_key(k));
                let indexed_offset = new_volatile_state[k].unwrap().item_offset;
                assert(old_durable_state.index_to_key_map.contains_key(indexed_offset));
                assert(old_durable_state.index_to_key_map[indexed_offset] == new_durable_state.index_to_key_map[indexed_offset]);
            }
        }
    }
}

================
File: ./storage_node/src/kv/kvspec_t.rs
================

//! This file contains the unverified specification for the high-level KV store.
//! We also define the crash-consistency-related TrustedKvPermission structure
//! here.
//!
//! This file should be audited for correctness.

#![allow(unused_imports)]
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use crate::pmem::wrpm_t::*;

use crate::kv::durable::durableimpl_v::*;
use crate::kv::durable::durablespec_t::*;
use crate::kv::kvimpl_t::*;
use crate::kv::kvimpl_v::*;
use crate::kv::volatile::volatileimpl_v::*;
use crate::kv::volatile::volatilespec_t::*;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use std::hash::Hash;

verus! {

    // Since the durable part of the PagedKV is a list of PM regions,
    // we use Seq<Seq<u8>> to determine whether states are crash-consistent.
    pub struct TrustedKvPermission<PM, K, I, L, D, E>
        where
            PM: PersistentMemoryRegions,
            K: Hash + Eq + Clone + PmCopy + std::fmt::Debug,
            I: PmCopy + Item<K> + std::fmt::Debug,
            L: PmCopy + std::fmt::Debug,
            D: DurableKvStore<PM, K, I, L, E>,
            E: std::fmt::Debug,
    {
        ghost is_state_allowable: spec_fn(Seq<Seq<u8>>) -> bool,
        _phantom:  Ghost<core::marker::PhantomData<(PM, K, I, L, D, E)>>
    }

    impl<PM, K, I, L, D, E> CheckPermission<Seq<Seq<u8>>> for TrustedKvPermission<PM, K, I, L, D, E>
        where
            PM: PersistentMemoryRegions,
            K: Hash + Eq + Clone + PmCopy + std::fmt::Debug,
            I: PmCopy + Item<K> + std::fmt::Debug,
            L: PmCopy + std::fmt::Debug,
            D: DurableKvStore<PM, K, I, L, E>,
            E: std::fmt::Debug,
    {
        closed spec fn check_permission(&self, state: Seq<Seq<u8>>) -> bool
        {
            (self.is_state_allowable)(state)
        }
    }

    impl<PM, K, I, L, D, E> TrustedKvPermission<PM, K, I, L, D, E>
        where
            PM: PersistentMemoryRegions,
            K: Hash + Eq + Clone + PmCopy + std::fmt::Debug,
            I: PmCopy + Item<K> + std::fmt::Debug,
            L: PmCopy + std::fmt::Debug,
            D: DurableKvStore<PM, K, I, L, E>,
            E: std::fmt::Debug,
    {
        // methods copied from multilogimpl_t and updated for PagedKV structures

        // This is one of two constructors for `TrustedKvPermission`.
        // It conveys permission to do any update as long as a
        // subsequent crash and recovery can only lead to given
        // abstract state `state`.
        pub proof fn new_one_possibility(kv_id: u128, state: AbstractKvStoreState<K, I, L, E>) -> (tracked perm: Self)
            ensures
                forall |s| #[trigger] perm.check_permission(s) <==>
                    D::recover_to_kv_state(s, kv_id) == Some(state)
        {
            Self {
                is_state_allowable: |s| D::recover_to_kv_state(s, kv_id) == Some(state),
                _phantom: Ghost(spec_phantom_data())
            }
        }

        // This is the second of two constructors for
        // `TrustedKvPermission`.  It conveys permission to do any
        // update as long as a subsequent crash and recovery can only
        // lead to one of two given abstract states `state1` and
        // `state2`.
        pub proof fn new_two_possibilities(
            kv_id: u128,
            state1: AbstractKvStoreState<K, I, L, E>,
            state2: AbstractKvStoreState<K, I, L, E>
        ) -> (tracked perm: Self)
            ensures
                forall |s| #[trigger] perm.check_permission(s) <==> {
                    ||| D::recover_to_kv_state(s, kv_id) == Some(state1)
                    ||| D::recover_to_kv_state(s, kv_id) == Some(state2)
                }
        {
            Self {
                is_state_allowable: |s| {
                    ||| D::recover_to_kv_state(s, kv_id) == Some(state1)
                    ||| D::recover_to_kv_state(s, kv_id) == Some(state2)
                },
                _phantom: Ghost(spec_phantom_data())
            }
        }
    }


    /// An `AbstractKvStoreState` is an abstraction of
    /// an entire `KvStoreStore`.
    /// TODO: Should this be generic over the key/header/page
    /// types used in the kv store, or over their views?
    #[verifier::reject_recursive_types(K)]
    pub struct AbstractKvStoreState<K, I, L, E>
    where
        K: Hash + Eq,
        I: Item<K>,
    {
        pub id: u128,
        pub contents: Map<K, (I, Seq<L>)>,
        pub _phantom: Option<E>
    }

    impl<K, I, L, E> AbstractKvStoreState<K, I, L, E>
    where
        K: Hash + Eq + std::fmt::Debug,
        I: Item<K>,
        E: std::fmt::Debug,
    {
        pub open spec fn spec_index(self, key: K) -> Option<(I, Seq<L>)>
        {
            if self.contents.contains_key(key) {
                Some(self.contents[key])
            } else {
                None
            }
        }

        pub open spec fn empty(self) -> bool
        {
            self.contents.is_empty()
        }

        pub open spec fn contains_key(&self, key: K) -> bool
        {
            self.contents.contains_key(key)
        }

        pub open spec fn construct_view_contents(
            volatile_store_state: VolatileKvIndexView<K>,
            durable_store_state: DurableKvStoreView<K, I, L, E>
        ) -> Map<K, (I, Seq<L>)> {
            Map::new(
                |k| { volatile_store_state.contains_key(k) },
                |k| {
                    let index_entry = volatile_store_state[k].unwrap();
                    let durable_entry = durable_store_state[index_entry.item_offset].unwrap();
                    (durable_entry.item(), durable_entry.list().list)
                }
            )
        }

        pub open spec fn create(self, key: K, item: I) -> Result<Self, KvError<K, E>>
        {
            if self.contents.contains_key(key) {
                Err(KvError::KeyAlreadyExists)
            } else {
                Ok(Self {
                    id: self.id,
                    contents: self.contents.insert(key, (item, Seq::empty())),
                    _phantom: None
                })
            }

        }

        pub open spec fn read_item_and_list(self, key: K) -> Option<(I, Seq<L>)>
        {
            if self.contents.contains_key(key) {
                Some(self.contents[key])
            } else {
                None
            }
        }

        pub open spec fn read_list_entry_at_index(self, key: K, idx: int) -> Result<L, KvError<K, E>>
        {
            if self.contents.contains_key(key) {
                let (offset, list) = self.contents[key];
                if list.len() > idx {
                    Ok(list[idx])
                } else {
                    Err(KvError::IndexOutOfRange)
                }
            } else {
                Err(KvError::KeyNotFound)
            }
        }

        pub open spec fn update_item(self, key: K, new_item: I) -> Result<Self, KvError<K, E>>
        {
            let val = self.read_item_and_list(key);
            match val {
                Some((old_item, pages)) => {
                    Ok(Self {
                        id: self.id,
                        contents: self.contents.insert(key, (new_item, pages)),
                        _phantom: None
                    })
                }
                None => Err(KvError::KeyNotFound)
            }

        }

        pub open spec fn delete(self, key: K) -> Result<Self, KvError<K, E>>
        {
            if self.contents.contains_key(key) {
                Ok(Self {
                    id: self.id,
                    contents: self.contents.remove(key),
                    _phantom: None
                })
            } else {
                Err(KvError::KeyNotFound)
            }

        }

        pub open spec fn append_to_list(self, key: K, new_list_entry: L) -> Result<Self, KvError<K, E>>
        {
            let result = self.read_item_and_list(key);
            match result {
                Some((item, pages)) => {
                    Ok(Self {
                        id: self.id,
                        contents: self.contents.insert(key, (item, pages.push(new_list_entry))),
                        _phantom: None
                    })
                }
                None => Err(KvError::KeyNotFound)
            }
        }

        pub open spec fn append_to_list_and_update_item(self, key: K, new_list_entry: L, new_item: I) -> Result<Self, KvError<K, E>>
        {
            let result = self.read_item_and_list(key);
            match result {
                Some((item, pages)) => {
                    Ok(Self {
                        id: self.id,
                        contents: self.contents.insert(key, (new_item, pages.push(new_list_entry))),
                        _phantom: None
                    })
                }
                None => Err(KvError::KeyNotFound)
            }
        }

        pub open spec fn update_list_entry_at_index(self, key: K, idx: usize, new_list_entry: L) -> Result<Self, KvError<K, E>>
        {
            let result = self.read_item_and_list(key);
            match result {
                Some((item, pages)) => {
                    let pages = pages.update(idx as int, new_list_entry);
                    Ok(Self {
                        id: self.id,
                        contents: self.contents.insert(key, (item, pages)),
                        _phantom: None
                    })
                }
                None => Err(KvError::KeyNotFound)
            }
        }

        pub open spec fn update_entry_at_index_and_item(self, key: K, idx: usize, new_list_entry: L, new_item: I) -> Result<Self, KvError<K, E>>
        {
            let result = self.read_item_and_list(key);
            match result {
                Some((item, pages)) => {
                    let pages = pages.update(idx as int, new_list_entry);
                    Ok(Self {
                        id: self.id,
                        contents: self.contents.insert(key, (new_item, pages)),
                        _phantom: None
                    })
                }
                None => Err(KvError::KeyNotFound)
            }
        }

        pub open spec fn trim_list(self, key: K, trim_length: int) -> Result<Self, KvError<K, E>>
        {
            let result = self.read_item_and_list(key);
            match result {
                Some((item, pages)) => {
                    let pages = pages.subrange(trim_length, pages.len() as int);
                    Ok(Self {
                        id: self.id,
                        contents: self.contents.insert(key, (item, pages)),
                        _phantom: None
                    })
                }
                None => Err(KvError::KeyNotFound)
            }
        }

        pub open spec fn trim_list_and_update_item(self, key: K, trim_length: int, new_item: I) -> Result<Self, KvError<K, E>>
        {
            let result = self.read_item_and_list(key);
            match result {
                Some((item, pages)) => {
                    let pages = pages.subrange(trim_length, pages.len() as int);
                    Ok(Self {
                        id: self.id,
                        contents: self.contents.insert(key, (new_item, pages)),
                        _phantom: None
                    })
                }
                None => Err(KvError::KeyNotFound)
            }
        }

        pub open spec fn get_keys(self) -> Set<K>
        {
            self.contents.dom()
        }
    }

}

================
File: ./storage_node/src/lib.rs
================

#![feature(maybe_uninit_as_bytes)]
#![feature(maybe_uninit_slice)]
#![feature(maybe_uninit_write_slice)]
#![feature(new_uninit)]
#![allow(unused_imports)]

use builtin::*;
use builtin_macros::*;
use vstd::pervasive::runtime_assert;
use vstd::prelude::*;

pub mod kv;
pub mod log;
pub mod multilog;
pub mod pmem;

use crate::log::logimpl_t::*;
use crate::multilog::layout_v::*;
use crate::multilog::multilogimpl_t::*;
use crate::multilog::multilogimpl_v::*;
use crate::multilog::multilogspec_t::*;
#[cfg(target_os = "linux")]
use crate::pmem::linux_pmemfile_t::*;
#[cfg(target_os = "windows")]
use crate::pmem::windows_pmemfile_t::*;
use crate::pmem::pmemmock_t::*;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmemutil_v::*;

mod tests {

use super::*;
/// This test ensures that the hardcoded constant size of each metadata structure
/// matches the actual size at runtime. This helps ensure that the serde specification
/// for each structure is correct.
// #[verifier::external_body]
#[test]
fn check_layout() {
    let global_metadata_size =
        core::mem::size_of::<crate::multilog::layout_v::GlobalMetadata>();
    let region_metadata_size =
        core::mem::size_of::<crate::multilog::layout_v::RegionMetadata>();
    let log_metadata_size = core::mem::size_of::<crate::multilog::layout_v::LogMetadata>();

    println!("global metadata struct size: {:?}\n", global_metadata_size);
    println!("region metadata struct size: {:?}\n", region_metadata_size);
    println!("log metadata struct size: {:?}\n", log_metadata_size);

    assert!(global_metadata_size == LENGTH_OF_GLOBAL_METADATA.try_into().unwrap());
    assert!(region_metadata_size == LENGTH_OF_REGION_METADATA.try_into().unwrap());
    assert!(log_metadata_size == LENGTH_OF_LOG_METADATA.try_into().unwrap());
}

#[test]
fn check_multilog_in_volatile_memory() {
    assert!(test_multilog_in_volatile_memory());
}
    
}

verus! {
// this function is defined outside of the test module so that we can both
// run verification on it and call it in a test to ensure that all operations
// succeed
#[allow(dead_code, unused_variables, unused_mut)]
fn test_multilog_in_volatile_memory() -> bool {
    // set up vectors to mock persistent memory
    let mut region_sizes = Vec::<u64>::new();
    region_sizes.push(512);
    region_sizes.push(512);
    let mut regions = VolatileMemoryMockingPersistentMemoryRegions::new(region_sizes.as_slice());

    let result = MultiLogImpl::setup(&mut regions);
    let (_log_capacities, multilog_id) = match result {
        Ok((log_capacities, multilog_id)) => (log_capacities, multilog_id),
        Err(_) => return false,
    };

    // start the log
    let result = MultiLogImpl::start(regions, multilog_id);
    let mut multilog = match result {
        Ok(multilog) => multilog,
        Err(_) => return false,
    };

    let mut vec = Vec::new();
    vec.push(1); vec.push(2); vec.push(3);

    let result1 = multilog.tentatively_append(0, vec.as_slice());
    let result2 = multilog.tentatively_append(1, vec.as_slice());
    match (result1, result2) {
        (Ok(_), Ok(_)) => {},
        _=> return false,
    }

    let result = multilog.commit();
    match result {
        Ok(_) => {},
        _ => return false,
    }

    let result = multilog.advance_head(0, 2);
    match result {
        Ok(_) => {}
        _ => return false
    }

    true
}

fn test_multilog_on_memory_mapped_file() -> Option<()>
{
    // To test the multilog, we use files in the current directory that mock persistent-memory
    // regions. Here we use such regions, one of size 4096 and one of size 1024.
    let mut region_sizes: Vec<u64> = Vec::<u64>::new();
    region_sizes.push(4096);
    region_sizes.push(1024);

    // Create the multipersistent memory out of the two regions.
    let file_name = "test_multilog";
    #[cfg(target_os = "windows")]
    let mut pm_regions = FileBackedPersistentMemoryRegions::new(
        &file_name,
        MemoryMappedFileMediaType::SSD,
        region_sizes.as_slice(),
        FileCloseBehavior::TestingSoDeleteOnClose
    ).ok()?;
    #[cfg(target_os = "linux")]
    let mut pm_regions = FileBackedPersistentMemoryRegions::new(
        &file_name,
        region_sizes.as_slice(),
        PersistentMemoryCheck::DontCheckForPersistentMemory,
    ).ok()?;

    // Set up the memory regions to contain a multilog. The capacities will be less
    // than 4096 and 1024 because a few bytes are needed in each region for metadata.
    let (capacities, multilog_id) = MultiLogImpl::setup(&mut pm_regions).ok()?;
    runtime_assert(capacities.len() == 2);
    runtime_assert(capacities[0] <= 4096);
    runtime_assert(capacities[1] <= 1024);

    // Start accessing the multilog.
    let mut multilog = MultiLogImpl::start(pm_regions, multilog_id).ok()?;

    // Tentatively append [30, 42, 100] to log #0 of the multilog.
    let mut v: Vec<u8> = Vec::<u8>::new();
    v.push(30); v.push(42); v.push(100);
    let pos = multilog.tentatively_append(0, v.as_slice()).ok()?;
    runtime_assert(pos == 0);

    // Note that a tentative append doesn't actually advance the tail. That
    // doesn't happen until the next commit.
    let (head, tail, _capacity) = multilog.get_head_tail_and_capacity(0).ok()?;
    runtime_assert(head == 0);
    runtime_assert(tail == 0);

    // Also tentatively append [30, 42, 100, 152] to log #1. This still doesn't
    // commit anything to the log.
    v.push(152);
    let pos = multilog.tentatively_append(1, v.as_slice()).ok()?;
    runtime_assert(pos == 0);

    // Now commit the tentative appends. This causes log #0 to have tail 3
    // and log #1 to have tail 4.
    if multilog.commit().is_err() {
        runtime_assert(false); // can't fail
    }
    match multilog.get_head_tail_and_capacity(0) {
        Ok((head, tail, _capacity)) => {
            runtime_assert(head == 0);
            runtime_assert(tail == 3);
        },
        _ => runtime_assert(false) // can't fail
    }
    match multilog.get_head_tail_and_capacity(1) {
        Ok((head, tail, _capacity)) => {
            runtime_assert(head == 0);
            runtime_assert(tail == 4);
        },
        _ => runtime_assert(false) // can't fail
    }

    // We read the 2 bytes starting at position 1 of log #0. We should
    // read bytes [42, 100]. This is only guaranteed if the memory
    // wasn't corrupted.
    if let Ok(bytes) = multilog.read(0, 1, 2) {
        runtime_assert(bytes.len() == 2);
        assert(multilog.constants().impervious_to_corruption ==> bytes[0] == 42);
    }

    // We now advance the head of log #0 to position 2. This causes the
    // head to become 2 and the tail stays at 3.
    match multilog.advance_head(0, 2) {
        Ok(()) => runtime_assert(true),
        _ => runtime_assert(false) // can't fail
    }
    match multilog.get_head_tail_and_capacity(0) {
        Ok((head, tail, _capacity)) => {
            runtime_assert(head == 2);
            runtime_assert(tail == 3);
        },
        _ => runtime_assert(false) // can't fail
    }

    // If we read from position 2 of log #0, we get the same thing we
    // would have gotten before the advance-head operation.
    if let Ok(_bytes) = multilog.read(0, 2, 1) {
        assert(multilog.constants().impervious_to_corruption ==> _bytes[0] == 100);
    }

    // But if we try to read from position 0 of log #0, we get an
    // error because we're not allowed to read from before the head.
    match multilog.read(0, 0, 1) {
        Err(MultiLogErr::CantReadBeforeHead{head}) => runtime_assert(head == 2),
        _ => runtime_assert(false) // can't succeed, and can't fail with any other error
    }
    Some(())
}

fn test_log_on_memory_mapped_file() -> Option<()>
{
    let region_size = 1024;

    // Create the memory out of a single file.
    let file_name = "test_log";
    #[cfg(target_os = "windows")]
    let mut pm_region = FileBackedPersistentMemoryRegion::new(
        &file_name, MemoryMappedFileMediaType::SSD,
        region_size,
        FileCloseBehavior::TestingSoDeleteOnClose
    ).ok()?;
    #[cfg(target_os = "linux")]
    let mut pm_region = FileBackedPersistentMemoryRegion::new(
        &file_name,
        region_size,
        PersistentMemoryCheck::DontCheckForPersistentMemory,
    ).ok()?;

    // Set up the memory region to contain a log. The capacity will be less than
    // the file size because a few bytes are needed for metadata.
    let (capacity, log_id) = LogImpl::setup(&mut pm_region).ok()?;
    runtime_assert(capacity <= 1024);

    // Start accessing the log.
    let mut log = LogImpl::start(pm_region, log_id).ok()?;

    // Tentatively append [30, 42, 100] to the log.
    let mut v: Vec<u8> = Vec::<u8>::new();
    v.push(30); v.push(42); v.push(100);
    let pos = log.tentatively_append(v.as_slice()).ok()?;
    runtime_assert(pos == 0);

    // Note that a tentative append doesn't actually advance the tail. That
    // doesn't happen until the next commit.
    let (head, tail, _capacity) = log.get_head_tail_and_capacity().ok()?;
    runtime_assert(head == 0);
    runtime_assert(tail == 0);

    // Now commit the tentative appends. This causes the log to have tail 3.
    if log.commit().is_err() {
        runtime_assert(false); // can't fail
    }
    match log.get_head_tail_and_capacity() {
        Ok((head, tail, _capacity)) => {
            runtime_assert(head == 0);
            runtime_assert(tail == 3);
        },
        _ => runtime_assert(false) // can't fail
    }

    // We read the 2 bytes starting at position 1 of the log. We should
    // read bytes [42, 100]. This is only guaranteed if the memory
    // wasn't corrupted.
    if let Ok(bytes) = log.read(1, 2) {
        runtime_assert(bytes.len() == 2);
        assert(log.constants().impervious_to_corruption ==> bytes[0] == 42);
    }

    // We now advance the head of the log to position 2. This causes the
    // head to become 2 and the tail stays at 3.
    match log.advance_head(2) {
        Ok(()) => runtime_assert(true),
        _ => runtime_assert(false) // can't fail
    }
    match log.get_head_tail_and_capacity() {
        Ok((head, tail, capacity)) => {
            runtime_assert(head == 2);
            runtime_assert(tail == 3);
        },
        _ => runtime_assert(false) // can't fail
    }

    // If we read from position 2 of the log, we get the same thing we
    // would have gotten before the advance-head operation.
    if let Ok(bytes) = log.read(2, 1) {
        assert(log.constants().impervious_to_corruption ==> bytes[0] == 100);
    }

    // But if we try to read from position 0, we get an
    // error because we're not allowed to read from before the head.
    match log.read(0, 1) {
        Err(LogErr::CantReadBeforeHead{head}) => runtime_assert(head == 2),
        Err(LogErr::PmemErr { err: PmemError::AccessOutOfRange }) => {}
        _ => runtime_assert(false) // can't succeed, and can't fail with any other error
    }
    Some(())
}

#[allow(dead_code)]
fn main()
{
    test_multilog_in_volatile_memory();
    test_multilog_on_memory_mapped_file();
    test_log_on_memory_mapped_file();
}
}

================
File: ./storage_node/src/verify.sh
================

#!/bin/bash

VERUS_PATH=/home/$USER/verus/source/target-verus/release/verus

clear
# The --compile flag is necessary to run checks like the compile-time assertions on size and alignment calculations.
$VERUS_PATH lib.rs --compile --expand-errors -L dependency=../../deps_hack/target/debug/deps --extern=deps_hack=../../deps_hack/target/debug/libdeps_hack.rlib $@
================
File: ./storage_node/src/pmem/windows_pmemfile_t.rs
================

//! This file contains the trusted implementation for
//! `FileBackedPersistentMemoryRegions`, a collection of persistent
//! memory regions backed by files. It implements trait
//! `PersistentMemoryRegions`.

use builtin::*;
use builtin_macros::*;
use crate::pmem::pmemspec_t::{
    copy_from_slice, maybe_corrupted, PersistentMemoryByte, PersistentMemoryConstants, PersistentMemoryRegion,
    PersistentMemoryRegionView, PersistentMemoryRegions, PersistentMemoryRegionsView,
    PmemError,
};
use crate::pmem::pmcopy_t::*;
use deps_hack::rand::Rng;
use deps_hack::winapi::ctypes::c_void;
use deps_hack::winapi::shared::winerror::SUCCEEDED;
use deps_hack::winapi::um::errhandlingapi::GetLastError;
use deps_hack::winapi::um::fileapi::{CreateFileA, CREATE_NEW, DeleteFileA, OPEN_EXISTING};
use deps_hack::winapi::um::handleapi::{CloseHandle, INVALID_HANDLE_VALUE};
use deps_hack::winapi::um::memoryapi::{FILE_MAP_ALL_ACCESS, FlushViewOfFile, MapViewOfFile, UnmapViewOfFile};
use deps_hack::winapi::um::winbase::CreateFileMappingA;
use deps_hack::winapi::um::winnt::{
    FILE_ATTRIBUTE_NORMAL, FILE_ATTRIBUTE_TEMPORARY, FILE_SHARE_DELETE, FILE_SHARE_READ,
    FILE_SHARE_WRITE, GENERIC_READ, GENERIC_WRITE, HANDLE, PAGE_READWRITE, ULARGE_INTEGER,
};
use std::cell::RefCell;
use std::convert::*;
use std::ffi::CString;
use std::rc::Rc;
use std::slice;
use vstd::prelude::*;

#[cfg(target_arch = "x86_64")]
use core::arch::x86_64::_mm_clflush;
#[cfg(target_arch = "x86_64")]
use core::arch::x86_64::_mm_sfence;
    
// The `MemoryMappedFile` struct represents a memory-mapped file.

pub struct MemoryMappedFile {
    media_type: MemoryMappedFileMediaType,  // type of media on which the file is stored
    size: usize,                            // number of bytes in the file
    h_file: HANDLE,                         // handle to the file
    h_map_file: HANDLE,                     // handle to the mapping
    h_map_addr: HANDLE,                     // address of the first byte of the mapping
    num_bytes_sectioned: usize,             // how many bytes allocated to `MemoryMappedFileSection`s
}

impl MemoryMappedFile {
    // The function `from_file` memory-maps a file and returns a
    // `MemoryMappedFile` to represent it.

    fn from_file(path: &str, size: usize, media_type: MemoryMappedFileMediaType,
                 open_behavior: FileOpenBehavior, close_behavior: FileCloseBehavior)
                 -> Result<Self, PmemError>
    {
        unsafe {
            // Since str in rust is not null terminated, we need to convert it to a null-terminated string.
            let path_cstr = match std::ffi::CString::new(path) {
                Ok(p) => p,
                Err(_) => {
                    eprintln!("Could not convert path {} to string", path);
                    return Err(PmemError::InvalidFileName);
                }
            };

            // Windows can only create files with size < 2^64 so we need to convert `size` to a `u64`.
            let size_as_u64: u64 =
                match size.try_into() {
                    Ok(sz) => sz,
                    Err(_) => {
                        eprintln!("Could not convert size {} into u64", size);
                        return Err(PmemError::CannotOpenPmFile);
                    }
                };

            let create_or_open = match open_behavior {
                FileOpenBehavior::CreateNew => CREATE_NEW,
                FileOpenBehavior::OpenExisting => OPEN_EXISTING,
            };
            let attributes = match close_behavior {
                FileCloseBehavior::TestingSoDeleteOnClose => FILE_ATTRIBUTE_TEMPORARY,
                FileCloseBehavior::Persistent => FILE_ATTRIBUTE_NORMAL,
            };

            // Open or create the file with `CreateFileA`.
            let h_file = CreateFileA(
                path_cstr.as_ptr(),
                GENERIC_READ | GENERIC_WRITE,
                FILE_SHARE_WRITE | FILE_SHARE_READ | FILE_SHARE_DELETE,
                core::ptr::null_mut(),
                create_or_open,
                attributes,
                core::ptr::null_mut()
            );

            if h_file.is_null() || h_file == INVALID_HANDLE_VALUE {
                let error_code = GetLastError();
                match open_behavior {
                    FileOpenBehavior::CreateNew =>
                        eprintln!("Could not create new file {}. err={}", path, error_code),
                    FileOpenBehavior::OpenExisting =>
                        eprintln!("Could not open existing file {}. err={}", path, error_code),
                };
                return Err(PmemError::CannotOpenPmFile);
            }

            let mut li: ULARGE_INTEGER = std::mem::zeroed();
            *li.QuadPart_mut() = size_as_u64;

            // Create a file mapping object backed by the file
            let h_map_file = CreateFileMappingA(
                h_file,
                core::ptr::null_mut(),
                PAGE_READWRITE,
                li.u().HighPart,
                li.u().LowPart,
                core::ptr::null_mut()
            );

            if h_map_file.is_null() {
                eprintln!("Could not create file mapping object for {}.", path);
                return Err(PmemError::CannotOpenPmFile);
            }

            // Map a view of the file mapping into the address space of the process
            let h_map_addr = MapViewOfFile(
                h_map_file,
                FILE_MAP_ALL_ACCESS,
                0,
                0,
                size,
            );

            if h_map_addr.is_null() {
                let err = GetLastError();
                eprintln!("Could not map view of file, got error {}", err);
                return Err(PmemError::CannotOpenPmFile);
            }

            if let FileCloseBehavior::TestingSoDeleteOnClose = close_behavior {
                // After opening the file, mark it for deletion when the file is closed.
                // Obviously, we should only do this during testing!
                DeleteFileA(path_cstr.as_ptr());
            }

            let mmf = MemoryMappedFile {
                media_type,
                size,
                h_file,
                h_map_file,
                h_map_addr,
                num_bytes_sectioned: 0,
            };
            Ok(mmf)
        }
    }
}

impl Drop for MemoryMappedFile {
    fn drop(&mut self)
    {
        unsafe {
            UnmapViewOfFile(self.h_map_addr);
            CloseHandle(self.h_map_file);
            CloseHandle(self.h_file);
        }
    }
}

// The `MemoryMappedFileSection` struct represents a section of a memory-mapped file.
// It contains a reference to the `MemoryMappedFile` it's a section of so that the
// `MemoryMappedFile` isn't dropped until this `MemoryMappedFileSection1 is dropped.

#[verifier::external_body]
pub struct MemoryMappedFileSection {
    mmf: Rc<RefCell<MemoryMappedFile>>,     // the memory-mapped file this is a section of
    media_type: MemoryMappedFileMediaType,  // type of media on which the file is stored
    size: usize,                            // number of bytes in the section
    h_map_addr: HANDLE,                     // address of the first byte of the section
}

impl MemoryMappedFileSection {
    fn new(mmf: Rc<RefCell<MemoryMappedFile>>, len: usize) -> Result<Self, PmemError>
    {
        let mut mmf_borrowed = mmf.borrow_mut();
        let offset = mmf_borrowed.num_bytes_sectioned;
        let offset_as_isize: isize = match offset.try_into() {
            Ok(off) => off,
            Err(_) => {
                eprintln!("Can't express offset {} as isize", offset);
                return Err(PmemError::AccessOutOfRange)
            },
        };

        if offset + len > mmf_borrowed.size {
            eprintln!("Can't allocate {} bytes because only {} remain", len, mmf_borrowed.size - offset);
            return Err(PmemError::AccessOutOfRange);
        }
        
        let h_map_addr = unsafe { (mmf_borrowed.h_map_addr as *mut u8).offset(offset_as_isize) };

        mmf_borrowed.num_bytes_sectioned += len;
        let media_type = mmf_borrowed.media_type.clone();

        std::mem::drop(mmf_borrowed);
        
        let section = Self {
            mmf,
            media_type,
            size: len,
            h_map_addr: h_map_addr as HANDLE,
        };
        Ok(section)
    }

    // The function `flush` flushes updated parts of the
    // memory-mapped file back to the media.

    fn flush(&mut self) {
        unsafe {
            match self.media_type {
                MemoryMappedFileMediaType::BatteryBackedDRAM => {
                    // If using battery-backed DRAM, there's no need
                    // to flush cache lines, since those will be
                    // flushed during the battery-enabled graceful
                    // shutdown after power loss.
                    _mm_sfence();
                },
                _ => {
                    let hr = FlushViewOfFile(
                        self.h_map_addr as *const c_void,
                        self.size
                    );

                    if !SUCCEEDED(hr) {
                        panic!("Failed to flush view of file. err={}", hr);
                    }
                },
            }
        }
    }
}

verus! {

// The `MemoryMappedFileMediaType` enum represents a type of media
// from which a file can be memory-mapped.

#[derive(Clone)]
pub enum MemoryMappedFileMediaType {
    HDD,
    SSD,
    BatteryBackedDRAM,
}

#[derive(Clone, Copy)]
pub enum FileOpenBehavior {
    CreateNew,
    OpenExisting,
}

#[derive(Clone, Copy)]
pub enum FileCloseBehavior {
    TestingSoDeleteOnClose,
    Persistent,
}

// The `FileBackedPersistentMemoryRegion` struct represents a
// persistent-memory region backed by a memory-mapped file.

#[allow(dead_code)]
pub struct FileBackedPersistentMemoryRegion
{
    section: MemoryMappedFileSection,
}

impl FileBackedPersistentMemoryRegion
{
    #[verifier::external_body]
    fn new_internal(path: &str, media_type: MemoryMappedFileMediaType, region_size: u64,
                    open_behavior: FileOpenBehavior, close_behavior: FileCloseBehavior)
                    -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(region) => region.inv() && region@.len() == region_size,
                Err(_) => true,
            }
    {
        let mmf = MemoryMappedFile::from_file(
            path,
            region_size as usize,
            media_type,
            open_behavior,
            close_behavior
        )?;
        let mmf =
            Rc::<RefCell<MemoryMappedFile>>::new(RefCell::<MemoryMappedFile>::new(mmf));
        let section = MemoryMappedFileSection::new(mmf, region_size as usize)?;
        Ok(Self { section })
    }

    pub fn new(path: &str, media_type: MemoryMappedFileMediaType, region_size: u64,
               close_behavior: FileCloseBehavior) -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(region) => region.inv() && region@.len() == region_size,
                Err(_) => true,
            }
    {
        Self::new_internal(path, media_type, region_size, FileOpenBehavior::CreateNew, close_behavior)
    }

    pub fn restore(path: &str, media_type: MemoryMappedFileMediaType, region_size: u64)
               -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(region) => region.inv() && region@.len() == region_size,
                Err(_) => true,
            }
    {
        Self::new_internal(path, media_type, region_size, FileOpenBehavior::OpenExisting, FileCloseBehavior::Persistent)
    }

    #[verifier::external_body]
    fn new_from_section(section: MemoryMappedFileSection) -> (result: Self)
    {
        Self{ section }
    }

    #[verifier::external_body]
    fn get_slice_at_offset(&self, addr: u64, len: u64) -> (result: Result<&[u8], PmemError>)
        requires 
            0 <= addr <= addr + len <= self@.len()
        ensures 
            match result {
                Ok(slice) => if self.constants().impervious_to_corruption {
                    slice@ == self@.committed().subrange(addr as int, addr + len)
                } else {
                    let addrs = Seq::new(len as nat, |i: int| addr + i);
                    maybe_corrupted(slice@, self@.committed().subrange(addr as int, addr + len), addrs)
                }
                _ => false
            }
    {
        // SAFETY: The `offset` method is safe as long as both the start
        // and resulting pointer are in bounds and the computed offset does
        // not overflow `isize`. The precondition ensures that addr + len are 
        // in bounds, and when we set up the region we ensured that 
        // in-bounds accesses cannot overflow isize.
        let addr_on_pm: *const u8 = unsafe {
            (self.section.h_map_addr as *const u8).offset(addr.try_into().unwrap())
        };

        // SAFETY: The precondition establishes that num_bytes bytes
        // from addr_on_pmem are valid bytes on PM. The bytes will not 
        // be modified during this call since the system is single threaded.
        let pm_slice: &[u8] = unsafe {
            core::slice::from_raw_parts(addr_on_pm, len as usize)
        };

        Ok(pm_slice)
    }
}

impl PersistentMemoryRegion for FileBackedPersistentMemoryRegion
{
    closed spec fn view(&self) -> PersistentMemoryRegionView;
    closed spec fn inv(&self) -> bool;
    closed spec fn constants(&self) -> PersistentMemoryConstants;

    #[verifier::external_body]
    fn get_region_size(&self) -> u64
    {
        self.section.size as u64
    }

    fn read_aligned<S>(&self, addr: u64, Ghost(true_val): Ghost<S>) -> (bytes: Result<MaybeCorruptedBytes<S>, PmemError>)
        where
            S: PmCopy 
    {
        let pm_slice = self.get_slice_at_offset(addr, S::size_of() as u64)?;
        let ghost addrs = Seq::new(S::spec_size_of() as nat, |i: int| addr + i);
        let mut maybe_corrupted_val = MaybeCorruptedBytes::new();

        maybe_corrupted_val.copy_from_slice(pm_slice, Ghost(true_val), Ghost(addrs),
                                            Ghost(self.constants().impervious_to_corruption));
        
        Ok(maybe_corrupted_val)
    }

    #[verifier::external_body]
    fn read_unaligned(&self, addr: u64, num_bytes: u64) -> (bytes: Result<Vec<u8>, PmemError>)
    {
        let pm_slice = self.get_slice_at_offset(addr, num_bytes)?;

        // Allocate an unaligned buffer to copy the bytes into
        let unaligned_buffer = copy_from_slice(pm_slice);

        Ok(unaligned_buffer)
    }

    #[verifier::external_body]
    fn write(&mut self, addr: u64, bytes: &[u8])
    {
        let addr_on_pm: *mut u8 = unsafe {
            (self.section.h_map_addr as *mut u8).offset(addr.try_into().unwrap())
        };
        let slice: &mut [u8] = unsafe { core::slice::from_raw_parts_mut(addr_on_pm, bytes.len()) };
        slice.copy_from_slice(bytes)
    }

    #[verifier::external_body]
    #[allow(unused_variables)]
    fn serialize_and_write<S>(&mut self, addr: u64, to_write: &S)
        where
            S: PmCopy + Sized
    {
        let num_bytes: usize = S::size_of() as usize;

        // SAFETY: The `offset` method is safe as long as both the start
        // and resulting pointer are in bounds and the computed offset does
        // not overflow `isize`. `addr` and `num_bytes` are unsigned and
        // the precondition requires that `addr + num_bytes` is in bounds.
        // The precondition does not technically prevent overflowing `isize`
        // but the value is large enough (assuming a 64-bit architecture)
        // that we will not violate this restriction in practice.
        // TODO: put it in the precondition anyway
        let addr_on_pm: *mut u8 = unsafe {
            (self.section.h_map_addr as *mut u8).offset(addr.try_into().unwrap())
        };

        // convert the given &S to a pointer, then a slice of bytes
        let s_pointer = to_write as *const S as *const u8;

        unsafe {
            std::ptr::copy_nonoverlapping(s_pointer, addr_on_pm, num_bytes);
        }
    }

    #[verifier::external_body]
    fn flush(&mut self)
    {
        self.section.flush();
    }
}

// The `FileBackedPersistentMemoryRegions` struct contains a
// vector of volatile memory regions. It implements the trait
// `PersistentMemoryRegions` so that it can be used by a multilog.

pub struct FileBackedPersistentMemoryRegions
{
    media_type: MemoryMappedFileMediaType,           // common media file type used
    regions: Vec<FileBackedPersistentMemoryRegion>,  // all regions
}

impl FileBackedPersistentMemoryRegions {
    #[verifier::external_body]
    fn new_internal(path: &str, media_type: MemoryMappedFileMediaType, region_sizes: &[u64],
                    open_behavior: FileOpenBehavior, close_behavior: FileCloseBehavior)
                    -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(regions) => {
                    &&& regions.inv()
                    &&& regions@.no_outstanding_writes()
                    &&& regions@.len() == region_sizes@.len()
                    &&& forall |i| 0 <= i < region_sizes@.len() ==> #[trigger] regions@[i].len() == region_sizes@[i]
                },
                Err(_) => true
            }
    {
        let mut total_size: usize = 0;
        for &region_size in region_sizes {
            let region_size = region_size as usize;
            if region_size >= usize::MAX - total_size {
                eprintln!("Cannot allocate {} bytes because, combined with the {} allocated so far, it would exceed usize::MAX", region_size, total_size);
                return Err(PmemError::AccessOutOfRange);
            }
            total_size += region_size;
        }
        let mmf = MemoryMappedFile::from_file(
            path,
            total_size,
            media_type.clone(),
            open_behavior,
            close_behavior
        )?;
        let mmf =
            Rc::<RefCell<MemoryMappedFile>>::new(RefCell::<MemoryMappedFile>::new(mmf));
        let mut regions = Vec::<FileBackedPersistentMemoryRegion>::new();
        for &region_size in region_sizes {
            let region_size: usize = region_size as usize;
            let section = MemoryMappedFileSection::new(mmf.clone(), region_size)?;
            let region = FileBackedPersistentMemoryRegion::new_from_section(section);
            regions.push(region);
        }
        Ok(Self { media_type, regions })
    }

    // The static function `new` creates a
    // `FileBackedPersistentMemoryRegions` object by creating a file
    // and dividing it into memory-mapped sections.
    //
    // `path` -- the path to use for the file
    //
    // `media_type` -- the type of media the path refers to
    //
    // `region_sizes` -- a vector of region sizes, where
    // `region_sizes[i]` is the length of file `log<i>`
    //
    // `close_behavior` -- what to do when the file is closed
    pub fn new(path: &str, media_type: MemoryMappedFileMediaType, region_sizes: &[u64],
               close_behavior: FileCloseBehavior)
               -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(regions) => {
                    &&& regions.inv()
                    &&& regions@.no_outstanding_writes()
                    &&& regions@.len() == region_sizes@.len()
                    &&& forall |i| 0 <= i < region_sizes@.len() ==> #[trigger] regions@[i].len() == region_sizes@[i]
                },
                Err(_) => true
            }
    {
        Self::new_internal(path, media_type, region_sizes, FileOpenBehavior::CreateNew, close_behavior)
    }

    // The static function `restore` creates a
    // `FileBackedPersistentMemoryRegions` object by opening an
    // existing file and dividing it into memory-mapped sections.
    //
    // `path` -- the path to use for the file
    //
    // `media_type` -- the type of media the path refers to
    //
    // `region_sizes` -- a vector of region sizes, where
    // `region_sizes[i]` is the length of file `log<i>`
    pub fn restore(path: &str, media_type: MemoryMappedFileMediaType, region_sizes: &[u64])
                   -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(regions) => {
                    &&& regions.inv()
                    &&& regions@.no_outstanding_writes()
                    &&& regions@.len() == region_sizes@.len()
                    &&& forall |i| 0 <= i < region_sizes@.len() ==> #[trigger] regions@[i].len() == region_sizes@[i]
                },
                Err(_) => true
            }
    {
        Self::new_internal(
            path, media_type, region_sizes, FileOpenBehavior::OpenExisting, FileCloseBehavior::Persistent
        )
    }
}

impl PersistentMemoryRegions for FileBackedPersistentMemoryRegions {
    closed spec fn view(&self) -> PersistentMemoryRegionsView;
    closed spec fn inv(&self) -> bool;
    closed spec fn constants(&self) -> PersistentMemoryConstants;

    #[verifier::external_body]
    fn get_num_regions(&self) -> usize
    {
        self.regions.len()
    }

    #[verifier::external_body]
    fn get_region_size(&self, index: usize) -> u64
    {
        self.regions[index].get_region_size()
    }


    #[verifier::external_body]
    fn read_aligned<S>(&self, index: usize, addr: u64, Ghost(true_val): Ghost<S>) -> (bytes: Result<MaybeCorruptedBytes<S>, PmemError>)
        where
            S: PmCopy
    {
        self.regions[index].read_aligned::<S>(addr, Ghost(true_val))
    }

    #[verifier::external_body]
    fn read_unaligned(&self, index: usize, addr: u64, num_bytes: u64) -> (bytes: Result<Vec<u8>, PmemError>)
    {
        self.regions[index].read_unaligned(addr, num_bytes)
    }

    #[verifier::external_body]
    fn write(&mut self, index: usize, addr: u64, bytes: &[u8])
    {
        self.regions[index].write(addr, bytes)
    }

    #[verifier::external_body]
    fn serialize_and_write<S>(&mut self, index: usize, addr: u64, to_write: &S)
        where
            S: PmCopy + Sized
    {
        self.regions[index].serialize_and_write(addr, to_write);
    }

    #[verifier::external_body]
    fn flush(&mut self)
    {
        match self.media_type {
            MemoryMappedFileMediaType::BatteryBackedDRAM => {
                // If using battery-backed DRAM, a single sfence
                // instruction will fence all of memory, so
                // there's no need to iterate through all the
                // regions. Also, there's no need to flush cache
                // lines, since those will be flushed during the
                // battery-enabled graceful shutdown after power
                // loss.
                unsafe {
                    core::arch::x86_64::_mm_sfence();
                }
            },
            _ => {
                for region in &mut self.regions {
                    region.flush();
                }
            },
        }
    }
}

}

================
File: ./storage_node/src/pmem/crc_t.rs
================

use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::bytes::*;
use vstd::prelude::*;

use deps_hack::crc64fast::Digest;

verus! {

    // Helper struct to hide the external crc64fast Digest type
    // from Verus while still keeping ghost state associated with it
    #[verifier::external_body]
    struct ExternalDigest
    {
        digest: Digest
    }

    // A structure for obtaining CRCs of multiple PmCopy objects
    // and writing proofs about them.
    pub struct CrcDigest
    {
        digest: ExternalDigest,
        bytes_in_digest: Ghost<Seq<Seq<u8>>>,
    }

    impl CrcDigest
    {
        pub closed spec fn bytes_in_digest(self) -> Seq<Seq<u8>>
        {
            self.bytes_in_digest@
        }

        #[verifier::external_body]
        pub fn new() -> (output: Self)
            ensures
                output.bytes_in_digest() == Seq::<Seq<u8>>::empty()
        {
            Self {
                digest: ExternalDigest{ digest: Digest::new() },
                bytes_in_digest: Ghost(Seq::<Seq<u8>>::empty())
            }
        }

        #[verifier::external_body]
        pub fn write<S>(&mut self, val: &S)
            where
                S: PmCopy,
            ensures
                self.bytes_in_digest() == old(self).bytes_in_digest().push(val.spec_to_bytes())
        {
            // Cast `val` to bytes, then add them to the digest.
            // The crc64fast crate that we use computes the CRC iteratively and does
            // not store copies of the bytes we write to the digest, so this
            // will (hopefully) not incur any copying beyond what is directly
            // necessary to compute the CRC.
            let num_bytes: usize = S::size_of().try_into().unwrap();
            let s_pointer = val as *const S;
            let bytes_pointer = s_pointer as *const u8;
            // SAFETY: `bytes_pointer` always points to `num_bytes` consecutive, initialized
            // bytes because it was obtained by casting a regular Rust object reference
            // to a raw pointer.
            let bytes = unsafe {
                std::slice::from_raw_parts(bytes_pointer, num_bytes)
            };
            self.digest.digest.write(bytes);
        }

        #[verifier::external_body]
        pub fn write_bytes(&mut self, val: &[u8])
            ensures 
                self.bytes_in_digest() == old(self).bytes_in_digest().push(val@)
        {
            self.digest.digest.write(val);
        }

        // Compute and return the CRC for all bytes in the digest.
        #[verifier::external_body]
        pub fn sum64(&self) -> (output: u64)
            requires
                self.bytes_in_digest().len() != 0
            ensures
                ({
                    let all_bytes_seq = self.bytes_in_digest().flatten();
                    &&& output == spec_crc_u64(all_bytes_seq)
                    &&& output.spec_to_bytes() == spec_crc_bytes(all_bytes_seq)
                })

        {
            self.digest.digest.sum64()
        }
    }
}
================
File: ./storage_node/src/pmem/traits_t.rs
================

//! This file defines external and unsafe traits PmSized 
//! and PmSafe that are used to prove that accesses to PM
//! are safe. Aside from the hardcoded unsafe implementations
//! in this file, these traits should *ONLY* be implemented
//! via derive macros, which are defined in the pmsafe crate.
//! 
//! Both reading and writing to PM are potentially dangerous
//! operations. It is not crash-safe to write a structure with
//! references to external resources (e.g., references, raw pointers,
//! file descriptors), as these resources may be lost in a crash,
//! causing a dangling reference upon recovery. The PmSafe marker
//! trait establishes which types are safe to write to PM and can
//! only be derived for safe types.
//! 
//! To read safely from PM, we need to know the runtime size and
//! alignment of the structure we are reading so that we can 
//! eventually cast the read bytes to a more useful type without
//! risking UB. The PmSized and ConstPmSized traits provide 
//! methods to calculate the runtime size and alignment alongside
//! the SpecPmSized trait (defined in pmem/pmcopy_t.rs) and check
//! that the calculated size is correct, which helps us ensure 
//! that proofs use the correct size for structures. 

use builtin_macros::*;
use builtin::*;
use vstd::prelude::*;
use deps_hack::PmSafe;

use super::pmcopy_t::SpecPmSized;

// The `PmSafe` marker trait indicates whether a structure is safe to 
// write to PM. This trait should only implemented via its derive macro
// for user-defined structures.
pub unsafe trait PmSafe {}

// Numeric types and arrays are all always PmSafe.
// The PmSafe derive macro can derive PmSafe for any structure
// that contains only PmSafe types.
unsafe impl PmSafe for u8 {}
unsafe impl PmSafe for u16 {}
unsafe impl PmSafe for u32 {}
unsafe impl PmSafe for u64 {}
unsafe impl PmSafe for u128 {}
unsafe impl PmSafe for usize {}
unsafe impl PmSafe for i8 {}
unsafe impl PmSafe for i16 {}
unsafe impl PmSafe for i32 {}
unsafe impl PmSafe for i64 {}
unsafe impl PmSafe for i128 {}
unsafe impl PmSafe for isize {}
unsafe impl<T: PmSafe, const N: usize> PmSafe for [T; N] {}

// These types are PmSafe because they do not contain references to external
// resources. Note, however, that extra care must be taken when *reading* them,
// as attempting to cast invalid bytes to one of these types will cause UB.
// Any type that we wish to read from PM must implement the PmCopy trait,
// defined in pmem/pmcopy_t.rs, which provides additional conditions and methods
// for making sure such reads are safe.
unsafe impl PmSafe for bool {}
unsafe impl PmSafe for char {}
unsafe impl PmSafe for f32 {} 
unsafe impl PmSafe for f64 {}

verus! {
    #[verifier::external_trait_specification]
    pub trait ExPmSafe {
        type ExternalTraitSpecificationFor: PmSafe;
    }

    #[verifier::external_trait_specification]
    pub trait ExPmSized : SpecPmSized {
        type ExternalTraitSpecificationFor: PmSized;

        fn size_of() -> (out: usize)
            ensures 
                out as int == Self::spec_size_of();
        fn align_of() -> (out: usize)
            ensures 
                out as int == Self::spec_align_of();
    }

    #[verifier::external_trait_specification]
    pub trait ExUnsafeSpecPmSized {
        type ExternalTraitSpecificationFor: UnsafeSpecPmSized;
    }

    // The specifications of these methods in ExPmSized are 
    // not useable in verified code; use these verified wrappers
    // instead to obtain the runtime size and alignment of a type.
    pub fn size_of<S: PmSized>() -> (out: usize)
        ensures 
            out as int == S::spec_size_of()
    {
        S::size_of()
    }

    pub fn align_of<S: PmSized>() -> (out: usize)
        ensures 
            out as int == S::spec_align_of()
    {
        S::align_of()
    }
}

// The unsafe trait PmSized provides non-const exec methods that return the size and alignment
// of a type as calculated by the PmSize derive macro. This trait is visible to Verus via 
// an external trait specification, which axiomatizes that the size and alignment given by these 
// methods match that which is given by the spec functions. Due to limitations in Verus and Rust,
// we can't make implementations of this trait or its methods constant. We use the trait 
// ConstPmSized below, which is not visible to Verus, to obtain constant size and alignment values,
// which are checked at compile time and should be returned by the methods of this trait.
//
// Ideally, this would be a constant trait defined within Verus, with verified methods. This is 
// not currently possible due to limitations in Verus, so we have to use this workaround.
pub unsafe trait PmSized : SpecPmSized {
    fn size_of() -> usize;
    fn align_of() -> usize;
}

// ConstPmSized's associated constants store the size and alignment of an implementing 
// type as calculated by the PmSized derive macro. This trait is not visible to Verus,
// since Verus does not currently support associated constants. The size_of and align_of
// methods in PmSized, which ARE visible to Verus but are external-body, return 
// these associated constants.
pub unsafe trait ConstPmSized {
    const SIZE: usize;
    const ALIGN: usize;
}

// This unsafe marker trait is a supertrait of SpecPmSized to ensure that
// types cannot safely provide their own implementations of SpecPmSized. 
// This is a workaround for the fact that Verus does not support unsafe traits;
// only externally-defined traits can be unsafe.
pub unsafe trait UnsafeSpecPmSized {}
================
File: ./storage_node/src/pmem/linux_pmemfile_t.rs
================

use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use core::ffi::c_void;
use core::slice;
use std::{cell::RefCell, convert::TryInto, ffi::CString, rc::Rc};

use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use deps_hack::{
    pmem::pmem_memcpy_nodrain_helper, pmem_drain, pmem_errormsg, pmem_flush, pmem_map_file,
    pmem_memcpy_nodrain, pmem_unmap, rand::Rng, PMEM_FILE_CREATE, PMEM_FILE_EXCL,
};

pub struct MemoryMappedFile {
    virt_addr: *mut u8,
    size: usize,
    num_bytes_sectioned: usize,
}

impl Drop for MemoryMappedFile
{
    fn drop(&mut self)
    {
        unsafe { pmem_unmap(self.virt_addr as *mut c_void, self.size) };
    }
}

impl MemoryMappedFile
{
    // TODO: detailed information for error returns
    fn from_file<'a>(file_to_map: &str, size: usize, file_open_behavior: FileOpenBehavior,
                     persistent_memory_check: PersistentMemoryCheck) -> Result<Self, PmemError>
    {
        let mut mapped_len = 0;
        let mut is_pm = 0;
        let file = CString::new(file_to_map).map_err(|_| PmemError::InvalidFileName )?;
        let file = file.as_c_str();

        let require_pm = match persistent_memory_check {
            PersistentMemoryCheck::CheckForPersistentMemory => true,
            PersistentMemoryCheck::DontCheckForPersistentMemory => false,
        };
        let create_flags = match file_open_behavior {
            FileOpenBehavior::CreateNew => PMEM_FILE_CREATE | PMEM_FILE_EXCL,
            FileOpenBehavior::OpenExisting => 0,
        };

        let addr = unsafe {
            pmem_map_file(
                file.as_ptr(),
                size,
                create_flags.try_into().unwrap(),
                0666,
                &mut mapped_len,
                &mut is_pm,
            )
        };

        if addr.is_null() {
            eprintln!("{}", unsafe {
                CString::from_raw(pmem_errormsg() as *mut i8)
                    .into_string()
                    .unwrap()
            });
            Err(PmemError::CannotOpenPmFile)
        } else if is_pm == 0 && require_pm {
            eprintln!("{}", unsafe {
                CString::from_raw(pmem_errormsg() as *mut i8)
                    .into_string()
                    .unwrap()
            });
            Err(PmemError::NotPm)
        } else if addr as isize >= isize::MAX + mapped_len as isize {
            // By checking that no access to bytes between 0 and the length of the PM region
            // will overflow isize, we can assume later that all accesses are valid and will 
            // not violate the safety conditions of methods like raw ptr offset on the virtual 
            // address.
            Err(PmemError::AccessOutOfRange)
        } else {
            Ok(Self {
                virt_addr: addr as *mut u8,
                size: mapped_len.try_into().unwrap(),
                num_bytes_sectioned: 0,
            })
        }
    }
}

#[verifier::external_body]
pub struct MemoryMappedFileSection {
    mmf: Rc<RefCell<MemoryMappedFile>>,
    virt_addr: *mut u8,
    size: usize,
}

impl MemoryMappedFileSection
{
    fn new(mmf: Rc<RefCell<MemoryMappedFile>>, len: usize) -> Result<Self, PmemError>
    {
        let mut mmf_borrowed = mmf.borrow_mut();
        let offset = mmf_borrowed.num_bytes_sectioned;
        let offset_as_isize: isize = match offset.try_into() {
            Ok(off) => off,
            Err(_) => {
                eprintln!("Can't express offset {} as isize", offset);
                return Err(PmemError::AccessOutOfRange)
            },
        };

        if offset + len > mmf_borrowed.size {
            eprintln!("Can't allocate {} bytes because only {} remain", len, mmf_borrowed.size - offset);
            return Err(PmemError::AccessOutOfRange);
        }

        mmf_borrowed.num_bytes_sectioned += len;
        let new_virt_addr = unsafe { mmf_borrowed.virt_addr.offset(offset_as_isize) };

        std::mem::drop(mmf_borrowed);

        let section = Self {
            mmf,
            virt_addr: new_virt_addr,
            size: len,
        };
        Ok(section)
    }
}

verus! {

#[derive(Clone, Copy)]
pub enum FileOpenBehavior {
    CreateNew,
    OpenExisting,
}

#[derive(Clone, Copy)]
pub enum PersistentMemoryCheck {
    CheckForPersistentMemory,
    DontCheckForPersistentMemory,
}

pub struct FileBackedPersistentMemoryRegion
{
    section: MemoryMappedFileSection,
}

impl FileBackedPersistentMemoryRegion
{
    #[verifier::external_body]
    fn new_internal(path: &str, region_size: u64, open_behavior: FileOpenBehavior,
                    persistent_memory_check: PersistentMemoryCheck)
                    -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(region) => region.inv() && region@.len() == region_size,
                Err(_) => true,
            }
    {
        let mmf = MemoryMappedFile::from_file(
            path,
            region_size as usize,
            open_behavior,
            persistent_memory_check,
        )?;
        let mmf = Rc::<RefCell<MemoryMappedFile>>::new(RefCell::<MemoryMappedFile>::new(mmf));
        let section = MemoryMappedFileSection::new(mmf, region_size as usize)?;
        Ok(Self { section })
    }

    pub fn new(path: &str, region_size: u64, persistent_memory_check: PersistentMemoryCheck)
               -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(region) => region.inv() && region@.len() == region_size,
                Err(_) => true,
            }
    {
        Self::new_internal(path, region_size, FileOpenBehavior::CreateNew, persistent_memory_check)
    }

    pub fn restore(path: &str, region_size: u64) -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(region) => region.inv() && region@.len() == region_size,
                Err(_) => true,
            }
    {
        Self::new_internal(path, region_size, FileOpenBehavior::OpenExisting,
                           PersistentMemoryCheck::DontCheckForPersistentMemory)
    }

    #[verifier::external_body]
    fn new_from_section(section: MemoryMappedFileSection) -> (result: Self)
    {
        Self{ section }
    }

    #[verifier::external_body]
    fn get_slice_at_offset(&self, addr: u64, len: u64) -> (result: Result<&[u8], PmemError>)
        requires 
            0 <= addr <= addr + len <= self@.len()
        ensures 
            match result {
                Ok(slice) => if self.constants().impervious_to_corruption {
                    slice@ == self@.committed().subrange(addr as int, addr + len)
                } else {
                    let addrs = Seq::new(len as nat, |i: int| addr + i);
                    maybe_corrupted(slice@, self@.committed().subrange(addr as int, addr + len), addrs)
                }
                _ => false
            }
    {
        let raw_addr = addr as isize;

        // SAFETY: The `offset` method is safe as long as both the start
        // and resulting pointer are in bounds and the computed offset does
        // not overflow `isize`. The precondition ensures that addr + len are 
        // in bounds, and when we set up the region we ensured that 
        // in-bounds accesses cannot overflow isize.
        let addr_on_pm: *const u8 = unsafe {
            self.section.virt_addr.offset(raw_addr)
        };

        // SAFETY: The precondition establishes that num_bytes bytes
        // from addr_on_pmem are valid bytes on PM. The bytes will not 
        // be modified during this call since the system is single threaded.
        let pm_slice: &[u8] = unsafe {
            std::slice::from_raw_parts(addr_on_pm, len as usize)
        };

        Ok(pm_slice)
    }
}

impl PersistentMemoryRegion for FileBackedPersistentMemoryRegion
{
    closed spec fn view(&self) -> PersistentMemoryRegionView;

    closed spec fn inv(&self) -> bool;

    closed spec fn constants(&self) -> PersistentMemoryConstants;

    #[verifier::external_body]
    fn get_region_size(&self) -> u64
    {
        self.section.size as u64
    }

    fn read_aligned<S>(&self, addr: u64, Ghost(true_val): Ghost<S>) -> (bytes: Result<MaybeCorruptedBytes<S>, PmemError>)
        where
            S: PmCopy 
    {
        let pm_slice = self.get_slice_at_offset(addr, S::size_of() as u64)?;
        let ghost addrs = Seq::new(S::spec_size_of() as nat, |i: int| addr + i);
        let mut maybe_corrupted_val = MaybeCorruptedBytes::new();

        maybe_corrupted_val.copy_from_slice(pm_slice, Ghost(true_val), Ghost(addrs), Ghost(self.constants().impervious_to_corruption));
        
        Ok(maybe_corrupted_val)
    }

    fn read_unaligned(&self, addr: u64, num_bytes: u64) -> (bytes: Result<Vec<u8>, PmemError>)
    {
        let pm_slice = self.get_slice_at_offset(addr, num_bytes)?;

        // Allocate an unaligned buffer to copy the bytes into
        let unaligned_buffer = copy_from_slice(pm_slice);

        Ok(unaligned_buffer)
    }

    #[verifier::external_body]
    fn write(&mut self, addr: u64, bytes: &[u8])
    {
        // SAFETY: The `offset` method is safe as long as both the start
        // and resulting pointer are in bounds and the computed offset does
        // not overflow `isize`. `addr` and `num_bytes` are unsigned and
        // the precondition requires that `addr + num_bytes` is in bounds.
        // The precondition does not technically prevent overflowing `isize`
        // but the value is large enough (assuming a 64-bit architecture)
        // that we will not violate this restriction in practice.
        // TODO: put it in the precondition anyway
        let addr_on_pm: *mut u8 = unsafe {
            self.section.virt_addr.offset(addr.try_into().unwrap())
        };

        // pmem_memcpy_nodrain() does a memcpy to PM with no cache line flushes or
        // ordering; it makes no guarantees about durability. pmem_flush() does cache
        // line flushes but does not use an ordering primitive, so updates are still
        // not guaranteed to be durable yet.
        // Verus doesn't like calling pmem_memcpy_nodrain directly because it returns
        // a raw pointer, so we define a wrapper around pmem_memcpy_nodrain in deps_hack
        // that does not return anything and call that instead
        unsafe {
            pmem_memcpy_nodrain_helper(
                addr_on_pm as *mut c_void,
                bytes.as_ptr() as *const c_void,
                bytes.len()
            );
        }
    }

    #[verifier::external_body]
    #[allow(unused_variables)]
    fn serialize_and_write<S>(&mut self, addr: u64, to_write: &S)
        where
            S: PmCopy + Sized
    {
        let num_bytes: usize = S::size_of() as usize;

        // SAFETY: The `offset` method is safe as long as both the start
        // and resulting pointer are in bounds and the computed offset does
        // not overflow `isize`. `addr` and `num_bytes` are unsigned and
        // the precondition requires that `addr + num_bytes` is in bounds.
        // The precondition does not technically prevent overflowing `isize`
        // but the value is large enough (assuming a 64-bit architecture)
        // that we will not violate this restriction in practice.
        // TODO: put it in the precondition anyway
        let addr_on_pm: *mut u8 = unsafe {
            self.section.virt_addr.offset(addr.try_into().unwrap())
        };

        // convert the given &S to a pointer, then a slice of bytes
        let s_pointer = to_write as *const S as *const u8;

        // pmem_memcpy_nodrain() does a memcpy to PM with no cache line flushes or
        // ordering; it makes no guarantees about durability. pmem_flush() does cache
        // line flushes but does not use an ordering primitive, so updates are still
        // not guaranteed to be durable yet.
        // Verus doesn't like calling pmem_memcpy_nodrain directly because it returns
        // a raw pointer, so we define a wrapper around pmem_memcpy_nodrain in deps_hack
        // that does not return anything and call that instead
        unsafe {
            pmem_memcpy_nodrain_helper(
                addr_on_pm as *mut c_void,
                s_pointer as *const c_void,
                num_bytes
            );
        }
    }

    #[verifier::external_body]
    fn flush(&mut self)
    {
        // `pmem_drain()` invokes an ordering primitive to drain store buffers and
        // ensure that all cache lines that were flushed since the previous ordering
        // primitive are durable. This guarantees that all updates made with `write`/
        // `serialize_and_write` since the last `flush` call will be durable before
        // any new updates become durable.
        unsafe { pmem_drain(); }
    }
}

pub struct FileBackedPersistentMemoryRegions {
    regions: Vec<FileBackedPersistentMemoryRegion>,
}

impl FileBackedPersistentMemoryRegions {
    // TODO: detailed information for error returns
    #[verifier::external_body]
    #[allow(dead_code)]
    pub fn new_internal(path: &str, region_sizes: &[u64], open_behavior: FileOpenBehavior,
                        persistent_memory_check: PersistentMemoryCheck) -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(regions) => {
                    &&& regions.inv()
                    &&& regions@.no_outstanding_writes()
                    &&& regions@.len() == region_sizes@.len()
                    &&& forall |i| 0 <= i < regions@.len() ==> #[trigger] regions@[i].len() == region_sizes@[i]
                },
                Err(_) => true,
            }
    {
        let mut total_size: usize = 0;
        for &region_size in region_sizes {
            let region_size = region_size as usize;
            if region_size >= usize::MAX - total_size {
                return Err(PmemError::AccessOutOfRange);
            }
            total_size += region_size;
        }
        let mmf = MemoryMappedFile::from_file(
            path,
            total_size,
            open_behavior,
            persistent_memory_check,
        )?;
        let mmf = Rc::<RefCell<MemoryMappedFile>>::new(RefCell::<MemoryMappedFile>::new(mmf));
        let mut regions = Vec::<FileBackedPersistentMemoryRegion>::new();
        for &region_size in region_sizes {
            let region_size: usize = region_size as usize;
            let section = MemoryMappedFileSection::new(mmf.clone(), region_size)?;
            let region = FileBackedPersistentMemoryRegion::new_from_section(section);
            regions.push(region);
        }
        Ok(Self { regions })
    }
    
    pub fn new(path: &str, region_sizes: &[u64], persistent_memory_check: PersistentMemoryCheck)
               -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(regions) => {
                    &&& regions.inv()
                    &&& regions@.no_outstanding_writes()
                    &&& regions@.len() == region_sizes@.len()
                    &&& forall |i| 0 <= i < regions@.len() ==> #[trigger] regions@[i].len() == region_sizes@[i]
                },
                Err(_) => true,
            }
    {
        Self::new_internal(path, region_sizes, FileOpenBehavior::CreateNew, persistent_memory_check)
    }
    
    pub fn restore(path: &str, region_sizes: &[u64], persistent_memory_check: PersistentMemoryCheck)
                   -> (result: Result<Self, PmemError>)
        ensures
            match result {
                Ok(regions) => {
                    &&& regions.inv()
                    &&& regions@.no_outstanding_writes()
                    &&& regions@.len() == region_sizes@.len()
                    &&& forall |i| 0 <= i < regions@.len() ==> #[trigger] regions@[i].len() == region_sizes@[i]
                },
                Err(_) => true,
            }
    {
        Self::new_internal(path, region_sizes, FileOpenBehavior::OpenExisting, persistent_memory_check)
    }
}

impl PersistentMemoryRegions for FileBackedPersistentMemoryRegions {
    closed spec fn view(&self) -> PersistentMemoryRegionsView;
    closed spec fn inv(&self) -> bool;
    closed spec fn constants(&self) -> PersistentMemoryConstants;

    #[verifier::external_body]
    fn get_num_regions(&self) -> usize
    {
        self.regions.len()
    }

    #[verifier::external_body]
    fn get_region_size(&self, index: usize) -> u64
    {
        self.regions[index].get_region_size()
    }

    #[verifier::external_body]
    fn read_aligned<S>(&self, index: usize, addr: u64, Ghost(true_val): Ghost<S>) -> (bytes: Result<MaybeCorruptedBytes<S>, PmemError>)
        where
            S: PmCopy
    {
        self.regions[index].read_aligned::<S>(addr, Ghost(true_val))
    }

    #[verifier::external_body]
    fn read_unaligned(&self, index: usize, addr: u64, num_bytes: u64) -> (bytes: Result<Vec<u8>, PmemError>)
    {
        self.regions[index].read_unaligned(addr, num_bytes)
    }

    #[verifier::external_body]
    fn write(&mut self, index: usize, addr: u64, bytes: &[u8])
    {
        self.regions[index].write(addr, bytes)
    }

    #[verifier::external_body]
    fn serialize_and_write<S>(&mut self, index: usize, addr: u64, to_write: &S)
        where
            S: PmCopy + Sized
    {
        self.regions[index].serialize_and_write(addr, to_write);
    }

    #[verifier::external_body]
    fn flush(&mut self)
    {
        unsafe { pmem_drain(); }
    }
}

}
================
File: ./storage_node/src/pmem/mod.rs
================

#[cfg(target_os = "linux")]
pub mod linux_pmemfile_t;
#[cfg(target_os = "windows")]
pub mod windows_pmemfile_t;
pub mod pmemmock_t;
pub mod pmemspec_t;
pub mod pmemutil_v;
pub mod pmcopy_t;
pub mod subregion_v;
pub mod wrpm_t;
pub mod crc_t;
pub mod traits_t;
================
File: ./storage_node/src/pmem/pmemspec_t.rs
================

//! This file contains the trusted specification for how a collection
//! of persistent memory regions (implementing trait
//! `PersistentMemoryRegions`) behaves.
//!
//! One of the things it models is what can happen to a persistent
//! memory region if the system crashes in the middle of a write.
//! Specifically, it says that on a crash some subset of the
//! outstanding byte writes will be flushed (performed before the
//! crash) and the rest of the outstanding byte writes will be
//! discarded. Furthermore, any 8-byte-aligned 8-byte chunk either has
//! all its outstanding writes flushed or all of them discarded.
//!
//! To obviate the need to model what happens when there are multiple
//! outstanding writes to the same byte, the specification says that
//! writes are only allowed to bytes that have no outstanding writes.
//! To obviate the need to model what happens when a byte with an
//! outstanding write is read, the specification says that reads (like
//! writes) are only allowed to access bytes that have no outstanding
//! writes.
//!
//! Another thing this file models is how bit corruption manifests. It
//! models a collection of persistent memory regions as either
//! impervious to corruption or not so. If a memory is impervious to
//! corruption, then bit corruption never occurs and reads always
//! return the last-written bytes. However, if memory isn't impervious
//! to corruption, then all that's guaranteed is that the bytes that
//! are read and the last-written bytes are related by
//! `maybe_corrupted`.
//!
//! This file also provides axioms allowing possibly-corrupted bytes
//! to be freed of suspicion of corruption. Both axioms require the
//! use of CRCs to detect possible corruption, and model a CRC match
//! as showing evidence of an absence of corruption.

use crate::pmem::pmcopy_t::*;
use builtin::*;
use builtin_macros::*;
use core::fmt::Debug;
use vstd::bytes::*;
use vstd::prelude::*;

use deps_hack::crc64fast::Digest;

verus! {

    // map_err
    #[verifier::external_fn_specification]
    pub fn map_err<T, E, F, O: FnOnce(E) -> F>(result: Result<T, E>, op: O) -> (mapped_result: Result<T, F>)
        requires 
            result.is_err() ==> op.requires((result.get_Err_0(),)), 
        ensures 
            result.is_err() ==> mapped_result.is_err() && op.ensures(
                (result.get_Err_0(),),
                mapped_result.get_Err_0(),
            ),
            result.is_ok() ==> mapped_result == Result::<T, F>::Ok(result.get_Ok_0()),
    {
        result.map_err(op)
    }

    // This function is used to copy bytes from a slice to a newly-allocated vector.
    // `std::slice::copy_from_slice` requires that the source and destination have the
    // same length, so this function allocates a buffer with the correct length, 
    // obtains a mutable reference to the vector as a slice, and copies the 
    // source bytes in. 
    //
    // This must be implemented in an external_body function because Verus does not
    // support the vec! macro and does not support mutable references.
    #[verifier::external_body]
    pub fn copy_from_slice(bytes: &[u8]) -> (out: Vec<u8>)
        ensures 
            out@ == bytes@
    {
        let mut buffer = vec![0; bytes.len()];
        let mut buffer_slice = buffer.as_mut_slice();
        buffer_slice.copy_from_slice(bytes);
        buffer
    }

    #[derive(Debug, Eq, PartialEq, Clone)]
    pub enum PmemError {
        InvalidFileName,
        CannotOpenPmFile,
        NotPm,
        PmdkError,
        AccessOutOfRange,
    }

    /// This is our model of bit corruption. It models corruption of a
    /// read byte sequence as a sequence of corruptions happening to
    /// each byte. This way, we can concatenate two read byte
    /// sequences (say, to do a wrapping read) and consider them to be
    /// analogously corrupted. We don't allow arbitrary concatenation
    /// of bytes to prevent proofs from assembling CRC collisions and
    /// thereby proving `false`. Specifically, we only allow byte
    /// sequences to be put together if they all came from different
    /// addresses.

    // A byte `byte` read from address `addr` is a possible corruption
    // of the actual last-written byte `true_byte` to that address if
    // they're related by `maybe_corrupted_byte`.
    pub closed spec fn maybe_corrupted_byte(byte: u8, true_byte: u8, addr: int) -> bool;

    pub open spec fn all_elements_unique(seq: Seq<int>) -> bool {
        forall |i: int, j: int| 0 <= i < j < seq.len() ==> seq[i] != seq[j]
    }

    // A sequence of bytes `bytes` read from addresses `addrs` is a
    // possible corruption of the actual last-written bytes
    // `true_bytes` to those addresses if those addresses are all
    // distinct and if each corresponding byte pair is related by
    // `maybe_corrupted_byte`.
    pub open spec fn maybe_corrupted(bytes: Seq<u8>, true_bytes: Seq<u8>, addrs: Seq<int>) -> bool {
        &&& bytes.len() == true_bytes.len() == addrs.len()
        &&& forall |i: int| #![auto] 0 <= i < bytes.len() ==> maybe_corrupted_byte(bytes[i], true_bytes[i], addrs[i])
    }

    pub const CRC_SIZE: u64 = 8;

    pub open spec fn spec_crc_bytes(bytes: Seq<u8>) -> Seq<u8> {
        spec_crc_u64(bytes).spec_to_bytes()
    }

    pub closed spec fn spec_crc_u64(bytes: Seq<u8>) -> u64;

    // This executable method can be called to compute the CRC of a
    // sequence of bytes. It uses the `crc` crate.
    #[verifier::external_body]
    pub exec fn bytes_crc(bytes: &[u8]) -> (out: Vec<u8>)
        ensures
            spec_u64_to_le_bytes(spec_crc_u64(bytes@)) == out@,
            out@.len() == CRC_SIZE
    {
        let mut digest = Digest::new();
        digest.write(bytes);
        u64_to_le_bytes(digest.sum64())
    }

    /// We make two assumptions about how CRCs can be used to detect
    /// corruption.

    /// The first assumption, encapsulated in
    /// `axiom_bytes_uncorrupted`, is that if we store byte sequences
    /// `x` and `y` to persistent memory where `y` is the CRC of `x`,
    /// then we can detect an absence of corruption by reading both of
    /// them. Specifically, if we read from those locations and get
    /// `x_c` and `y_c` (corruptions of `x` and `y` respectively), and
    /// `y_c` is the CRC of `x_c`, then we can conclude that `x` wasn't
    /// corrupted, i.e., that `x_c == x`.

    #[verifier(external_body)]
    pub proof fn axiom_bytes_uncorrupted2(x_c: Seq<u8>, x: Seq<u8>, x_addrs: Seq<int>,
                                         y_c: Seq<u8>, y: Seq<u8>, y_addrs: Seq<int>)
        requires
            maybe_corrupted(x_c, x, x_addrs),
            maybe_corrupted(y_c, y, y_addrs),
            y_c == spec_crc_bytes(x_c),
            y == spec_crc_bytes(x),
            all_elements_unique(x_addrs),
            all_elements_unique(y_addrs),
        ensures
            x == x_c
    {}

    #[verifier::external_body]
    #[inline(always)]
    pub exec fn compare_crcs(crc1: &[u8], crc2: u64) -> (out: bool)
        requires 
            crc1@.len() == u64::spec_size_of()
        ensures 
            out ==> crc1@ == crc2.spec_to_bytes(),
            !out ==> crc1@ != crc2.spec_to_bytes()
    {
        let crc1_u64 = u64_from_le_bytes(crc1);
        crc1_u64 == crc2
    }

    /// The second assumption, encapsulated in
    /// `axiom_corruption_detecting_boolean`, is that the values
    /// `CDB_FALSE` and `CDB_TRUE` are so randomly different from each
    /// other that corruption can't make one appear to be the other.
    /// That is, if we know we wrote either `CDB_FALSE` or `CDB_TRUE`
    /// to a certain part of persistent memory, and when we read that
    /// same part we get `CDB_FALSE` or `CDB_TRUE`, we can conclude it
    /// matches what we last wrote to it. To justify the assumption
    /// that `CDB_FALSE` and `CDB_TRUE` are different from each other,
    /// we set them to CRC(b"0") and CRC(b"1"), respectively.

    pub const CDB_FALSE: u64 = 0xa32842d19001605e; // CRC(b"0")
    pub const CDB_TRUE: u64  = 0xab21aa73069531b7; // CRC(b"1")

    pub const CDB_SIZE: u64 = 8;

    #[verifier(external_body)]
    pub proof fn axiom_corruption_detecting_boolean(cdb_c: Seq<u8>, cdb: Seq<u8>, addrs: Seq<int>)
        requires
            maybe_corrupted(cdb_c, cdb, addrs),
            all_elements_unique(addrs),
            cdb.len() == u64::spec_size_of(),
            cdb_c == CDB_FALSE.spec_to_bytes() || cdb_c == CDB_TRUE.spec_to_bytes(),
            cdb == CDB_FALSE.spec_to_bytes() || cdb == CDB_TRUE.spec_to_bytes(),
        ensures
            cdb_c == cdb
    {}

    /// We model the persistent memory as getting flushed in chunks,
    /// where each chunk has `const_persistence_chunk_size()` bytes. We refer
    /// to chunk number `c` as the set of addresses `addr` such that
    /// `addr / const_persistence_chunk_size() == c`.

    pub open spec fn const_persistence_chunk_size() -> int { 8 }

    /// We model the state of each byte of persistent memory as
    /// follows. `state_at_last_flush` contains the contents
    /// immediately after the most recent flush. `outstanding_write`
    /// contains `None` if there's no outstanding write, or `Some(b)`
    /// if there's an outstanding write of `b`. We don't model the
    /// possibility of there being multiple outstanding writes because
    /// we restrict reads and writes to not be allowed at locations
    /// with currently outstanding writes.

    #[verifier::ext_equal]
    pub struct PersistentMemoryByte {
        pub state_at_last_flush: u8,
        pub outstanding_write: Option<u8>,
    }

    impl PersistentMemoryByte {
        pub open spec fn write(self, byte: u8) -> Self
        {
            Self {
                state_at_last_flush: self.state_at_last_flush,
                outstanding_write: Some(byte),
            }
        }

        pub open spec fn flush_byte(self) -> u8
        {
            match self.outstanding_write {
                None => self.state_at_last_flush,
                Some(b) => b
            }
        }

        pub open spec fn flush(self) -> Self
        {
            Self {
                state_at_last_flush: self.flush_byte(),
                outstanding_write: None,
            }
        }
    }

    /// We model the state of a region of persistent memory as a
    /// `PersistentMemoryRegionView`, which is essentially just a sequence
    /// of `PersistentMemoryByte` values.

    #[verifier::ext_equal]
    pub struct PersistentMemoryRegionView
    {
        pub state: Seq<PersistentMemoryByte>,
    }

    impl PersistentMemoryRegionView
    {
        pub open spec fn len(self) -> nat
        {
            self.state.len()
        }

        pub open spec fn write(self, addr: int, bytes: Seq<u8>) -> Self
        {
            Self {
                state: self.state.map(|pos: int, pre_byte: PersistentMemoryByte|
                                         if addr <= pos < addr + bytes.len() { pre_byte.write(bytes[pos - addr]) }
                                         else { pre_byte }),
            }
        }

        pub open spec fn flush(self) -> Self
        {
            Self {
                state: self.state.map(|_addr, b: PersistentMemoryByte| b.flush()),
            }
        }

        pub open spec fn no_outstanding_writes_in_range(self, i: int, j: int) -> bool
        {
            forall |k| i <= k < j ==> (#[trigger] self.state[k].outstanding_write).is_none()
        }

        pub open spec fn no_outstanding_writes(self) -> bool
        {
            Self::no_outstanding_writes_in_range(self, 0, self.state.len() as int)
        }

        pub open spec fn committed(self) -> Seq<u8>
        {
            self.state.map(|_addr, b: PersistentMemoryByte| b.state_at_last_flush)
        }

        // This specification function describes what it means for
        // chunk number `chunk` in `self` to match the corresponding
        // bytes in `bytes` if outstanding writes to those bytes in
        // `self` haven't happened yet.
        pub open spec fn chunk_corresponds_ignoring_outstanding_writes(self, chunk: int, bytes: Seq<u8>) -> bool
        {
            forall |addr: int| {
                &&& 0 <= addr < self.len()
                &&& addr / const_persistence_chunk_size() == chunk
            } ==> #[trigger] bytes[addr] == self.state[addr].state_at_last_flush
        }

        // This specification function describes what it means for
        // chunk number `chunk` in `self` to match the corresponding
        // bytes in `bytes` if outstanding writes to those bytes in
        // `self` have all been performed.
        pub open spec fn chunk_corresponds_after_flush(self, chunk: int, bytes: Seq<u8>) -> bool
        {
            forall |addr: int| {
                &&& 0 <= addr < self.len()
                &&& addr / const_persistence_chunk_size() == chunk
            } ==> #[trigger] bytes[addr] == self.state[addr].flush_byte()
        }

        // This specification function describes whether `self` can
        // crash as a sequence of bytes `bytes`. It can do so if, for
        // each chunk, that chunk either matches the corresponding
        // part of `bytes` ignoring outstanding writes to that chunk
        // or matches it after performing outstanding writes to that
        // chunk. In other words, each byte can be flushed or
        // unflushed, but bytes in the same chunk must always make the
        // same flushed/unflushed choice.
        pub open spec fn can_crash_as(self, bytes: Seq<u8>) -> bool
        {
            &&& bytes.len() == self.len()
            &&& forall |chunk| {
                  ||| self.chunk_corresponds_ignoring_outstanding_writes(chunk, bytes)
                  ||| self.chunk_corresponds_after_flush(chunk, bytes)
              }
        }
    }

    /// We model the state of a sequence of regions of persistent
    /// memory as a `PersistentMemoryRegionsView`, which is essentially
    /// just a sequence of `PersistentMemoryRegionView` values.

    #[verifier::ext_equal]
    pub struct PersistentMemoryRegionsView {
        pub regions: Seq<PersistentMemoryRegionView>,
    }

    impl PersistentMemoryRegionsView {
        pub open spec fn len(self) -> nat
        {
            self.regions.len()
        }

        pub open spec fn spec_index(self, i: int) -> PersistentMemoryRegionView
        {
            self.regions[i]
        }

        pub open spec fn write(self, index: int, addr: int, bytes: Seq<u8>) -> Self
        {
            Self {
                regions: self.regions.map(|pos: int, pre_view: PersistentMemoryRegionView|
                    if pos == index {
                        pre_view.write(addr, bytes)
                    } else {
                        pre_view
                    }
                ),
            }
        }


        pub open spec fn flush(self) -> Self
        {
            Self {
                regions: self.regions.map(|_pos, pm: PersistentMemoryRegionView| pm.flush()),
            }
        }

        pub open spec fn no_outstanding_writes(self) -> bool {
            forall |i: int| #![auto] 0 <= i < self.len() ==> self[i].no_outstanding_writes()
        }

        pub open spec fn no_outstanding_writes_in_range(self, index: int, start: int, end: int) -> bool
        {
            self[index].no_outstanding_writes_in_range(start, end)
        }

        pub open spec fn committed(self) -> Seq<Seq<u8>>
        {
            Seq::<Seq<u8>>::new(self.len(), |i: int| self[i].committed())
        }

        pub open spec fn can_crash_as(self, crash_regions: Seq<Seq<u8>>) -> bool
        {
            &&& crash_regions.len() == self.len()
            &&& forall |i: int| #![auto] 0 <= i < self.len() ==> self[i].can_crash_as(crash_regions[i])
        }
    }

    // The struct `PersistentMemoryConstants` contains fields that
    // remain the same across all operations on persistent memory.

    pub struct PersistentMemoryConstants {
        pub impervious_to_corruption: bool
    }

    pub trait PersistentMemoryRegion : Sized
    {
        spec fn view(&self) -> PersistentMemoryRegionView;

        spec fn inv(&self) -> bool;

        spec fn constants(&self) -> PersistentMemoryConstants;

        fn get_region_size(&self) -> (result: u64)
            requires
                self.inv()
            ensures
                result == self@.len()
        ;

        // This function takes a ghost `true_val` representing the value we originally wrote to this location, rather than 
        // choosing it internally, so that the caller can get more specific information about this structure if they want.
        fn read_aligned<S>(&self, addr: u64, Ghost(true_val): Ghost<S>) -> (bytes: Result<MaybeCorruptedBytes<S>, PmemError>)
            where 
                S: PmCopy + Sized,
            requires
                self.inv(),
                0 <= addr < addr + S::spec_size_of() <= self@.len(),
                self@.no_outstanding_writes_in_range(addr as int, addr + S::spec_size_of()),
                // We must have previously written a serialized S -- specifically, the serialization of `true_val` -- to this addr
                self@.committed().subrange(addr as int, addr + S::spec_size_of()) == true_val.spec_to_bytes(),
            ensures
                match bytes {
                    Ok(bytes) => {
                        let true_bytes = self@.committed().subrange(addr as int, addr + S::spec_size_of());
                        let addrs = Seq::<int>::new(S::spec_size_of() as nat, |i: int| i + addr);
                        // If the persistent memory regions are impervious
                        // to corruption, read returns the last bytes
                        // written. Otherwise, it returns a
                        // possibly-corrupted version of those bytes.
                        if self.constants().impervious_to_corruption {
                            bytes@ == true_bytes
                        }
                        else {
                            maybe_corrupted(bytes@, true_bytes, addrs)
                        }
                    }
                    _ => false
                }
            ;

        fn read_unaligned(&self, addr: u64, num_bytes: u64) -> (bytes: Result<Vec<u8>, PmemError>) 
            requires 
                self.inv(),
                addr + num_bytes <= self@.len(),
                self@.no_outstanding_writes_in_range(addr as int, addr + num_bytes),
            ensures 
                match bytes {
                    Ok(bytes) => {
                        let true_bytes = self@.committed().subrange(addr as int, addr + num_bytes);
                        let addrs = Seq::<int>::new(num_bytes as nat, |i: int| i + addr);
                        &&& // If the persistent memory regions are impervious
                            // to corruption, read returns the last bytes
                            // written. Otherwise, it returns a
                            // possibly-corrupted version of those bytes.
                            if self.constants().impervious_to_corruption {
                                bytes@ == true_bytes
                            }
                            else {
                                maybe_corrupted(bytes@, true_bytes, addrs)
                            }
                        }
                    _ => false
                }
                
        ;

        fn write(&mut self, addr: u64, bytes: &[u8])
            requires
                old(self).inv(),
                addr + bytes@.len() <= old(self)@.len(),
                addr + bytes@.len() <= u64::MAX,
                // Writes aren't allowed where there are already outstanding writes.
                old(self)@.no_outstanding_writes_in_range(addr as int, addr + bytes@.len()),
            ensures
                self.inv(),
                self.constants() == old(self).constants(),
                self@ == old(self)@.write(addr as int, bytes@),
        ;

        fn serialize_and_write<S>(&mut self, addr: u64, to_write: &S)
            where
                S: PmCopy + Sized
            requires
                old(self).inv(),
                addr + S::spec_size_of() <= old(self)@.len(),
                old(self)@.no_outstanding_writes_in_range(addr as int, addr + S::spec_size_of()),
            ensures
                self.inv(),
                self.constants() == old(self).constants(),
                self@ == old(self)@.write(addr as int, to_write.spec_to_bytes()),
                self@.flush().committed().subrange(addr as int, addr + S::spec_size_of()) == to_write.spec_to_bytes(),
        ;


        fn flush(&mut self)
            requires
                old(self).inv()
            ensures
                self.inv(),
                self.constants() == old(self).constants(),
                self@ == old(self)@.flush(),
        ;
    }

    /// The `PersistentMemoryRegions` trait represents an ordered list
    /// of one or more persistent memory regions.

    pub trait PersistentMemoryRegions : Sized
    {
        spec fn view(&self) -> PersistentMemoryRegionsView;

        spec fn inv(&self) -> bool;

        spec fn constants(&self) -> PersistentMemoryConstants;

        fn get_num_regions(&self) -> (result: usize)
            requires
                self.inv()
            ensures
                result == self@.len(),
        ;

        fn get_region_size(&self, index: usize) -> (result: u64)
            requires
                self.inv(),
                index < self@.len()
            ensures
                result == self@[index as int].len(),
        ;

        fn read_aligned<S>(&self, index: usize, addr: u64, Ghost(true_val): Ghost<S>) -> (bytes: Result<MaybeCorruptedBytes<S>, PmemError>)
            where 
                S: PmCopy,
            requires 
                self.inv(),
                index < self@.len(),
                0 <= addr < addr + S::spec_size_of() <= self@[index as int].len(),
                self@.no_outstanding_writes_in_range(index as int, addr as int, addr + S::spec_size_of()),
                // We must have previously written a serialized S -- specifically, the serialization of `true_val` -- to this addr
                self@[index as int].committed().subrange(addr as int, addr + S::spec_size_of()) == true_val.spec_to_bytes(),
            ensures 
                match bytes {
                    Ok(bytes) => {
                        let addrs = Seq::<int>::new(S::spec_size_of() as nat, |i: int| i + addr);
                        &&& // If the persistent memory regions are impervious
                            // to corruption, read returns the last bytes
                            // written. Otherwise, it returns a
                            // possibly-corrupted version of those bytes.
                            if self.constants().impervious_to_corruption {
                                bytes@ == true_val.spec_to_bytes()
                            }
                            else {
                                maybe_corrupted(bytes@, true_val.spec_to_bytes(), addrs)
                            }
                        }
                    _ => false,
                }
        ;

        fn read_unaligned(&self, index: usize, addr: u64, num_bytes: u64) -> (bytes: Result<Vec<u8>, PmemError>)
            requires 
                self.inv(),
                index < self@.len(),
                addr + num_bytes <= self@[index as int].len(),
                self@.no_outstanding_writes_in_range(index as int, addr as int, addr + num_bytes),
            ensures 
            match bytes {
                Ok(bytes) => {
                    let true_bytes = self@[index as int].committed().subrange(addr as int, addr + num_bytes);
                    let addrs = Seq::<int>::new(num_bytes as nat, |i: int| i + addr);
                    &&& // If the persistent memory regions are impervious
                        // to corruption, read returns the last bytes
                        // written. Otherwise, it returns a
                        // possibly-corrupted version of those bytes.
                        if self.constants().impervious_to_corruption {
                            bytes@ == true_bytes
                        }
                        else {
                            maybe_corrupted(bytes@, true_bytes, addrs)
                        }
                    }
                _ => false
            }
        ;

        fn write(&mut self, index: usize, addr: u64, bytes: &[u8])
            requires
                old(self).inv(),
                index < old(self)@.len(),
                addr + bytes@.len() <= old(self)@[index as int].len(),
                // Writes aren't allowed where there are already outstanding writes.
                old(self)@.no_outstanding_writes_in_range(index as int, addr as int, addr + bytes@.len()),
            ensures
                self.inv(),
                self.constants() == old(self).constants(),
                self@ == old(self)@.write(index as int, addr as int, bytes@),
        ;

        // Note that addr is a regular offset in terms of bytes, but to_write is type S
        fn serialize_and_write<S>(&mut self, index: usize, addr: u64, to_write: &S)
            where
                S: PmCopy + Sized,
            requires
                old(self).inv(),
                index < old(self)@.len(),
                addr + S::spec_size_of() <= old(self)@[index as int].len(),
                old(self)@.no_outstanding_writes_in_range(index as int, addr as int, addr + S::spec_size_of()),
            ensures
                self.inv(),
                self.constants() == old(self).constants(),
                self@ == old(self)@.write(index as int, addr as int, to_write.spec_to_bytes()),
        ;

        fn flush(&mut self)
            requires
                old(self).inv(),
            ensures
                self.inv(),
                self.constants() == old(self).constants(),
                self@ == old(self)@.flush(),
        ;
    }
}
================
File: ./storage_node/src/pmem/pmemutil_v.rs
================

//! This file contains lemmas and utility executable functions about
//! persistent memory regions.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::crc_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::bytes::*;
use vstd::prelude::*;

verus! {

    broadcast use pmcopy_axioms;

    // This lemma establishes that if there are no outstanding writes
    // to a particular location in memory, then there's only one
    // possibility for how the byte at that address can crash: it will
    // always crash into its committed state.
    pub proof fn lemma_if_no_outstanding_writes_at_addr_then_persistent_memory_view_can_only_crash_as_committed(
        pm_region_view: PersistentMemoryRegionView,
        addr: int,
    )
        requires
            0 <= addr < pm_region_view.len(),
            pm_region_view.state[addr].outstanding_write.is_none(),
        ensures
            forall |s| pm_region_view.can_crash_as(s) ==> #[trigger] s[addr] == pm_region_view.committed()[addr]
    {
        assert forall |s| pm_region_view.can_crash_as(s) implies #[trigger] s[addr] == pm_region_view.committed()[addr] by {
            let chunk = addr / const_persistence_chunk_size();
            // There are two cases to consider. Each is fairly trivial
            // for Z3, once we cue it to consider the two cases
            // separately with the following `if`.
            if pm_region_view.chunk_corresponds_after_flush(chunk, s) {
            }
            else {
            }
        }
    }

    // This lemma establishes that any persistent memory byte that has
    // no outstanding writes has one possible crash state, which is
    // the same as its committed state.
    pub proof fn lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(
        pm_region_view: PersistentMemoryRegionView,
    )
        ensures
            forall |s, addr: int| {
                &&& pm_region_view.can_crash_as(s)
                &&& 0 <= addr < s.len()
                &&& pm_region_view.state[addr].outstanding_write.is_none()
            } ==> #[trigger] s[addr] == pm_region_view.committed()[addr]
    {
        assert forall |s, addr: int| {
                   &&& pm_region_view.can_crash_as(s)
                   &&& 0 <= addr < s.len()
                   &&& pm_region_view.state[addr].outstanding_write.is_none()
               } implies #[trigger] s[addr] == pm_region_view.committed()[addr] by {
            lemma_if_no_outstanding_writes_at_addr_then_persistent_memory_view_can_only_crash_as_committed(
                pm_region_view, addr);
        }
    }

    // This lemma establishes that if there are no outstanding writes
    // anywhere in a persistent memory region's view, then it can only
    // crash in one state, which is the same as its committed state.
    pub proof fn lemma_if_no_outstanding_writes_then_persistent_memory_view_can_only_crash_as_committed(
        pm_region_view: PersistentMemoryRegionView
    )
        requires
            pm_region_view.no_outstanding_writes()
        ensures
            forall |s| pm_region_view.can_crash_as(s) ==> s == pm_region_view.committed()
    {
        lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(pm_region_view);
        assert (forall |s| pm_region_view.can_crash_as(s) ==> s =~= pm_region_view.committed());
    }

    // This lemma establishes that if there are no outstanding writes
    // anywhere in the view of a collection of persistent memory
    // regions, then the collection can only crash in one state, which
    // is the same as its committed state.
    pub proof fn lemma_if_no_outstanding_writes_then_persistent_memory_regions_view_can_only_crash_as_committed(
        pm_regions_view: PersistentMemoryRegionsView
    )
        requires
            pm_regions_view.no_outstanding_writes()
        ensures
            forall |s| pm_regions_view.can_crash_as(s) ==> s == pm_regions_view.committed()
    {
        assert forall |s| pm_regions_view.can_crash_as(s) implies s == pm_regions_view.committed() by {
            assert forall |i| 0 <= i < pm_regions_view.len() implies s[i] == #[trigger] pm_regions_view[i].committed() by {
                lemma_if_no_outstanding_writes_then_persistent_memory_view_can_only_crash_as_committed(
                    pm_regions_view[i]);
            }
            assert (s =~= pm_regions_view.committed());
        }
    }

    // This lemma establishes that if a persistent memory region has
    // no outstanding writes, then a flush of them does nothing.
    pub proof fn lemma_if_no_outstanding_writes_to_region_then_flush_is_idempotent(
        region_view: PersistentMemoryRegionView,
    )
        requires
            region_view.no_outstanding_writes(),
        ensures
            region_view.flush() == region_view,
    {
        assert(region_view.flush() =~= region_view);
    }

    // This lemma establishes that if a collection of persistent
    // memory regions has no outstanding writes anywhere, then a flush
    // of them does nothing.
    pub proof fn lemma_if_no_outstanding_writes_then_flush_is_idempotent(
        regions_view: PersistentMemoryRegionsView,
    )
        requires
            regions_view.no_outstanding_writes(),
        ensures
            regions_view.flush() == regions_view,
    {
        assert(regions_view.flush().len() == regions_view.len());
        assert forall |i| 0 <= i < regions_view.len() implies
               #[trigger] regions_view.flush().regions[i] == regions_view.regions[i] by {
            assert(regions_view[i].no_outstanding_writes());
            lemma_if_no_outstanding_writes_to_region_then_flush_is_idempotent(regions_view.regions[i]);
        }
        assert(regions_view.flush() =~= regions_view);
    }

    // This is an auto lemma for lemma_if_no_outstanding_writes_then_flush_is_idempotent.
    pub proof fn lemma_auto_if_no_outstanding_writes_then_flush_is_idempotent()
        ensures
            forall |r: PersistentMemoryRegionsView| r.no_outstanding_writes() ==> r.flush() == r
    {
        assert forall |r: PersistentMemoryRegionsView| r.no_outstanding_writes() implies r.flush() == r by {
            lemma_if_no_outstanding_writes_then_flush_is_idempotent(r);
        };
    }

    // This executable function returns a vector containing the sizes
    // of the regions in the given collection of persistent memory
    // regions.
    pub fn get_region_sizes<PMRegions: PersistentMemoryRegions>(pm_regions: &PMRegions) -> (result: Vec<u64>)
        requires
            pm_regions.inv()
        ensures
            result@.len() == pm_regions@.len(),
            forall |i: int| 0 <= i < pm_regions@.len() ==> result@[i] == #[trigger] pm_regions@[i].len()
    {
        let mut result: Vec<u64> = Vec::<u64>::new();
        for which_region in iter: 0..pm_regions.get_num_regions()
            invariant
                iter.end == pm_regions@.len(),
                pm_regions.inv(),
                result@.len() == which_region,
                forall |i: int| 0 <= i < which_region ==> result@[i] == #[trigger] pm_regions@[i].len(),
        {
            result.push(pm_regions.get_region_size(which_region));
        }
        result
    }

    // This executable function checks whether the given CRC read from
    // persistent memory is the actual CRC of the given bytes read
    // from persistent memory. It returns a boolean indicating whether
    // the check succeeds.
    //
    // It guarantees that, if the CRC last written to memory really
    // was the CRC of the data last written to memory, then:
    //
    // 1) If the function returns `true`, then the data read from
    // memory (`data_c`) constitute an uncorrupted copy of the correct
    // bytes last written to memory.
    //
    // 2) If the function returns `false`, then the persistent memory
    // region collection the data and CRC were read from are not
    // impervious to corruption.
    //
    // Parameters:
    //
    // `data_c` -- the possibly corrupted data read from memory
    //
    // `crc_c` -- the possibly corrupted CRC read from memory
    //
    // `mem` (ghost) -- the true contents of the memory that was read from
    //
    // `impervious_to_corruption` (ghost) -- whether that memory is
    // impervious to corruption
    //
    // `data_addr` (ghost) -- where the data were read from in memory
    //
    // `data_length` (ghost) -- the length of the data read from memory
    //
    // `crc_addr` (ghost) -- where the CRC was read from in memory
    pub fn check_crc(
        data_c: &[u8],
        crc_c: &[u8],
        Ghost(mem): Ghost<Seq<u8>>,
        Ghost(impervious_to_corruption): Ghost<bool>,
        Ghost(data_addrs): Ghost<Seq<int>>,
        Ghost(crc_addrs): Ghost<Seq<int>>,
    ) -> (b: bool)
        requires
            data_addrs.len() <= mem.len(),
            crc_addrs.len() <= mem.len(),
            crc_c@.len() == CRC_SIZE,
            all_elements_unique(data_addrs),
            all_elements_unique(crc_addrs),
            ({
                let true_data_bytes = Seq::new(data_addrs.len(), |i: int| mem[data_addrs[i] as int]);
                let true_crc_bytes = Seq::new(crc_addrs.len(), |i: int| mem[crc_addrs[i]]);
                &&& if impervious_to_corruption {
                        &&& data_c@ == true_data_bytes
                        &&& crc_c@ == true_crc_bytes
                    }
                    else {
                        &&& maybe_corrupted(data_c@, true_data_bytes, data_addrs)
                        &&& maybe_corrupted(crc_c@, true_crc_bytes, crc_addrs)
                    }
            })
        ensures
            ({
                let true_data_bytes = Seq::new(data_addrs.len(), |i: int| mem[data_addrs[i] as int]);
                let true_crc_bytes = Seq::new(crc_addrs.len(), |i: int| mem[crc_addrs[i]]);
                &&& true_crc_bytes == spec_crc_bytes(true_data_bytes) ==> {
                    if b {
                        &&& data_c@ == true_data_bytes
                        &&& crc_c@ == true_crc_bytes
                    }
                    else {
                        !impervious_to_corruption
                    }
                }
            })
    {
        // Compute a CRC for the bytes we read.
        let computed_crc = calculate_crc_bytes(data_c);

        // Check whether the CRCs match. This is done in an external body function so that we can convert the maybe-corrupted
        // CRC to a u64 for comparison to the computed CRC.
        let crcs_match = compare_crcs(crc_c, computed_crc);

        proof {
            let true_data_bytes = Seq::new(data_addrs.len(), |i: int| mem[data_addrs[i] as int]);
            let true_crc_bytes = Seq::new(crc_addrs.len(), |i: int| mem[crc_addrs[i]]);

            // We may need to invoke `axiom_bytes_uncorrupted` to justify that since the CRCs match,
            // we can conclude that the data matches as well. That axiom only applies in the case
            // when all three of the following conditions hold: (1) the last-written CRC really is
            // the CRC of the last-written data; (2) the persistent memory regions aren't impervious
            // to corruption; and (3) the CRC read from disk matches the computed CRC. If any of
            // these three is false, we can't invoke `axiom_bytes_uncorrupted`, but that's OK
            // because we don't need it. If #1 is false, then this lemma isn't expected to prove
            // anything. If #2 is false, then no corruption has happened. If #3 is false, then we've
            // detected corruption.
            if {
                &&& true_crc_bytes == spec_crc_bytes(true_data_bytes)
                &&& !impervious_to_corruption
                &&& crcs_match
            } {
                axiom_bytes_uncorrupted2(data_c@, true_data_bytes, data_addrs, crc_c@, true_crc_bytes, crc_addrs);
            }
        }
        crcs_match
    }


    // This function converts the given encoded CDB read from persistent
    // memory into a boolean. It checks for corruption as it does so. It
    // guarantees that if it returns `Some` then there was no corruption,
    // and that if it returns `None` then the memory isn't impervious to
    // corruption.
    //
    // Parameters:
    //
    // `cdb_c` -- the possibly corrupted encoded CDB read from memory
    //
    // `mem` (ghost) -- the true contents of the memory that was read from
    //
    // `impervious_to_corruption` (ghost) -- whether that memory is
    // impervious to corruption
    //
    // `cdb_addr` (ghost) -- where the CDB was read from in memory
    //
    // Return value:
    //
    // `Some(b)` -- the encoded CDB is uncorrupted and encodes the boolean
    // `b`
    //
    // `None` -- corruption was detected, so the persistent memory regions
    // can't be impervious to corruption
    pub fn check_cdb(
        cdb_c: MaybeCorruptedBytes<u64>,
        Ghost(true_cdb): Ghost<u64>,
        Ghost(mem): Ghost<Seq<u8>>,
        Ghost(impervious_to_corruption): Ghost<bool>,
        Ghost(cdb_addrs): Ghost<Seq<int>>,
    ) -> (result: Option<bool>)
        requires
            forall |i: int| 0 <= i < cdb_addrs.len() ==> cdb_addrs[i] <= mem.len(),
            all_elements_unique(cdb_addrs),
            ({
                let true_cdb_bytes = Seq::new(u64::spec_size_of() as nat, |i: int| mem[cdb_addrs[i]]);
                &&& true_cdb.spec_to_bytes() == true_cdb_bytes
                &&& true_cdb == CDB_FALSE || true_cdb == CDB_TRUE
                &&& if impervious_to_corruption { cdb_c@ == true_cdb_bytes }
                        else { maybe_corrupted(cdb_c@, true_cdb_bytes, cdb_addrs) }
            })
        ensures
            ({
                let true_cdb_bytes = Seq::new(u64::spec_size_of() as nat, |i: int| mem[cdb_addrs[i]]);
                let true_cdb = u64::spec_from_bytes(true_cdb_bytes);
                match result {
                    Some(b) => if b { true_cdb == CDB_TRUE }
                               else { true_cdb == CDB_FALSE },
                    None => !impervious_to_corruption,
                }
            })
    {
        let ghost true_cdb_bytes = Seq::new(u64::spec_size_of() as nat, |i: int| mem[cdb_addrs[i]]);
        proof {
            // We may need to invoke the axiom
            // `axiom_corruption_detecting_boolean` to justify concluding
            // that, if we read `CDB_FALSE` or `CDB_TRUE`, it can't have
            // been corrupted.
            if !impervious_to_corruption && (cdb_c@ == CDB_FALSE.spec_to_bytes() || cdb_c@ == CDB_TRUE.spec_to_bytes()) {
                axiom_corruption_detecting_boolean(cdb_c@, true_cdb_bytes, cdb_addrs);
            }  
        }
        
        let cdb_val = cdb_c.extract_cdb(Ghost(true_cdb), Ghost(true_cdb_bytes), Ghost(cdb_addrs), Ghost(impervious_to_corruption));
        assert(cdb_val.spec_to_bytes() == cdb_c@);

        // If the read encoded CDB is one of the expected ones, translate
        // it into a boolean; otherwise, indicate corruption.

        if *cdb_val == CDB_FALSE {
            Some(false)
        }
        else if *cdb_val == CDB_TRUE {
            Some(true)
        }
        else {
            proof {
                // This part of the proof can be flaky -- invoking this axiom helps stabilize it
                // by helping Z3 prove that the real CDB is neither valid value, which implies we are 
                // not impervious to corruption
               axiom_to_from_bytes::<u64>(*cdb_val);
            }
            None
        }
    }

    /// If the only outstanding write is `const_persistence_chunk_size()`-sized and
    /// -aligned, then there are only two possible resulting crash states,
    /// one with the write and one without.

    // This lemma establishes that, if there are no outstanding writes and
    // we write a `const_persistence_chunk_size()`-aligned segment of length
    // `const_persistence_chunk_size()`, then there are only two possible crash
    // states that can happen after the write is initiated. In one of those
    // crash states, nothing has changed; in the other, all the written
    // bytes have been updated according to this write.
    pub proof fn lemma_single_write_crash_effect_on_pm_region_view(
        pm_region_view: PersistentMemoryRegionView,
        write_addr: int,
        bytes_to_write: Seq<u8>,
    )
        requires
            bytes_to_write.len() == const_persistence_chunk_size(),
            write_addr % const_persistence_chunk_size() == 0,
            0 <= write_addr,
            write_addr + const_persistence_chunk_size() <= pm_region_view.len(),
            pm_region_view.no_outstanding_writes()
        ensures
            ({
                let new_pm_region_view = pm_region_view.write(write_addr, bytes_to_write);
                forall |crash_bytes: Seq<u8>| new_pm_region_view.can_crash_as(crash_bytes) ==> {
                    ||| crash_bytes == pm_region_view.committed()
                    ||| crash_bytes == new_pm_region_view.flush().committed()
                }
            })
    {
        assert forall |crash_bytes: Seq<u8>|
                   pm_region_view.write(write_addr, bytes_to_write).can_crash_as(crash_bytes) implies {
                       ||| crash_bytes == pm_region_view.committed()
                       ||| crash_bytes == pm_region_view.write(write_addr, bytes_to_write).flush().committed()
                   } by {
            let chunk = write_addr / const_persistence_chunk_size();
            let new_pm_region_view = pm_region_view.write(write_addr, bytes_to_write);

            // To reason about all the bytes we haven't written, it's useful to invoke
            // the lemma that says that wherever there's no outstanding write, there's
            // only one possible crash state, the committed state.

            lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(new_pm_region_view);

            // Then, we just have to reason about this one written chunk. There are two cases:
            // (1) the chunk isn't flushed at all and (2) the chunk is entirely flushed.

            if new_pm_region_view.chunk_corresponds_ignoring_outstanding_writes(chunk, crash_bytes) {
                assert(crash_bytes =~= pm_region_view.committed());
            }
            if new_pm_region_view.chunk_corresponds_after_flush(chunk, crash_bytes) {
                assert(crash_bytes =~= pm_region_view.write(write_addr, bytes_to_write).flush().committed());
            }
        }
    }

    // This lemma establishes that, if there are no outstanding writes and
    // we write a `const_persistence_chunk_size()`-aligned segment of length
    // `const_persistence_chunk_size()` to a single region among a collection of
    // persistent memory regions, then there are only two possible crash
    // states that can happen after the write is initiated. In one of those
    // crash states, nothing has changed; in the other, all the written
    // bytes have been updated according to this write.
    pub proof fn lemma_single_write_crash_effect_on_pm_regions_view(
        pm_regions_view: PersistentMemoryRegionsView,
        index: int,
        write_addr: int,
        bytes_to_write: Seq<u8>,
    )
        requires
            0 <= index < pm_regions_view.len(),
            bytes_to_write.len() == const_persistence_chunk_size(),
            write_addr % const_persistence_chunk_size() == 0,
            0 <= write_addr,
            write_addr + const_persistence_chunk_size() <= pm_regions_view[index as int].len(),
            pm_regions_view.no_outstanding_writes(),

        ensures
            ({
                let new_pm_regions_view = pm_regions_view.write(index, write_addr, bytes_to_write);
                let flushed_pm_regions_view = new_pm_regions_view.flush();
                forall |crash_bytes: Seq<Seq<u8>>| new_pm_regions_view.can_crash_as(crash_bytes) ==> {
                    ||| crash_bytes == pm_regions_view.committed()
                    ||| crash_bytes == flushed_pm_regions_view.committed()
                }
            })
    {
        let new_pm_regions_view = pm_regions_view.write(index, write_addr, bytes_to_write);
        let flushed_pm_regions_view = new_pm_regions_view.flush();
        assert forall |crash_bytes: Seq<Seq<u8>>| new_pm_regions_view.can_crash_as(crash_bytes) implies {
            ||| crash_bytes == pm_regions_view.committed()
            ||| crash_bytes == flushed_pm_regions_view.committed()
        } by {
            // All other regions other than the written-to one can only
            // crash into their `committed` state, since they have no outstanding writes.
            assert forall |other: int| 0 <= other < pm_regions_view.len() && other != index implies
                       #[trigger] crash_bytes[other] == pm_regions_view[other].committed() by {
                // The following `assert` is required to trigger the `forall`
                // in `PersistentMemoryRegionsView::no_outstanding_writes`:
                assert(pm_regions_view[other].no_outstanding_writes());

                // Knowing that there are no outstanding writes, we can now call the following lemma.
                lemma_if_no_outstanding_writes_then_persistent_memory_view_can_only_crash_as_committed(
                    new_pm_regions_view[other]);
            }

            // This lemma says that there are only two possible states for
            // region number `index` after this write is initiated.
            lemma_single_write_crash_effect_on_pm_region_view(pm_regions_view[index], write_addr, bytes_to_write);

            // We now use extensional equality to show the final result.
            // There are two cases to consider: (1) the write doesn't get
            // flushed; (2) the write gets flushed.

            if crash_bytes[index] == pm_regions_view[index].committed() {
                assert(crash_bytes =~= pm_regions_view.committed());
            }
            if crash_bytes[index] == pm_regions_view[index].write(write_addr, bytes_to_write).flush().committed() {
                let new_regions_view = pm_regions_view.write(index, write_addr, bytes_to_write);
                let flushed_regions_view = new_regions_view.flush();
                assert(forall |any| 0 <= any < pm_regions_view.len() ==> #[trigger] crash_bytes[any] =~=
                    flushed_regions_view.committed()[any]);
                assert(crash_bytes =~= flushed_regions_view.committed());
            }
        }
    }

    // This lemma establishes that if one performs a write and then a
    // flush, then the committed contents reflect that write.
    pub proof fn lemma_write_reflected_after_flush_committed(
        pm_region_view: PersistentMemoryRegionView,
        addr: int,
        bytes: Seq<u8>,
    )
        requires
            0 <= addr,
            addr + bytes.len() <= pm_region_view.len(),
        ensures
            pm_region_view.write(addr, bytes).flush().committed().subrange(addr as int, addr + bytes.len()) == bytes
    {
        // All we need is to get Z3 to consider extensional equality.
        assert(pm_region_view.write(addr, bytes).flush().committed().subrange(addr as int, addr + bytes.len()) =~=
               bytes);
    }

    // Calculates the CRC for a single `PmCopy` object.
    pub fn calculate_crc<S>(val: &S) -> (out: u64)
        where
            S: PmCopy + Sized,
        requires
            // this is true in the default implementation of `spec_crc`, but
            // an impl of `PmCopy` can override the default impl, so
            // we have to require it here
            val.spec_crc() == spec_crc_u64(val.spec_to_bytes())
        ensures
            val.spec_crc() == out,
            spec_crc_u64(val.spec_to_bytes()) == out,
    {
        let mut digest = CrcDigest::new();
        digest.write(val);
        proof {
            lemma_auto_spec_u64_to_from_le_bytes();
            digest.bytes_in_digest().lemma_flatten_one_element();
        }
        digest.sum64()
    }

    pub fn calculate_crc_bytes(val: &[u8]) -> (out: u64) 
        ensures 
            out == spec_crc_u64(val@),
            out.spec_to_bytes() == spec_crc_bytes(val@),
    {
        let mut digest = CrcDigest::new();
        digest.write_bytes(val);
        proof {
            lemma_auto_spec_u64_to_from_le_bytes();
            digest.bytes_in_digest().lemma_flatten_one_element();
        }
        digest.sum64()
    }
}

================
File: ./storage_node/src/pmem/pmemmock_t.rs
================

//! This file contains the trusted implementation for
//! `VolatileMemoryMockingPersistentMemoryRegions`, a collection of
//! volatile memory regions. It serves as a mock of persistent memory
//! regions by implementing trait `PersistentMemoryRegions`.
//!
//! THIS IS ONLY INTENDED FOR USE IN TESTING! In practice, one should
//! use actually persistent memory to implement persistent memory!

use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use builtin::*;
use builtin_macros::*;
use deps_hack::rand::Rng;
use std::convert::*;
use vstd::prelude::*;

verus! {

    // The `VolatileMemoryMockingPersistentMemoryRegion` struct
    // contains a vector of volatile memory to hold the contents, as
    // well as a ghost field that keeps track of the virtual modeled
    // state. This ghost field pretends that outstanding writes remain
    // outstanding even though in the concrete `contents` field we
    // actually overwrite all data in place immediately.
    pub struct VolatileMemoryMockingPersistentMemoryRegion
    {
        contents: Vec<u8>,
    }

    impl VolatileMemoryMockingPersistentMemoryRegion
    {
        #[verifier::external_body]
        fn new(region_size: u64) -> (result: Self)
            ensures
                result.inv(),
                result@.len() == region_size,
        {
            let contents: Vec<u8> = vec![0; region_size as usize];
            Self { contents }
        }
    }

    impl PersistentMemoryRegion for VolatileMemoryMockingPersistentMemoryRegion
    {
        #[verifier::external_body]
        closed spec fn view(&self) -> PersistentMemoryRegionView;

        closed spec fn inv(&self) -> bool
        {
            // We maintain the invariant that our size fits in a `u64`.
            &&& self.contents.len() <= u64::MAX
            &&& self.contents.len() == self@.len()

            // We also maintain the invariant that the contents of our
            // volatile buffer matches the result of flushing the
            // abstract state.
            &&& self.contents@ == self@.flush().committed()
        }

        closed spec fn constants(&self) -> PersistentMemoryConstants;

        fn get_region_size(&self) -> (result: u64)
        {
            self.contents.len() as u64
        }

        #[verifier::external_body]
        fn read_aligned<S>(&self, addr: u64, Ghost(true_val): Ghost<S>) -> (bytes: Result<MaybeCorruptedBytes<S>, PmemError>)
            where 
                S: PmCopy 
        {
            let pm_slice = &self.contents[addr as usize..addr as usize + S::size_of() as usize];
            let ghost addrs = Seq::new(S::spec_size_of() as nat, |i: int| addr + i);

            let mut maybe_corrupted_val = MaybeCorruptedBytes::new();
            maybe_corrupted_val.copy_from_slice(pm_slice, Ghost(true_val), Ghost(addrs), Ghost(self.constants().impervious_to_corruption));

            Ok(maybe_corrupted_val)
        }

        #[verifier::external_body]
        fn read_unaligned(&self, addr: u64, num_bytes: u64) -> (bytes: Result<Vec<u8>, PmemError>)
        {
            let pm_slice = &self.contents[addr as usize..addr as usize + num_bytes as usize];
            let unaligned_buffer = copy_from_slice(pm_slice);
            Ok(unaligned_buffer)
        }

        #[verifier::external_body]
        fn write(&mut self, addr: u64, bytes: &[u8])
        {
            let addr_usize: usize = addr.try_into().unwrap();
            self.contents.splice(addr_usize..addr_usize+bytes.len(), bytes.iter().cloned());
        }

        #[verifier::external_body]
        fn serialize_and_write<S>(&mut self, addr: u64, to_write: &S)
            where
                S: PmCopy + Sized
        {
            let addr_usize: usize = addr.try_into().unwrap();
            let num_bytes: usize = S::size_of().try_into().unwrap();
            let s_pointer = to_write as *const S;
            let bytes_pointer = s_pointer as *const u8;
            // SAFETY: `bytes_pointer` always points to `num_bytes` consecutive, initialized
            // bytes because it was obtained by casting a regular Rust object reference
            // to a raw pointer. The borrow checker will ensure that `to_write` is not modified
            // by someone else during this function.
            let bytes = unsafe {
                std::slice::from_raw_parts(bytes_pointer, num_bytes)
            };
            self.contents.splice(addr_usize..addr_usize+num_bytes, bytes.iter().cloned());
        }

        #[verifier::external_body]
        fn flush(&mut self)
        {
        }
    }

    // The `VolatileMemoryMockingPersistentMemoryRegions` struct
    // contains a vector of volatile memory regions.
    pub struct VolatileMemoryMockingPersistentMemoryRegions
    {
        pub regions: Vec<VolatileMemoryMockingPersistentMemoryRegion>,
    }

    impl VolatileMemoryMockingPersistentMemoryRegions
    {
        #[verifier::external_body]
        pub fn new(region_sizes: &[u64]) -> (result: Self)
            ensures
                result.inv(),
                result@.len() == region_sizes@.len(),
                forall |i| 0 <= i < region_sizes@.len() ==> #[trigger] result@[i].len() == region_sizes[i],
        {
            let mut regions = Vec::<VolatileMemoryMockingPersistentMemoryRegion>::new();
            let num_regions = region_sizes.len();
            for pos in 0..num_regions
                invariant
                    regions.len() == pos,
                    forall |i| 0 <= i < pos ==> regions[i]@.len() == region_sizes[i],
            {
                let region = VolatileMemoryMockingPersistentMemoryRegion::new(region_sizes[pos]);
                regions.push(region);
            }
            Self{ regions }
        }
    }

    /// So that `VolatileMemoryMockingPersistentMemoryRegions` can be
    /// used to mock a collection of persistent memory regions, it
    /// implements the trait `PersistentMemoryRegions`.
    impl PersistentMemoryRegions for VolatileMemoryMockingPersistentMemoryRegions {
        #[verifier::external_body]
        closed spec fn view(&self) -> PersistentMemoryRegionsView
        {
            PersistentMemoryRegionsView{
                regions: self.regions@.map(|_i, r: VolatileMemoryMockingPersistentMemoryRegion| r@)
            }
        }

        closed spec fn inv(&self) -> bool
        {
            forall |i| 0 <= i < self.regions.len() ==> #[trigger] self.regions[i].inv()
        }

        #[verifier::external_body]
        closed spec fn constants(&self) -> PersistentMemoryConstants;

        #[verifier::external_body]
        fn get_num_regions(&self) -> usize
        {
            self.regions.len()
        }

        #[verifier::external_body]
        fn get_region_size(&self, index: usize) -> u64
        {
            self.regions[index].get_region_size()
        }

        #[verifier::external_body]
        fn read_aligned<S>(&self, index: usize, addr: u64, Ghost(true_val): Ghost<S>) -> (bytes: Result<MaybeCorruptedBytes<S>, PmemError>)
            where 
                S: PmCopy
        {
            self.regions[index].read_aligned::<S>(addr, Ghost(true_val))
        }

        #[verifier::external_body]
        fn read_unaligned(&self, index: usize, addr: u64, num_bytes: u64) -> (bytes: Result<Vec<u8>, PmemError>)
        {
            self.regions[index].read_unaligned(addr, num_bytes)
        }

        #[verifier::external_body]
        fn write(&mut self, index: usize, addr: u64, bytes: &[u8])
        {
            self.regions[index].write(addr, bytes)
        }

        #[verifier::external_body]
        fn serialize_and_write<S>(&mut self, index: usize, addr: u64, to_write: &S)
            where
                S: PmCopy + Sized
        {
            self.regions[index].serialize_and_write(addr, to_write);
        }

        #[verifier::external_body]
        fn flush(&mut self)
        {
        }
    }
}
================
File: ./storage_node/src/pmem/wrpm_t.rs
================

use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {
/// A `WriteRestrictedPersistentMemoryRegions` is a wrapper around a
/// collection of persistent memory regions that restricts how it can
/// be written. Specifically, it only permits a write if it's
/// accompanied by a tracked permission authorizing that write. The
/// tracked permission must authorize every possible state that could
/// result from crashing while the write is ongoing.

pub trait CheckPermission<State>
{
    spec fn check_permission(&self, state: State) -> bool;
}

#[allow(dead_code)]
pub struct WriteRestrictedPersistentMemoryRegions<Perm, PMRegions>
    where
        Perm: CheckPermission<Seq<Seq<u8>>>,
        PMRegions: PersistentMemoryRegions
{
    pm_regions: PMRegions,
    ghost perm: Option<Perm>, // Needed to work around Rust limitation that Perm must be referenced
}

impl<Perm, PMRegions> WriteRestrictedPersistentMemoryRegions<Perm, PMRegions>
    where
        Perm: CheckPermission<Seq<Seq<u8>>>,
        PMRegions: PersistentMemoryRegions
{
    pub closed spec fn view(&self) -> PersistentMemoryRegionsView
    {
        self.pm_regions@
    }

    pub closed spec fn inv(&self) -> bool
    {
        self.pm_regions.inv()
    }

    pub closed spec fn constants(&self) -> PersistentMemoryConstants
    {
        self.pm_regions.constants()
    }

    pub exec fn new(pm_regions: PMRegions) -> (wrpm_regions: Self)
        requires
            pm_regions.inv()
        ensures
            wrpm_regions.inv(),
            wrpm_regions@ == pm_regions@,
            wrpm_regions.constants() == pm_regions.constants(),
    {
        Self {
            pm_regions: pm_regions,
            perm: None
        }
    }

    // This executable function returns an immutable reference to the
    // persistent memory regions. This can be used to perform any
    // operation (e.g., read) that can't mutate the memory. After all,
    // this is a write-restricted memory, not a read-restricted one.
    pub exec fn get_pm_regions_ref(&self) -> (pm_regions: &PMRegions)
        requires
            self.inv(),
        ensures
            pm_regions.inv(),
            pm_regions@ == self@,
            pm_regions.constants() == self.constants(),
    {
        &self.pm_regions
    }

    // This executable function is the only way to perform a write, and
    // it requires the caller to supply permission authorizing the
    // write. The caller must prove that for every state this memory
    // can crash and recover into, the permission authorizes that
    // state.
    #[allow(unused_variables)]
    pub exec fn write(&mut self, index: usize, addr: u64, bytes: &[u8], perm: Tracked<&Perm>)
        requires
            old(self).inv(),
            index < old(self)@.len(),
            addr + bytes@.len() <= old(self)@[index as int].len(),
            addr + bytes@.len() <= u64::MAX,
            old(self)@.no_outstanding_writes_in_range(index as int, addr as int, addr + bytes@.len()),
            // The key thing the caller must prove is that all crash states are authorized by `perm`
            forall |s| old(self)@.write(index as int, addr as int, bytes@).can_crash_as(s)
                  ==> #[trigger] perm@.check_permission(s),
        ensures
            self.inv(),
            self.constants() == old(self).constants(),
            self@ == old(self)@.write(index as int, addr as int, bytes@),
    {
        self.pm_regions.write(index, addr, bytes)
    }

    #[allow(unused_variables)]
    pub exec fn serialize_and_write<S>(&mut self, index: usize, addr: u64, to_write: &S, perm: Tracked<&Perm>)
        where
            S: PmCopy + Sized
        requires
            old(self).inv(),
            index < old(self)@.len(),
            addr + S::spec_size_of() <= old(self)@[index as int].len(),
            old(self)@.no_outstanding_writes_in_range(index as int, addr as int, addr + S::spec_size_of()),
            // The key thing the caller must prove is that all crash states are authorized by `perm`
            forall |s| old(self)@.write(index as int, addr as int, to_write.spec_to_bytes()).can_crash_as(s)
                  ==> #[trigger] perm@.check_permission(s),
        ensures
            self.inv(),
            self.constants() == old(self).constants(),
            self@ == old(self)@.write(index as int, addr as int, to_write.spec_to_bytes()),
    {
        self.pm_regions.serialize_and_write(index, addr, to_write);
    }

    // Even though the memory is write-restricted, no restrictions are
    // placed on calling `flush`. After all, `flush` can only narrow
    // the possible states the memory can crash into. So if the memory
    // is already restricted to only crash into good states, `flush`
    // automatically maintains that restriction.
    pub exec fn flush(&mut self)
        requires
            old(self).inv(),
        ensures
            self.inv(),
            self@ == old(self)@.flush(),
            self.constants() == old(self).constants(),
            forall |i: int| #![auto] 0 <= i < self@.len() ==> old(self)@[i].len() == self@[i].len()
    {
        self.pm_regions.flush()
    }
}

#[allow(dead_code)]
pub struct WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>
    where
        Perm: CheckPermission<Seq<u8>>,
        PMRegion: PersistentMemoryRegion
{
    pm_region: PMRegion,
    ghost perm: Option<Perm>, // Needed to work around Rust limitation that Perm must be referenced
}

impl<Perm, PMRegion> WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>
    where
        Perm: CheckPermission<Seq<u8>>,
        PMRegion: PersistentMemoryRegion
{
    pub closed spec fn view(&self) -> PersistentMemoryRegionView
    {
        self.pm_region@
    }

    pub closed spec fn inv(&self) -> bool
    {
        self.pm_region.inv()
    }

    pub closed spec fn constants(&self) -> PersistentMemoryConstants
    {
        self.pm_region.constants()
    }

    pub exec fn new(pm_region: PMRegion) -> (wrpm_region: Self)
        requires
            pm_region.inv(),
        ensures
            wrpm_region.inv(),
            wrpm_region@ == pm_region@,
            wrpm_region.constants() == pm_region.constants(),
    {
        Self {
            pm_region: pm_region,
            perm: None
        }
    }

    // This executable function returns an immutable reference to the
    // persistent memory region. This can be used to perform any
    // operation (e.g., read) that can't mutate the memory. After all,
    // this is a write-restricted memory, not a read-restricted one.
    pub exec fn get_pm_region_ref(&self) -> (pm_region: &PMRegion)
        requires
            self.inv(),
        ensures
            pm_region.inv(),
            pm_region@ == self@,
            pm_region.constants() == self.constants(),
    {
        &self.pm_region
    }

    // This executable function is the only way to perform a write, and
    // it requires the caller to supply permission authorizing the
    // write. The caller must prove that for every state this memory
    // can crash and recover into, the permission authorizes that
    // state.
    #[allow(unused_variables)]
    pub exec fn write(&mut self, addr: u64, bytes: &[u8], perm: Tracked<&Perm>)
        requires
            old(self).inv(),
            addr + bytes@.len() <= old(self)@.len(),
            addr + bytes@.len() <= u64::MAX,
            old(self)@.no_outstanding_writes_in_range(addr as int, addr + bytes@.len()),
                // The key thing the caller must prove is that all crash states are authorized by `perm`
            forall |s| old(self)@.write(addr as int, bytes@).can_crash_as(s)
                  ==> #[trigger] perm@.check_permission(s),
        ensures
            self.inv(),
            self.constants() == old(self).constants(),
            self@ == old(self)@.write(addr as int, bytes@),
    {
        let ghost pmr = self.pm_region;
        self.pm_region.write(addr, bytes);
    }

    #[allow(unused_variables)]
    pub exec fn serialize_and_write<S>(&mut self, addr: u64, to_write: &S, perm: Tracked<&Perm>)
        where
            S: PmCopy + Sized
        requires
            old(self).inv(),
            addr + S::spec_size_of() <= old(self)@.len(),
            old(self)@.no_outstanding_writes_in_range(addr as int, addr + S::spec_size_of()),
            // The key thing the caller must prove is that all crash states are authorized by `perm`
            forall |s| old(self)@.write(addr as int, to_write.spec_to_bytes()).can_crash_as(s)
                  ==> #[trigger] perm@.check_permission(s),
        ensures
            self.inv(),
            self.constants() == old(self).constants(),
            self@ == old(self)@.write(addr as int, to_write.spec_to_bytes()),
    {
        self.pm_region.serialize_and_write(addr, to_write);
    }

    // Even though the memory is write-restricted, no restrictions are
    // placed on calling `flush`. After all, `flush` can only narrow
    // the possible states the memory can crash into. So if the memory
    // is already restricted to only crash into good states, `flush`
    // automatically maintains that restriction.
    pub exec fn flush(&mut self)
        requires
            old(self).inv(),
        ensures
            self.inv(),
            self@ == old(self)@.flush(),
            self.constants() == old(self).constants(),
    {
        self.pm_region.flush()
    }
}
}

================
File: ./storage_node/src/pmem/pmcopy_t.rs
================

//! This file contains the trusted specification of several traits that 
//! help us prove that reading from PM is safe. Some of the traits in this 
//! file correspond to macros defined in the `pmsafe` crate in this repository.
//! It is also related to some external traits defined and specified 
//! in pmem/traits_t.rs.
//! 
//! Both reading and writing to PM is potentially dangerous; this file 
//! focuses on ensuring that reads are safe. We want to be able to 
//! read bytes from PM and then cast them to some target type T,
//! but this may result in undefined behavior if the bytes are 
//! an invalid T value. Thus, before doing such a cast, we 
//! need to be sure that 1) the bytes we are reading have been 
//! properly initialized with a valid T and 2) they have not been
//! corrupted. 
//! 
//! We also need to be sure that proofs use the correct size for T.
//! This is tricky, because Verus has no way of obtaining the size 
//! in bytes of a structure. Rust's std::mem::size_of function
//! can give the size of a type in constant contexts, but this is 
//! an exec function that cannot be used in spec or proof code.
//! 
//! This file contains the `PmCopy` trait, which indicates whether
//! a type is safe to copy from PM and defines some methods and 
//! axioms for dealing with such structures.

use crate::pmem::pmemspec_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::bytes;
use vstd::bytes::*;
use vstd::prelude::*;
use vstd::ptr::*;
use vstd::layout::*;
use crate::pmem::traits_t::{PmSafe, PmSized, ConstPmSized, UnsafeSpecPmSized};

use deps_hack::{crc64fast::Digest, pmsized_primitive};
use core::slice;
use std::convert::TryInto;
use std::ptr;
use std::mem::MaybeUninit;

verus! {
    pub broadcast group pmcopy_axioms {
        axiom_bytes_len,
        axiom_to_from_bytes
    }

    // PmCopy provides functions to help reason about copying data to and from persistent memory.
    // It is a subtrait of PmSafe (a marker trait indicating that the type is safe to write to persistent memory)
    // and PmSized/SpecPmSized (which provide reliable information about the size of the implementing struct in 
    // bytes for use in ghost code).

    // PmCopy (together with the PmCopyHelper trait, which provides a blanket implementation of 
    // method we want PmCopy objects to have) indicates whether an object is safe to 
    // read from PM and provides methods to help prove that such reads are safe. 
    // It is a subtrait of several unsafe traits -- PmSafe and PmSized -- that should
    // only be implemented by structures using #[derive]. 
    // 
    // There is currently no derive macro for PmCopy, but since it is a safe trait with 
    // no associated functions, it can be implemented directly in safe, verified code.
    // If compilation of such an implementation succeeds, the implementor is safe
    // to read from PM.
    //
    // Implicit in this definition is that `PmCopy` types must be repr(C);
    // the macros that derive `PmSized` and `PmSafe` require that the deriving
    // type be repr(C), as this is the best way to ensure a predictable in-memory
    // layout and size.
    pub trait PmCopy : PmSized + SpecPmSized + Sized + PmSafe + Copy {}

    // PmCopyHelper is a subtrait of PmCopy that exists to provide a blanket
    // implementation of these methods for all PmCopy objects. 
    pub trait PmCopyHelper : PmCopy {
        // `spec_to_bytes` is an uninterpreted method that returns a byte
        // representation of `self`. 
        spec fn spec_to_bytes(self) -> Seq<u8>;

        // `spec_from_bytes` is the inverse of spec_to_bytes. It only returns
        // valid instances of T, because only valid instances of T can be
        // converted to bytes using `spec_to_bytes`. Its relationship 
        // to `spec_to_bytes` is axiomatized by `axiom_to_from_bytes`.
        spec fn spec_from_bytes(bytes: Seq<u8>) -> Self;

        spec fn bytes_parseable(bytes: Seq<u8>) -> bool;

        // `spec_crc` returns the CRC of the given value as a u64. 
        spec fn spec_crc(self) -> u64;
    }

    impl<T> PmCopyHelper for T where T: PmCopy {
        closed spec fn spec_to_bytes(self) -> Seq<u8>;

        // The definition is closed because no one should need to reason about it,
        // thanks to `axiom_to_from_bytes`.
        closed spec fn spec_from_bytes(bytes: Seq<u8>) -> Self
        {
            // If the bytes represent some valid `Self`, pick such a `Self`.
            // Otherwise, pick an arbitrary `Self`. (That's how `choose` works.)
            choose |x: T| x.spec_to_bytes() == bytes
        }

        open spec fn spec_crc(self) -> u64 {
            spec_crc_u64(self.spec_to_bytes())
        }

        open spec fn bytes_parseable(bytes: Seq<u8>) -> bool
        {
            Self::spec_from_bytes(bytes).spec_to_bytes() == bytes
        }
    }

    // The two following axioms are brodcast in the `pmcopy_axioms`
    // group. 

    // `axiom_bytes_len` axiomatizes the fact that a byte 
    // representation of some PmCopy value S has size
    // S::spec_size_of(). Since `spec_to_bytes` is uninterpreted,
    // we cannot prove that this is true; the PmSized macro helps
    // us check that this is the case, but that macro still must 
    // be audited to ensure that S::spec_size_of() and the runtime 
    // size_of functions return the same value.
    pub broadcast proof fn axiom_bytes_len<S: PmCopy>(s: S)
        ensures 
            #[trigger] s.spec_to_bytes().len() == S::spec_size_of()
    {
        admit();
    }

    // `axiom_to_from_bytes` axiomatizes the fact that converting any
    // PmCopy value `s` to bytes and back results in the original value. 
    // This axiom has no preconditions because `s` is necessarily
    // a valid instance of its type.
    pub broadcast proof fn axiom_to_from_bytes<S: PmCopy>(s: S)
        ensures 
            s == #[trigger] S::spec_from_bytes(s.spec_to_bytes())
    {
        admit();
    }

    // u64 must implement PmCopy for CRC management
    impl PmCopy for u64 {}

    // MaybeCorruptedBytes<S> is a container for bytes that have been read 
    // from PM and represent a valid S in the absence of corruption. It 
    // provides methods to help check that the bytes have not been corrupted
    // and then cast them to an S. 
    // 
    // This structure is external_body because it uses a `MaybeUninit` to store
    // the bytes. We cannot cast the bytes to an S before checking for corruption, 
    // as this could cause UB if the bytes are corrupted. Using `MaybeUninit` 
    // avoids UB and also ensures that the DRAM location of the S is properly
    // aligned, which is also important for safety when we eventually cast 
    // the bytes to an S.
    #[verifier::external_body]
    #[verifier::reject_recursive_types(S)]
    pub struct MaybeCorruptedBytes<S>
        where 
            S: PmCopy
    {
        val: Box<MaybeUninit<S>>,
    }

    impl<S> MaybeCorruptedBytes<S>
        where 
            S: PmCopy 
    {
        // The constructor doesn't have a postcondition because we do not know anything about 
        // the state of the bytes yet.
        #[verifier::external_body]
        pub exec fn new() -> (out: Self)
        {
            MaybeCorruptedBytes { 
                val: Box::<S>::new_uninit()
            }
        }

        // The view of a `MaybeCorruptedBytes<S>` is a sequence of bytes.
        // This spec fn is uninterpreted; its relationship to S's `spec_to_bytes`
        // is established in the postconditions of other `MaybeCorruptedBytes`
        // methods.
        pub closed spec fn view(self) -> Seq<u8>;

        // `copy_from_slice` copies bytes from a location on PM to a 
        // `MaybeCorruptedBytes<S>`. The precondition requires that we 
        // have previously initialized the PM bytes as a valid S `true_val`,
        // but not that the bytes are uncorrupted. 
        #[verifier::external_body]
        pub exec fn copy_from_slice(
            &mut self, 
            bytes: &[u8], 
            Ghost(true_val): Ghost<S>,
            Ghost(addrs): Ghost<Seq<int>>,
            Ghost(impervious_to_corruption): Ghost<bool>
        )
            requires 
                if impervious_to_corruption {
                    bytes@ == true_val.spec_to_bytes()
                } else {
                    maybe_corrupted(bytes@, true_val.spec_to_bytes(), addrs)
                },
                bytes@.len() == S::spec_size_of(),
            ensures 
                self@ == bytes@
        {
            self.copy_from_slice_helper(bytes);
        }

        // This helper method lets us work around the lack of Verus support for &mut and
        // a bug where the body of some external_body functions may be checked by the verifier.
        // It casts the contents of self to a slice of `MaybeUninit<u8>`, then copies the given
        // byte slice to this location. All of the code in this function is safe -- it's safe
        // to cast a MaybeUninit value to a slice of MaybeUninit bytes, and it is safe to 
        // copy initialized bytes to this slice. `MaybeUninit::write_slice` will panic if 
        // the two slices have different lengths; since this method is external, it cannot 
        // have a precondition, but the public wrapper around this function `copy_from_slice`
        // requires that the provided PM byte slice is the correct size.
        // TODO: remove this helper function and move its body back into `copy_from_slice` once 
        // https://github.com/verus-lang/verus/issues/1151 is fixed
        #[verifier::external]
        #[inline(always)]
        fn copy_from_slice_helper(
            &mut self, 
            bytes: &[u8], 
        ) 
        {
            // convert the MaybeUninit<S> to a mutable slice of `MaybeUninit<u8>`
            let mut self_bytes = self.val.as_bytes_mut();
            // copy bytes from the given slice to the mutable slice of `MaybeUninit<u8>`.
            // This returns a slice of initialized bytes, but it does NOT change the fact that 
            // the original S is still MaybeUninit
            // TODO: in newer versions of Rust, write_slice is renamed to copy_from_slice
            MaybeUninit::write_slice(self_bytes, bytes);
        }


        // This method returns the `MaybeUninit` value stored in self as an immutable slice of 
        // bytes. This is safe here because there are no invalid values of u8; even if the slice
        // constitutes an invalid S, the returned &[u8] will be valid, so assuming that it is 
        // initialized is safe. We use this method to obtain a concrete byte representation of
        // for use in CRC checking.
        #[verifier::external_body]
        pub exec fn as_slice(&self) -> (out: &[u8])
            ensures 
                out@ == self@
        {
            let bytes = self.val.as_bytes();
            // SAFETY: even if we haven't initialized the bytes, there are no invalid values of u8, so we can 
            // safely assume that these bytes are initialized (even if the S may not be)
            unsafe { MaybeUninit::slice_assume_init_ref(bytes) }
        }

        // This method assumes that the `MaybeUninit` value of S in self is, in fact, init. 
        // This method can only be invoked once the caller has proven that the bytes are not 
        // corrupted and that they represent a valid S.
        #[verifier::external_body]
        pub exec fn extract_init_val(self, Ghost(true_val): Ghost<S>, Ghost(true_bytes): Ghost<Seq<u8>>, Ghost(impervious_to_corruption): Ghost<bool>) -> (out: Box<S>)
            requires 
                if impervious_to_corruption {
                    true 
                } else {
                    &&& true_bytes == true_val.spec_to_bytes()
                    &&& self@ == true_bytes
                }
            ensures 
                out == true_val
        {
            // SAFETY: The precondition establishes that self@ -- the ghost view of the maybe-corrupted bytes
            // written to self.val -- are equivalent to the serialization of the true value; i.e., we must have 
            // proven that the bytes are not corrupted, and therefore self.val is initialized.
            unsafe { self.val.assume_init() }
        }
    }

    impl MaybeCorruptedBytes<u64> {
        // This method assumes that the CDB is initialized and returns a Box<u64> without
        // requiring a CRC check first. This is necessary because we check the CDB for 
        // corruption by checking its own value directly; it does not have a separate CRC
        // to compare to. Thus, we need an initialized u64 representation of the CDB
        // prior to the corruption check, which we cannot obtain with `extract_init_val`.
        // Since there are no invalid values of u64, it is always safe to assume that
        // a u64 is initialized. We further require that we've previously durably initialized
        // these bytes as one of the two valid CDB values, so this method cannot be used
        // on arbitrary u64s. 
        #[verifier::external_body]
        pub exec fn extract_cdb(
            self, 
            Ghost(true_val): Ghost<u64>, 
            Ghost(true_bytes): Ghost<Seq<u8>>, 
            Ghost(addrs): Ghost<Seq<int>>,
            Ghost(impervious_to_corruption): Ghost<bool>
        ) -> (out: Box<u64>)
            requires 
                if impervious_to_corruption {
                    self@ == true_bytes
                } else {
                    maybe_corrupted(self@, true_bytes, addrs)
                },
                true_val.spec_to_bytes() == true_bytes,
                true_val == CDB_TRUE || true_val == CDB_FALSE,
            ensures 
                out.spec_to_bytes() == self@
        {
            // SAFETY: there are not invalid u64 values, so it is safe to assume that any value we read
            // from a CDB location can be treated as initialized. This function makes no promises about 
            // the value of the CDB -- just that we can treat the view as equal to the output of this
            // function -- so we still have to check the CDB for corruption.
            unsafe { self.val.assume_init() }
        }  
    }

    // Our trusted specification of primitive sizes assumes that usize and 
    // isize are 8 bytes. These directives have Verus check this assumption.
    global size_of usize == 8;
    global size_of isize == 8;

    // SpecPmSized defines the spec counterparts of the external trait
    // PmSized, which provides methods for calculating the size 
    // of a user-defined structure that needs to be stored on PM. 
    // PmSized can only be derived via a trusted macro (defined in the
    // pmsafe crate) and uses compile-time assertions to check that
    // the calculated size is the same as the true size. This helps
    // us check that we are using the correct size for PM-resident
    // structures in ghost code, where their size is not usually
    // accessible.
    //
    // Due to restrictions on const traits and associated constants
    // in Rust and Verus, these spec methods must be defined in a separate
    // trait from the exec methods they specify. SpecPmSized is a subtrait
    // of the external unsafe trait UnsafeSpecPmSized to ensure that 
    // structures only implement it if they use the provided derive macros.
    pub trait SpecPmSized : UnsafeSpecPmSized {
        spec fn spec_size_of() -> int;
        spec fn spec_align_of() -> int;
    }

    // User-defined structures that derive PmSized have their 
    // size calculated based on the size and alignment of the
    // struct's fields. The size and alignment of safe primitives
    // are hardcoded in pmsafe/pmsafe_macros.rs and implementations
    // for primitive types based on these values are generated
    // by the pmsized_primitive! macro. The macro also generates
    // compile-time assertions to check that the hardcoded sizes
    // and alignments are correct.
    pmsized_primitive!(u8);
    pmsized_primitive!(u16);
    pmsized_primitive!(u32);
    pmsized_primitive!(u64);
    pmsized_primitive!(u128);
    pmsized_primitive!(usize);
    pmsized_primitive!(i8);
    pmsized_primitive!(i16);
    pmsized_primitive!(i32);
    pmsized_primitive!(i64);
    pmsized_primitive!(i128);
    pmsized_primitive!(isize);
    pmsized_primitive!(bool);
    pmsized_primitive!(char);
    // floats are not currently supported by the verifier
    // pmsized_primitive!(f32);
    // pmsized_primitive!(f64);

    // TODO: Manually implement the array case

    // impl<T: PmSafe + PmSized + PmCheckSize, const N: usize> PmSized for [T; N] {
    //     open spec fn spec_size_of() -> int
    //     {
    //         N * T::spec_size_of()
    //     }     

    //     fn size_of() -> usize 
    //     {
    //         N * T::size_of()
    //     }
        
    //     open spec fn spec_align_of() -> int
    //     {
    //         T::spec_align_of()
    //     }

    //     fn align_of() -> usize {
    //         T::align_of()
    //     }
    // }

    // This spec function implements an algorithm for determining the amount of 
    // padding needed before the next field in a repr(C) structure to ensure it is aligned.
    // This is the same algorithm described here:
    // https://doc.rust-lang.org/reference/type-layout.html#the-c-representation
    pub open spec fn spec_padding_needed(offset: int, align: int) -> int
    {
        let misalignment = offset % align;
        if misalignment > 0 {
            align - misalignment
        } else {
            0
        }
    }

    // This function calculates the amount of padding needed to align the next field in a struct.
    // It's const, so we can use it const contexts to calculate the size of a struct at compile time.
    // This function is also verified.
    pub const fn padding_needed(offset: usize, align: usize) -> (out: usize) 
        requires 
            align > 0,
        ensures 
            out <= align,
            out as int == spec_padding_needed(offset as int, align as int)
    {
        let misalignment = offset % align;
        if misalignment > 0 {
            align - misalignment
        } else {
            0
        }
    }

}

================
File: ./storage_node/src/pmem/subregion_v.rs
================

use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::wrpm_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::bytes::*;
use vstd::invariant::*;
use vstd::prelude::*;
use vstd::seq::*;
use vstd::seq_lib::*;

verus! {

broadcast use pmcopy_axioms;

pub open spec fn get_subregion_view(
    region: PersistentMemoryRegionView,
    start: int,
    len: int,
) -> PersistentMemoryRegionView
    recommends
        0 <= start,
        0 <= len,
        start + len <= region.len(),
{
    PersistentMemoryRegionView{ state: region.state.subrange(start as int, start + len) }
}

pub open spec fn memories_differ_only_where_subregion_allows(
    mem1: Seq<u8>,
    mem2: Seq<u8>,                                                         
    start: int,
    len: int,
    is_writable_absolute_addr_fn: spec_fn(int) -> bool
) -> bool
    recommends
        0 <= start,
        0 <= len,
        mem1.len() == mem2.len(),
        start + len <= mem1.len(),
{
    forall |addr: int| {
       ||| 0 <= addr < start
       ||| start + len <= addr < mem1.len()
       ||| start <= addr < start + len && !is_writable_absolute_addr_fn(addr)
    } ==> mem1[addr] == #[trigger] mem2[addr]
}

pub open spec fn views_differ_only_where_subregion_allows(
    v1: PersistentMemoryRegionView,
    v2: PersistentMemoryRegionView,
    start: int,
    len: int,
    is_writable_absolute_addr_fn: spec_fn(int) -> bool
) -> bool
    recommends
        0 <= start,
        0 <= len,
        start + len <= v1.len(),
        v1.len() == v2.len()
{
    forall |addr: int| {
       ||| 0 <= addr < start
       ||| start + len <= addr < v1.len()
       ||| start <= addr < start + len && !is_writable_absolute_addr_fn(addr)
    } ==> v1.state[addr] == #[trigger] v2.state[addr]
}

pub open spec fn condition_sufficient_to_create_wrpm_subregion<Perm>(
    region_view: PersistentMemoryRegionView,
    perm: &Perm,
    start: u64,
    len: int,
    is_writable_absolute_addr_fn: spec_fn(int) -> bool,
    condition: spec_fn(Seq<u8>) -> bool,
) -> bool
    where
        Perm: CheckPermission<Seq<u8>>,
{
    &&& 0 <= len
    &&& start + len <= region_view.len() <= u64::MAX
    &&& forall |crash_state| region_view.can_crash_as(crash_state) ==> condition(crash_state)
    &&& forall |crash_state| condition(crash_state) ==> perm.check_permission(crash_state)
    &&& forall |s1: Seq<u8>, s2: Seq<u8>| {
           &&& condition(s1)
           &&& s1.len() == s2.len() == region_view.len()
           &&& #[trigger] memories_differ_only_where_subregion_allows(s1, s2, start as int, len,
                                                                    is_writable_absolute_addr_fn)
       } ==> condition(s2)
}

pub proof fn lemma_condition_sufficient_to_create_wrpm_subregion<Perm>(
    region_view: PersistentMemoryRegionView,
    perm: &Perm,
    start: u64,
    len: int,
    is_writable_absolute_addr_fn: spec_fn(int) -> bool,
    condition: spec_fn(Seq<u8>) -> bool,
)
    where
        Perm: CheckPermission<Seq<u8>>,
    requires
        condition_sufficient_to_create_wrpm_subregion(region_view, perm, start, len, is_writable_absolute_addr_fn,
                                                      condition),
    ensures
        forall |alt_region_view: PersistentMemoryRegionView, alt_crash_state: Seq<u8>| {
            &&& #[trigger] alt_region_view.can_crash_as(alt_crash_state)
            &&& region_view.len() == alt_region_view.len()
            &&& views_differ_only_where_subregion_allows(region_view, alt_region_view, start as int, len,
                                                       is_writable_absolute_addr_fn)
        } ==> perm.check_permission(alt_crash_state),
{
    assert forall |alt_region_view: PersistentMemoryRegionView, alt_crash_state: Seq<u8>| {
        &&& #[trigger] alt_region_view.can_crash_as(alt_crash_state)
        &&& region_view.len() == alt_region_view.len()
        &&& views_differ_only_where_subregion_allows(region_view, alt_region_view, start as int, len,
                                                   is_writable_absolute_addr_fn)
    } implies perm.check_permission(alt_crash_state) by {
        let crash_state = Seq::<u8>::new(
            alt_crash_state.len(),
            |addr| {
                if !(start <= addr < start + len && is_writable_absolute_addr_fn(addr)) {
                    alt_crash_state[addr]
                }
                else {
                    let chunk = addr / const_persistence_chunk_size();
                    if alt_region_view.chunk_corresponds_ignoring_outstanding_writes(chunk, alt_crash_state) {
                        region_view.state[addr].state_at_last_flush
                    }
                    else {
                        region_view.state[addr].flush_byte()
                    }
                }
            }
        );
        assert(memories_differ_only_where_subregion_allows(crash_state, alt_crash_state, start as int, len,
                                                           is_writable_absolute_addr_fn));
        assert(region_view.can_crash_as(crash_state));
        assert(region_view.can_crash_as(crash_state)) by {
            assert forall |chunk| {
                ||| region_view.chunk_corresponds_ignoring_outstanding_writes(chunk, crash_state)
                ||| region_view.chunk_corresponds_after_flush(chunk, crash_state)
            } by {
                if alt_region_view.chunk_corresponds_ignoring_outstanding_writes(chunk, alt_crash_state) {
                    assert(region_view.chunk_corresponds_ignoring_outstanding_writes(chunk, crash_state));
                }
                else {
                    assert(region_view.chunk_corresponds_after_flush(chunk, crash_state));
                }
            }
        }
        assert(condition(crash_state));
    }
}

pub struct WriteRestrictedPersistentMemorySubregion
{
    start_: u64,
    len_: Ghost<int>,
    constants_: Ghost<PersistentMemoryConstants>,
    initial_region_view_: Ghost<PersistentMemoryRegionView>,
    is_writable_absolute_addr_fn_: Ghost<spec_fn(int) -> bool>,
}

impl WriteRestrictedPersistentMemorySubregion
{
    pub exec fn new<Perm, PMRegion>(
        wrpm: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        Tracked(perm): Tracked<&Perm>,
        start: u64,
        Ghost(len): Ghost<int>,
        Ghost(is_writable_absolute_addr_fn): Ghost<spec_fn(int) -> bool>,
    ) -> (result: Self)
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            wrpm.inv(),
            0 <= len,
            start + len <= wrpm@.len() <= u64::MAX,
            forall |alt_region_view: PersistentMemoryRegionView, alt_crash_state: Seq<u8>| {
                &&& #[trigger] alt_region_view.can_crash_as(alt_crash_state)
                &&& wrpm@.len() == alt_region_view.len()
                &&& views_differ_only_where_subregion_allows(wrpm@, alt_region_view, start as int, len,
                                                           is_writable_absolute_addr_fn)
            } ==> perm.check_permission(alt_crash_state),
        ensures
            result.inv(wrpm, perm),
            result.constants() == wrpm.constants(),
            result.start() == start,
            result.len() == len,
            result.initial_region_view() == wrpm@,
            result.is_writable_absolute_addr_fn() == is_writable_absolute_addr_fn,
            result.view(wrpm) == result.initial_subregion_view(),
            result.view(wrpm) == get_subregion_view(wrpm@, start as int, len),
    {
        let result = Self{
            start_: start,
            len_: Ghost(len),
            constants_: Ghost(wrpm.constants()),
            initial_region_view_: Ghost(wrpm@),
            is_writable_absolute_addr_fn_: Ghost(is_writable_absolute_addr_fn),
        };
        result
    }

    pub exec fn new_with_condition<Perm, PMRegion>(
        wrpm: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        Tracked(perm): Tracked<&Perm>,
        start: u64,
        Ghost(len): Ghost<int>,
        Ghost(is_writable_absolute_addr_fn): Ghost<spec_fn(int) -> bool>,
        Ghost(condition): Ghost<spec_fn(Seq<u8>) -> bool>,
    ) -> (result: Self)
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            wrpm.inv(),
            condition_sufficient_to_create_wrpm_subregion(wrpm@, perm, start, len, is_writable_absolute_addr_fn,
                                                          condition),
        ensures
            result.inv(wrpm, perm),
            result.constants() == wrpm.constants(),
            result.start() == start,
            result.len() == len,
            result.initial_region_view() == wrpm@,
            result.is_writable_absolute_addr_fn() == is_writable_absolute_addr_fn,
            result.view(wrpm) == result.initial_subregion_view(),
            result.view(wrpm) == get_subregion_view(wrpm@, start as int, len),
    {
        proof {
            lemma_condition_sufficient_to_create_wrpm_subregion(wrpm@, perm, start, len, is_writable_absolute_addr_fn,
                                                                condition);
        }
        let result = Self{
            start_: start,
            len_: Ghost(len),
            constants_: Ghost(wrpm.constants()),
            initial_region_view_: Ghost(wrpm@),
            is_writable_absolute_addr_fn_: Ghost(is_writable_absolute_addr_fn),
        };
        result
    }

    pub closed spec fn constants(self) -> PersistentMemoryConstants
    {
        self.constants_@
    }

    pub closed spec fn start(self) -> int
    {
        self.start_
 as int
    }

    pub closed spec fn len(self) -> int
    {
        self.len_@
    }

    pub open spec fn end(self) -> int
    {
        self.start() + self.len()
    }

    pub closed spec fn initial_region_view(self) -> PersistentMemoryRegionView
    {
        self.initial_region_view_@
    }

    pub closed spec fn is_writable_absolute_addr_fn(self) -> spec_fn(int) -> bool
    {
        self.is_writable_absolute_addr_fn_@
    }

    pub open spec fn is_writable_relative_addr(self, addr: int) -> bool
    {
        self.is_writable_absolute_addr_fn()(addr + self.start())
    }

    pub closed spec fn initial_subregion_view(self) -> PersistentMemoryRegionView
    {
        get_subregion_view(self.initial_region_view(), self.start(), self.len())
    }

    pub closed spec fn view<Perm, PMRegion>(
        self,
        wrpm: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>
    ) -> PersistentMemoryRegionView
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
    {
        get_subregion_view(wrpm@, self.start(), self.len())
    }

    pub closed spec fn opaque_inv<Perm, PMRegion>(
        self,
        wrpm: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        perm: &Perm
    ) -> bool
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
    {
        &&& wrpm.inv()
        &&& wrpm.constants() == self.constants()
        &&& wrpm@.len() == self.initial_region_view().len()
        &&& self.initial_region_view().len() <= u64::MAX
        &&& self.start() + self.len() <= wrpm@.len()
        &&& self.view(wrpm).len() == self.len()
        &&& views_differ_only_where_subregion_allows(self.initial_region_view(), wrpm@, self.start(),
                                                   self.len(), self.is_writable_absolute_addr_fn())
        &&& forall |alt_region_view: PersistentMemoryRegionView, alt_crash_state: Seq<u8>| {
              &&& #[trigger] alt_region_view.can_crash_as(alt_crash_state)
              &&& self.initial_region_view().len() == alt_region_view.len()
              &&& views_differ_only_where_subregion_allows(self.initial_region_view(), alt_region_view,
                                                         self.start(), self.len(),
                                                         self.is_writable_absolute_addr_fn())
           } ==> perm.check_permission(alt_crash_state)
    }

    pub open spec fn inv<Perm, PMRegion>(
        self,
        wrpm: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        perm: &Perm
    ) -> bool
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
    {
        &&& self.view(wrpm).len() == self.len()
        &&& self.opaque_inv(wrpm, perm)
    }

    pub exec fn read_relative_unaligned<Perm, PMRegion>(
        self: &Self,
        wrpm: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        relative_addr: u64,
        num_bytes: u64,
        Tracked(perm): Tracked<&Perm>,
    ) ->(result: Result<Vec<u8>, PmemError>)
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(wrpm, perm),
            relative_addr + num_bytes <= self.len(),
            self.view(wrpm).no_outstanding_writes_in_range(relative_addr as int, relative_addr + num_bytes),
        ensures
            match result {
                Ok(bytes) => {
                    let true_bytes = self.view(wrpm).committed().subrange(relative_addr as int, relative_addr + num_bytes);
                    // If the persistent memory region is impervious
                    // to corruption, read returns the last bytes
                    // written. Otherwise, it returns a
                    // possibly-corrupted version of those bytes.
                    if wrpm.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    }
                    else {
                        // The addresses in `maybe_corrupted` reflect the fact
                        // that we're reading from a subregion at a certain
                        // start.
                        let absolute_addrs = Seq::<int>::new(num_bytes as nat, |i: int| relative_addr + self.start() + i);
                        maybe_corrupted(bytes@, true_bytes, absolute_addrs)
                    }
                }
                Err(e) => e == PmemError::AccessOutOfRange
            }
    {
        self.read_absolute_unaligned(wrpm, relative_addr + self.start_, num_bytes, Tracked(perm))
    }

    pub exec fn read_absolute_unaligned<Perm, PMRegion>(
        self: &Self,
        wrpm: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        absolute_addr: u64,
        num_bytes: u64,
        Tracked(perm): Tracked<&Perm>,
    ) -> (result: Result<Vec<u8>, PmemError>)
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(wrpm, perm),
            self.start() <= absolute_addr,
            absolute_addr + num_bytes <= self.end(),
            self.view(wrpm).no_outstanding_writes_in_range(
                absolute_addr - self.start(),
                absolute_addr + num_bytes - self.start(),
            ),
        ensures
            match result {
                Ok(bytes) => {
                    let true_bytes = self.view(wrpm).committed().subrange(
                        absolute_addr - self.start(),
                        absolute_addr + num_bytes - self.start()
                    );
                    // If the persistent memory region is impervious
                    // to corruption, read returns the last bytes
                    // written. Otherwise, it returns a
                    // possibly-corrupted version of those bytes.
                    if wrpm.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    }
                    else {
                        // The addresses in `maybe_corrupted` reflect the fact
                        // that we're reading from a subregion at a certain
                        // start.
                        let absolute_addrs = Seq::<int>::new(num_bytes as nat, |i: int| absolute_addr + i);
                        maybe_corrupted(bytes@, true_bytes, absolute_addrs)
                    }
                }
                Err(e) => e == PmemError::AccessOutOfRange
            }
    {
        let ghost true_bytes1 = self.view(wrpm).committed().subrange(
            absolute_addr - self.start(),
            absolute_addr + num_bytes - self.start(),
        );
        let ghost true_bytes2 = wrpm@.committed().subrange(
            absolute_addr as int,
            absolute_addr + num_bytes
        );
        assert(true_bytes1 =~= true_bytes2);
        assert forall |i| #![trigger wrpm@.state[i]]
                   absolute_addr <= i < absolute_addr + num_bytes implies
                   wrpm@.state[i].outstanding_write.is_none() by {
            assert(wrpm@.state[i] == self.view(wrpm).state[i - self.start()]);
        }
        wrpm.get_pm_region_ref().read_unaligned(absolute_addr, num_bytes)
    }

    pub exec fn read_relative_aligned<'a, S, Perm, PMRegion>(
        self: &Self,
        wrpm: &'a WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        relative_addr: u64,
        Ghost(true_val): Ghost<S>,
        Tracked(perm): Tracked<&Perm>,
    ) -> (result: Result<MaybeCorruptedBytes<S>, PmemError>)
        where
            S: PmCopy + Sized,
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(wrpm, perm),
            relative_addr < relative_addr + S::spec_size_of() <= self.len(),
            self.view(wrpm).no_outstanding_writes_in_range(
                relative_addr as int,
                relative_addr + S::spec_size_of(),
            ),
            self.view(wrpm).committed().subrange(relative_addr as int, relative_addr + S::spec_size_of()) == true_val.spec_to_bytes(),
        ensures
            match result {
                Ok(bytes) => {
                    let true_bytes = self.view(wrpm).committed().subrange(
                        relative_addr as int,
                        relative_addr + S::spec_size_of(),
                    );
                    if self.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    } else {
                        let absolute_addrs = Seq::<int>::new(S::spec_size_of() as nat, |i: int| relative_addr + self.start() + i);
                        maybe_corrupted(bytes@, true_bytes, absolute_addrs)
                    }
                }
                Err(e) => e == PmemError::AccessOutOfRange
            }
    {
        self.read_absolute_aligned(wrpm, relative_addr + self.start_, Ghost(true_val), Tracked(perm))
    }

    pub exec fn read_absolute_aligned<'a, S, Perm, PMRegion>(
        self: &Self,
        wrpm: &'a WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        absolute_addr: u64,
        Ghost(true_val): Ghost<S>,
        Tracked(perm): Tracked<&Perm>,
    ) -> (result: Result<MaybeCorruptedBytes<S>, PmemError>)
        where
            S: PmCopy + Sized,
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(wrpm, perm),
            self.start() <= absolute_addr,
            absolute_addr < absolute_addr + S::spec_size_of() <= self.end(),
            self.view(wrpm).no_outstanding_writes_in_range(
                absolute_addr - self.start(),
                absolute_addr + S::spec_size_of() - self.start(),
            ),
            self.view(wrpm).committed().subrange(absolute_addr - self.start(), absolute_addr + S::spec_size_of() - self.start()) == true_val.spec_to_bytes(),
        ensures
            match result {
                Ok(bytes) => {
                    let true_bytes = self.view(wrpm).committed().subrange(
                        absolute_addr - self.start(),
                        absolute_addr + S::spec_size_of() - self.start()
                    );
                    if self.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    } else {
                        let absolute_addrs = Seq::<int>::new(S::spec_size_of() as nat, |i: int| absolute_addr + i);
                        maybe_corrupted(bytes@, true_bytes, absolute_addrs)
                    }
                }
                Err(e) => e == PmemError::AccessOutOfRange
            }
    {
        let ghost true_bytes1 = self.view(wrpm).committed().subrange(
            absolute_addr - self.start(),
            absolute_addr + S::spec_size_of() - self.start(),
        );
        let ghost true_bytes2 = wrpm@.committed().subrange(
            absolute_addr as int,
            absolute_addr + S::spec_size_of()
        );
        assert(true_bytes1 =~= true_bytes2);
        assert forall |i| #![trigger wrpm@.state[i]]
                   absolute_addr <= i < absolute_addr + S::spec_size_of() implies
                   wrpm@.state[i].outstanding_write.is_none() by {
            assert(wrpm@.state[i] == self.view(wrpm).state[i - self.start()]);
        }

        wrpm.get_pm_region_ref().read_aligned::<S>(absolute_addr, Ghost(true_val))
    }

    pub exec fn write_relative<Perm, PMRegion>(
        self: &Self,
        wrpm: &mut WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        relative_addr: u64,
        bytes: &[u8],
        Tracked(perm): Tracked<&Perm>,
    )
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(old::<&mut _>(wrpm), perm),
            relative_addr + bytes@.len() <= self.view(old::<&mut _>(wrpm)).len(),
            self.view(old::<&mut _>(wrpm)).no_outstanding_writes_in_range(relative_addr as int,
                                                                        relative_addr + bytes.len()),
            forall |i: int| relative_addr <= i < relative_addr + bytes@.len() ==> self.is_writable_relative_addr(i),
        ensures
            self.inv(wrpm, perm),
            self.view(wrpm) == self.view(old::<&mut _>(wrpm)).write(relative_addr as int, bytes@),
    {
        let ghost subregion_view = self.view(wrpm).write(relative_addr as int, bytes@);
        assert(forall |addr| #![trigger self.is_writable_absolute_addr_fn()(addr)]
                   !self.is_writable_absolute_addr_fn()(addr) ==> !self.is_writable_relative_addr(addr - self.start()));
        assert forall |i| #![trigger wrpm@.state[i]]
                   relative_addr + self.start_ <= i < relative_addr + self.start_ + bytes@.len() implies
                   wrpm@.state[i].outstanding_write.is_none() by {
            assert(wrpm@.state[i] == self.view(wrpm).state[i - self.start()]);
        }
        assert forall |alt_crash_state| wrpm@.write(relative_addr + self.start_, bytes@).can_crash_as(alt_crash_state)
                   implies perm.check_permission(alt_crash_state) by {
            let alt_region_view = wrpm@.write(relative_addr + self.start_, bytes@);
            assert(alt_region_view.len() == wrpm@.len());
            assert forall |addr: int| {
                       ||| 0 <= addr < self.start()
                       ||| self.start() + self.len() <= addr < alt_region_view.len()
                       ||| self.start() <= addr < self.end() && !self.is_writable_absolute_addr_fn()(addr)
                   } implies self.initial_region_view().state[addr] == #[trigger] alt_region_view.state[addr] by {
                assert(!(relative_addr + self.start_ <= addr < relative_addr + self.start_ + bytes@.len()));
                assert(self.initial_region_view().state[addr] == wrpm@.state[addr]);
            }
        }
        wrpm.write(relative_addr + self.start_, bytes, Tracked(perm));
        assert(self.view(wrpm) =~= subregion_view);
    }

    pub exec fn write_absolute<Perm, PMRegion>(
        self: &Self,
        wrpm: &mut WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        absolute_addr: u64,
        bytes: &[u8],
        Tracked(perm): Tracked<&Perm>,
    )
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(old::<&mut _>(wrpm), perm),
            self.start() <= absolute_addr,
            absolute_addr + bytes@.len() <= self.len(),
            self.view(old::<&mut _>(wrpm)).no_outstanding_writes_in_range(
                absolute_addr - self.start(),
                absolute_addr + bytes@.len() - self.start()
            ),
            forall |i: int| absolute_addr <= i < absolute_addr + bytes@.len() ==>
                #[trigger] self.is_writable_absolute_addr_fn()(i),
        ensures
            self.inv(wrpm, perm),
            self.view(wrpm) == self.view(old::<&mut _>(wrpm)).write(absolute_addr - self.start(), bytes@),
    {
        let ghost subregion_view = self.view(wrpm).write(absolute_addr - self.start(), bytes@);
        assert forall |i| #![trigger wrpm@.state[i]]
                   absolute_addr <= i < absolute_addr + bytes@.len() implies
                   wrpm@.state[i].outstanding_write.is_none() by {
            assert(wrpm@.state[i] == self.view(wrpm).state[i - self.start()]);
        }
        wrpm.write(absolute_addr, bytes, Tracked(perm));
        assert(self.view(wrpm) =~= subregion_view);
    }

    pub exec fn serialize_and_write_relative<S, Perm, PMRegion>(
        self: &Self,
        wrpm: &mut WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        relative_addr: u64,
        to_write: &S,
        Tracked(perm): Tracked<&Perm>,
    )
        where
            S: PmCopy + Sized,
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(old::<&mut _>(wrpm), perm),
            relative_addr + S::spec_size_of() <= self.view(old::<&mut _>(wrpm)).len(),
            self.view(old::<&mut _>(wrpm)).no_outstanding_writes_in_range(relative_addr as int,
                                                                        relative_addr + S::spec_size_of()),
            forall |i: int| relative_addr <= i < relative_addr + S::spec_size_of() ==>
                self.is_writable_relative_addr(i),
        ensures
            self.inv(wrpm, perm),
            self.view(wrpm) == self.view(old::<&mut _>(wrpm)).write(relative_addr as int, to_write.spec_to_bytes()),
    {
        let ghost bytes = to_write.spec_to_bytes();
        assert(bytes.len() == S::spec_size_of());
        let ghost subregion_view = self.view(wrpm).write(relative_addr as int, bytes);
        assert(forall |addr| #![trigger self.is_writable_absolute_addr_fn()(addr)]
                   !self.is_writable_absolute_addr_fn()(addr) ==> !self.is_writable_relative_addr(addr - self.start()));
        assert forall |i| #![trigger wrpm@.state[i]]
                   relative_addr + self.start_ <= i < relative_addr + self.start_ + S::spec_size_of() implies
                   wrpm@.state[i].outstanding_write.is_none() by {
            assert(wrpm@.state[i] == self.view(wrpm).state[i - self.start()]);
        }
        assert forall |alt_crash_state| wrpm@.write(relative_addr + self.start_, bytes).can_crash_as(alt_crash_state)
                   implies perm.check_permission(alt_crash_state) by {
            let alt_region_view = wrpm@.write(relative_addr + self.start_, bytes);
            assert(alt_region_view.len() == wrpm@.len());
            assert forall |addr: int| {
                       ||| 0 <= addr < self.start()
                       ||| self.start() + self.len() <= addr < alt_region_view.len()
                       ||| self.start() <= addr < self.end() && !self.is_writable_absolute_addr_fn()(addr)
                   } implies self.initial_region_view().state[addr] == #[trigger] alt_region_view.state[addr] by {
                assert(!(relative_addr + self.start_ <= addr < relative_addr + self.start_ + S::spec_size_of()));
                assert(self.initial_region_view().state[addr] == wrpm@.state[addr]);
            }
        }
        wrpm.serialize_and_write(relative_addr + self.start_, to_write, Tracked(perm));
        assert(self.view(wrpm) =~= subregion_view);
    }

    pub exec fn serialize_and_write_absolute<S, Perm, PMRegion>(
        self: &Self,
        wrpm: &mut WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        absolute_addr: u64,
        to_write: &S,
        Tracked(perm): Tracked<&Perm>,
    )
        where
            S: PmCopy + Sized,
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(old::<&mut _>(wrpm), perm),
            self.start() <= absolute_addr,
            absolute_addr + S::spec_size_of() <= self.len(),
            self.view(old::<&mut _>(wrpm)).no_outstanding_writes_in_range(
                absolute_addr - self.start(),
                absolute_addr + S::spec_size_of() - self.start()
            ),
            forall |i: int| absolute_addr <= i < absolute_addr + S::spec_size_of() ==>
                #[trigger] self.is_writable_absolute_addr_fn()(i),
        ensures
            self.inv(wrpm, perm),
            self.view(wrpm) == self.view(old::<&mut _>(wrpm)).write(absolute_addr - self.start(),
                                                                  to_write.spec_to_bytes()),
    {
        let ghost bytes = to_write.spec_to_bytes();
        // assert(bytes.len() == S::spec_size_of()) by {
        //     S::lemma_auto_serialized_len();
        // }
        let ghost subregion_view = self.view(wrpm).write(absolute_addr - self.start(), bytes);
        assert forall |i| #![trigger wrpm@.state[i]]
                   absolute_addr <= i < absolute_addr + S::spec_size_of() implies
                   wrpm@.state[i].outstanding_write.is_none() by {
            assert(wrpm@.state[i] == self.view(wrpm).state[i - self.start()]);
        }
        wrpm.serialize_and_write(absolute_addr, to_write, Tracked(perm));
        assert(self.view(wrpm) =~= subregion_view);
    }

    pub proof fn lemma_reveal_opaque_inv<Perm, PMRegion>(
        self,
        wrpm: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
        perm: &Perm
    )
        where
            Perm: CheckPermission<Seq<u8>>,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(wrpm, perm),
        ensures
            wrpm.inv(),
            wrpm.constants() == self.constants(),
            wrpm@.len() == self.initial_region_view().len(),
            views_differ_only_where_subregion_allows(self.initial_region_view(), wrpm@, self.start(), self.len(),
                                                     self.is_writable_absolute_addr_fn()),
            self.view(wrpm) == get_subregion_view(wrpm@, self.start(), self.len()),
            forall |addr: int| 0 <= addr < self.len() ==>
                #[trigger] self.view(wrpm).state[addr] == wrpm@.state[addr + self.start()],
    {
    }
}


pub struct PersistentMemorySubregion
{
    start_: u64,
    len_: Ghost<int>,
}

impl PersistentMemorySubregion
{
    pub exec fn new<PMRegion: PersistentMemoryRegion>(
        pm: &PMRegion,
        start: u64,
        Ghost(len): Ghost<int>,
    ) -> (result: Self)
        requires
            pm.inv(),
            start + len <= pm@.len() <= u64::MAX,
        ensures
            result.start() == start,
            result.len() == len,
            result.view(pm) == get_subregion_view(pm@, start as int, len),
    {
        let result = Self{
            start_: start,
            len_: Ghost(len),
        };
        result
    }

    pub closed spec fn start(self) -> int
    {
        self.start_ as int
    }

    pub closed spec fn len(self) -> int
    {
        self.len_@
    }

    pub open spec fn end(self) -> int
    {
        self.start() + self.len()
    }

    pub closed spec fn view<PMRegion: PersistentMemoryRegion>(
        self,
        pm: &PMRegion,
    ) -> PersistentMemoryRegionView
    {
        get_subregion_view(pm@, self.start(), self.len())
    }

    pub open spec fn inv<PMRegion: PersistentMemoryRegion>(
        self,
        pm: &PMRegion,
    ) -> bool
    {
        &&& pm.inv()
        &&& pm@.len() <= u64::MAX
        &&& self.view(pm).len() == self.len()
        &&& self.start() + self.len() <= pm@.len()
    }

    pub exec fn read_relative_unaligned<'a, PMRegion>(
        self: &Self,
        pm: &'a PMRegion,
        relative_addr: u64,
        num_bytes: u64,
    ) ->(result: Result<Vec<u8>, PmemError>)
        where
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(pm),
            relative_addr + num_bytes <= self.len(),
            self.view(pm).no_outstanding_writes_in_range(relative_addr as int, relative_addr + num_bytes),
        ensures
            match result {
                Ok(bytes) => {
                    let true_bytes = self.view(pm).committed().subrange(relative_addr as int, relative_addr + num_bytes);
                    // If the persistent memory region is impervious
                    // to corruption, read returns the last bytes
                    // written. Otherwise, it returns a
                    // possibly-corrupted version of those bytes.
                    if pm.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    }
                    else {
                        // The addresses in `maybe_corrupted` reflect the fact
                        // that we're reading from a subregion at a certain
                        // start.
                        let absolute_addrs = Seq::<int>::new(num_bytes as nat, |i: int| relative_addr + self.start() + i);
                        maybe_corrupted(bytes@, true_bytes, absolute_addrs)
                    }
                }
                Err(e) => e == PmemError::AccessOutOfRange
            }
    {
        self.read_absolute_unaligned(pm, relative_addr + self.start_, num_bytes)
    }

    pub exec fn read_absolute_unaligned<'a, PMRegion>(
        self: &Self,
        pm: &'a PMRegion,
        absolute_addr: u64,
        num_bytes: u64,
    ) -> (result: Result<Vec<u8>, PmemError>)
        where
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(pm),
            self.start() <= absolute_addr,
            absolute_addr + num_bytes <= self.end(),
            self.view(pm).no_outstanding_writes_in_range(
                absolute_addr - self.start(),
                absolute_addr + num_bytes - self.start(),
            ),
        ensures
            match result {
                Ok(bytes) => {
                    let true_bytes = self.view(pm).committed().subrange(
                        absolute_addr - self.start(),
                        absolute_addr + num_bytes - self.start()
                    );
                    // If the persistent memory region is impervious
                    // to corruption, read returns the last bytes
                    // written. Otherwise, it returns a
                    // possibly-corrupted version of those bytes.
                    if pm.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    }
                    else {
                        // The addresses in `maybe_corrupted` reflect the fact
                        // that we're reading from a subregion at a certain
                        // start.
                        let absolute_addrs = Seq::<int>::new(num_bytes as nat, |i: int| absolute_addr + i);
                        maybe_corrupted(bytes@, true_bytes, absolute_addrs)
                    }
                }
                Err(e) => e == PmemError::AccessOutOfRange
            }
    {
        let ghost true_bytes1 = self.view(pm).committed().subrange(
            absolute_addr - self.start(),
            absolute_addr + num_bytes - self.start(),
        );
        let ghost true_bytes2 = pm@.committed().subrange(
            absolute_addr as int,
            absolute_addr + num_bytes
        );
        assert(true_bytes1 =~= true_bytes2);
        assert forall |i| #![trigger pm@.state[i]]
                   absolute_addr <= i < absolute_addr + num_bytes implies
                   pm@.state[i].outstanding_write.is_none() by {
            assert(pm@.state[i] == self.view(pm).state[i - self.start()]);
        }
        pm.read_unaligned(absolute_addr, num_bytes)
    }

    pub exec fn read_relative_aligned<'a, S, PMRegion>(
        self: &Self,
        pm: &'a PMRegion,
        relative_addr: u64,
        Ghost(true_val): Ghost<S>,
    ) -> (result: Result<MaybeCorruptedBytes<S>, PmemError>)
        where
            S: PmCopy + Sized,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(pm),
            relative_addr < relative_addr + S::spec_size_of() <= self.len(),
            self.view(pm).no_outstanding_writes_in_range(
                relative_addr as int,
                relative_addr + S::spec_size_of(),
            ),
            self.view(pm).committed().subrange(relative_addr as int, relative_addr + S::spec_size_of()) == true_val.spec_to_bytes(),
        ensures
            match result {
                Ok(bytes) => {
                    let true_bytes = self.view(pm).committed().subrange(
                        relative_addr as int,
                        relative_addr + S::spec_size_of(),
                    );
                    if pm.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    } else {
                        let absolute_addrs = Seq::<int>::new(S::spec_size_of() as nat, |i: int| relative_addr + self.start() + i);
                        maybe_corrupted(bytes@, true_bytes, absolute_addrs)
                    }
                }
                Err(e) => e == PmemError::AccessOutOfRange
            }
    {
        self.read_absolute_aligned(pm, relative_addr + self.start_, Ghost(true_val))
    }

    pub exec fn read_absolute_aligned<'a, S, PMRegion>(
        self: &Self,
        pm: &'a PMRegion,
        absolute_addr: u64,
        Ghost(true_val): Ghost<S>,
    ) -> (result: Result<MaybeCorruptedBytes<S>, PmemError>)
        where
            S: PmCopy + Sized,
            PMRegion: PersistentMemoryRegion,
        requires
            self.inv(pm),
            self.start() <= absolute_addr,
            absolute_addr < absolute_addr + S::spec_size_of() <= self.end(),
            self.view(pm).no_outstanding_writes_in_range(
                absolute_addr - self.start(),
                absolute_addr + S::spec_size_of() - self.start(),
            ),
            self.view(pm).committed().subrange(absolute_addr - self.start(), absolute_addr + S::spec_size_of() - self.start()) == true_val.spec_to_bytes(),
        ensures
            match result {
                Ok(bytes) => {
                    let true_bytes = self.view(pm).committed().subrange(
                        absolute_addr - self.start(),
                        absolute_addr + S::spec_size_of() - self.start()
                    );
                    if pm.constants().impervious_to_corruption {
                        bytes@ == true_bytes
                    } else {
                        let absolute_addrs = Seq::<int>::new(S::spec_size_of() as nat, |i: int| absolute_addr + i);
                        maybe_corrupted(bytes@, true_bytes, absolute_addrs)
                    }
                }
                Err(e) => e == PmemError::AccessOutOfRange
            }
    {
        let ghost true_bytes1 = self.view(pm).committed().subrange(
            absolute_addr - self.start(),
            absolute_addr + S::spec_size_of() - self.start(),
        );
        let ghost true_bytes2 = pm@.committed().subrange(
            absolute_addr as int,
            absolute_addr + S::spec_size_of()
        );
        assert(true_bytes1 =~= true_bytes2);
        assert forall |i| #![trigger pm@.state[i]]
                   absolute_addr <= i < absolute_addr + S::spec_size_of() implies
                   pm@.state[i].outstanding_write.is_none() by {
            assert(pm@.state[i] == self.view(pm).state[i - self.start()]);
        }

        pm.read_aligned::<S>(absolute_addr, Ghost(true_val))
    }
}

}

================
File: ./storage_node/src/multilog/layout_v.rs
================

//! This file describes the persistent-memory layout used by the
//! multilog implementation.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.
//!
//! Each persistent-memory region used to store a log will have the following layout.
//!
//! Global metadata:   Metadata whose length is constant across all versions and
//!                    the same for each region/log
//! Region metadata:   Per-region metadata that does not change over the course
//!                    of execution.
//! Log metadata:      Per-log metadata that changes as the data changes, so it
//!                    has two versions and a corruption-detecting boolean
//!                    distinguishing which of those two versions is active
//! Log area:          Area where log is written
//!
//! Only the first region's corruption-detecting boolean is used, and
//! it dictates which log metadata is used on *all* regions. The
//! corruption-detecting boolean on all other regions is ignored.
//!
//! Global metadata (absolute offsets):
//!   bytes 0..8:     Version number of the program that created this metadata
//!   bytes 8..16:    Length of region metadata, not including CRC
//!   bytes 16..32:   Program GUID for this program  
//!   bytes 32..40:   CRC of the above 32 bytes
//!
//! Region metadata (absolute offsets):
//!   bytes 40..44:   Number of logs in the multilog
//!   bytes 44..48:   Index of this log in the multilog
//!   bytes 48..56:   Unused padding bytes
//!   bytes 56..64:   This region's size
//!   bytes 64..72:   Length of log area (LoLA)
//!   bytes 72..88:   Multilog ID
//!   bytes 88..96:   CRC of the above 48 bytes
//!
//! Log metadata (relative offsets):
//!   bytes 0..8:     Log length
//!   bytes 8..16:    Unused padding bytes
//!   bytes 16..32:   Log head virtual position
//!   bytes 32..40:   CRC of the above 32 bytes
//!
//! Log area (relative offsets):
//!   bytes 0..LoLA:   Byte #n is the one whose virtual log position modulo LoLA is n
//!
//! The log area starts at absolute offset 256 to improve Intel Optane DC PMM performance.
//!
//! The way the corruption-detecting boolean (CDB) detects corruption
//! is as follows. To write a CDB to persistent memory, we store one
//! of two eight-byte values: `CDB_FALSE` or `CDB_TRUE`. These are
//! sufficiently different from one another that each is extremely
//! unlikely to be corrupted to become the other. So, if corruption
//! happens, we can detect it by the fact that something other than
//! `CDB_FALSE` or `CDB_TRUE` was read.
//!

use crate::multilog::multilogspec_t::{AbstractLogState, AbstractMultiLogState};
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmemutil_v::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::traits_t::*;
use deps_hack::{PmSafe, PmSized};
use builtin::*;
use builtin_macros::*;
use core::fmt::Debug;
use vstd::bytes::*;
use vstd::prelude::*;
use vstd::ptr::*;


verus! {

    /// Constants

    // These constants describe the absolute or relative positions of
    // various parts of the layout.
    // TODO: clean these up
    pub const ABSOLUTE_POS_OF_GLOBAL_METADATA: u64 = 0;
    pub const RELATIVE_POS_OF_GLOBAL_VERSION_NUMBER: u64 = 0;
    pub const RELATIVE_POS_OF_GLOBAL_LENGTH_OF_REGION_METADATA: u64 = 8;
    pub const RELATIVE_POS_OF_GLOBAL_PROGRAM_GUID: u64 = 16;
    pub const LENGTH_OF_GLOBAL_METADATA: u64 = 32;
    pub const ABSOLUTE_POS_OF_GLOBAL_CRC: u64 = 32;

    pub const ABSOLUTE_POS_OF_REGION_METADATA: u64 = 40;
    pub const RELATIVE_POS_OF_REGION_NUM_LOGS: u64 = 0;
    pub const RELATIVE_POS_OF_REGION_WHICH_LOG: u64 = 4;
    pub const RELATIVE_POS_OF_REGION_PADDING: u64 = 8;
    pub const RELATIVE_POS_OF_REGION_REGION_SIZE: u64 = 16;
    pub const RELATIVE_POS_OF_REGION_LENGTH_OF_LOG_AREA: u64 = 24;
    pub const RELATIVE_POS_OF_REGION_MULTILOG_ID: u64 = 32;
    pub const LENGTH_OF_REGION_METADATA: u64 = 48;
    pub const ABSOLUTE_POS_OF_REGION_CRC: u64 = 88;

    pub const ABSOLUTE_POS_OF_LOG_CDB: u64 = 96;
    pub const ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE: u64 = 104;
    pub const ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE: u64 = 144;
    pub const RELATIVE_POS_OF_LOG_LOG_LENGTH: u64 = 0;
    pub const RELATIVE_POS_OF_LOG_PADDING: u64 = 8;
    pub const RELATIVE_POS_OF_LOG_HEAD: u64 = 16;
    pub const LENGTH_OF_LOG_METADATA: u64 = 32;
    pub const ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE: u64 = 136;
    pub const ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE: u64 = 176;
    pub const ABSOLUTE_POS_OF_LOG_AREA: u64 = 256;
    pub const MIN_LOG_AREA_SIZE: u64 = 1;

    // This GUID was generated randomly and is meant to describe the
    // multilog program, even if it has future versions.

    pub const MULTILOG_PROGRAM_GUID: u128 = 0x21b8b4b3c7d140a9abf7e80c07b7f01fu128;

    // The current version number, and the only one whose contents
    // this program can read, is the following:

    pub const MULTILOG_PROGRAM_VERSION_NUMBER: u64 = 1;

    // These structs represent the different levels of metadata.

    #[repr(C)]
    #[derive(PmSized, PmSafe, Copy, Clone, Default)]
    pub struct GlobalMetadata {
        pub version_number: u64,
        pub length_of_region_metadata: u64,
        pub program_guid: u128,
    }
    
    impl PmCopy for GlobalMetadata {}

    #[repr(C)]
    #[derive(PmSized, PmSafe, Copy, Clone, Default)]
    pub struct RegionMetadata {
        pub num_logs: u32,
        pub which_log: u32,
        pub _padding: u64,
        pub region_size: u64,
        pub log_area_len: u64,
        pub multilog_id: u128,
    }
    
    impl PmCopy for RegionMetadata {}

    #[repr(C)]
    #[derive(PmSized, PmSafe, Copy, Clone, Default)]
    pub struct LogMetadata {
        pub log_length: u64,
        pub _padding: u64,
        pub head: u128,
    }
    
    impl PmCopy for LogMetadata {}


    /// Specification functions for extracting metadata from a
    /// persistent-memory region.

    // This function extracts the subsequence of `bytes` that lie
    // between `pos` and `pos + len` inclusive of `pos` but exclusive
    // of `pos + len`.
    pub open spec fn extract_bytes(bytes: Seq<u8>, pos: int, len: int) -> Seq<u8>
    {
        bytes.subrange(pos, pos + len)
    }

    // This function extracts the bytes encoding global metadata from
    // the contents `mem` of a persistent memory region.
    pub open spec fn extract_global_metadata(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of())
    }

    pub open spec fn deserialize_global_metadata(mem: Seq<u8>) -> GlobalMetadata
    {
        let bytes = extract_global_metadata(mem);
        GlobalMetadata::spec_from_bytes(bytes)
    }

    // This function extracts the CRC of the global metadata from the
    // contents `mem` of a persistent memory region.
    pub open spec fn extract_global_crc(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of())
    }

    pub open spec fn deserialize_global_crc(mem: Seq<u8>) -> u64
    {
        let bytes = extract_global_crc(mem);
        u64::spec_from_bytes(bytes)
    }

    // This function extracts the bytes encoding region metadata
    // from the contents `mem` of a persistent memory region.
    pub open spec fn extract_region_metadata(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of())
    }

    pub open spec fn deserialize_region_metadata(mem: Seq<u8>) -> RegionMetadata
    {
        let bytes = extract_region_metadata(mem);
        RegionMetadata::spec_from_bytes(bytes)
    }

    // This function extracts the CRC of the region metadata from the
    // contents `mem` of a persistent memory region.
    pub open spec fn extract_region_crc(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of())
    }

    pub open spec fn deserialize_region_crc(mem: Seq<u8>) -> u64
    {
        let bytes = extract_region_crc(mem);
        u64::spec_from_bytes(bytes)
    }

    // This function extracts the bytes encoding the log metadata's
    // corruption-detecting boolean (i.e., CDB) from the contents
    // `mem` of a persistent memory region.
    pub open spec fn extract_log_cdb(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of())
    }

    // This function extracts the log metadata's corruption-detecting boolean
    // (i.e., CDB) from the contents `mem` of a persistent memory
    // region. It returns an Option<bool> with the following meanings:
    //
    // None -- Corruption was detected when reading the CDB
    // Some(true) -- No corruption was detected and the CDB is true
    // Some(false) -- No corruption was detected and the CDB is false
    //
    pub open spec fn extract_and_parse_log_cdb(mem: Seq<u8>) -> Option<bool>
    {
        let log_cdb = extract_log_cdb(mem);
        if spec_u64_from_le_bytes(log_cdb) == CDB_FALSE {
            Some(false)
        }
        else if spec_u64_from_le_bytes(log_cdb) == CDB_TRUE {
            Some(true)
        }
        else {
            None
        }
    }

    pub open spec fn deserialize_log_cdb(mem: Seq<u8>) -> u64
    {
        let bytes = extract_log_cdb(mem);
        u64::spec_from_bytes(bytes)
    }

    pub open spec fn deserialize_and_check_log_cdb(mem: Seq<u8>) -> Option<bool>
    {
        let log_cdb = deserialize_log_cdb(mem);
        if log_cdb == CDB_FALSE {
            Some(false)
        } else if log_cdb == CDB_TRUE {
            Some(true)
        } else {
            None
        }
    }

    // This function computes where the log metadata will be in a
    // persistent-memory region given the current boolean value `cdb`
    // of the corruption-detecting boolean.
    pub open spec fn get_log_metadata_pos(cdb: bool) -> u64
    {
        if cdb { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE } else { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE }
    }

    // This function computes where the log metadata ends in a
    // persistent-memory region (i.e., the index of the byte just past
    // the end of the log metadata) given the current boolean
    // value `cdb` of the corruption-detecting boolean.
    pub open spec fn get_log_crc_end(cdb: bool) -> u64
    {
        (get_log_metadata_pos(cdb) + LogMetadata::spec_size_of() + u64::spec_size_of()) as u64
    }

    // This function extracts the bytes encoding log metadata from
    // the contents `mem` of a persistent memory region. It needs to
    // know the current boolean value `cdb` of the
    // corruption-detecting boolean because there are two possible
    // places for such metadata.
    pub open spec fn extract_log_metadata(mem: Seq<u8>, cdb: bool) -> Seq<u8>
    {
        let pos = get_log_metadata_pos(cdb);
        extract_bytes(mem, pos as int, LogMetadata::spec_size_of() as int)
    }

    pub open spec fn deserialize_log_metadata(mem: Seq<u8>, cdb: bool) -> LogMetadata
    {
        let bytes = extract_log_metadata(mem, cdb);
        LogMetadata::spec_from_bytes(bytes)
    }

    // This function extracts the CRC of the log metadata from the
    // contents `mem` of a persistent memory region. It needs to know
    // the current boolean value `cdb` of the corruption-detecting
    // boolean because there are two possible places for that CRC.
    pub open spec fn extract_log_crc(mem: Seq<u8>, cdb: bool) -> Seq<u8>
    {
        let pos = if cdb { ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE }
                  else { ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE };
        extract_bytes(mem, pos as int, u64::spec_size_of())
    }

    pub open spec fn deserialize_log_crc(mem: Seq<u8>, cdb: bool) -> u64
    {
        let bytes = extract_log_crc(mem, cdb);
        u64::spec_from_bytes(bytes)
    }

    // This function returns the 4-byte unsigned integer (i.e., u32)
    // encoded at position `pos` in byte sequence `bytes`.
    pub open spec fn parse_u32(bytes: Seq<u8>, pos: int) -> u32
    {
        spec_u32_from_le_bytes(extract_bytes(bytes, pos, 4))
    }

    // This function returns the 8-byte unsigned integer (i.e., u64)
    // encoded at position `pos` in byte sequence `bytes`.
    pub open spec fn parse_u64(bytes: Seq<u8>, pos: int) -> u64
    {
        spec_u64_from_le_bytes(extract_bytes(bytes, pos, 8))
    }

    // This function returns the 16-byte unsigned integer (i.e., u128)
    // encoded at position `pos` in byte sequence `bytes`.
    pub open spec fn parse_u128(bytes: Seq<u8>, pos: int) -> u128
    {
        spec_u128_from_le_bytes(extract_bytes(bytes, pos, 16))
    }

    // This function returns the global metadata encoded as the given
    // bytes `bytes`.
    pub open spec fn parse_global_metadata(bytes: Seq<u8>) -> GlobalMetadata
    {
        let program_guid = parse_u128(bytes, RELATIVE_POS_OF_GLOBAL_PROGRAM_GUID as int);
        let version_number = parse_u64(bytes, RELATIVE_POS_OF_GLOBAL_VERSION_NUMBER as int);
        let length_of_region_metadata = parse_u64(bytes, RELATIVE_POS_OF_GLOBAL_LENGTH_OF_REGION_METADATA as int);
        GlobalMetadata { program_guid, version_number, length_of_region_metadata }
    }

    // This function returns the region metadata encoded as the given
    // bytes `bytes`.
    pub open spec fn parse_region_metadata(bytes: Seq<u8>) -> RegionMetadata
    {
        let region_size = parse_u64(bytes, RELATIVE_POS_OF_REGION_REGION_SIZE as int);
        let multilog_id = parse_u128(bytes, RELATIVE_POS_OF_REGION_MULTILOG_ID as int);
        let num_logs = parse_u32(bytes, RELATIVE_POS_OF_REGION_NUM_LOGS as int);
        let which_log = parse_u32(bytes, RELATIVE_POS_OF_REGION_WHICH_LOG as int);
        let log_area_len = parse_u64(bytes, RELATIVE_POS_OF_REGION_LENGTH_OF_LOG_AREA as int);
        RegionMetadata { region_size, multilog_id, _padding: 0, num_logs, which_log, log_area_len }
    }

    // This function returns the log metadata encoded as the given
    // bytes `bytes`.
    pub open spec fn parse_log_metadata(bytes: Seq<u8>) -> LogMetadata
    {
        let head = parse_u128(bytes, RELATIVE_POS_OF_LOG_HEAD as int);
        let log_length = parse_u64(bytes, RELATIVE_POS_OF_LOG_LOG_LENGTH as int);
        LogMetadata { head, _padding: 0, log_length }
    }

    /// Specification functions for extracting log data from a
    /// persistent-memory region.

    // This function converts a virtual log position (given relative
    // to the virtual log's head) to a memory location (given relative
    // to the beginning of the log area in memory).
    //
    // `pos_relative_to_head` -- the position in the virtual log being
    // asked about, expressed as the number of positions past the
    // virtual head (e.g., if the head is 3 and this is 7, it
    // means position 10 in the virtual log).
    //
    // `head_log_area_offset` -- the offset from the location in the
    // log area in memory containing the head position of the virtual
    // log (e.g., if this is 3, that means the log's head byte is at
    // address ABSOLUTE_POS_OF_LOG_AREA + 3 in the persistent memory
    // region)
    //
    // `log_area_len` -- the length of the log area in memory
    pub open spec fn relative_log_pos_to_log_area_offset(
        pos_relative_to_head: int,
        head_log_area_offset: int,
        log_area_len: int
    ) -> int
    {
        let log_area_offset = head_log_area_offset + pos_relative_to_head;
        if log_area_offset >= log_area_len {
            log_area_offset - log_area_len
        }
        else {
            log_area_offset
        }
    }

    // This function extracts the virtual log from the contents of a
    // persistent-memory region.
    //
    // `mem` -- the contents of the persistent-memory region
    //
    // `log_area_len` -- the size of the log area in that region
    //
    // `head` -- the virtual log position of the head
    //
    // `log_length` -- the current length of the virtual log past the
    // head
    pub open spec fn extract_log(mem: Seq<u8>, log_area_len: int, head: int, log_length: int) -> Seq<u8>
    {
        let head_log_area_offset = head % log_area_len;
        Seq::<u8>::new(log_length as nat, |pos_relative_to_head: int| mem[ABSOLUTE_POS_OF_LOG_AREA +
            relative_log_pos_to_log_area_offset(pos_relative_to_head, head_log_area_offset, log_area_len)])
    }

    /// Specification functions for recovering data and metadata from
    /// persistent memory after a crash

    // This function specifies how recovery should treat the contents
    // of a single persistent-memory region as an abstract log state.
    // It only deals with data; it assumes the metadata has already
    // been recovered. Relevant aspects of that metadata are passed in
    // as parameters.
    //
    // `mem` -- the contents of the persistent-memory region
    //
    // `log_area_len` -- the size of the log area in that region
    //
    // `head` -- the virtual log position of the head
    //
    // `log_length` -- the current length of the virtual log past the
    // head
    //
    // Returns an `Option<AbstractLogState>` with the following
    // meaning:
    //
    // `None` -- the given metadata isn't valid
    // `Some(s)` -- `s` is the abstract state represented in memory
    pub open spec fn recover_abstract_log_from_region_given_metadata(
        mem: Seq<u8>,
        log_area_len: u64,
        head: u128,
        log_length: u64,
    ) -> Option<AbstractLogState>
    {
        if log_length > log_area_len || head + log_length > u128::MAX
        {
            None
        }
        else {
            Some(AbstractLogState {
                head: head as int,
                log: extract_log(mem, log_area_len as int, head as int, log_length as int),
                pending: Seq::<u8>::empty(),
                capacity: log_area_len as int
            })
        }
    }

    // This function specifies how recovery should treat the contents
    // of a single persistent-memory region as an abstract log state.
    // It assumes the corruption-detecting boolean has already been
    // read and is given by `cdb`.
    //
    // `mem` -- the contents of the persistent-memory region
    //
    // `multilog_id` -- the GUID associated with the multilog when it
    // was initialized
    //
    // `num_logs` -- the number of logs overall in the multilog that
    // this region's log is part of
    //
    // `which_log` -- which log, among the logs in the multilog,
    // that this region stores
    //
    // `cdb` -- what value the corruption-detecting boolean has,
    // according to the metadata in region 0
    //
    // Returns an `Option<AbstractLogState>` with the following
    // meaning:
    //
    // `None` -- the metadata on persistent memory isn't consistent
    // with it having been used as a multilog with the given
    // parameters
    //
    // `Some(s)` -- `s` is the abstract state represented in memory
    pub open spec fn recover_abstract_log_from_region_given_cdb(
        mem: Seq<u8>,
        multilog_id: u128,
        num_logs: int,
        which_log: int,
        cdb: bool
    ) -> Option<AbstractLogState>
    {
        if mem.len() < ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE {
            // To be valid, the memory's length has to be big enough to store at least
            // `MIN_LOG_AREA_SIZE` in the log area.
            None
        }
        else {
            let global_metadata = deserialize_global_metadata(mem);
            let global_crc = deserialize_global_crc(mem);
            if global_crc != global_metadata.spec_crc() {
                // To be valid, the global metadata CRC has to be a valid CRC of the global metadata
                // encoded as bytes.
                None
            }
            else {
                if global_metadata.program_guid != MULTILOG_PROGRAM_GUID {
                    // To be valid, the global metadata has to refer to this program's GUID.
                    // Otherwise, it wasn't created by this program.
                    None
                }
                else if global_metadata.version_number == 1 {
                    // If this metadata was written by version #1 of this code, then this is how to
                    // interpret it:

                    if global_metadata.length_of_region_metadata != RegionMetadata::spec_size_of() {
                        // To be valid, the global metadata's encoding of the region metadata's
                        // length has to be what we expect. (This version of the code doesn't
                        // support any other length of region metadata.)
                        None
                    }
                    else {
                        let region_metadata = deserialize_region_metadata(mem);
                        let region_crc = deserialize_region_crc(mem);
                        if region_crc != region_metadata.spec_crc() {
                            // To be valid, the region metadata CRC has to be a valid CRC of the region
                            // metadata encoded as bytes.
                            None
                        }
                        else {
                            // To be valid, the region metadata's region size has to match the size of the
                            // region given to us. Also, its metadata has to match what we expect
                            // from the list of regions given to us. Finally, there has to be
                            // sufficient room for the log area.
                            if {
                                ||| region_metadata.region_size != mem.len()
                                ||| region_metadata.multilog_id != multilog_id
                                ||| region_metadata.num_logs != num_logs
                                ||| region_metadata.which_log != which_log
                                ||| region_metadata.log_area_len < MIN_LOG_AREA_SIZE
                                ||| mem.len() < ABSOLUTE_POS_OF_LOG_AREA + region_metadata.log_area_len
                            } {
                                None
                            }
                            else {
                                let log_metadata = deserialize_log_metadata(mem, cdb);
                                let log_crc = deserialize_log_crc(mem, cdb);
                                if log_crc != log_metadata.spec_crc() {
                                    // To be valid, the log metadata CRC has to be a valid CRC of the
                                    // log metadata encoded as bytes. (This only applies to the
                                    // "active" log metadata, i.e., the log metadata
                                    // corresponding to the current CDB.)
                                    None
                                }
                                else {
                                    recover_abstract_log_from_region_given_metadata(
                                        mem, region_metadata.log_area_len, log_metadata.head,
                                        log_metadata.log_length)
                                }
                            }
                        }
                    }
                }
                else {
                    // This version of the code doesn't know how to parse metadata for any other
                    // versions of this code besides 1. If we reach this point, we're presumably
                    // reading metadata written by a future version of this code, which we can't
                    // interpret.
                    None
                }
            }
        }
    }

    // This function specifies how recovery should treat the contents
    // of a sequence of persistent memory regions as an abstract
    // multilog state. It assumes the corruption-detecting boolean has
    // already been read and is given by `cdb`.
    //
    // `mems` -- the contents of the sequence of persistent memory
    // regions, i.e., a sequence of sequences of bytes, with one
    // sequence of bytes per persistent-memory region
    //
    // `multilog_id` -- the GUID associated with the multilog when it
    // was initialized
    //
    // `cdb` -- what value the corruption-detecting boolean has,
    // according to the metadata in region 0
    //
    // Returns an `Option<AbstractMultiLogState>` with the following
    // meaning:
    //
    // `None` -- the metadata on persistent memory isn't consistent
    // with it having been used as a multilog with the given
    // parameters
    //
    // `Some(s)` -- `s` is the abstract state represented in memory
    pub open spec fn recover_given_cdb(
        mems: Seq<Seq<u8>>,
        multilog_id: u128,
        cdb: bool
    ) -> Option<AbstractMultiLogState>
    {
        // For each region, use `recover_abstract_log_from_region_given_cdb` to recover it.  One of
        // the parameters to that function is `which_log`, which we fill in with the index of the
        // memory region within the sequence `mems`.
        let seq_option = mems.map(|idx, c| recover_abstract_log_from_region_given_cdb(c, multilog_id, mems.len() as int,
                                                                                      idx, cdb));

        // If any of those recoveries failed, fail this recovery. Otherwise, amass all the recovered
        // `AbstractLogState` values into a sequence to construct an `AbstractMultiLogState`.
        if forall |i| 0 <= i < seq_option.len() ==> seq_option[i].is_Some() {
            Some(AbstractMultiLogState{ states: seq_option.map(|_idx, ot: Option<AbstractLogState>| ot.unwrap()) })
        }
        else {
            None
        }
    }

    // This function specifies how recovery should recover the
    // corruption-detecting boolean. The input `mem` is the contents
    // of region #0 of the persistent memory regions, since the CDB is
    // only stored there.
    //
    // Returns an `Option<bool>` with the following meaning:
    //
    // `None` -- the metadata on this region isn't consistent
    // with it having been used as a multilog
    //
    // `Some(cdb)` -- `cdb` is the corruption-detecting boolean
    pub open spec fn recover_cdb(mem: Seq<u8>) -> Option<bool>
    {
        if mem.len() < ABSOLUTE_POS_OF_REGION_METADATA {
            // If there isn't space in memory to store the global metadata
            // and CRC, then this region clearly isn't a valid multilog
            // region #0.
            None
        }
        else {
            let global_metadata = deserialize_global_metadata(mem);
            let global_crc = deserialize_global_crc(mem);
            if global_crc != global_metadata.spec_crc() {
                // To be valid, the global metadata CRC has to be a valid CRC of the global metadata
                // encoded as bytes.
                None
            }
            else {
                if global_metadata.program_guid != MULTILOG_PROGRAM_GUID {
                    // To be valid, the global metadata has to refer to this program's GUID.
                    // Otherwise, it wasn't created by this program.
                    None
                }
                else if global_metadata.version_number == 1 {
                    // If this metadata was written by version #1 of this code, then this is how to
                    // interpret it:

                    if mem.len() < ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of() {
                        // If memory isn't big enough to store the CDB, then this region isn't
                        // valid.
                        None
                    }
                    else {
                        // Extract and parse the log metadata CDB
                        deserialize_and_check_log_cdb(mem)
                    }
                }
                else {
                    // This version of the code doesn't know how to parse metadata for any other
                    // versions of this code besides 1. If we reach this point, we're presumably
                    // reading metadata written by a future version of this code, which we can't
                    // interpret.
                    None
                }
            }
        }
    }

    // This function specifies how recovery should treat the contents
    // of a sequence of persistent-memory regions as an abstract
    // multilog state.
    //
    // `mems` -- the contents of the persistent memory regions, i.e.,
    // a sequence of sequences of bytes, with one sequence of bytes
    // per persistent-memory region
    //
    // `multilog_id` -- the GUID associated with the multilog when it
    // was initialized
    //
    // Returns an `Option<AbstractMultiLogState>` with the following
    // meaning:
    //
    // `None` -- the metadata on persistent memory isn't consistent
    // with it having been used as a multilog with the given multilog
    // ID
    //
    // `Some(s)` -- `s` is the abstract state represented in memory
    pub open spec fn recover_all(mems: Seq<Seq<u8>>, multilog_id: u128) -> Option<AbstractMultiLogState>
    {
        if mems.len() < 1 || mems.len() > u32::MAX {
            // There needs to be at least one region for it to be
            // valid, and there can't be more regions than can fit in
            // a u32.
            None
        }
        else {
            // To recover, first recover the CDB from region #0, then
            // use it to recover the abstract state from all the
            // regions (including region #0).
            match recover_cdb(mems[0]) {
                Some(cdb) => recover_given_cdb(mems, multilog_id, cdb),
                None => None
            }
        }
    }

    /// Useful utility proofs about layout that other files use.

    // This lemma establishes that if a persistent memory regions view
    // `pm_regions_view` has no outstanding writes, and if its committed byte
    // sequence recovers to abstract state `state`, then any state
    // `pm_regions_view` can crash into also recovers that same abstract state.
    pub proof fn lemma_if_no_outstanding_writes_then_can_only_crash_as_state(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        state: AbstractMultiLogState,
    )
        requires
            pm_regions_view.no_outstanding_writes(),
            recover_all(pm_regions_view.committed(), multilog_id) == Some(state),
        ensures
            forall |s| #[trigger] pm_regions_view.can_crash_as(s) ==> recover_all(s, multilog_id) == Some(state)
    {
        // This follows trivially from the observation that the only
        // byte sequence `pm_regions_view` can crash into is its committed byte
        // sequence. (It has no outstanding writes, so there's nothing
        // else it could crash into.)
        lemma_if_no_outstanding_writes_then_persistent_memory_regions_view_can_only_crash_as_committed(pm_regions_view);
    }

    // This lemma establishes that if persistent memory regions'
    // contents `mems` can successfully be recovered from, then each
    // of its regions has size large enough to hold at least
    // `MIN_LOG_AREA_SIZE` bytes in its log area.
    pub proof fn lemma_recover_all_successful_implies_region_sizes_sufficient(mems: Seq<Seq<u8>>, multilog_id: u128)
        requires
            recover_all(mems, multilog_id).is_Some()
        ensures
            forall |i| 0 <= i < mems.len() ==> #[trigger] mems[i].len() >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
    {
        assert forall |i| 0 <= i < mems.len() implies
                   #[trigger] mems[i].len() >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE by
        {
            let cdb = recover_cdb(mems[0]).get_Some_0();
            let recovered_mems = mems.map(|idx, c| recover_abstract_log_from_region_given_cdb(
                c, multilog_id, mems.len() as int, idx, cdb));
            // We have to mention `recovered_mems[i]` to trigger the `forall` in `recover_given_cdb`
            // and thereby learn that it's Some. Everything we need follows easily from that.
            assert(recovered_mems[i].is_Some());
        }
    }

    // This lemma establishes that for any `i` and `n`, if
    //
    // `forall |k| 0 <= k < n ==> mem1[i+k] == mem2[i+k]`
    //
    // holds, then
    //
    // `extract_bytes(mem1, i, n) == mem2.extract_bytes(mem2, i, n)`
    //
    // also holds.
    //
    // This is an obvious fact, so the body of the lemma is
    // empty. Nevertheless, the lemma is useful because it establishes
    // a trigger. Specifically, it hints Z3 that whenever Z3 is
    // thinking about two terms `extract_bytes(mem1, i, n)` and
    // `extract_bytes(mem2, i, n)` where `mem1` and `mem2` are the
    // specific memory byte sequences passed to this lemma, Z3 should
    // also think about this lemma's conclusion. That is, it should
    // try to prove that
    //
    // `forall |k| 0 <= k < n ==> mem1[i+k] == mem2[i+k]`
    //
    // and, whenever it can prove that, conclude that
    //
    // `extract_bytes(mem1, i, n) == mem2.extract_bytes(mem2, i, n)`
    pub proof fn lemma_establish_extract_bytes_equivalence(
        mem1: Seq<u8>,
        mem2: Seq<u8>,
    )
        ensures
            forall |i: int, n: int| extract_bytes(mem1, i, n) =~= extract_bytes(mem2, i, n) ==>
                #[trigger] extract_bytes(mem1, i, n) == #[trigger] extract_bytes(mem2, i, n)
    {
    }

    pub proof fn lemma_same_bytes_same_deserialization<S>(mem1: Seq<u8>, mem2: Seq<u8>)
        where
            S: PmCopy + Sized
        ensures
            forall |i: int, n: int| extract_bytes(mem1, i, n) =~= extract_bytes(mem2, i, n) ==>
                S::spec_from_bytes(#[trigger] extract_bytes(mem1, i, n)) == S::spec_from_bytes(#[trigger] extract_bytes(mem2, i, n))
    {}

    // This lemma establishes that if the given persistent memory
    // regions' contents can be recovered to a valid abstract state,
    // then that abstract state is unaffected by
    // `drop_pending_appends`.
    pub proof fn lemma_recovered_state_is_crash_idempotent(mems: Seq<Seq<u8>>, multilog_id: u128)
        requires
            recover_all(mems, multilog_id).is_Some()
        ensures
            ({
                let state = recover_all(mems, multilog_id).unwrap();
                state == state.drop_pending_appends()
            })
    {
        let state = recover_all(mems, multilog_id).unwrap();
        assert forall |which_log: int| #![trigger state[which_log]] 0 <= which_log < state.num_logs()
            implies state[which_log].pending.len() == 0 by {
        }
        assert(state =~= state.drop_pending_appends());
    }
}
================
File: ./storage_node/src/multilog/append_v.rs
================

//! This file contains lemmas about tentatively appending to a
//! multilog.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::multilog::inv_v::*;
use crate::multilog::layout_v::*;
use crate::multilog::multilogimpl_v::LogInfo;
use crate::multilog::multilogspec_t::AbstractMultiLogState;
use crate::pmem::pmemspec_t::PersistentMemoryRegionsView;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {

    // This lemma establishes useful facts about performing a
    // contiguous write to effect a tentative append:
    //
    // 1) It's permitted because a crash after the write is initiated
    //    doesn't affect the post-recovery abstract state.
    //
    // 2) It maintains invariants, if `infos` and `state` are updated
    //    in a certain way.
    //
    // Parameters:
    //
    // `pm_regions_view` -- the view of the persistent memory regions
    // before the write
    //
    // `multilog_id` -- the ID of the multilog stored on that memory
    //
    // `num_logs` -- the number of logs in the multilog
    //
    // `which_log` -- which log among the logs is being tentatively
    // appended to
    //
    // `bytes_to_append` -- what bytes are being tentatively appended
    //
    // `cdb` -- the current corruption-detecting boolean value
    //
    // `prev_infos` -- the pre-append `infos` value
    //
    // `prev_state` -- the pre-append abstract state
    pub proof fn lemma_tentatively_append(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        num_logs: u32,
        which_log: u32,
        bytes_to_append: Seq<u8>,
        cdb: bool,
        prev_infos: Seq<LogInfo>,
        prev_state: AbstractMultiLogState,
    )
        requires
            is_valid_log_index(which_log, num_logs),
            memory_matches_deserialized_cdb(pm_regions_view, cdb),
            each_metadata_consistent_with_info(pm_regions_view, multilog_id, num_logs, cdb, prev_infos),
            each_info_consistent_with_log_area(pm_regions_view, num_logs, prev_infos, prev_state),
            ({
                let prev_info = prev_infos[which_log as int];
                let log_area_len = prev_info.log_area_len;
                let num_bytes = bytes_to_append.len();
                let max_len_without_wrapping = log_area_len -
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int, log_area_len as int);
                &&& 0 < num_bytes <= max_len_without_wrapping // no wrapping is necessary
                &&& prev_info.log_plus_pending_length + num_bytes <= log_area_len
                &&& prev_info.head + prev_info.log_plus_pending_length + num_bytes <= u128::MAX
            })
        ensures
            ({
                let prev_info = prev_infos[which_log as int];
                let log_area_len = prev_info.log_area_len;
                let num_bytes = bytes_to_append.len();
                let max_len_without_wrapping = log_area_len -
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int, log_area_len as int);
                // This is how you should update `infos`
                let new_infos = prev_infos.update(which_log as int, LogInfo{
                    log_plus_pending_length: (prev_info.log_plus_pending_length + num_bytes) as u64,
                    ..prev_infos[which_log as int]
                });
                // This is how you should update `state`
                let new_state = prev_state.tentatively_append(which_log as int, bytes_to_append);
                let write_addr = ABSOLUTE_POS_OF_LOG_AREA +
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int,
                                                        log_area_len as int);
                let pm_regions_view2 = pm_regions_view.write(which_log as int, write_addr, bytes_to_append);
                &&& each_metadata_consistent_with_info(pm_regions_view, multilog_id, num_logs, cdb, new_infos)
                // The write doesn't conflict with any outstanding writes
                &&& pm_regions_view.no_outstanding_writes_in_range(which_log as int, write_addr, write_addr + num_bytes)
                &&& memory_matches_deserialized_cdb(pm_regions_view2, cdb)
                &&& each_metadata_consistent_with_info(pm_regions_view2, multilog_id, num_logs, cdb, new_infos)
                &&& each_info_consistent_with_log_area(pm_regions_view2, num_logs, new_infos, new_state)
                &&& new_state.drop_pending_appends() == prev_state.drop_pending_appends()
                // After initiating the write, any crash and recovery will enter the abstract state
                // `new_state.drop_pending_appends()`, so as long as that state is permitted the
                // write will be.
                &&& forall |mem| pm_regions_view2.can_crash_as(mem) ==>
                      recover_all(mem, multilog_id) == Some(new_state.drop_pending_appends())
            }),
    {
        let w = which_log as int;
        let prev_info = prev_infos[w];
        let log_area_len = prev_info.log_area_len;
        let new_infos = prev_infos.update(which_log as int, LogInfo{
            log_plus_pending_length: (prev_info.log_plus_pending_length + bytes_to_append.len()) as u64,
            ..prev_infos[which_log as int]
        });
        let new_state = prev_state.tentatively_append(which_log as int, bytes_to_append);
        let write_addr = ABSOLUTE_POS_OF_LOG_AREA +
            relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                prev_info.head_log_area_offset as int,
                                                log_area_len as int);
        let pm_regions_view2 = pm_regions_view.write(which_log as int, write_addr, bytes_to_append);
        let new_state = prev_state.tentatively_append(w, bytes_to_append);

        // To prove that the post-write metadata is consistent with
        // `new_infos`, we have to reason about the equivalence of
        // extracted byte sequences that match between the old and new
        // metadata regions.

        assert forall |any_log: u32| #[trigger] is_valid_log_index(any_log, num_logs) implies {
            let a = any_log as int;
            metadata_consistent_with_info(pm_regions_view2[a], multilog_id, num_logs, any_log, cdb, new_infos[a])
        } by {
            let a = any_log as int;
            lemma_establish_extract_bytes_equivalence(pm_regions_view[a].committed(), pm_regions_view2[a].committed());
        }

        // To prove that the post-write CDB is the same as the
        // pre-write CDB, we have to reason about the equivalence of
        // extracted byte sequences that match between the old and new
        // region #0, where the CDB is stored.

        assert (memory_matches_deserialized_cdb(pm_regions_view2, cdb)) by {
            assert(is_valid_log_index(0, num_logs));
            lemma_establish_extract_bytes_equivalence(pm_regions_view[0].committed(), pm_regions_view2[0].committed());
        }

        // We need extensional equality to reason that the old and new
        // abstract states are the same after dropping pending appends.

        assert(new_state.drop_pending_appends() =~= prev_state.drop_pending_appends());

        // To prove that all crash states after initiating the write
        // are OK, we just have to invoke the following lemma, which
        // requires that our invariants hold.

        lemma_invariants_imply_crash_recover_forall(pm_regions_view2, multilog_id, num_logs, cdb, new_infos, new_state);

        // To prove that there are no outstanding writes in the range
        // where we plan to write, we need to reason about how
        // addresses in the log area correspond to relative log
        // positions. This is because the invariant talks about
        // relative log positions but we're trying to prove something
        // about addresses in the log area (that there are no
        // outstanding writes to certain of them).

        lemma_addresses_in_log_area_correspond_to_relative_log_positions(pm_regions_view[w], prev_info);
    }

    // This lemma establishes useful facts about performing two
    // contiguous writes, one at the end of the log area and one at
    // the beginning, to effect a tentative append:
    //
    // 1) Each write is permitted because a crash after it's initiated
    // doesn't affect the post-recovery abstract state.
    //
    // 2) The pair of writes maintains invariants, if `infos` and
    // `state` are updated in a certain way.
    //
    // Parameters:
    //
    // `pm_regions_view` -- the view of the persistent memory regions
    // before the write
    //
    // `multilog_id` -- the ID of the multilog stored on that memory
    //
    // `num_logs` -- the number of logs in the multilog
    //
    // `which_log` -- which log among the logs is being tentatively
    // appended to
    //
    // `bytes_to_append` -- what bytes are being tentatively appended
    //
    // `cdb` -- the current corruption-detecting boolean value
    //
    // `prev_infos` -- the pre-append `infos` value
    //
    // `prev_state` -- the pre-append abstract state
    pub proof fn lemma_tentatively_append_wrapping(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        num_logs: u32,
        which_log: u32,
        bytes_to_append: Seq<u8>,
        cdb: bool,
        prev_infos: Seq<LogInfo>,
        prev_state: AbstractMultiLogState,
    )
        requires
            is_valid_log_index(which_log, num_logs),
            memory_matches_deserialized_cdb(pm_regions_view, cdb),
            each_metadata_consistent_with_info(pm_regions_view, multilog_id, num_logs, cdb, prev_infos),
            each_info_consistent_with_log_area(pm_regions_view, num_logs, prev_infos, prev_state),
            ({
                let w = which_log as int;
                let prev_info = prev_infos[w];
                let log_area_len = prev_info.log_area_len;
                let num_bytes = bytes_to_append.len();
                let max_len_without_wrapping = log_area_len -
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int, log_area_len as int);
                &&& num_bytes > max_len_without_wrapping // wrapping is required
                &&& prev_info.head + prev_info.log_plus_pending_length + num_bytes <= u128::MAX
                &&& num_bytes <= log_area_len - prev_info.log_plus_pending_length
            }),
        ensures
            ({
                let w = which_log as int;
                let prev_info = prev_infos[w];
                let log_area_len = prev_info.log_area_len;
                let max_len_without_wrapping = log_area_len -
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int, log_area_len as int);
                let new_infos = prev_infos.update(w, LogInfo{
                    log_plus_pending_length: (prev_info.log_plus_pending_length + bytes_to_append.len()) as u64,
                    ..prev_infos[w]
                });
                let new_state = prev_state.tentatively_append(w, bytes_to_append);
                let bytes_to_append_part1 = bytes_to_append.subrange(0, max_len_without_wrapping as int);
                let bytes_to_append_part2 = bytes_to_append.subrange(max_len_without_wrapping as int,
                                                                     bytes_to_append.len() as int);
                let write_addr = ABSOLUTE_POS_OF_LOG_AREA +
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int,
                                                        log_area_len as int);
                let pm_regions_view2 = pm_regions_view.write(w, write_addr, bytes_to_append_part1);
                let pm_regions_view3 = pm_regions_view2.write(w, ABSOLUTE_POS_OF_LOG_AREA as int, bytes_to_append_part2);
                &&& each_metadata_consistent_with_info(pm_regions_view, multilog_id, num_logs, cdb, new_infos)
                // The first write doesn't conflict with any outstanding writes
                &&& pm_regions_view.no_outstanding_writes_in_range(w, write_addr,
                                                                 write_addr + bytes_to_append_part1.len())
                // The second write also doesn't conflict with any outstanding writes
                &&& pm_regions_view2.no_outstanding_writes_in_range(
                       w,
                       ABSOLUTE_POS_OF_LOG_AREA as int,
                       ABSOLUTE_POS_OF_LOG_AREA + bytes_to_append_part2.len())
                &&& each_metadata_consistent_with_info(pm_regions_view3, multilog_id, num_logs, cdb, new_infos)
                &&& each_info_consistent_with_log_area(pm_regions_view3, num_logs, new_infos, new_state)
                &&& memory_matches_deserialized_cdb(pm_regions_view3, cdb)
                &&& new_state.drop_pending_appends() == prev_state.drop_pending_appends()
                // After initiating the first write, any crash and recovery will enter the abstract
                // state `prev_state.drop_pending_appends()`, so as long as that state is permitted
                // the write will be.
                &&& forall |mem| pm_regions_view2.can_crash_as(mem) ==>
                       recover_all(mem, multilog_id) == Some(prev_state.drop_pending_appends())
                // After initiating the second write, any crash and recovery will enter the abstract
                // state `prev_state.drop_pending_appends()`, so as long as that state is permitted
                // the write will be.
                &&& forall |mem| pm_regions_view3.can_crash_as(mem) ==>
                       recover_all(mem, multilog_id) == Some(prev_state.drop_pending_appends())
            }),
    {
        let w = which_log as int;
        let prev_info = prev_infos[w];
        let log_area_len = prev_info.log_area_len;
        let max_len_without_wrapping = log_area_len -
            relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                prev_info.head_log_area_offset as int, log_area_len as int);
        let bytes_to_append_part1 = bytes_to_append.subrange(0, max_len_without_wrapping as int);
        let bytes_to_append_part2 = bytes_to_append.subrange(max_len_without_wrapping as int,
                                                             bytes_to_append.len() as int);
        let intermediate_infos = prev_infos.update(w, LogInfo{
            log_plus_pending_length: (prev_info.log_plus_pending_length + max_len_without_wrapping) as u64,
            ..prev_infos[w]
        });
        let intermediate_state = prev_state.tentatively_append(w, bytes_to_append_part1);
        let new_infos = prev_infos.update(w, LogInfo{
            log_plus_pending_length: (prev_info.log_plus_pending_length + bytes_to_append.len()) as u64,
            ..prev_infos[w]
        });
        let new_state = prev_state.tentatively_append(w, bytes_to_append);
        let write_addr = ABSOLUTE_POS_OF_LOG_AREA +
            relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                prev_info.head_log_area_offset as int,
                                                log_area_len as int);
        let pm_regions_view2 = pm_regions_view.write(w, write_addr, bytes_to_append_part1);
        let pm_regions_view3 = pm_regions_view2.write(w, ABSOLUTE_POS_OF_LOG_AREA as int, bytes_to_append_part2);

        // Invoke `lemma_tentatively_append` on each write.

        lemma_tentatively_append(pm_regions_view, multilog_id, num_logs, which_log, bytes_to_append_part1, cdb,
                                 prev_infos, prev_state);
        lemma_tentatively_append(pm_regions_view2, multilog_id, num_logs, which_log, bytes_to_append_part2, cdb,
                                 intermediate_infos, intermediate_state);

        // Use extensional equality to prove the equivalence of the
        // intermediate abstract state between writes and the previous
        // state, if both drop pending appends.

        assert(intermediate_state.drop_pending_appends() =~= prev_state.drop_pending_appends());

        // Use extensional equality to prove the equivalence of the new
        // abstract state, which the caller of this lemma cares about, with
        // an update to the intermediate abstract state, which is what the
        // second call to `lemma_tentatively_append` proves things about.

        assert(new_state =~= intermediate_state.tentatively_append(w, bytes_to_append_part2)) by {
            assert(prev_state[w].pending + bytes_to_append_part1 + bytes_to_append_part2 =~=
                   prev_state[w].pending + bytes_to_append);
        }
    }

}

================
File: ./storage_node/src/multilog/multilogimpl_v.rs
================

//! This file contains the implementation of `UntrustedMultiLogImpl`,
//! which implements a provably correct multilog.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::log::inv_v::lemma_auto_smaller_range_of_seq_is_subrange;
use crate::log::inv_v::lemma_metadata_matches_implies_metadata_types_set;
use crate::multilog::append_v::*;
use crate::multilog::inv_v::*;
use crate::multilog::layout_v::*;
use crate::multilog::multilogimpl_t::*;
use crate::multilog::multilogspec_t::AbstractMultiLogState;
use crate::multilog::setup_v::{
    check_for_required_space, compute_log_capacities, write_setup_metadata_to_all_regions,
};
use crate::multilog::start_v::{read_cdb, read_logs_variables};
use crate::pmem::crc_t::*;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmemutil_v::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::wrpm_t::*;
use crate::pmem::traits_t::size_of;
use builtin::*;
use builtin_macros::*;
use vstd::arithmetic::div_mod::*;
use vstd::bytes::*;
use vstd::prelude::*;
use vstd::slice::*;

verus! {

    // This structure, `LogInfo`, is used by `UntrustedMultiLogImpl`
    // to store information about a single log. Its fields are:
    //
    // `log_area_len` -- how many bytes are in the log area on
    //     persistent memory
    //
    // `head` -- the logical position of the log's head
    //
    // `head_log_area_offset` -- the offset into the log area
    //     holding the byte at the head position. This is
    //     always equal to `head % log_area_len`, and is
    //     cached in this variable to avoid expensive modulo
    //     operations.
    //
    // `log_length` -- the number of bytes in the log beyond the head
    //
    // `log_plus_pending_length` -- the number of bytes in the log and
    //     the pending appends to the log combined
    pub struct LogInfo {
        pub log_area_len: u64,
        pub head: u128,
        pub head_log_area_offset: u64,
        pub log_length: u64,
        pub log_plus_pending_length: u64,
    }

    // This structure, `UntrustedMultiLogImpl`, implements a
    // multilog. Its fields are:
    //
    // `num_logs` -- the number of logs in the multilog
    // `cdb` -- the current value of the corruption-detecting boolean
    // `infos` -- a vector of `LogInfo`s, one per log
    // `state` -- the abstract view of the multilog
    pub struct UntrustedMultiLogImpl {
        num_logs: u32,
        cdb: bool,
        infos: Vec<LogInfo>,
        state: Ghost<AbstractMultiLogState>
    }

    impl UntrustedMultiLogImpl
    {
        // This static function specifies how multiple regions'
        // contents should be viewed upon recovery as an abstract
        // multilog state.
        pub closed spec fn recover(mems: Seq<Seq<u8>>, multilog_id: u128) -> Option<AbstractMultiLogState>
        {
            if !metadata_types_set(mems) {
                // If the metadata types aren't properly set up, the log is unrecoverable.
                None
            } else {
                recover_all(mems, multilog_id)
            }
        }

        // This method specifies an invariant on `self` that all
        // `UntrustedMultiLogImpl` methods maintain. It requires this
        // invariant to hold on any method invocation, and ensures it
        // in any method invocation that takes `&mut self`.
        //
        // Most of the conjuncts in this invariant are defined in the
        // file `inv_v.rs`. See that file for detailed explanations.
        pub closed spec fn inv<Perm, PMRegions>(
            &self,
            wrpm_regions: &WriteRestrictedPersistentMemoryRegions<Perm, PMRegions>,
            multilog_id: u128,
        ) -> bool
            where
                Perm: CheckPermission<Seq<Seq<u8>>>,
                PMRegions: PersistentMemoryRegions
        {
            &&& wrpm_regions.inv() // whatever the persistent memory regions require as an invariant
            &&& no_outstanding_writes_to_metadata(wrpm_regions@, self.num_logs)
            &&& memory_matches_deserialized_cdb(wrpm_regions@, self.cdb)
            &&& each_metadata_consistent_with_info(wrpm_regions@, multilog_id, self.num_logs, self.cdb, self.infos@)
            &&& each_info_consistent_with_log_area(wrpm_regions@, self.num_logs, self.infos@, self.state@)
            &&& can_only_crash_as_state(wrpm_regions@, multilog_id, self.state@.drop_pending_appends())
            &&& metadata_types_set(wrpm_regions@.committed())
            &&& forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> ABSOLUTE_POS_OF_LOG_AREA < wrpm_regions@[i].len()
        }

        pub proof fn lemma_inv_implies_wrpm_inv<Perm, PMRegions>(
            &self,
            wrpm_regions: &WriteRestrictedPersistentMemoryRegions<Perm, PMRegions>,
            multilog_id: u128
        )
            where
                Perm: CheckPermission<Seq<Seq<u8>>>,
                PMRegions: PersistentMemoryRegions
            requires
                self.inv(wrpm_regions, multilog_id)
            ensures
                wrpm_regions.inv()
        {}

        pub proof fn lemma_inv_implies_can_only_crash_as<Perm, PMRegions>(
            &self,
            wrpm_regions: &WriteRestrictedPersistentMemoryRegions<Perm, PMRegions>,
            multilog_id: u128
        )
            where
                Perm: CheckPermission<Seq<Seq<u8>>>,
                PMRegions: PersistentMemoryRegions
            requires
                self.inv(wrpm_regions, multilog_id)
            ensures
                can_only_crash_as_state(wrpm_regions@, multilog_id, self@.drop_pending_appends())
        {}

        // This function specifies how to view the in-memory state of
        // `self` as an abstract multilog state.
        pub closed spec fn view(&self) -> AbstractMultiLogState
        {
            self.state@
        }

        // The `setup` method sets up persistent memory objects `pm_regions`
        // to store an initial empty multilog. It returns a vector
        // listing the capacities of the logs. See `README.md` for more
        // documentation.
        pub exec fn setup<PMRegions>(
            pm_regions: &mut PMRegions,
            multilog_id: u128,
        ) -> (result: Result<Vec<u64>, MultiLogErr>)
            where
                PMRegions: PersistentMemoryRegions
            requires
                old(pm_regions).inv(),
            ensures
                pm_regions.inv(),
                pm_regions.constants() == old(pm_regions).constants(),
                pm_regions@.no_outstanding_writes(),
                match result {
                    Ok(log_capacities) => {
                        let state = AbstractMultiLogState::initialize(log_capacities@);
                        &&& pm_regions@.len() == old(pm_regions)@.len()
                        &&& pm_regions@.len() >= 1
                        &&& pm_regions@.len() <= u32::MAX
                        &&& log_capacities@.len() == pm_regions@.len()
                        &&& forall |i: int| 0 <= i < pm_regions@.len() ==> #[trigger] log_capacities@[i] <= pm_regions@[i].len()
                        &&& forall |i: int| 0 <= i < pm_regions@.len() ==>
                               #[trigger] pm_regions@[i].len() == old(pm_regions)@[i].len()
                        &&& can_only_crash_as_state(pm_regions@, multilog_id, state)
                        &&& Self::recover(pm_regions@.committed(), multilog_id) == Some(state)
                        &&& Self::recover(pm_regions@.flush().committed(), multilog_id) == Some(state)
                        &&& state == state.drop_pending_appends()
                    },
                    Err(MultiLogErr::InsufficientSpaceForSetup { which_log, required_space }) => {
                        let flushed_regions = old(pm_regions)@.flush();
                        &&& pm_regions@ == flushed_regions
                        &&& pm_regions@[which_log as int].len() < required_space
                    },
                    Err(MultiLogErr::CantSetupWithFewerThanOneRegion { }) => {
                        let flushed_regions = old(pm_regions)@.flush();
                        &&& pm_regions@ == flushed_regions
                        &&& pm_regions@.len() < 1
                    },
                    Err(MultiLogErr::CantSetupWithMoreThanU32MaxRegions { }) => {
                        let flushed_regions = old(pm_regions)@.flush();
                        &&& pm_regions@ == flushed_regions
                        &&& pm_regions@.len() > u32::MAX
                    },
                    _ => false
                }
        {
            let ghost original_pm_regions = pm_regions@;

            // We can't write without proving that there are no
            // outstanding writes where we're writing. So just start
            // out by flushing, so it's clear we can write anywhere.
            //
            // Why can't we write without proving there isn't a
            // conflicting outstanding write, you ask? Two reasons:
            //
            // First, to simplify the specification of how persistent
            // memory behaves, in `pmem::pmemspec_t.rs` we don't specify
            // what happens when there are multiple outstanding writes
            // to the same address. Instead, we just forbid that
            // case.
            //
            // Second, even if we did specify what happened in that
            // case, in this function we have no idea what's already
            // been written. If there were outstanding writes and they
            // got reordered after our writes, the resulting state
            // might be invalid. So we need to flush before writing
            // anything anyway.

            pm_regions.flush();

            // Get the list of region sizes and make sure they support
            // storing a multilog. If not, return an appropriate
            // error.

            let region_sizes = get_region_sizes(pm_regions);
            let num_regions = region_sizes.len();

            if num_regions < 1 {
                return Err(MultiLogErr::CantSetupWithFewerThanOneRegion { });
            }
            if num_regions > u32::MAX as usize {
                return Err(MultiLogErr::CantSetupWithMoreThanU32MaxRegions { });
            }
            let num_logs = num_regions as u32;
            check_for_required_space(&region_sizes, num_logs)?;

            // Compute log capacities so we can return them.

            let log_capacities = compute_log_capacities(&region_sizes);

            // Write setup metadata to all regions.

            write_setup_metadata_to_all_regions(pm_regions, &region_sizes, Ghost(log_capacities@), multilog_id);

            proof {
                // Prove various postconditions about how we can
                // crash. Specifically, (1) we can only crash as
                // `AbstractMultiLogState::initialize(log_capacities@)`,
                // (2) if we recover after flushing then we get that
                // state, and (3) that state has no pending appends.

                let state = AbstractMultiLogState::initialize(log_capacities@);
                assert(state =~= state.drop_pending_appends());
                lemma_if_no_outstanding_writes_then_flush_is_idempotent(pm_regions@);
                lemma_if_no_outstanding_writes_then_persistent_memory_regions_view_can_only_crash_as_committed(
                    pm_regions@);
            }

            Ok(log_capacities)
        }

        // The `start` static method creates an
        // `UntrustedMultiLogImpl` out of a set of persistent memory
        // regions. It's assumed that those regions were initialized
        // with `setup` and then only `UntrustedMultiLogImpl` methods
        // were allowed to mutate them. See `README.md` for more
        // documentation and an example of its use.
        //
        // This method is passed a write-restricted collection of
        // persistent memory regions `wrpm_regions`. This restricts
        // how we can write `wrpm_regions`. This is moot, though,
        // because we don't ever write to the memory.
        pub exec fn start<PMRegions>(
            wrpm_regions: &mut WriteRestrictedPersistentMemoryRegions<TrustedMultiLogPermission, PMRegions>,
            multilog_id: u128,
            Tracked(perm): Tracked<&TrustedMultiLogPermission>,
            Ghost(state): Ghost<AbstractMultiLogState>,
        ) -> (result: Result<Self, MultiLogErr>)
            where
                PMRegions: PersistentMemoryRegions
            requires
                Self::recover(old(wrpm_regions)@.flush().committed(), multilog_id) == Some(state),
                old(wrpm_regions).inv(),
                forall |s| #[trigger] perm.check_permission(s) <==> Self::recover(s, multilog_id) == Some(state),
            ensures
                wrpm_regions.inv(),
                wrpm_regions.constants() == old(wrpm_regions).constants(),
                match result {
                    Ok(log_impl) => {
                        &&& log_impl.inv(wrpm_regions, multilog_id)
                        &&& log_impl@ == state
                        &&& can_only_crash_as_state(wrpm_regions@, multilog_id, state.drop_pending_appends())
                    },
                    Err(MultiLogErr::CRCMismatch) => !wrpm_regions.constants().impervious_to_corruption,
                    Err(MultiLogErr::InsufficientSpaceForSetup { which_log, required_space }) => {
                        let flushed_regions = old(wrpm_regions)@.flush();
                        &&& 0 <= which_log < flushed_regions.len()
                        &&& wrpm_regions@ == flushed_regions
                        &&& wrpm_regions@[which_log as int].len() < required_space
                    },
                    _ => false
                }
        {
            // The invariants demand that there are no outstanding
            // writes to various location. To make sure of this, we
            // flush all memory regions.

            wrpm_regions.flush();

            // Out of paranoia, we check to make sure that the number
            // of regions is sensible. Both cases are technically
            // precluded by the assumptions about how `start` is
            // invoked, since it's assumed the user invokes `start` on
            // a properly set-up collection of persistent memory
            // regions. We check for them anyway in case that
            // assumption doesn't hold.

            let pm_regions = wrpm_regions.get_pm_regions_ref();
            let num_regions = pm_regions.get_num_regions();
            let region_sizes = get_region_sizes(pm_regions);
            if num_regions < 1 {
                assert(false);
                return Err(MultiLogErr::CantSetupWithFewerThanOneRegion { });
            }
            if num_regions > u32::MAX as usize {
                assert(false);
                return Err(MultiLogErr::CantSetupWithMoreThanU32MaxRegions { });
            }
            let num_logs = num_regions as u32;
            check_for_required_space(&region_sizes, num_logs)?;

            // First, we read the corruption-detecting boolean and
            // return an error if that fails.

            let cdb = read_cdb(pm_regions)?;

            // Second, we read the logs variables to store in
            // `infos`. If that fails, we return an error.

            let infos = read_logs_variables(pm_regions, multilog_id, cdb, num_logs, Ghost(state))?;
            proof {
                // We have to prove that we can only crash as the given abstract
                // state with all pending appends dropped. We prove this with two
                // lemmas. The first says that since we've established certain
                // invariants, we can only crash as `state`. The second says that,
                // because this is a recovered state, it's unaffected by dropping
                // all pending appends.

                lemma_invariants_imply_crash_recover_forall(pm_regions@, multilog_id, num_logs, cdb,
                                                            infos@, state);
                lemma_recovered_state_is_crash_idempotent(wrpm_regions@.committed(), multilog_id);
                assert(no_outstanding_writes_to_metadata(wrpm_regions@, num_logs));
                lemma_no_outstanding_writes_to_metadata_implies_no_outstanding_writes_to_active_metadata(wrpm_regions@, num_logs, cdb);

                lemma_metadata_set_after_crash(wrpm_regions@, cdb);
            }
            Ok(Self{ num_logs, cdb, infos, state: Ghost(state) })
        }

        // The `tentatively_append` method tentatively appends
        // `bytes_to_append` to the end of log number `which_log` in
        // the multilog. It's tentative in that crashes will undo the
        // appends, and reads aren't allowed in the tentative part of
        // the log. See `README.md` for more documentation and examples
        // of its use.
        //
        // This method is passed a write-restricted collection of
        // persistent memory regions `wrpm_regions`. This restricts
        // how it can write `wrpm_regions`. It's only given permission
        // (in `perm`) to write if it can prove that any crash after
        // initiating the write is safe. That is, any such crash must
        // put the memory in a state that recovers as the current
        // abstract state with all pending appends dropped.
        pub exec fn tentatively_append<PMRegions>(
            &mut self,
            wrpm_regions: &mut WriteRestrictedPersistentMemoryRegions<TrustedMultiLogPermission, PMRegions>,
            which_log: u32,
            bytes_to_append: &[u8],
            Ghost(multilog_id): Ghost<u128>,
            Tracked(perm): Tracked<&TrustedMultiLogPermission>,
        ) -> (result: Result<u128, MultiLogErr>)
            where
                PMRegions: PersistentMemoryRegions
            requires
                old(self).inv(&*old(wrpm_regions), multilog_id),
                forall |s| #[trigger] perm.check_permission(s) <==>
                    Self::recover(s, multilog_id) == Some(old(self)@.drop_pending_appends()),
            ensures
                self.inv(wrpm_regions, multilog_id),
                wrpm_regions.constants() == old(wrpm_regions).constants(),
                can_only_crash_as_state(wrpm_regions@, multilog_id, self@.drop_pending_appends()),
                match result {
                    Ok(offset) => {
                        let state = old(self)@[which_log as int];
                        &&& which_log < old(self)@.num_logs()
                        &&& offset == state.head + state.log.len() + state.pending.len()
                        &&& self@ == old(self)@.tentatively_append(which_log as int, bytes_to_append@)
                    },
                    Err(MultiLogErr::InvalidLogIndex { }) => {
                        &&& self@ == old(self)@
                        &&& which_log >= self@.num_logs()
                    },
                    Err(MultiLogErr::InsufficientSpaceForAppend { available_space }) => {
                        &&& self@ == old(self)@
                        &&& which_log < self@.num_logs()
                        &&& available_space < bytes_to_append@.len()
                        &&& {
                               let state = self@[which_log as int];
                               ||| available_space == state.capacity - state.log.len() - state.pending.len()
                               ||| available_space == u128::MAX - state.head - state.log.len() - state.pending.len()
                           }
                    },
                    _ => false
                }
        {
            // If an invalid log index was requested, return an error.

            if which_log >= self.num_logs {
                return Err(MultiLogErr::InvalidLogIndex{ });
            }

            // Mention `is_valid_log_index` for `which_log` to trigger
            // various useful `forall`s in invariants and thereby make
            // the verifier aware of important per-log invariants
            // about log number `which_log`.

            assert(is_valid_log_index(which_log, self.num_logs));

            let info = &self.infos[which_log as usize];

            // For instance, one useful invariant we triggered above
            // implies that `info.log_plus_pending_length <=
            // info.log_area_len`, so we know we can safely do the
            // following subtraction without underflow.

            let available_space: u64 = info.log_area_len - info.log_plus_pending_length as u64;

            // Check to make sure we have enough available space, and
            // return an error otherwise. There are two ways we might
            // not have available space. The first is that doing the
            // append would overfill the log area. The second (which
            // will probably never happen) is that doing this append
            // and a subsequent commit would make the logical tail
            // exceed u128::MAX.

            let num_bytes: u64 = bytes_to_append.len() as u64;
            if num_bytes > available_space {
                return Err(MultiLogErr::InsufficientSpaceForAppend{ available_space })
            }
            if num_bytes as u128 > u128::MAX - info.log_plus_pending_length as u128 - info.head {
                return Err(MultiLogErr::InsufficientSpaceForAppend{
                    available_space: (u128::MAX - info.log_plus_pending_length as u128 - info.head) as u64
                })
            }

            // Compute the current logical offset of the end of the
            // log, including any earlier pending appends. This is the
            // offset at which we'll be logically appending, and so is
            // the offset we're expected to return. After all, the
            // caller wants to know what virtual log position they
            // need to use to read this data in the future.

            let old_pending_tail: u128 = info.head + info.log_plus_pending_length as u128;

            let ghost w = which_log as int;
            let ghost state = self.state@[w];

            // The simple case is that we're being asked to append the
            // empty string. If so, do nothing and return.

            if num_bytes == 0 {
                assert(forall |a: Seq<u8>, b: Seq<u8>| b == Seq::<u8>::empty() ==> a + b == a);
                assert(bytes_to_append@ =~= Seq::<u8>::empty());
                assert(self@ =~= old(self)@.tentatively_append(w, bytes_to_append@));
                return Ok(old_pending_tail);
            }

            // If the number of bytes in the log plus pending appends
            // is at least as many bytes as are beyond the head in the
            // log area, there's obviously enough room to append all
            // the bytes without wrapping. So just write the bytes
            // there.

            if info.log_plus_pending_length >= info.log_area_len - info.head_log_area_offset {

                // We could compute the address to write to with:
                //
                // `write_addr = ABSOLUTE_POS_OF_LOG_AREA + old_pending_tail % info.log_area_len;`
                //
                // But we can replace the expensive modulo operation above with two subtraction
                // operations as follows. This is somewhat subtle, but we have verification backing
                // us up and proving this optimization correct.

                let write_addr: u64 = ABSOLUTE_POS_OF_LOG_AREA +
                    info.log_plus_pending_length - (info.log_area_len - info.head_log_area_offset);
                assert(write_addr == ABSOLUTE_POS_OF_LOG_AREA +
                       relative_log_pos_to_log_area_offset(info.log_plus_pending_length as int,
                                                           info.head_log_area_offset as int,
                                                           info.log_area_len as int));

                proof {
                    lemma_tentatively_append(wrpm_regions@, multilog_id, self.num_logs, which_log,
                                             bytes_to_append@, self.cdb, self.infos@, self.state@);

                    // Prove that the metadata types are still set based on the fact that we have not modified any metadata bytes.
                    let wrpm_regions_new = wrpm_regions@.write(which_log as int, write_addr as int, bytes_to_append@);
                    lemma_no_outstanding_writes_to_metadata_implies_no_outstanding_writes_to_active_metadata(wrpm_regions_new, self.num_logs, self.cdb);
                    assert(wrpm_regions@[which_log as int].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) =~= 
                        wrpm_regions_new[which_log as int].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));
                    let log_metadata_pos = get_log_metadata_pos(self.cdb);
                    assert(wrpm_regions@[which_log as int].committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                        wrpm_regions_new[which_log as int].committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()));
                    assert(active_metadata_is_equal(wrpm_regions@, wrpm_regions_new));
                    lemma_regions_metadata_matches_implies_metadata_types_set(wrpm_regions@, wrpm_regions_new, self.cdb);
                    assert(metadata_types_set(wrpm_regions_new.committed()));
                    lemma_metadata_set_after_crash(wrpm_regions_new, self.cdb);

                    assert forall |s| #[trigger] wrpm_regions@.write(which_log as int, write_addr as int, bytes_to_append@).can_crash_as(s) implies
                        #[trigger] perm.check_permission(s) 
                    by {
                        lemma_invariants_imply_crash_recover_forall(wrpm_regions@, multilog_id, 
                            self.num_logs, self.cdb, self.infos@, self.state@);
                        assert(metadata_types_set(s));
                    }
                }
                    
                wrpm_regions.write(which_log as usize, write_addr, bytes_to_append, Tracked(perm));
            }
            else {
                // We could compute the address to write to with:
                //
                // `write_addr = ABSOLUTE_POS_OF_LOG_AREA + old_pending_tail % info.log_area_len`
                //
                // But we can replace the expensive modulo operation above with an addition
                // operation as follows. This is somewhat subtle, but we have verification backing
                // us up and proving this optimization correct.

                let write_addr: u64 = ABSOLUTE_POS_OF_LOG_AREA +
                    info.log_plus_pending_length + info.head_log_area_offset;
                assert(write_addr == ABSOLUTE_POS_OF_LOG_AREA +
                       relative_log_pos_to_log_area_offset(info.log_plus_pending_length as int,
                                                           info.head_log_area_offset as int,
                                                           info.log_area_len as int));

                // There's limited space beyond the pending bytes in the log area, so as we write
                // the bytes we may have to wrap around the end of the log area. So we must compute
                // how many bytes we can write without having to wrap:

                let max_len_without_wrapping: u64 =
                    info.log_area_len - info.head_log_area_offset - info.log_plus_pending_length;
                assert(max_len_without_wrapping == info.log_area_len -
                       relative_log_pos_to_log_area_offset(info.log_plus_pending_length as int,
                                                           info.head_log_area_offset as int, info.log_area_len as int));

                if num_bytes <= max_len_without_wrapping {

                    // If there's room for all the bytes we need to write, we just need one write.

                    proof {
                        lemma_tentatively_append(wrpm_regions@, multilog_id, self.num_logs, which_log,
                                                 bytes_to_append@, self.cdb, self.infos@, self.state@);

                        // Prove that the metadata types are still set based on the fact that we have not modified any metadata bytes.
                        let wrpm_regions_new = wrpm_regions@.write(which_log as int, write_addr as int, bytes_to_append@);
                        lemma_no_outstanding_writes_to_metadata_implies_no_outstanding_writes_to_active_metadata(wrpm_regions_new, self.num_logs, self.cdb);
                        assert(wrpm_regions@[which_log as int].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) =~= 
                            wrpm_regions_new[which_log as int].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));
                        let log_metadata_pos = get_log_metadata_pos(self.cdb);
                        assert(wrpm_regions@[which_log as int].committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                            wrpm_regions_new[which_log as int].committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()));
                        assert(active_metadata_is_equal(wrpm_regions@, wrpm_regions_new));
                        lemma_regions_metadata_matches_implies_metadata_types_set(wrpm_regions@, wrpm_regions_new, self.cdb);
                        assert(metadata_types_set(wrpm_regions_new.committed()));
                        lemma_metadata_set_after_crash(wrpm_regions_new, self.cdb);
                    }
                    wrpm_regions.write(which_log as usize, write_addr, bytes_to_append, Tracked(perm));
                }
                else {

                    // If there isn't room for all the bytes we need to write, we need two writes,
                    // one writing the first `max_len_without_wrapping` bytes to address
                    // `write_addr` and the other writing the remaining bytes to the beginning of
                    // the log area (`ABSOLUTE_POS_OF_LOG_AREA`).
                    //
                    // There are a lot of things we have to prove about these writes, like the fact
                    // that they're both permitted by `perm`. We offload those proofs to a lemma in
                    // `append_v.rs` that we invoke here.

                    proof {
                        lemma_tentatively_append_wrapping(wrpm_regions@, multilog_id, self.num_logs, which_log,
                                                          bytes_to_append@, self.cdb, self.infos@, self.state@);
                    
                        // Prove that the metadata types are still set based on the fact that we have not modified any metadata bytes.
                        // TODO: refactor this into a separate proof fn
                        let wrpm_regions_new = wrpm_regions@.write(which_log as int, write_addr as int, bytes_to_append@.subrange(0, max_len_without_wrapping as int));
                        lemma_establish_extract_bytes_equivalence(wrpm_regions@[0].committed(), wrpm_regions_new[0].committed());
                        lemma_no_outstanding_writes_to_metadata_implies_no_outstanding_writes_to_active_metadata(wrpm_regions_new, self.num_logs, self.cdb);
                        assert(wrpm_regions@[which_log as int].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) =~= 
                            wrpm_regions_new[which_log as int].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));
                        let log_metadata_pos = get_log_metadata_pos(self.cdb);
                        assert(wrpm_regions@[which_log as int].committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                            wrpm_regions_new[which_log as int].committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()));
                        assert(active_metadata_is_equal(wrpm_regions@, wrpm_regions_new));
                            lemma_regions_metadata_matches_implies_metadata_types_set(wrpm_regions@, wrpm_regions_new, self.cdb);
                        lemma_regions_metadata_matches_implies_metadata_types_set(wrpm_regions@, wrpm_regions_new, self.cdb);
                        assert(metadata_types_set(wrpm_regions_new.committed()));
                        lemma_metadata_set_after_crash(wrpm_regions_new, self.cdb);


                        let wrpm_regions_new = wrpm_regions_new.write(which_log as int, ABSOLUTE_POS_OF_LOG_AREA as int, bytes_to_append@.subrange(max_len_without_wrapping as int, bytes_to_append.len() as int));
                        lemma_no_outstanding_writes_to_metadata_implies_no_outstanding_writes_to_active_metadata(wrpm_regions_new, self.num_logs, self.cdb);
                        assert(wrpm_regions@[which_log as int].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) =~= 
                            wrpm_regions_new[which_log as int].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));
                        let log_metadata_pos = get_log_metadata_pos(self.cdb);
                        assert(wrpm_regions@[which_log as int].committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                            wrpm_regions_new[which_log as int].committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()));
                        assert(active_metadata_is_equal(wrpm_regions@, wrpm_regions_new));
                        lemma_regions_metadata_matches_implies_metadata_types_set(wrpm_regions@, wrpm_regions_new, self.cdb);
                        assert(metadata_types_set(wrpm_regions_new.committed()));
                        lemma_metadata_set_after_crash(wrpm_regions_new, self.cdb);
                    
                    }
                    wrpm_regions.write(which_log as usize, write_addr,
                                slice_subrange(bytes_to_append, 0, max_len_without_wrapping as usize),
                                Tracked(perm));
                    wrpm_regions.write(which_log as usize, ABSOLUTE_POS_OF_LOG_AREA,
                                slice_subrange(bytes_to_append, max_len_without_wrapping as usize,
                                               bytes_to_append.len()),
                                Tracked(perm));
                }
            }

            // We now update our `infos` field to reflect the new
            // `log_plus_pending_length` value.

            let new_info = LogInfo{
                log_plus_pending_length: (info.log_plus_pending_length + num_bytes) as u64,
                ..self.infos[which_log as usize]
            };
            self.infos.set(which_log as usize, new_info);

            // We update our `state` field to reflect the tentative append.

            self.state = Ghost(self.state@.tentatively_append(which_log as int, bytes_to_append@));

            Ok(old_pending_tail)
        }

        // This local helper method updates the log metadata on
        // persistent memory to be consistent with `self.infos` and
        // `self.state`. It does so in the following steps: (1) update, in
        // each region, the log metadata corresponding to the inactive
        // CDB; (2) flush; (3) swap the CDB in region #0; (4) flush again.
        //
        // The first of these steps only writes to inactive metadata, i.e.,
        // metadata that's ignored during recovery. So even if a crash
        // happens during or immediately after this call, recovery will be
        // unaffected.
        //
        // Before calling this function, the caller should make sure that
        // `self.infos` and `self.state` contain the data that the inactive
        // log metadata should reflect. But, since this function has to
        // reason about crashes, it also needs to know things about the
        // *previous* values of `self.infos` and `self.state`, since those
        // are the ones that the active log metadata is consistent with
        // and will stay consistent with until we write the new CDB. These
        // previous values are passed as ghost parameters since they're
        // only needed for proving things.
        //
        // The caller of this function is responsible for making sure that
        // the contents of the log area are compatible with both the old
        // and the new `infos` and `state`. However, the log area contents
        // only need to be compatible with the new `infos` and `state`
        // after the next flush, since we're going to be doing a flush.
        // This weaker requirement allows a performance optimization: the
        // caller doesn't have to flush before calling this function.
        exec fn update_log_metadata<PMRegions>(
            &mut self,
            wrpm_regions: &mut WriteRestrictedPersistentMemoryRegions<TrustedMultiLogPermission, PMRegions>,
            Ghost(multilog_id): Ghost<u128>,
            Ghost(prev_infos): Ghost<Seq<LogInfo>>,
            Ghost(prev_state): Ghost<AbstractMultiLogState>,
            Tracked(perm): Tracked<&TrustedMultiLogPermission>,
        )
            where
                PMRegions: PersistentMemoryRegions
            requires
                old(wrpm_regions).inv(),
                memory_matches_deserialized_cdb(old(wrpm_regions)@, old(self).cdb),
                no_outstanding_writes_to_metadata(old(wrpm_regions)@, old(self).num_logs),
                each_metadata_consistent_with_info(old(wrpm_regions)@, multilog_id, old(self).num_logs,
                                                   old(self).cdb, prev_infos),
                each_info_consistent_with_log_area(old(wrpm_regions)@.flush(), old(self).num_logs,
                                                old(self).infos@, old(self).state@),
                each_info_consistent_with_log_area(old(wrpm_regions)@, old(self).num_logs, prev_infos, prev_state),
                forall |which_log: u32| #[trigger] is_valid_log_index(which_log, old(self).num_logs) ==>
                    old(self).infos@[which_log as int].log_area_len == prev_infos[which_log as int].log_area_len,
                forall |s| {
                          ||| Self::recover(s, multilog_id) == Some(prev_state.drop_pending_appends())
                          ||| Self::recover(s, multilog_id) == Some(old(self).state@.drop_pending_appends())
                      } ==> #[trigger] perm.check_permission(s),
                regions_metadata_types_set(old(wrpm_regions)@),
                forall |i: int| #![auto] 0 <= i < old(wrpm_regions)@.len() ==> ABSOLUTE_POS_OF_LOG_AREA < old(wrpm_regions)@[i].len(),
                old(wrpm_regions)@.len() > 0,
            ensures
                self.inv(wrpm_regions, multilog_id),
                wrpm_regions.constants() == old(wrpm_regions).constants(),
                self.state == old(self).state,
        {
            broadcast use pmcopy_axioms;
                
            // Set the `unused_metadata_pos` to be the position corresponding to !self.cdb
            // since we're writing in the inactive part of the metadata.

            let unused_metadata_pos = if self.cdb { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE }
                                      else { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE };
            assert(unused_metadata_pos == get_log_metadata_pos(!self.cdb));
            assert(is_valid_log_index(0, self.num_logs));

            // Loop, each time performing the update of the inactive log
            // metadata for log number `current_log`.

            let ghost old_wrpm_regions = wrpm_regions@;
            proof {
                // Before we enter the loop, we need to prove that there are no outstanding writes to active
                // metadata to satisfy the invariant. This follows from the fact that there are no outstanding 
                // writes to *any* metadata, but Z3 needs a hint from the lemma.
                lemma_no_outstanding_writes_to_metadata_implies_no_outstanding_writes_to_active_metadata(
                    wrpm_regions@, self.num_logs, self.cdb);
            }

            self.update_inactive_log_metadata(wrpm_regions, Ghost(multilog_id), Ghost(prev_infos), Ghost(prev_state), Tracked(perm));

            assert(self.num_logs == wrpm_regions@.len());
            assert(forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==>
                    inactive_metadata_types_set_in_region(wrpm_regions@.flush().committed()[i], self.cdb));

            proof {
                // Prove that after the flush we're about to do, all our
                // invariants will continue to hold (using the still-unchanged
                // CDB and the old metadata, infos, and state).
                lemma_flushing_metadata_maintains_invariants(wrpm_regions@, multilog_id, self.num_logs, self.cdb,
                                                             prev_infos, prev_state);

                // Also, prove that metadata types will still be set after the flush.
                lemma_no_outstanding_writes_to_active_metadata_implies_metadata_types_set_after_flush(wrpm_regions@, self.cdb);
            }

            // Next, flush all outstanding writes to memory. This is
            // necessary so that those writes are ordered before the update
            // to the CDB.
            wrpm_regions.flush();

            // Next, compute the new encoded CDB to write.

            let new_cdb = if self.cdb { CDB_FALSE } else { CDB_TRUE };
            let ghost new_cdb_bytes = new_cdb.spec_to_bytes();

            // Show that after writing and flushing, the CDB will be !self.cdb

            let ghost pm_regions_after_write = wrpm_regions@.write(0int, ABSOLUTE_POS_OF_LOG_CDB as int, new_cdb_bytes);
            let ghost flushed_mem_after_write = pm_regions_after_write.flush();
            assert(memory_matches_deserialized_cdb(flushed_mem_after_write, !self.cdb)) by {
                let flushed_regions = pm_regions_after_write.flush();
                lemma_write_reflected_after_flush_committed(wrpm_regions@[0], ABSOLUTE_POS_OF_LOG_CDB as int, new_cdb_bytes);
                assert(deserialize_log_cdb(flushed_regions[0].committed()) == new_cdb);
            }

            // Show that after writing and flushing, our invariants will
            // hold for each log if we flip `self.cdb`.

            let ghost pm_regions_after_flush = pm_regions_after_write.flush();
            assert forall |which_log: u32| #[trigger] is_valid_log_index(which_log, self.num_logs) implies {
                let w = which_log as int;
                &&& metadata_consistent_with_info(pm_regions_after_flush[w], multilog_id, self.num_logs, which_log,
                                                 !self.cdb, self.infos@[w])
                &&& info_consistent_with_log_area(pm_regions_after_flush[w], self.infos@[w], self.state@[w])
                &&& metadata_types_set(pm_regions_after_flush.committed())
            } by {
                let w = which_log as int;
                lemma_establish_extract_bytes_equivalence(
                    wrpm_regions@[which_log as int].committed(),
                    pm_regions_after_flush[which_log as int].committed());

                lemma_each_metadata_consistent_with_info_after_cdb_update(
                    wrpm_regions@,
                    pm_regions_after_flush,
                    multilog_id,
                    self.num_logs,
                    new_cdb_bytes,
                    !self.cdb,
                    self.infos@
                );

                assert(pm_regions_after_flush.len() == wrpm_regions@.len());
                assert(forall |i: int| #![auto] 0 <= i < pm_regions_after_flush.len() ==> 
                    wrpm_regions@[i].len() == pm_regions_after_flush[i].len());
                assert(forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> 
                    ABSOLUTE_POS_OF_LOG_AREA < wrpm_regions@[i].len());
                assert(forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> 
                    inactive_metadata_types_set_in_region(wrpm_regions@.committed()[i], self.cdb));
                lemma_metadata_types_set_after_cdb_update(wrpm_regions@, pm_regions_after_flush, multilog_id, new_cdb_bytes, self.cdb);
            }
            assert(memory_matches_deserialized_cdb(pm_regions_after_flush, !self.cdb));

            // Show that if we crash after the write and flush, we recover
            // to an abstract state corresponding to `self.state@` after
            // dropping pending appends.

            proof {
                lemma_invariants_imply_crash_recover_forall(pm_regions_after_flush, multilog_id,
                                                            self.num_logs, !self.cdb, self.infos@, self.state@);
            }

            // Show that if we crash after initiating the write of the CDB,
            // we'll recover to a permissible state. There are two cases:
            //
            // If we crash without any updating, then we'll recover to
            // state `prev_state.drop_pending_appends()` with the current
            // CDB.
            //
            // If we crash after writing, then we'll recover to state
            // `self.state@.drop_pending_appends()` with the flipped CDB.
            //
            // Because we're only writing within the persistence
            // granularity of the persistent memory, a crash in the middle
            // will either leave the persistent memory in the pre-state or
            // the post-state.
            //
            // This means we're allowed to do the write because if we
            // crash, we'll either be in state wrpm_regions@.committed() or
            // pm_regions_after_write.flush().committed(). In the former
            // case, we'll be in state `prev_state.drop_pending_appends()`
            // and in the latter case, as shown above, we'll be in state
            // `self.state@.drop_pending_appends()`.

            assert forall |crash_bytes| pm_regions_after_write.can_crash_as(crash_bytes) implies
                       #[trigger] perm.check_permission(crash_bytes) by {
                lemma_invariants_imply_crash_recover_forall(wrpm_regions@, multilog_id, self.num_logs,
                                                            self.cdb, prev_infos, prev_state);
                lemma_single_write_crash_effect_on_pm_regions_view(wrpm_regions@, 0int,
                                                                   ABSOLUTE_POS_OF_LOG_CDB as int, new_cdb_bytes);
                assert(metadata_types_set(crash_bytes));
            }

            // Finally, update the CDB, then flush, then flip `self.cdb`.
            // There's no need to flip `self.cdb` atomically with the write
            // since the flip of `self.cdb` is happening in local
            // non-persistent memory so if we crash it'll be lost anyway.
            // wrpm_regions.write(0, ABSOLUTE_POS_OF_LOG_CDB, new_cdb.as_slice(), Tracked(perm));
            wrpm_regions.serialize_and_write(0, ABSOLUTE_POS_OF_LOG_CDB, &new_cdb, Tracked(perm));
            wrpm_regions.flush();
            self.cdb = !self.cdb;

            proof {
                lemma_if_no_outstanding_writes_then_persistent_memory_regions_view_can_only_crash_as_committed(wrpm_regions@);
                assert(can_only_crash_as_state(wrpm_regions@, multilog_id, self.state@.drop_pending_appends()));

                // We need these asserts to hit the triggers needed to prove that the size of each region has not been changed.
                assert(forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> old(wrpm_regions)@[i].len() == wrpm_regions@[i].len());
                assert(forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> ABSOLUTE_POS_OF_LOG_AREA < wrpm_regions@[i].len());
            }
        }

        fn update_inactive_log_metadata<PMRegions>(
            &mut self,
            wrpm_regions: &mut WriteRestrictedPersistentMemoryRegions<TrustedMultiLogPermission, PMRegions>,
            Ghost(multilog_id): Ghost<u128>,
            Ghost(prev_infos): Ghost<Seq<LogInfo>>,
            Ghost(prev_state): Ghost<AbstractMultiLogState>,
            Tracked(perm): Tracked<&TrustedMultiLogPermission>,
        )
            where
                PMRegions: PersistentMemoryRegions,
            requires
                old(wrpm_regions).inv(),
                memory_matches_deserialized_cdb(old(wrpm_regions)@, old(self).cdb),
                no_outstanding_writes_to_metadata(old(wrpm_regions)@, old(self).num_logs),
                each_metadata_consistent_with_info(old(wrpm_regions)@, multilog_id, old(self).num_logs,
                                                   old(self).cdb, prev_infos),
                each_info_consistent_with_log_area(old(wrpm_regions)@.flush(), old(self).num_logs,
                                                old(self).infos@, old(self).state@),
                each_info_consistent_with_log_area(old(wrpm_regions)@, old(self).num_logs, prev_infos, prev_state),
                forall |which_log: u32| #[trigger] is_valid_log_index(which_log, old(self).num_logs) ==>
                    old(self).infos@[which_log as int].log_area_len == prev_infos[which_log as int].log_area_len,
                forall |s| {
                          ||| Self::recover(s, multilog_id) == Some(prev_state.drop_pending_appends())
                          ||| Self::recover(s, multilog_id) == Some(old(self).state@.drop_pending_appends())
                      } ==> #[trigger] perm.check_permission(s),
                regions_metadata_types_set(old(wrpm_regions)@),
                forall |i: int| #![auto] 0 <= i < old(wrpm_regions)@.len() ==> ABSOLUTE_POS_OF_LOG_AREA < old(wrpm_regions)@[i].len(),
                old(wrpm_regions)@.len() > 0,
            ensures
                wrpm_regions.inv(),
                self.state == old(self).state,
                wrpm_regions.constants() == old(wrpm_regions).constants(),
                memory_matches_deserialized_cdb(wrpm_regions@, self.cdb),
                each_metadata_consistent_with_info(wrpm_regions@, multilog_id, self.num_logs, self.cdb, prev_infos),
                each_info_consistent_with_log_area(wrpm_regions@, self.num_logs, prev_infos, prev_state),
                each_info_consistent_with_log_area(wrpm_regions@.flush(), self.num_logs, self.infos@, self.state@),
                forall |s| Self::recover(s, multilog_id) == Some(prev_state.drop_pending_appends()) ==>
                    #[trigger] perm.check_permission(s),
                forall |which_log: u32| #[trigger] is_valid_log_index(which_log, self.num_logs) ==>
                    self.infos@[which_log as int].log_area_len == prev_infos[which_log as int].log_area_len,
                forall |which_log: u32| #[trigger] is_valid_log_index(which_log, self.num_logs) ==> {
                    let w = which_log as int;
                    let flushed = wrpm_regions@.flush();
                    &&& metadata_consistent_with_info(flushed[w], multilog_id, self.num_logs, which_log,
                                                    !self.cdb, self.infos@[w])
                },
                no_outstanding_writes_to_active_metadata(wrpm_regions@, self.cdb),
                metadata_types_set(wrpm_regions@.committed()),
                active_metadata_is_equal(old(wrpm_regions)@, wrpm_regions@),
                forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> wrpm_regions@[i].len() == old(wrpm_regions)@[i].len(),
                forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> ABSOLUTE_POS_OF_LOG_AREA < wrpm_regions@[i].len(),
                forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> 
                    inactive_metadata_types_set_in_region(wrpm_regions@.flush().committed()[i], self.cdb),
                wrpm_regions@.len() > 0,
                
        {
            broadcast use pmcopy_axioms;

            let unused_metadata_pos = if self.cdb { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE }
                                            else { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE };
            assert(unused_metadata_pos == get_log_metadata_pos(!self.cdb));
            assert(is_valid_log_index(0, self.num_logs));

            let ghost old_wrpm_regions = wrpm_regions@;
            proof {
                // Before we enter the loop, we need to prove that there are no outstanding writes to active
                // metadata to satisfy the invariant. This follows from the fact that there are no outstanding 
                // writes to *any* metadata, but Z3 needs a hint from the lemma.
                lemma_no_outstanding_writes_to_metadata_implies_no_outstanding_writes_to_active_metadata(
                    wrpm_regions@, self.num_logs, self.cdb);
            }

            let ghost old_prev_infos = prev_infos;
            let ghost old_prev_state = prev_state;

            for current_log in 0..self.num_logs
                invariant
                    wrpm_regions.inv(),
                    wrpm_regions.constants() == old(wrpm_regions).constants(),
                    unused_metadata_pos == get_log_metadata_pos(!self.cdb),
                    memory_matches_deserialized_cdb(wrpm_regions@, self.cdb),
                    each_metadata_consistent_with_info(wrpm_regions@, multilog_id, self.num_logs, self.cdb, prev_infos),
                    each_info_consistent_with_log_area(wrpm_regions@, self.num_logs, prev_infos, prev_state),
                    each_info_consistent_with_log_area(wrpm_regions@.flush(), self.num_logs, self.infos@, self.state@),
                    forall |s| Self::recover(s, multilog_id) == Some(prev_state.drop_pending_appends()) ==>
                        #[trigger] perm.check_permission(s),
                    forall |which_log: u32| #[trigger] is_valid_log_index(which_log, self.num_logs) ==>
                        self.infos@[which_log as int].log_area_len == prev_infos[which_log as int].log_area_len,

                    // For logs we haven't updated the metadata for
                    // yet, there are still no outstanding writes in
                    // the inactive metadata part, and the region's
                    // contents are unchanged since the beginning of
                    // this function.

                    forall |which_log: u32|
                        current_log <= which_log && #[trigger] is_valid_log_index(which_log, self.num_logs) ==>
                        wrpm_regions@[which_log as int].no_outstanding_writes_in_range(
                            unused_metadata_pos as int,
                            unused_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()),
                    forall |which_log: u32|
                        current_log <= which_log && #[trigger] is_valid_log_index(which_log, self.num_logs) ==> {
                        let w = which_log as int;
                        wrpm_regions@[w] == old(wrpm_regions)@[w]
                    },

                    // For logs that we *have* updated the metadata
                    // for, we've made the metadata corresponding to
                    // !self.cdb consistent with self.infos@.

                    forall |which_log: u32| #[trigger] is_valid_log_index(which_log, self.num_logs)
                                       && which_log < current_log ==> {
                        let w = which_log as int;
                        let flushed = wrpm_regions@.flush();
                        &&& metadata_consistent_with_info(flushed[w], multilog_id, self.num_logs, which_log,
                                                        !self.cdb, self.infos@[w])
                    },

                    // Despite potential updates to each log, their active metadata is never 
                    // modified by the loop.

                    no_outstanding_writes_to_active_metadata(wrpm_regions@, self.cdb),
                    metadata_types_set(wrpm_regions@.committed()),
                    active_metadata_is_equal(old_wrpm_regions, wrpm_regions@),

                    // The loop does not change the size of any of the regions
                    forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> wrpm_regions@[i].len() == old(wrpm_regions)@[i].len(),
                    forall |i: int| #![auto] 0 <= i < wrpm_regions@.len() ==> ABSOLUTE_POS_OF_LOG_AREA < wrpm_regions@[i].len(),
                    forall |i: int| 0 <= i < current_log ==> 
                        inactive_metadata_types_set_in_region(#[trigger] wrpm_regions@.flush().committed()[i], self.cdb),
                    wrpm_regions@.len() > 0,

                    self.cdb == old(self).cdb,
                    prev_infos == old_prev_infos,
                    prev_state == old_prev_state,
            {
                broadcast use pmcopy_axioms; // Remove this workaround once https://github.com/verus-lang/verus/issues/1166 is fixed

                assert(is_valid_log_index(current_log, self.num_logs));
                let ghost cur = current_log as int;

                // Encode the log metadata as bytes, and compute the CRC of those bytes

                let info = &self.infos[current_log as usize];
                let log_metadata = LogMetadata {
                    head: info.head,
                    _padding: 0,
                    log_length: info.log_length
                };
                let log_crc = calculate_crc(&log_metadata);

                let ghost log_metadata_bytes = log_metadata.spec_to_bytes();
                let ghost log_crc_bytes = log_crc.spec_to_bytes();

                // Prove that updating the inactive metadata+CRC maintains
                // all invariants that held before. We prove this separately
                // for metadata and CRC because they are updated in two separate
                // writes.

                proof {
                    lemma_updating_inactive_metadata_maintains_invariants(
                        wrpm_regions@, multilog_id, self.num_logs, self.cdb, prev_infos, prev_state, current_log,
                        log_metadata_bytes);

                    let wrpm_regions_new = wrpm_regions@.write(cur, unused_metadata_pos as int, log_metadata_bytes);
                    lemma_updating_inactive_crc_maintains_invariants(
                        wrpm_regions_new, multilog_id, self.num_logs, self.cdb, prev_infos, prev_state, current_log,
                        log_crc_bytes);
                }

                let ghost wrpm_regions_new = wrpm_regions@.write(cur, unused_metadata_pos as int, log_metadata_bytes);
                proof {
                    // The proofs in this block apply to both the crash case and the regular case, and help us prove
                    // that the metadata types are still set after the write regardless of whether it completes or not.
                    assert(forall |i: int| #![auto] 0 <= i < wrpm_regions_new.len() && i != cur ==> 
                        wrpm_regions@[i] == wrpm_regions_new[i]); 
                    lemma_write_to_inactive_metadata_implies_active_metadata_stays_equal(wrpm_regions@, wrpm_regions_new, 
                        cur, unused_metadata_pos as int, log_metadata_bytes, self.cdb);
                }

                // Use `lemma_invariants_imply_crash_recover_forall` to prove that it's OK to call
                // `write`. (One of the conditions for calling that lemma is that our invariants
                // hold, which we just proved above.)
                assert forall |crash_bytes| wrpm_regions_new.can_crash_as(crash_bytes)
                           implies #[trigger] perm.check_permission(crash_bytes) by {
                    lemma_invariants_imply_crash_recover_forall(
                        wrpm_regions_new, multilog_id, self.num_logs, self.cdb, prev_infos, prev_state);

                    lemma_metadata_set_after_crash(wrpm_regions_new, self.cdb);
                    assert(metadata_types_set(crash_bytes));
                }

                // Write the new metadata to the inactive header (without the CRC)
                let ghost old_wrpm_regions = wrpm_regions@;
                
                wrpm_regions.serialize_and_write(current_log as usize, unused_metadata_pos, &log_metadata, Tracked(perm));

                // Now prove that the CRC is safe to update as well, and write it.

                let ghost wrpm_regions_new = wrpm_regions@.write(cur, unused_metadata_pos + LogMetadata::spec_size_of(), log_crc_bytes);
                proof {
                    // Prove that there are no outstanding writes to active metadata in any of the logs
                    assert(forall |i: int| #![auto] 0 <= i < wrpm_regions_new.len() && i != cur ==> 
                        wrpm_regions@[i] == wrpm_regions_new[i]); 

                    lemma_write_to_inactive_metadata_implies_active_metadata_stays_equal(wrpm_regions@, wrpm_regions_new, 
                        cur, unused_metadata_pos + LogMetadata::spec_size_of(), log_crc_bytes, self.cdb);

                    // after this write, the inactive CRC will be set
                    assert(wrpm_regions_new[cur].flush().committed().subrange(
                        unused_metadata_pos + LogMetadata::spec_size_of(), 
                        unused_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()
                    ) == log_crc_bytes);
                }

                assert forall |crash_bytes| wrpm_regions_new.can_crash_as(crash_bytes)
                           implies #[trigger] perm.check_permission(crash_bytes) by {
                    lemma_invariants_imply_crash_recover_forall(
                        wrpm_regions_new, multilog_id, self.num_logs, self.cdb, prev_infos, prev_state);

                    lemma_metadata_set_after_crash(wrpm_regions_new, self.cdb);
                    assert(metadata_types_set(crash_bytes));
                }

                wrpm_regions.serialize_and_write(current_log as usize, unused_metadata_pos + size_of::<LogMetadata>() as u64, &log_crc, Tracked(perm));
                
                assert forall |i: int| 0 <= i < wrpm_regions@.len() implies #[trigger] wrpm_regions@.flush().committed()[i].len() > ABSOLUTE_POS_OF_LOG_AREA by {
                    assert(wrpm_regions@[i].len() == wrpm_regions@.flush().committed()[i].len());
                    assert(wrpm_regions@[i].len() > ABSOLUTE_POS_OF_LOG_AREA);
                }
                assert forall |i: int| 0 <= i < cur implies 
                    inactive_metadata_types_set_in_region(#[trigger] wrpm_regions@.flush().committed()[i], self.cdb) 
                by {
                    assert(old_wrpm_regions.flush().committed()[i] == wrpm_regions@.flush().committed()[i]);
                }
                
                // Prove that after the flush, the log metadata corresponding to the unused CDB will
                // be reflected in memory.

                let ghost flushed = wrpm_regions_new.flush();
                assert (metadata_consistent_with_info(flushed[current_log as int], multilog_id,
                                                      self.num_logs, current_log, !self.cdb, self.infos@[cur])) by {
                    let mem1 = wrpm_regions@[cur].committed();
                    let mem2 = flushed[cur].committed();
                    lemma_establish_extract_bytes_equivalence(mem1, mem2);
                    lemma_write_reflected_after_flush_committed(wrpm_regions@[cur], unused_metadata_pos as int,
                                                                log_metadata_bytes + log_crc_bytes);
                    assert(extract_log_metadata(mem2, !self.cdb) =~= log_metadata_bytes);
                    assert(extract_log_crc(mem2, !self.cdb) =~= log_crc_bytes);
                    assert(deserialize_log_metadata(mem2, !self.cdb) == log_metadata);
                    assert(deserialize_log_crc(mem2, !self.cdb) == log_crc);
                }

                assert(wrpm_regions@.flush().committed()[cur].subrange(unused_metadata_pos as int, unused_metadata_pos + LogMetadata::spec_size_of()) ==
                        log_metadata.spec_to_bytes());
                assert forall |i: int| 0 <= i < cur implies 
                    inactive_metadata_types_set_in_region(#[trigger] wrpm_regions@.flush().committed()[i], self.cdb) 
                by {
                    assert(exists |log_metadata: LogMetadata| 
                        wrpm_regions@.flush().committed()[i].subrange(unused_metadata_pos as int, unused_metadata_pos + LogMetadata::spec_size_of()) ==
                            log_metadata.spec_to_bytes());
                }
            }
        }

        // The `commit` method commits all tentative appends that have been
        // performed since the last one. See `README.md` for more
        // documentation and examples of its use.
        //
        // This method is passed a write-restricted collection of
        // persistent memory regions `wrpm_regions`. This restricts
        // how it can write `wrpm_regions`. It's only given permission
        // (in `perm`) to write if it can prove that any crash after
        // initiating the write is safe. That is, any such crash must
        // put the memory in a state that recovers as either (1) the
        // current abstract state with all pending appends dropped, or
        // (2) the abstract state after all pending appends are
        // committed.
        pub exec fn commit<PMRegions>(
            &mut self,
            wrpm_regions: &mut WriteRestrictedPersistentMemoryRegions<TrustedMultiLogPermission, PMRegions>,
            Ghost(multilog_id): Ghost<u128>,
            Tracked(perm): Tracked<&TrustedMultiLogPermission>,
        ) -> (result: Result<(), MultiLogErr>)
            where
                PMRegions: PersistentMemoryRegions
            requires
                old(self).inv(&*old(wrpm_regions), multilog_id),
                forall |s| #[trigger] perm.check_permission(s) <==> {
                    ||| Self::recover(s, multilog_id) == Some(old(self)@.drop_pending_appends())
                    ||| Self::recover(s, multilog_id) == Some(old(self)@.commit().drop_pending_appends())
                },
            ensures
                self.inv(wrpm_regions, multilog_id),
                wrpm_regions.constants() == old(wrpm_regions).constants(),
                can_only_crash_as_state(wrpm_regions@, multilog_id, self@.drop_pending_appends()),
                result is Ok,
                self@ == old(self)@.commit(),
        {
            let ghost prev_infos = self.infos@;
            let ghost prev_state = self.state@;

            self.state = Ghost(self.state@.commit());

            // Loop, where `current_log` ranges through the log indices,
            // each time updating `self.infos[current_log]`. Each iteration
            // maintains the invariants that (1) the persistent memory is
            // compatible with `prev_infos` and `prev_state`, and (2) for
            // each log we've already updated, the persistent memory's log
            // area, if flushed, would be consistent with `self.infos` and
            // `self.state`.

            for current_log in iter: 0..self.num_logs
                invariant
                    iter.end == self.num_logs, // we need to remember this since `self` is changed in the loop body
                    wrpm_regions.inv(),

                    memory_matches_deserialized_cdb(wrpm_regions@, self.cdb),
                    each_metadata_consistent_with_info(wrpm_regions@, multilog_id, self.num_logs, self.cdb, prev_infos),
                    each_info_consistent_with_log_area(wrpm_regions@, self.num_logs, prev_infos, prev_state),
                    self.infos@.len() == self.state@.num_logs() == self.num_logs,
                    self.state@ == prev_state.commit(),

                    forall |which_log: u32| #[trigger] is_valid_log_index(which_log, self.num_logs) ==> {
                        let w = which_log as int;
                        if which_log < current_log {
                            info_consistent_with_log_area(wrpm_regions@.flush()[w], self.infos[w], self.state@[w])
                        }
                        else {
                            self.infos[w] == prev_infos[w]
                        }
                    },
            {
                assert(is_valid_log_index(current_log, self.num_logs)); // trigger various useful foralls in invariants

                // Update the `current_log`th entry in `self.infos` to
                // update the `log_length` field to be whatever is
                // currently in `log_plus_pending_length`. Verus currently
                // doesn't support updating a field of an element of a
                // vector, so we have to update the entire element. We must
                // furthermore use Verus's vector `set` method for this
                // because Verus doesn't support vector elements as
                // left-hand sides of assignments.

                let new_log_length = self.infos[current_log as usize].log_plus_pending_length;
                let new_info = LogInfo{
                    log_length: new_log_length,
                    ..self.infos[current_log as usize]
                };
                self.infos.set(current_log as usize, new_info);
            }

            // Update the inactive metadata on all regions and flush, then
            // swap the CDB to its opposite.

            self.update_log_metadata(wrpm_regions, Ghost(multilog_id), Ghost(prev_infos),
                                        Ghost(prev_state), Tracked(perm));

            Ok(())
        }

        // The `advance_head` method advances the head of one of the logs,
        // thereby making more space for appending but making log entries
        // before the new head unavailable for reading. Upon return from
        // this method, the head advancement is durable, i.e., it will
        // survive crashes. See `README.md` for more documentation and
        // examples of its use.
        //
        // This method is passed a write-restricted collection of
        // persistent memory regions `wrpm_regions`. This restricts how it
        // can write `wrpm_regions`. It's only given permission (in `perm`)
        // to write if it can prove that any crash after initiating the
        // write is safe. That is, any such crash must put the memory in a
        // state that recovers as either (1) the current abstract state
        // with all pending appends dropped, or (2) the state after
        // advancing the head and then dropping all pending appends.
        pub exec fn advance_head<PMRegions>(
            &mut self,
            wrpm_regions: &mut WriteRestrictedPersistentMemoryRegions<TrustedMultiLogPermission, PMRegions>,
            which_log: u32,
            new_head: u128,
            Ghost(multilog_id): Ghost<u128>,
            Tracked(perm): Tracked<&TrustedMultiLogPermission>,
        ) -> (result: Result<(), MultiLogErr>)
            where
                PMRegions: PersistentMemoryRegions
            requires
                old(self).inv(&*old(wrpm_regions), multilog_id),
                forall |s| #[trigger] perm.check_permission(s) <==> {
                    ||| Self::recover(s, multilog_id) == Some(old(self)@.drop_pending_appends())
                    ||| Self::recover(s, multilog_id) ==
                        Some(old(self)@.advance_head(which_log as int, new_head as int).drop_pending_appends())
                },
            ensures
                self.inv(wrpm_regions, multilog_id),
                wrpm_regions.constants() == old(wrpm_regions).constants(),
                can_only_crash_as_state(wrpm_regions@, multilog_id, self@.drop_pending_appends()),
                match result {
                    Ok(()) => {
                        let w = which_log as int;
                        &&& which_log < self@.num_logs()
                        &&& old(self)@[w].head <= new_head <= old(self)@[w].head + old(self)@[w].log.len()
                        &&& self@ == old(self)@.advance_head(w, new_head as int)
                    },
                    Err(MultiLogErr::InvalidLogIndex{ }) => {
                        &&& self@ == old(self)@
                        &&& which_log >= self@.num_logs()
                    },
                    Err(MultiLogErr::CantAdvanceHeadPositionBeforeHead { head }) => {
                        &&& self@ == old(self)@
                        &&& which_log < self@.num_logs()
                        &&& head == self@[which_log as int].head
                        &&& new_head < head
                    },
                    Err(MultiLogErr::CantAdvanceHeadPositionBeyondTail { tail }) => {
                        &&& self@ == old(self)@
                        &&& which_log < self@.num_logs()
                        &&& tail == self@[which_log as int].head + self@[which_log as int].log.len()
                        &&& new_head > tail
                    },
                    _ => false
                }
        {
            // Even if we return an error code, we still have to prove that
            // upon return the states we can crash into recover into valid
            // abstract states.

            proof {
                assert(is_valid_log_index(0, self.num_logs));
                lemma_invariants_imply_crash_recover_forall(wrpm_regions@, multilog_id, self.num_logs, self.cdb,
                                                            self.infos@, self.state@);
            }

            // Handle error cases due to improper parameters passed to the
            // function.

            if which_log >= self.num_logs {
                return Err(MultiLogErr::InvalidLogIndex{ });
            }

            assert(is_valid_log_index(which_log, self.num_logs)); // trigger useful foralls in invariants
            let info = &self.infos[which_log as usize];
            if new_head < info.head {
                return Err(MultiLogErr::CantAdvanceHeadPositionBeforeHead{ head: info.head })
            }
            if new_head - info.head > info.log_length as u128 {
                return Err(MultiLogErr::CantAdvanceHeadPositionBeyondTail{ tail: info.head + info.log_length as u128 })
            }

            // To compute the new head mod n (where n is the log area
            // length), take the old head mod n, add the amount by
            // which the head is advancing, then subtract n if
            // necessary.

            let amount_of_advancement: u64 = (new_head - info.head) as u64;
            let new_head_log_area_offset =
                if amount_of_advancement < info.log_area_len - info.head_log_area_offset {
                    amount_of_advancement + info.head_log_area_offset
                }
                else {
                    // To compute `info.head_log_area_offset` [the old
                    // head] plus `amount_of_advancement` [the amount
                    // by which the head is advancing] minus
                    // `info.log_area_len` [the log area length], we
                    // do it in the following order that guarantees no
                    // overflow/underflow.
                    amount_of_advancement - (info.log_area_len - info.head_log_area_offset)
                };

            assert(new_head_log_area_offset == new_head as int % info.log_area_len as int) by {
                // Here's a mathematical proof that doing the above
                // calculation of `new_head_log_area_offset` achieves the
                // desired computation of `new_head % log_area_len`.

                let n = info.log_area_len as int;
                let advancement = amount_of_advancement as int;
                let head = info.head as int;
                let head_mod_n = info.head_log_area_offset as int;
                let supposed_new_head_mod_n = new_head_log_area_offset as int;

                // First, observe that `advancement` plus `head` is
                // congruent modulo n to `advancement` plus `head` % n.

                assert((advancement + head) % n == (advancement + head_mod_n) % n) by {
                    assert(head == n * (head / n) + head % n) by {
                        lemma_fundamental_div_mod(head, n);
                    }
                    assert((n * (head / n) + (advancement + head_mod_n)) % n == (advancement + head_mod_n) % n) by {
                        lemma_mod_multiples_vanish(head / n, advancement + head_mod_n, n);
                    }
                }

                // Next, observe that `advancement` + `head` % n is
                // congruent modulo n to itself minus n. This is
                // relevant because there are two cases for computing
                // `new_head_mod_log_area_offset`. In one case, it's
                // computed as `advancement` + `head` % n. In the
                // other case, it's that quantity minus n.

                assert((advancement + head % n) % n == (advancement + head_mod_n - n) % n) by {
                    lemma_mod_sub_multiples_vanish(advancement + head_mod_n, n);
                }

                // So we know that in either case, `new_head` % n ==
                // `new_head_mod_log_area_offset` % n.

                assert(new_head as int % n == supposed_new_head_mod_n % n);

                // But what we want to prove is that `new_head` % n ==
                // `new_head_mod_log_area_offset`. So we need to show
                // that `new_head_mod_log_area_offset` % n ==
                // `new_head_mod_log_area_offset`.  We can deduce this
                // from the fact that 0 <= `new_head_mod_log_area_offset`
                // < n.

                assert(supposed_new_head_mod_n % n == supposed_new_head_mod_n) by {
                    lemma_small_mod(supposed_new_head_mod_n as nat, n as nat);
                }
            }

            // Update the `which_log`th entry in `self.infos` to reflect
            // the change to the head position. This necessitates updating
            // all the fields except the log area length. We have to use
            // Verus's vector `set` method for this because Verus doesn't
            // support vector elements as left-hand sides of assignments.

            let ghost prev_infos = self.infos@;
            let new_info = LogInfo{
                log_area_len: info.log_area_len,
                head: new_head,
                head_log_area_offset: new_head_log_area_offset,
                log_length: info.log_length - amount_of_advancement,
                log_plus_pending_length: info.log_plus_pending_length - amount_of_advancement,
            };
            self.infos.set(which_log as usize, new_info);

            // Update the abstract `self.state` to reflect the head update.

            let ghost prev_state = self.state@;
            self.state = Ghost(self.state@.advance_head(which_log as int, new_head as int));

            // To prove that the log area for log number `which_log` is
            // compatible with the new `self.infos` and `self.state`, we
            // need to reason about how addresses in the log area
            // correspond to relative log positions. That's because the
            // invariants we know about the log area talk about log
            // positions relative to the old head, but we want to know
            // things about log positions relative to the new head. What
            // connects those together is that they both talk about the
            // same addresses in the log area.

            let ghost w = which_log as int;
            let ghost flushed_regions = wrpm_regions@.flush();
            assert (info_consistent_with_log_area(flushed_regions[w], self.infos@[w], self.state@[w])) by {
                lemma_addresses_in_log_area_correspond_to_relative_log_positions(wrpm_regions@[w], prev_infos[w]);
            }

            // Update the inactive metadata on all regions and flush, then
            // swap the CDB to its opposite. We have to update the metadata
            // on all regions, even though we're only advancing the head on
            // one, for the following reason. The only way available to us
            // to update the active metadata is to flip the CDB, but this
            // flips which metadata is active on *all* regions. So we have
            // to update the inactive metadata on all regions.

            self.update_log_metadata(wrpm_regions, Ghost(multilog_id), Ghost(prev_infos), Ghost(prev_state),
                                        Tracked(perm));

            Ok(())
        }

        // This local helper method proves that we can read a portion of
        // the abstract log by reading a continuous range of the log area.
        // It requires that the position being read from is correct, and
        // that the read is short enough to not require wrapping around the
        // end of the log area.
        proof fn lemma_read_of_continuous_range(
            &self,
            pm_regions_view: PersistentMemoryRegionsView,
            multilog_id: u128,
            which_log: u32,
            pos: int,
            len: int,
            addr: int,
        )
            requires
                is_valid_log_index(which_log, self.num_logs),
                len > 0,
                each_metadata_consistent_with_info(pm_regions_view, multilog_id, self.num_logs, self.cdb, self.infos@),
                each_info_consistent_with_log_area(pm_regions_view, self.num_logs, self.infos@, self.state@),
                ({
                    let info = self.infos@[which_log as int];
                    let max_len_without_wrapping = info.log_area_len -
                        relative_log_pos_to_log_area_offset(pos - info.head,
                                                            info.head_log_area_offset as int,
                                                            info.log_area_len as int);
                    &&& pos >= info.head
                    &&& pos + len <= info.head + info.log_length
                    &&& len <= max_len_without_wrapping
                    &&& addr == ABSOLUTE_POS_OF_LOG_AREA +
                           relative_log_pos_to_log_area_offset(pos - info.head as int,
                                                               info.head_log_area_offset as int,
                                                               info.log_area_len as int)
                })
            ensures
                ({
                    let log = self@[which_log as int];
                    let pm_region_view = pm_regions_view[which_log as int];
                    &&& pm_region_view.no_outstanding_writes_in_range(addr, addr + len)
                    &&& pm_region_view.committed().subrange(addr, addr + len)
                           == log.log.subrange(pos - log.head, pos + len - log.head)
                })
        {
            let w = which_log as int;
            let info = self.infos@[w];
            let s = self.state@[w];
            let pm_region_view = pm_regions_view[w];

            // The key to the proof is that we need to reason about how
            // addresses in the log area correspond to relative log
            // positions. This is because the invariant talks about
            // relative log positions but this lemma is proving things
            // about addresses in the log area.

            lemma_addresses_in_log_area_correspond_to_relative_log_positions(pm_region_view, info);
            assert(pm_region_view.committed().subrange(addr, addr + len) =~=
                   s.log.subrange(pos - s.head, pos + len - s.head));
        }

        // The `read` method reads part of one of the logs, returning a
        // vector containing the read bytes. It doesn't guarantee that
        // those bytes aren't corrupted by persistent memory corruption.
        // See `README.md` for more documentation and examples of its use.
        pub exec fn read<Perm, PMRegions>(
            &self,
            wrpm_regions: &WriteRestrictedPersistentMemoryRegions<Perm, PMRegions>,
            which_log: u32,
            pos: u128,
            len: u64,
            Ghost(multilog_id): Ghost<u128>,
        ) -> (result: Result<Vec<u8>, MultiLogErr>)
            where
                Perm: CheckPermission<Seq<Seq<u8>>>,
                PMRegions: PersistentMemoryRegions
            requires
                self.inv(wrpm_regions, multilog_id),
                pos + len <= u128::MAX
            ensures
                ({
                    let log = self@[which_log as int];
                    match result {
                        Ok(bytes) => {
                            let true_bytes = self@.read(which_log as int, pos as int, len as int);
                            &&& which_log < self@.num_logs()
                            &&& pos >= log.head
                            &&& pos + len <= log.head + log.log.len()
                            &&& read_correct_modulo_corruption(bytes@, true_bytes,
                                                              wrpm_regions.constants().impervious_to_corruption)
                        },
                        Err(MultiLogErr::InvalidLogIndex{ }) => {
                            which_log >= self@.num_logs()
                        },
                        Err(MultiLogErr::CantReadBeforeHead{ head: head_pos }) => {
                            &&& which_log < self@.num_logs()
                            &&& pos < log.head
                            &&& head_pos == log.head
                        },
                        Err(MultiLogErr::CantReadPastTail{ tail }) => {
                            &&& which_log < self@.num_logs()
                            &&& pos + len > log.head + log.log.len()
                            &&& tail == log.head + log.log.len()
                        },
                        _ => false,
                    }
                })
        {
            // Handle error cases due to improper parameters passed to the
            // function.

            if which_log >= self.num_logs {
                return Err(MultiLogErr::InvalidLogIndex{ });
            }

            assert(is_valid_log_index(which_log, self.num_logs)); // triggers useful foralls in invariants

            let info = &self.infos[which_log as usize];
            if pos < info.head {
                return Err(MultiLogErr::CantReadBeforeHead{ head: info.head })
            }
            if len > info.log_length { // We have to do this check first to avoid underflow in the next comparison
                return Err(MultiLogErr::CantReadPastTail{ tail: info.head + info.log_length as u128 })
            }
            if pos - info.head > (info.log_length - len) as u128 { // we know `info.log_length - len` can't underflow
                return Err(MultiLogErr::CantReadPastTail{ tail: info.head + info.log_length as u128 })
            }

            let ghost s = self.state@[which_log as int];
            let ghost true_bytes = s.log.subrange(pos - s.head, pos + len - s.head);

            if len == 0 {
                // Case 0: The trivial case where we're being asked to read zero bytes.

                assert (true_bytes =~= Seq::<u8>::empty());
                assert (maybe_corrupted(Seq::<u8>::empty(), true_bytes, Seq::<int>::empty()));
                return Ok(Vec::<u8>::new());
            }

            let pm_regions = wrpm_regions.get_pm_regions_ref();

            let log_area_len: u64 = self.infos[which_log as usize].log_area_len;
            let relative_pos: u64 = (pos - info.head) as u64;
            if relative_pos >= log_area_len - info.head_log_area_offset {

                // Case 1: The position we're being asked to read appears
                // in the log area before the log head. So the read doesn't
                // need to wrap.
                //
                // We could compute the address to write to with:
                //
                // `write_addr = ABSOLUTE_POS_OF_LOG_AREA + pos % info.log_area_len;`
                //
                // But we can replace the expensive modulo operation above with two subtraction
                // operations as follows. This is somewhat subtle, but we have verification backing
                // us up and proving this optimization correct.

                let addr = ABSOLUTE_POS_OF_LOG_AREA + relative_pos - (info.log_area_len - info.head_log_area_offset);
                proof { self.lemma_read_of_continuous_range(pm_regions@, multilog_id, which_log, pos as int,
                                                            len as int, addr as int); }
                let bytes = match pm_regions.read_unaligned(which_log as usize, addr, len) {
                    Ok(bytes) => bytes,
                    Err(e) => {
                        assert(false);
                        return Err(MultiLogErr::PmemErr { err: e });
                    }
                };
                return Ok(bytes);
            }

            // The log area wraps past the point we're reading from, so we
            // need to compute the maximum length we can read without
            // wrapping to be able to figure out whether we need to wrap.

            let max_len_without_wrapping: u64 = log_area_len - info.head_log_area_offset - relative_pos;
            assert(max_len_without_wrapping == info.log_area_len -
                   relative_log_pos_to_log_area_offset(pos - info.head,
                                                       info.head_log_area_offset as int, info.log_area_len as int));

            // Whether we need to wrap or not, we know the address where
            // our read should start, so we can compute that and put it in
            // `addr`.
            //
            // We could compute the address to write to with:
            //
            // `write_addr = ABSOLUTE_POS_OF_LOG_AREA + pos % info.log_area_len;`
            //
            // But we can replace the expensive modulo operation above with
            // one addition operation as follows. This is somewhat subtle,
            // but we have verification backing us up and proving this
            // optimization correct.

            let addr: u64 = ABSOLUTE_POS_OF_LOG_AREA + relative_pos + info.head_log_area_offset;
            assert(addr == ABSOLUTE_POS_OF_LOG_AREA +
                   relative_log_pos_to_log_area_offset(pos - info.head,
                                                       info.head_log_area_offset as int,
                                                       info.log_area_len as int));

            if len <= max_len_without_wrapping {

                // Case 2: We're reading few enough bytes that we don't have to wrap.

                proof { self.lemma_read_of_continuous_range(pm_regions@, multilog_id, which_log, pos as int,
                                                            len as int, addr as int); }
                let bytes = match pm_regions.read_unaligned(which_log as usize, addr, len) {
                    Ok(bytes) => bytes,
                    Err(e) => {
                        assert(false);
                        return Err(MultiLogErr::PmemErr { err: e });
                    }
                };
                return Ok(bytes);
            }

            // Case 3: We're reading enough bytes that we have to wrap.
            // That necessitates doing two contiguous reads, one from the
            // end of the log area and one from the beginning, and
            // concatenating the results.

            proof {
                self.lemma_read_of_continuous_range(pm_regions@, multilog_id, which_log, pos as int,
                                                    max_len_without_wrapping as int, addr as int);
            }

            // let mut part1 = pm_regions.read_unaligned(which_log as usize, addr, max_len_without_wrapping).map_err(|e| MultiLogErr::PmemErr { err: e })?;
            let mut part1 = match pm_regions.read_unaligned(which_log as usize, addr, max_len_without_wrapping) {
                Ok(bytes) => bytes,
                Err(e) => {
                    assert(false);
                    return Err(MultiLogErr::PmemErr { err: e });
                }
            };

            proof {
                self.lemma_read_of_continuous_range(pm_regions@, multilog_id, which_log,
                                                    pos + max_len_without_wrapping,
                                                    len - max_len_without_wrapping,
                                                    ABSOLUTE_POS_OF_LOG_AREA as int);
            }

            // let mut part2 = pm_regions.read_unaligned(which_log as usize, ABSOLUTE_POS_OF_LOG_AREA, len - max_len_without_wrapping).map_err(|e| MultiLogErr::PmemErr { err: e })?;
            let mut part2 = match pm_regions.read_unaligned(which_log as usize, ABSOLUTE_POS_OF_LOG_AREA, len - max_len_without_wrapping) {
                Ok(bytes) => bytes,
                Err(e) => {
                    assert(false);
                    return Err(MultiLogErr::PmemErr { err: e });
                }
            };

            // Now, prove that concatenating them produces the correct
            // bytes to return. The subtle thing in this argument is that
            // the bytes are only correct modulo corruption. And the
            // "correct modulo corruption" specification function talks
            // about the concrete addresses the bytes were read from and
            // demands that those addresses all be distinct.

            proof {
                let true_part1 = s.log.subrange(pos - s.head, pos + max_len_without_wrapping - s.head);
                let true_part2 = s.log.subrange(pos + max_len_without_wrapping - s.head, pos + len - s.head);
                let addrs1 = Seq::<int>::new(max_len_without_wrapping as nat, |i: int| i + addr);
                let addrs2 = Seq::<int>::new((len - max_len_without_wrapping) as nat,
                                           |i: int| i + ABSOLUTE_POS_OF_LOG_AREA);
                assert(true_part1 + true_part2 =~= s.log.subrange(pos - s.head, pos + len - s.head));

                if !pm_regions.constants().impervious_to_corruption {
                    assert(maybe_corrupted(part1@ + part2@, true_part1 + true_part2, addrs1 + addrs2));
                    assert(all_elements_unique(addrs1 + addrs2));
                }
            }

            // Append the two byte vectors together and return the result.

            part1.append(&mut part2);
            Ok(part1)
        }

        // The `get_head_tail_and_capacity` method returns the head, tail,
        // and capacity of one of the logs. See `README.md` for more
        // documentation and examples of its use.
        #[allow(unused_variables)]
        pub exec fn get_head_tail_and_capacity<Perm, PMRegions>(
            &self,
            wrpm_regions: &WriteRestrictedPersistentMemoryRegions<Perm, PMRegions>,
            which_log: u32,
            Ghost(multilog_id): Ghost<u128>,
        ) -> (result: Result<(u128, u128, u64), MultiLogErr>)
            where
                Perm: CheckPermission<Seq<Seq<u8>>>,
                PMRegions: PersistentMemoryRegions
            requires
                self.inv(wrpm_regions, multilog_id)
            ensures
                ({
                    let log = self@[which_log as int];
                    match result {
                        Ok((result_head, result_tail, result_capacity)) => {
                            &&& which_log < self@.num_logs()
                            &&& result_head == log.head
                            &&& result_tail == log.head + log.log.len()
                            &&& result_capacity == log.capacity
                        },
                        Err(MultiLogErr::InvalidLogIndex{ }) => {
                            which_log >= self@.num_logs()
                        },
                        _ => false
                    }
                })
        {
            // Check for an invalid `which_log` parameter.

            if which_log >= self.num_logs {
                return Err(MultiLogErr::InvalidLogIndex{ });
            }

            let ghost w = which_log as int;
            assert(is_valid_log_index(which_log, self.num_logs)); // triggers useful foralls in invariants

            // We cache information in `self.infos` that lets us easily
            // compute the return values.

            let info = &self.infos[which_log as usize];
            Ok((info.head, info.head + info.log_length as u128, info.log_area_len))
        }
    }

}

================
File: ./storage_node/src/multilog/start_v.rs
================

//! This file contains functions for starting to use persistent memory
//! as a multilog. Such starting is done either after setup or after a
//! crash.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::multilog::inv_v::*;
use crate::multilog::layout_v::*;
use crate::multilog::multilogimpl_t::MultiLogErr;
use crate::multilog::multilogimpl_v::LogInfo;
use crate::multilog::multilogspec_t::AbstractMultiLogState;
use crate::pmem::pmemspec_t::{PersistentMemoryRegions, CRC_SIZE, CDB_SIZE};
use crate::pmem::pmemutil_v::{check_cdb, check_crc};
use crate::pmem::pmcopy_t::*;
use crate::pmem::traits_t::size_of;
use builtin::*;
use builtin_macros::*;
use vstd::arithmetic::div_mod::*;
use vstd::bytes::*;
use vstd::prelude::*;
use vstd::slice::*;

verus! {

    // This exported function reads the corruption-detecting boolean
    // and returns it.
    //
    // `pm_regions` -- the persistent-memory regions to read from
    //
    // The result is a `Result<bool, MultiLogErr>` with the following meanings:
    //
    // `Err(MultiLogErr::CRCMismatch)` -- The CDB couldn't be read due
    // to a CRC error.
    //
    // `Ok(b)` -- The CDB could be read and represents the boolean `b`.
    pub fn read_cdb<PMRegions: PersistentMemoryRegions>(pm_regions: &PMRegions) -> (result: Result<bool, MultiLogErr>)
        requires
            pm_regions.inv(),
            pm_regions@.len() > 0,
            recover_cdb(pm_regions@[0].committed()).is_Some(),
            pm_regions@.no_outstanding_writes(),
            metadata_types_set(pm_regions@.committed()),
        ensures
            match result {
                Ok(b) => Some(b) == recover_cdb(pm_regions@[0].committed()),
                // To make sure this code doesn't spuriously generate CRC-mismatch errors,
                // it's obligated to prove that it won't generate such an error when
                // the persistent memory is impervious to corruption.
                Err(MultiLogErr::CRCMismatch) => !pm_regions.constants().impervious_to_corruption,
                _ => false,
            }
    {
        let ghost mem = pm_regions@.committed()[0];
        assert(metadata_types_set_in_first_region(mem));
        let ghost log_cdb_addrs = Seq::new(u64::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_LOG_CDB + i);
        let ghost true_cdb_bytes = mem.subrange(ABSOLUTE_POS_OF_LOG_CDB as int, ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of());
        let ghost true_cdb = u64::spec_from_bytes(true_cdb_bytes);
       
        // check_cdb does not require that the true bytes be contiguous, so we need to make Z3 confirm that the 
        // contiguous region we are using as the true value matches the address sequence we pass in.
        assert(true_cdb_bytes == Seq::new(u64::spec_size_of() as nat, |i: int| mem[log_cdb_addrs[i]]));

        // let log_cdb = pm_regions.read_aligned::<u64>(0, ABSOLUTE_POS_OF_LOG_CDB, Ghost(true_cdb)).map_err(|e| MultiLogErr::PmemErr { err: e })?;
        let log_cdb = match pm_regions.read_aligned::<u64>(0, ABSOLUTE_POS_OF_LOG_CDB, Ghost(true_cdb)) {
            Ok(log_cdb) => log_cdb,
            Err(e) => {
                assert(false);
                return Err(MultiLogErr::PmemErr{ err: e });
            }
        };

        let result = check_cdb(log_cdb, Ghost(true_cdb), Ghost(mem),
                               Ghost(pm_regions.constants().impervious_to_corruption),
                               Ghost(log_cdb_addrs));
        match result {
            Some(b) => Ok(b),
            None => Err(MultiLogErr::CRCMismatch)
        }
    }

    // This function reads the log information for a single log from
    // persistent memory.
    //
    // `pm_regions` -- the persistent memory regions to read from
    //
    // `multilog_id` -- the GUID of the multilog
    //
    // `cdb` -- the corruption-detection boolean
    //
    // `num_logs` -- the number of logs in the multilog
    //
    // `which_log` -- which among the multilog's logs to read
    //
    // The result is a `Result<LogInfo, MultiLogErr>` with the following meanings:
    //
    // `Ok(log_info)` -- The information `log_info` has been
    // successfully read.
    //
    // `Err(MultiLogErr::CRCMismatch)` -- The region couldn't be read due
    // to a CRC error when reading data.
    //
    // `Err(MultiLogErr::StartFailedDueToProgramVersionNumberUnsupported)`
    // -- The program version number stored in persistent memory is
    // one that this code doesn't know how to recover from. It was
    // presumably created by a later version of this code.
    //
    // `Err(MultiLogErr::StartFailedDueToMultilogIDMismatch)` -- The
    // multilog ID stored in persistent memory doesn't match the one
    // passed to the `start` routine. So the caller of `start` gave
    // the wrong persistent memory region or the wrong ID.
    //
    // `Err(MultiLogErr::StartFailedDueToRegionSizeMismatch)` -- The
    // region size stored in persistent memory doesn't match the size
    // of the region passed to the `start` routine. So the caller of
    // `start` is likely using a persistent memory region that starts
    // in the right place but ends in the wrong place.
    //
    // `Err(MultiLogErr::StartFailedDueToInvalidMemoryContents)` --
    // The region's contents aren't valid, i.e., they're not
    // recoverable to a valid log. The user must have requested to
    // start using the wrong region of persistent memory.
    fn read_log_variables<PMRegions: PersistentMemoryRegions>(
        pm_regions: &PMRegions,
        multilog_id: u128,
        cdb: bool,
        num_logs: u32,
        which_log: u32,
    ) -> (result: Result<LogInfo, MultiLogErr>)
        requires
            pm_regions.inv(),
            is_valid_log_index(which_log, num_logs),
            num_logs == pm_regions@.len(),
            pm_regions@[which_log as int].no_outstanding_writes(),
            metadata_types_set(pm_regions@.committed()),
            deserialize_and_check_log_cdb(pm_regions@[0].committed()) is Some,
            cdb == deserialize_and_check_log_cdb(pm_regions@[0].committed()).unwrap(),
        ensures
            ({
                let w = which_log as int;
                let state = recover_abstract_log_from_region_given_cdb(pm_regions@[w].committed(), multilog_id,
                                                                       num_logs as int, w, cdb);
                match result {
                    Ok(info) => state.is_Some() ==> {
                        &&& metadata_consistent_with_info(pm_regions@[w], multilog_id, num_logs, which_log, cdb, info)
                        &&& info_consistent_with_log_area(pm_regions@[w], info, state.unwrap())
                    },
                    Err(MultiLogErr::CRCMismatch) =>
                        state.is_Some() ==> !pm_regions.constants().impervious_to_corruption,
                    _ => state.is_None()
                }
            })
    {
        let ghost mem = pm_regions@.committed()[which_log as int];
        let ghost w = which_log as int;
        let ghost state = recover_abstract_log_from_region_given_cdb(pm_regions@[w].committed(), multilog_id,
                                                                     num_logs as int, w, cdb);

        // Check that the region is at least the minimum required size. If
        // not, indicate invalid memory contents.

        let region_size = pm_regions.get_region_size(which_log as usize);
        if region_size < ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }

        // Read the global metadata and its CRC, and check that the
        // CRC matches.

        assert(metadata_types_set_in_region(mem, cdb));

        let ghost true_global_metadata = GlobalMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()));
        let ghost true_crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()));

        let ghost metadata_addrs = Seq::new(GlobalMetadata::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_GLOBAL_METADATA + i);
        let ghost crc_addrs = Seq::new(u64::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_GLOBAL_CRC + i);

        let ghost true_bytes = Seq::new(GlobalMetadata::spec_size_of()as nat, |i: int| mem[metadata_addrs[i] as int]);
        let ghost true_crc_bytes = Seq::new(u64::spec_size_of() as nat, |i: int| mem[crc_addrs[i] as int]);

        let global_metadata = match pm_regions.read_aligned::<GlobalMetadata>(which_log as usize, ABSOLUTE_POS_OF_GLOBAL_METADATA, Ghost(true_global_metadata)) {
            Ok(global_metadata) => global_metadata,
            Err(e) => {
                assert(false);
                return Err(MultiLogErr::PmemErr { err: e });
            }
        };
        let global_crc = match pm_regions.read_aligned::<u64>(which_log as usize, ABSOLUTE_POS_OF_GLOBAL_CRC, Ghost(true_crc)) {
            Ok(global_crc) => global_crc,
            Err(e) => {
                assert(false);
                return Err(MultiLogErr::PmemErr { err: e });
            }
        };

        assert(true_global_metadata.spec_to_bytes() == true_bytes && true_crc.spec_to_bytes() == true_crc_bytes);

        if !check_crc(global_metadata.as_slice(), global_crc.as_slice(),
                      Ghost(mem), Ghost(pm_regions.constants().impervious_to_corruption),
                      Ghost(metadata_addrs),
                      Ghost(crc_addrs)) {
            return Err(MultiLogErr::CRCMismatch);
        }
        
        let ghost true_bytes = Seq::new(metadata_addrs.len(), |i: int| mem[metadata_addrs[i] as int]);
        let global_metadata = global_metadata.extract_init_val(
            Ghost(true_global_metadata), 
            Ghost(true_bytes),
            Ghost(pm_regions.constants().impervious_to_corruption)
        );

        // Check the global metadata for validity. If it isn't valid,
        // e.g., due to the program GUID not matching, then return an
        // error. Such invalidity can't happen if the persistent
        // memory is recoverable.

        if global_metadata.program_guid != MULTILOG_PROGRAM_GUID {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }

        if global_metadata.version_number != MULTILOG_PROGRAM_VERSION_NUMBER {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToProgramVersionNumberUnsupported{
                which_log,
                version_number: global_metadata.version_number,
                max_supported: MULTILOG_PROGRAM_VERSION_NUMBER,
            })
        }

        if global_metadata.length_of_region_metadata != size_of::<RegionMetadata>() as u64 {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }

        // Read the region metadata and its CRC, and check that the
        // CRC matches.
        let ghost metadata_addrs = Seq::new(RegionMetadata::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_REGION_METADATA + i);
        let ghost crc_addrs = Seq::new(u64::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_REGION_CRC + i);

        let ghost true_region_metadata = RegionMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of()));
        let ghost true_crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of()));

        let region_metadata = match pm_regions.read_aligned::<RegionMetadata>(which_log as usize, ABSOLUTE_POS_OF_REGION_METADATA, Ghost(true_region_metadata)) {
            Ok(region_metadata) => region_metadata,
            Err(e) => {
                assert(false);
                return Err(MultiLogErr::PmemErr { err: e });
            }
        };
        let region_crc = match pm_regions.read_aligned::<u64>(which_log as usize, ABSOLUTE_POS_OF_REGION_CRC, Ghost(true_crc)) {
            Ok(region_crc) => region_crc,
            Err(e) => {
                assert(false);
                return Err(MultiLogErr::PmemErr { err: e });
            }
        };

        let ghost true_bytes = Seq::new(metadata_addrs.len(), |i: int| mem[metadata_addrs[i] as int]);
        let ghost true_crc_bytes = Seq::new(crc_addrs.len(), |i: int| mem[crc_addrs[i] as int]);
        assert(true_region_metadata.spec_to_bytes() == true_bytes && true_crc.spec_to_bytes() == true_crc_bytes);

        if !check_crc(region_metadata.as_slice(), region_crc.as_slice(),
                      Ghost(mem), Ghost(pm_regions.constants().impervious_to_corruption),
                      Ghost(metadata_addrs),
                      Ghost(crc_addrs)) {
            return Err(MultiLogErr::CRCMismatch);
        }

        let ghost true_bytes = Seq::new(metadata_addrs.len(), |i: int| mem[metadata_addrs[i] as int]);
        let region_metadata = region_metadata.extract_init_val(
            Ghost(true_region_metadata), 
            Ghost(true_bytes),
            Ghost(pm_regions.constants().impervious_to_corruption)
        );

        // Check the region metadata for validity. If it isn't valid,
        // e.g., due to the encoded region size not matching the
        // actual region size, then return an error. Such invalidity
        // can't happen if the persistent memory is recoverable.

        if region_metadata.region_size != region_size {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToRegionSizeMismatch{
                which_log,
                region_size_expected: region_size,
                region_size_read: region_metadata.region_size,
            })
        }

        if region_metadata.multilog_id != multilog_id {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToMultilogIDMismatch{
                which_log,
                multilog_id_expected: multilog_id,
                multilog_id_read: region_metadata.multilog_id,
            })
        }

        if region_metadata.num_logs != num_logs {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }

        if region_metadata.which_log != which_log {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }

        if region_metadata.log_area_len > region_size {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }
        if region_size - region_metadata.log_area_len < ABSOLUTE_POS_OF_LOG_AREA {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }
        if region_metadata.log_area_len < MIN_LOG_AREA_SIZE {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }

        // Read the log metadata and its CRC, and check that the
        // CRC matches. The position where to find the log
        // metadata depend on the CDB.

        let log_metadata_pos = if cdb { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE }
                                  else { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE };
        let log_crc_pos = if cdb { ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE }
                             else { ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE };
        assert(log_metadata_pos == get_log_metadata_pos(cdb));

        let ghost true_log_metadata = LogMetadata::spec_from_bytes(extract_bytes(mem, log_metadata_pos as int, LogMetadata::spec_size_of()));
        let ghost true_crc = u64::spec_from_bytes(extract_bytes(mem, log_metadata_pos + LogMetadata::spec_size_of(), u64::spec_size_of()));

        let ghost log_metadata_addrs = Seq::new(LogMetadata::spec_size_of() as nat, |i: int| log_metadata_pos + i);
        let ghost crc_addrs = Seq::new(u64::spec_size_of() as nat, |i: int| log_crc_pos + i);
        
        let ghost true_bytes = Seq::new(log_metadata_addrs.len(), |i: int| mem[log_metadata_addrs[i] as int]);
        let ghost true_crc_bytes = Seq::new(crc_addrs.len(), |i: int| mem[crc_addrs[i] as int]);

        assert(extract_bytes(mem, log_metadata_pos as int, LogMetadata::spec_size_of()) == true_bytes);
        let log_metadata = match pm_regions.read_aligned::<LogMetadata>(which_log as usize, log_metadata_pos, Ghost(true_log_metadata)) {
            Ok(log_metadata) => log_metadata,
            Err(e) => {
                assert(false);
                return Err(MultiLogErr::PmemErr { err: e });
            }
        };
        let log_crc = match pm_regions.read_aligned::<u64>(which_log as usize, log_crc_pos, Ghost(true_crc)) {
            Ok(log_crc) => log_crc,
            Err(e) => {
                assert(false);
                return Err(MultiLogErr::PmemErr { err: e });
            }
        };

        assert(true_log_metadata.spec_to_bytes() == true_bytes && true_crc.spec_to_bytes() == true_crc_bytes);
        if !check_crc(log_metadata.as_slice(), log_crc.as_slice(), Ghost(mem), Ghost(pm_regions.constants().impervious_to_corruption),
                                    Ghost(log_metadata_addrs), Ghost(crc_addrs)) {
            return Err(MultiLogErr::CRCMismatch);
        }

        let ghost true_bytes = Seq::new(log_metadata_addrs.len(), |i: int| mem[log_metadata_addrs[i] as int]);
        let log_metadata = log_metadata.extract_init_val(
            Ghost(true_log_metadata), 
            Ghost(true_bytes),
            Ghost(pm_regions.constants().impervious_to_corruption)
        );

        // Check the log metadata for validity. If it isn't valid,
        // e.g., due to the log length being greater than the log area
        // length, then return an error. Such invalidity can't happen
        // if the persistent memory is recoverable.

        let head = log_metadata.head;
        let log_length = log_metadata.log_length;
        if log_length > region_metadata.log_area_len {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }
        if log_length as u128 > u128::MAX - head {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(MultiLogErr::StartFailedDueToInvalidMemoryContents{ which_log })
        }

        // Compute the offset into the log area where the head of the
        // log is. This is the u128 `head` mod the u64
        // `log_area_len`. To prove that this will fit in a `u64`, we
        // need to invoke a math lemma saying that the result of a
        // modulo operation is always less than the divisor.

        proof { lemma_mod_bound(head as int, region_metadata.log_area_len as int); }
        let head_log_area_offset: u64 = (head % region_metadata.log_area_len as u128) as u64;

        // Return the log info. This necessitates computing the
        // pending tail position relative to the head, but this is
        // easy: It's the same as the log length. This is because,
        // upon recovery, there are no pending appends beyond the tail
        // of the log.

        Ok(LogInfo{
            log_area_len: region_metadata.log_area_len,
            head,
            head_log_area_offset,
            log_length,
            log_plus_pending_length: log_length
        })
    }

    // This function reads the log information for all logs in a
    // collection of persistent memory regions
    //
    // `pm_regions` -- the persistent memory regions to read from
    //
    // `multilog_id` -- the GUID of the multilog
    //
    // `cdb` -- the corruption-detection boolean
    //
    // `num_regions` -- the number of regions in the collection of
    // persistent memory regions
    //
    // `state` -- the abstract state that this memory is known to be
    // recoverable to
    //
    // The result is a `Result<LogInfo, MultiLogErr>` with the following meanings:
    //
    // `Err(MultiLogErr::CRCMismatch)` -- The region couldn't be read due
    // to a CRC error when reading data.
    //
    // `Ok(log_info)` -- The information `log_info` has been
    // successfully read.
    pub fn read_logs_variables<PMRegions: PersistentMemoryRegions>(
        pm_regions: &PMRegions,
        multilog_id: u128,
        cdb: bool,
        num_regions: u32,
        Ghost(state): Ghost<AbstractMultiLogState>,
    ) -> (result: Result<Vec<LogInfo>, MultiLogErr>)
        requires
            pm_regions.inv(),
            num_regions == pm_regions@.len(),
            num_regions > 0,
            pm_regions@.no_outstanding_writes(),
            memory_matches_deserialized_cdb(pm_regions@, cdb),
            recover_given_cdb(pm_regions@.committed(), multilog_id, cdb) == Some(state),
            metadata_types_set(pm_regions@.committed()),
            deserialize_and_check_log_cdb(pm_regions@[0].committed()) is Some,
            cdb == deserialize_and_check_log_cdb(pm_regions@[0].committed()).unwrap(),
        ensures
            match result {
                Ok(info) => {
                    &&& each_metadata_consistent_with_info(pm_regions@, multilog_id, num_regions, cdb, info@)
                    &&& each_info_consistent_with_log_area(pm_regions@, num_regions, info@, state)
                },
                Err(MultiLogErr::CRCMismatch) => !pm_regions.constants().impervious_to_corruption,
                _ => false,
            }
    {
        let mut infos = Vec::<LogInfo>::new();
        for which_log in 0..num_regions
            invariant
                pm_regions.inv(),
                which_log == infos.len(),
                forall |j:u32| j < which_log ==> {
                    &&& info_consistent_with_log_area(#[trigger] pm_regions@[j as int], infos[j as int], state[j as int])
                    &&& metadata_consistent_with_info(pm_regions@[j as int], multilog_id, num_regions, j, cdb,
                                                    infos[j as int])
                },
                num_regions == pm_regions@.len(),
                recover_given_cdb(pm_regions@.committed(), multilog_id, cdb) == Some(state),
                pm_regions@.no_outstanding_writes(),
                metadata_types_set(pm_regions@.committed()),
                deserialize_and_check_log_cdb(pm_regions@[0].committed()) is Some,
                cdb == deserialize_and_check_log_cdb(pm_regions@[0].committed()).unwrap(),
        {
            // Before calling `read_log_variables`, establish that
            // region `which_log` is recoverable. This is useful
            // because the postcondition of `read_log_variables`
            // doesn't let one draw many conclusions unless one knows
            // the region is recoverable.

            let ghost region_state = recover_abstract_log_from_region_given_cdb(
                pm_regions@[which_log as int].committed(), multilog_id, num_regions as int, which_log as int, cdb);
            assert (region_state.is_Some()) by {
                // To trigger the `forall` inside `recover_given_cdb`, we need to set a ghost
                // variable to the `seq_option` value created inside `recover_given_cdb`.
                let ghost seq_option = pm_regions@.committed().map(
                    |idx, c| recover_abstract_log_from_region_given_cdb(c, multilog_id, num_regions as int,
                                                                        idx, cdb));
                // Then, we trigger it by mentioning `seq_option[which_log as int]`.
                assert(region_state == seq_option[which_log as int]);
            }

            let info = read_log_variables(pm_regions, multilog_id, cdb, num_regions, which_log)?;
            infos.push(info);
        }
        Ok(infos)
    }
}

================
File: ./storage_node/src/multilog/README.md
================

# Multilog:  A proven-correct multilog using persistent memory

## Overview

A `MultiLogImpl` implements multiple logs that can be atomically
appended to. Each log is stored on its own persistent memory
region. The implementation handles crash consistency, ensuring
that even if the process or machine crashes, the multilog acts
like a multilog across the crashes.

The implementation handles bit corruption on the persistent
memory, but only for the implementation's internal metadata. If
you want to be sure that *data* you wrote to the multilog isn't
corrupted when you read it back out, you should include your own
corruption-detecting information like a CRC. You can be confident
that the implementation read your data from the same place on
persistent memory where it was stored, but the data still might
have gotten corrupted on that memory.

## Using the library

To create a `MultiLogImpl`, you need an object satisfying the
`PersistentMemoryRegions` trait, i.e., one implementing a list of
persistent-memory regions. The reason we take a single
`PersistentMemoryRegions` input rather than multiple
`PersistentMemory` inputs is to do efficient flushes to multiple
persistent memories at once. We anticipate that several persistent
memory regions will be on the same physical memory, and can thus be
efficiently flushed collectively with a single flush call.

To set up persistent memory objects to store an initial empty
multilog, you call `MultiLogImpl::setup`. For instance, here's
code to first create a `PersistentMemoryRegions` out of volatile
memory mocking persistent memory, then to set up a multilog on
them.

```
let mut region_sizes: Vec<u64> = Vec::<u64>::new();
region_sizes.push(4096);
region_sizes.push(1024);
if let Ok(mut pm_regions) =
    VolatileMemoryMockingPersistentMemoryRegions::new_mock_only_for_use_in_testing(region_sizes.as_slice()) {
   if let Ok(capacities, multilog_id) = MultiLogImpl::setup(&mut pm_regions) {
       println!("Multilog {} is set up with per-region capacities {:?}", multilog_id, capacities);
   }
}
```

Remember that such setup is only intended to be run a single time.
Once you've set up a multilog, you shouldn't set it up again as
that will clear its state. The number of logs in the multilog will
match the number of regions that you pass in, since it uses region
#`n` to store log #`n`.

Once you've set up a multilog, you can start using it. A multilog
is only intended to be used by one process at a time. But if the
process or the machine crashes, it's fine to start using it again.
Indeed, that's the whole point: the most interesting verified
property of this code is that the multilog acts consistently like
a multilog even across crashes.

To start using a multilog, run `MultiLogImpl::start`, passing the
persistent-memory regions and the multilog ID, as in the following
example:

```
match MultiLogImpl::start(pm_regions, multilog_id) {
    Ok(mut multilog) => ...,
    Err(e) => ...,
}
```

To use a multilog, you can do five operations: tentatively append,
commit, read, advance head, and get information. Here's a
description of them all:

To tentatively append some data to the end of log #n, use
`MultiLogImpl::tentatively_append`. For example, the following
code tentatively appends [30, 42, 100] to log #0 and then
tentatively appends [30, 42, 100, 152] to log #1:

```
let mut v: Vec<u8> = Vec::<u8>::new();
v.push(30); v.push(42); v.push(100);
if let Ok(pos) = multilog.tentatively_append(0, v.as_slice()) {
    if let Ok((head, tail, capacity)) = multilog.get_head_tail_and_capacity(0) {
        assert(head == 0);
        assert(tail == 0); // it's only tentative, so tail unchanged
    }
}
v.push(152);
multilog.tentatively_append(1, v.as_slice());
```

(That example also shows an example of the get-information call.)
Note that tentatively appending doesn't actually append to the
log, so the tail doesn't go up. It puts the append operation
inside of the current append transaction, which will be aborted if
the machine crashes before you commit the transaction. It will
also be aborted if the process accessing the multilog via a
`MultiLogImpl` crashes. You can tentatively append to one or more
logs before committing.

To atomically commit all tentative appends in the current append
transaction, use `MultiLogImpl::commit`, as in the following
example:

```
if let Ok(_) = multilog.commit() {
    if let Ok((head, tail, capacity)) = multilog.get_head_tail_and_capacity(0) {
        assert(head == 0);
        assert(tail == 3);
    }
    if let Ok((head, tail, capacity)) = multilog.get_head_tail_and_capacity(1) {
        assert(head == 0);
        assert(tail == 4);
    }
}
```

This, unlike `tentatively_append`, advances logs' tails.

Commit is atomic even with respect to crashes, so it logically
does all the tentative appends at once. So even if the machine
crashes in the middle of the commit operation, or if the process
that called `commit` crashes in the middle of the commit
operation, either all tentative appends will happen or none of
them will.

If a crash occurs in the middle of a commit but the tentative
appends aren't performed, those tentative appends are dropped and
a fresh empty append transaction is started. The same happens if
the crash occurs before you call `commit`.

Once you have data committed in the log, you can read it using
`MultiLogImpl::read`, as in the following example:

```
if let Ok((bytes)) = multilog.read(0, 1, 2) {
    assert(bytes.len() == 2);
    assert(pm_regions.constants().impervious_to_corruption ==> bytes[0] == 42);
}
```

Note, as discussed before, that the bytes returned might be
corruptions of the data you appended, since the implementation of
the multilog only checks for corruption of its own internal
metadata. If you want to use the bytes, we suggest including a CRC
and checking that CRC after any read.

If the memory storing the log is getting too full, you'll need to
advance the log's head with `MultiLogImpl::advance_head`.  This
doesn't affect the logical contents of the log or the positions of
bytes in the log, but does restrict you from reading earlier than
the new head. So, for instance, if you advance the head from 0 to
1000, you can still read byte #1042 by reading from position
#1042. You just can't read byte #42 anymore without getting an
error return value.

The advance-head operation is not tentative, so it doesn't need a
commit. By the time the operation returns, it will have
definitively happened, and that head advancement will survive a
subsequent crash.

The advance-head operation is atomic. That is, either the head
advances or it doesn't. Even if there's an intermediate crash, the
multilog will either be in a state where the advance-head happened
or a state where it didn't happen.

Here's an example of a call to `advance_head`:

```
if let Ok(()) = multilog.advance_head(0, 2) {
    if let Ok((head, tail, capacity)) = multilog.get_head_tail_and_capacity(0) {
        assert(head == 2);
        assert(tail == 3);
    }
    if let Ok((bytes)) = multilog.read(0, 2, 1) {
        assert(pm_regions.constants().impervious_to_corruption ==> bytes[0] == 100);
    }
    let e = multilog.read(0, 0, 1);
    assert(e == Result::<Vec<u8>, MultiLogErr>::Err(MultiLogErr::CantReadBeforeHead{head: 2}));
}
```

## Code organization

The code is organized into the following files. Files ending in `_t.rs` are
trusted specifications (e.g., of how persistent memory behaves and of how a
multilog should operate) that must be audited and read to understand the
semantics being proven. Files ending in `_v.rs` are verified and untrusted and
so do not have to be read to have confidence in the correctness of the code.

<!-- * `lib.rs` packages this crate as a library -->
* `multilogspec_t.rs` specifies the correct behavior of an abstract multilog,
  e.g., what should happen during a call to `tentatively_append`
* `multilogimpl_t.rs` implements `MultiLogImpl`, the main type used by
  clients of this library
* `multilogimpl_v.rs` implements `UntrustedMultiLogImpl`, verified for
  correctness and invoked by `MultiLogImpl` methods
* `inv_v.rs` provides invariants of the multilog code and proofs about those
  invariants
* `layout_v.rs` provides constants, functions, and proofs about how the
  multilog implementation lays out its metadata and data in persistent memory
* `append_v.rs` provides helper lemmas used by the multilog code to
  prove that tentative appends are done correctly
* `setup_v.rs` implements subroutines called when the multilog code is
  setting up a collection of persistent memory regions to act as a multilog
* `start_v.rs` implements subroutines called when the multilog code is
  starting up, either immediately after setup or to recover after a crash

## Example

Here's an example of a program that uses a `MultiLogImpl`:

```
fn test_multilog_on_memory_mapped_file() -> Option<()>
{
    // To test the multilog, we use files in the current directory that mock persistent-memory
    // regions. Here we use such regions, one of size 4096 and one of size 1024.
    let mut region_sizes: Vec<u64> = Vec::<u64>::new();
    region_sizes.push(4096);
    region_sizes.push(1024);

    // Create the multipersistent memory out of the two regions.
    let file_name = vstd::string::new_strlit("test_multilog");
    #[cfg(target_os = "windows")]
    let mut pm_regions = FileBackedPersistentMemoryRegions::new(
        &file_name,
        MemoryMappedFileMediaType::SSD,
        region_sizes.as_slice(),
        FileCloseBehavior::TestingSoDeleteOnClose
    ).ok()?;
    #[cfg(target_os = "linux")]
    let mut pm_regions = FileBackedPersistentMemoryRegions::new(
        &file_name,
        region_sizes.as_slice(),
        PersistentMemoryCheck::DontCheckForPersistentMemory,
    ).ok()?;

    // Set up the memory regions to contain a multilog. The capacities will be less
    // than 4096 and 1024 because a few bytes are needed in each region for metadata.
    let (capacities, multilog_id) = MultiLogImpl::setup(&mut pm_regions).ok()?;
    runtime_assert(capacities.len() == 2);
    runtime_assert(capacities[0] <= 4096);
    runtime_assert(capacities[1] <= 1024);

    // Start accessing the multilog.
    let mut multilog = MultiLogImpl::start(pm_regions, multilog_id).ok()?;

    // Tentatively append [30, 42, 100] to log #0 of the multilog.
    let mut v: Vec<u8> = Vec::<u8>::new();
    v.push(30); v.push(42); v.push(100);
    let pos = multilog.tentatively_append(0, v.as_slice()).ok()?;
    runtime_assert(pos == 0);

    // Note that a tentative append doesn't actually advance the tail. That
    // doesn't happen until the next commit.
    let (head, tail, _capacity) = multilog.get_head_tail_and_capacity(0).ok()?;
    runtime_assert(head == 0);
    runtime_assert(tail == 0);

    // Also tentatively append [30, 42, 100, 152] to log #1. This still doesn't
    // commit anything to the log.
    v.push(152);
    let pos = multilog.tentatively_append(1, v.as_slice()).ok()?;
    runtime_assert(pos == 0);

    // Now commit the tentative appends. This causes log #0 to have tail 3
    // and log #1 to have tail 4.
    if multilog.commit().is_err() {
        runtime_assert(false); // can't fail
    }
    match multilog.get_head_tail_and_capacity(0) {
        Ok((head, tail, capacity)) => {
            runtime_assert(head == 0);
            runtime_assert(tail == 3);
        },
        _ => runtime_assert(false) // can't fail
    }
    match multilog.get_head_tail_and_capacity(1) {
        Ok((head, tail, capacity)) => {
            runtime_assert(head == 0);
            runtime_assert(tail == 4);
        },
        _ => runtime_assert(false) // can't fail
    }

    // We read the 2 bytes starting at position 1 of log #0. We should
    // read bytes [42, 100]. This is only guaranteed if the memory
    // wasn't corrupted.
    if let Ok(bytes) = multilog.read(0, 1, 2) {
        runtime_assert(bytes.len() == 2);
        assert(multilog.constants().impervious_to_corruption ==> bytes[0] == 42);
    }

    // We now advance the head of log #0 to position 2. This causes the
    // head to become 2 and the tail stays at 3.
    match multilog.advance_head(0, 2) {
        Ok(()) => runtime_assert(true),
        _ => runtime_assert(false) // can't fail
    }
    match multilog.get_head_tail_and_capacity(0) {
        Ok((head, tail, capacity)) => {
            runtime_assert(head == 2);
            runtime_assert(tail == 3);
        },
        _ => runtime_assert(false) // can't fail
    }

    // If we read from position 2 of log #0, we get the same thing we
    // would have gotten before the advance-head operation.
    if let Ok(bytes) = multilog.read(0, 2, 1) {
        assert(multilog.constants().impervious_to_corruption ==> bytes[0] == 100);
    }

    // But if we try to read from position 0 of log #0, we get an
    // error because we're not allowed to read from before the head.
    match multilog.read(0, 0, 1) {
        Err(MultiLogErr::CantReadBeforeHead{head}) => runtime_assert(head == 2),
        _ => runtime_assert(false) // can't succeed, and can't fail with any other error
    }
    Some(())
}
```

================
File: ./storage_node/src/multilog/setup_v.rs
================

//! This file contains functions for setting up persistent memory
//! regions for use in a multilog.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::log::inv_v::lemma_auto_smaller_range_of_seq_is_subrange;
use crate::multilog::layout_v::*;
use crate::multilog::multilogimpl_t::MultiLogErr;
use crate::multilog::multilogspec_t::AbstractMultiLogState;
use crate::multilog::inv_v::*;
use crate::pmem::crc_t::*;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmemutil_v::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::traits_t::size_of;
use builtin::*;
use builtin_macros::*;
use vstd::bytes::*;
use vstd::prelude::*;

verus! {

    // This exported executable function checks whether there's enough
    // space on persistent memory regions to support a multilog.
    //
    // `region_sizes` -- a vector of sizes, one per region
    // `num_regions` -- the number of regions (equal to the length of the `region_sizes` array)
    //
    // The return value is a `Result<(), MultiLogErr>`, meaning the following:
    //
    // `Ok(())` -- there's enough space on each region
    // `Err(err)` -- there isn't enough space, so the caller should return the error `err`.
    pub fn check_for_required_space(region_sizes: &Vec<u64>, num_regions: u32) -> (result: Result<(), MultiLogErr>)
        requires
            num_regions == region_sizes.len()
        ensures
            ({
                match result {
                    Ok(()) => forall |i| 0 <= i < region_sizes@.len() ==>
                        region_sizes[i] >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE,
                    Err(MultiLogErr::InsufficientSpaceForSetup{ which_log, required_space }) => {
                        &&& 0 <= which_log < region_sizes@.len()
                        &&& region_sizes[which_log as int] < required_space
                        &&& required_space == ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
                    },
                    _ => false,
                }
            })
    {
        // Loop through all the regions, checking for sufficiency of
        // size.

        for which_log in 0..num_regions
            invariant
                num_regions == region_sizes.len(),
                forall |j| 0 <= j < which_log ==> region_sizes[j] >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
        {
            if region_sizes[which_log as usize] < ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE {
                return Err(MultiLogErr::InsufficientSpaceForSetup{
                    which_log,
                    required_space: ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
                });
            }
        }
        Ok(())
    }

    // This exported function computes the log capacities allowed by the given region sizes.
    pub fn compute_log_capacities(region_sizes: &Vec<u64>) -> (result: Vec<u64>)
        requires
            forall |i: int| 0 <= i < region_sizes.len() ==> region_sizes[i] >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
        ensures
            result.len() == region_sizes.len(),
            forall |i: int| 0 <= i < region_sizes.len() ==>
                #[trigger] result[i] + ABSOLUTE_POS_OF_LOG_AREA == region_sizes[i]
    {
        let mut result = Vec::<u64>::new();
        for which_region in iter: 0..region_sizes.len()
            invariant
                iter.end == region_sizes.len(),
                forall |i: int| 0 <= i < region_sizes.len() ==> region_sizes[i] >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE,
                result.len() == which_region,
                forall |i: int| 0 <= i < which_region ==>
                    #[trigger] result[i] + ABSOLUTE_POS_OF_LOG_AREA == region_sizes[i]
        {
            result.push(region_sizes[which_region] - ABSOLUTE_POS_OF_LOG_AREA);
        }
        result
    }

    // This function evaluates whether memory was correctly set up on
    // a single region. It's a helpful specification function for use
    // in later functions in this file.
    //
    // Because answering this question depends on knowing various
    // metadata about this region and the multilog it's part of, the
    // function needs various input parameters for that. Its
    // parameters are:
    //
    // `mem` -- the contents of memory for this region
    // `region_size` -- how big this region is
    // `multilog_id` -- the GUID of the multilog it's being used for
    // `num_logs` -- the number of logs in the multilog
    // `which_log` -- which among those logs this region is for
    spec fn memory_correctly_set_up_on_single_region(
        mem: Seq<u8>,
        region_size: u64,
        multilog_id: u128,
        num_logs: u32,
        which_log: u32,
    ) -> bool
    {
        let global_crc = deserialize_global_crc(mem);
        let global_metadata = deserialize_global_metadata(mem);
        let region_crc = deserialize_region_crc(mem);
        let region_metadata = deserialize_region_metadata(mem);
        let log_cdb = deserialize_and_check_log_cdb(mem);
        let log_metadata = deserialize_log_metadata(mem, false);
        let log_crc = deserialize_log_crc(mem, false);
        &&& mem.len() >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
        &&& mem.len() == region_size
        &&& global_crc == global_metadata.spec_crc()
        &&& region_crc == region_metadata.spec_crc()
        &&& log_crc == log_metadata.spec_crc()
        &&& global_metadata.program_guid == MULTILOG_PROGRAM_GUID
        &&& global_metadata.version_number == MULTILOG_PROGRAM_VERSION_NUMBER
        &&& global_metadata.length_of_region_metadata == RegionMetadata::spec_size_of()
        &&& region_metadata.region_size == region_size
        &&& region_metadata.multilog_id == multilog_id
        &&& region_metadata.num_logs == num_logs
        &&& region_metadata.which_log == which_log
        &&& region_metadata.log_area_len == region_size - ABSOLUTE_POS_OF_LOG_AREA
        &&& log_cdb == Some(false)
        &&& log_metadata.head == 0
        &&& log_metadata.log_length == 0
    }

    // This executable function sets up a single region for use in a
    // multilog. To do so, it needs various metadata about this region
    // and the multilog it's part of, so it needs some parameters:
    //
    // `region_size`: how big this region is
    // `multilog_id`: the GUID of the multilog it's being used for
    // `num_logs`: the number of logs in the multilog
    // `which_log`: which among those logs this region is for
    //
    // It also needs the parameter `pm_regions` that gives the
    // persistent memory regions for us to write to. It'll only write
    // to region number `which_log`.
    //
    // The main postcondition is:
    //
    // `memory_correctly_set_up_on_single_region(pm_regions@[which_log as int].flush().committed(),
    //                                           region_size, multilog_id, num_logs, which_log)`
    //
    // This means that, after the next flush, the memory in this
    // region will have been set up correctly. (This function doesn't
    // do the flush, for efficiency. That way we only need one flush
    // operation to flush all regions.)
    fn write_setup_metadata_to_single_region<PMRegions: PersistentMemoryRegions>(
        pm_regions: &mut PMRegions,
        region_size: u64,
        multilog_id: u128,
        num_logs: u32,
        which_log: u32,
    )
        requires
            old(pm_regions).inv(),
            num_logs == old(pm_regions)@.len(),
            which_log < num_logs,
            old(pm_regions)@[which_log as int].no_outstanding_writes(),
            old(pm_regions)@[which_log as int].len() == region_size,
            region_size >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE,
            which_log != 0,
        ensures
            pm_regions.inv(),
            pm_regions.constants() == old(pm_regions).constants(),
            pm_regions@.len() == old(pm_regions)@.len(),
            forall |i: int| 0 <= i < pm_regions@.len() && i != which_log ==> pm_regions@[i] =~= old(pm_regions)@[i],
            memory_correctly_set_up_on_single_region(
                pm_regions@[which_log as int].flush().committed(), // it'll be correct after the next flush
                region_size, multilog_id, num_logs, which_log),
            metadata_types_set_in_region(pm_regions@[which_log as int].flush().committed(), false),
    {
        broadcast use pmcopy_axioms;

        // Initialize global metadata and compute its CRC
        // We write this out for each log so that if, upon restore, our caller accidentally
        // sends us the wrong regions, we can detect it.
        let global_metadata = GlobalMetadata {
            program_guid: MULTILOG_PROGRAM_GUID,
            version_number: MULTILOG_PROGRAM_VERSION_NUMBER,
            length_of_region_metadata: size_of::<RegionMetadata>() as u64,
        };
        let global_crc = calculate_crc(&global_metadata);

        // Initialize region metadata and compute its CRC
        let region_metadata = RegionMetadata {
            region_size,
            multilog_id,
            num_logs,
            which_log,
            log_area_len: region_size - ABSOLUTE_POS_OF_LOG_AREA,
            _padding: 0,
        };
        let region_crc = calculate_crc(&region_metadata);

        // Obtain the initial CDB value
        let cdb = CDB_FALSE;

        // Initialize log metadata and compute its CRC
        let log_metadata = LogMetadata {
            head: 0,
            _padding: 0,
            log_length: 0
        };
        let log_crc = calculate_crc(&log_metadata);

        // Write all metadata structures and their CRCs to memory
        pm_regions.serialize_and_write(which_log as usize, ABSOLUTE_POS_OF_GLOBAL_METADATA, &global_metadata);
        pm_regions.serialize_and_write(which_log as usize, ABSOLUTE_POS_OF_GLOBAL_CRC, &global_crc);
        pm_regions.serialize_and_write(which_log as usize, ABSOLUTE_POS_OF_REGION_METADATA, &region_metadata);
        pm_regions.serialize_and_write(which_log as usize, ABSOLUTE_POS_OF_REGION_CRC, &region_crc);
        pm_regions.serialize_and_write(which_log as usize, ABSOLUTE_POS_OF_LOG_CDB, &cdb);
        pm_regions.serialize_and_write(which_log as usize, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE, &log_metadata);
        pm_regions.serialize_and_write(which_log as usize, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE, &log_crc);

        proof {
            // We want to prove that if we parse the result of
            // flushing memory, we get the desired metadata. 

            // Prove that if we extract pieces of the flushed memory,
            // we get the little-endian encodings of the desired
            // metadata. By using the `=~=` operator, we get Z3 to
            // prove this by reasoning about per-byte equivalence.

            let mem = pm_regions@[which_log as int].flush().committed();
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of())
                   =~= global_metadata.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of())
                   =~= global_crc.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of())
                   =~= region_metadata.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of())
                   =~= region_crc.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of())
                   =~= CDB_FALSE.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int,
                                 LogMetadata::spec_size_of())
                   =~= log_metadata.spec_to_bytes());
            assert (extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of())
                    =~= log_crc.spec_to_bytes());
        }
    }

    fn write_setup_metadata_to_first_region<PMRegions: PersistentMemoryRegions>(
        pm_regions: &mut PMRegions,
        region_size: u64,
        multilog_id: u128,
        num_logs: u32,
    )
        requires
            old(pm_regions).inv(),
            num_logs == old(pm_regions)@.len(),
            0 < num_logs,
            old(pm_regions)@[0].no_outstanding_writes(),
            old(pm_regions)@[0].len() == region_size,
            region_size >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE,
            
        ensures
            pm_regions.inv(),
            pm_regions.constants() == old(pm_regions).constants(),
            pm_regions@.len() == old(pm_regions)@.len(),
            forall |i: int| 1 <= i < pm_regions@.len() ==> pm_regions@[i] =~= old(pm_regions)@[i],
            memory_correctly_set_up_on_single_region(
                pm_regions@[0].flush().committed(), // it'll be correct after the next flush
                region_size, multilog_id, num_logs, 0),
            metadata_types_set_in_first_region(pm_regions@[0].flush().committed()),
            deserialize_and_check_log_cdb(pm_regions@[0].flush().committed()) is Some,
            !deserialize_and_check_log_cdb(pm_regions@[0].flush().committed()).unwrap(),
    {
        broadcast use pmcopy_axioms;

        // Initialize global metadata and compute its CRC
        // We write this out for each log so that if, upon restore, our caller accidentally
        // sends us the wrong regions, we can detect it.
        let global_metadata = GlobalMetadata {
            program_guid: MULTILOG_PROGRAM_GUID,
            version_number: MULTILOG_PROGRAM_VERSION_NUMBER,
            length_of_region_metadata: size_of::<RegionMetadata>() as u64,
        };
        let global_crc = calculate_crc(&global_metadata);

        // Initialize region metadata and compute its CRC
        let region_metadata = RegionMetadata {
            region_size,
            multilog_id,
            num_logs,
            which_log: 0,
            log_area_len: region_size - ABSOLUTE_POS_OF_LOG_AREA,
            _padding: 0,
        };
        let region_crc = calculate_crc(&region_metadata);

        // Obtain the initial CDB value
        let cdb = CDB_FALSE;

        // Initialize log metadata and compute its CRC
        let log_metadata = LogMetadata {
            head: 0,
            _padding: 0,
            log_length: 0
        };
        let log_crc = calculate_crc(&log_metadata);

        // Write all metadata structures and their CRCs to memory
        pm_regions.serialize_and_write(0, ABSOLUTE_POS_OF_GLOBAL_METADATA, &global_metadata);
        pm_regions.serialize_and_write(0, ABSOLUTE_POS_OF_GLOBAL_CRC, &global_crc);
        pm_regions.serialize_and_write(0, ABSOLUTE_POS_OF_REGION_METADATA, &region_metadata);
        pm_regions.serialize_and_write(0, ABSOLUTE_POS_OF_REGION_CRC, &region_crc);
        pm_regions.serialize_and_write(0, ABSOLUTE_POS_OF_LOG_CDB, &cdb);
        pm_regions.serialize_and_write(0, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE, &log_metadata);
        pm_regions.serialize_and_write(0, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE, &log_crc);

        proof {
            // We want to prove that if we parse the result of
            // flushing memory, we get the desired metadata. 

            // Prove that if we extract pieces of the flushed memory,
            // we get the little-endian encodings of the desired
            // metadata. By using the `=~=` operator, we get Z3 to
            // prove this by reasoning about per-byte equivalence.

            let mem = pm_regions@[0].flush().committed();
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of())
                   =~= global_metadata.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of())
                   =~= global_crc.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of())
                   =~= region_metadata.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of())
                   =~= region_crc.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of())
                   =~= CDB_FALSE.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int,
                                 LogMetadata::spec_size_of())
                   =~= log_metadata.spec_to_bytes());
            assert (extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of())
                    =~= log_crc.spec_to_bytes());

            assert(metadata_types_set_in_first_region(pm_regions@[0].flush().committed()));
        }
    }

    // This exported executable function writes to persistent memory
    // all the metadata necessary to set up a multilog. To do so, it
    // needs some parameters:
    //
    // `region_sizes`: for each region, how big it is
    //
    // `log_capacities`: for each region, what its capacity will be.
    // Note that this parameter is ghost because it's only needed to
    // establish the postcondition described below.
    //
    // `multilog_id`: the GUID of the multilog it's being used for
    //
    // It also needs the parameter `pm_regions` that gives the
    // persistent memory regions for us to write to.
    //
    // The main postcondition is:
    //
    // ```
    // recover_all(pm_regions@.committed(), multilog_id) ==
    //     Some(AbstractMultiLogState::initialize(log_capacities))
    // ```
    //
    // This means that if the recovery routine runs afterward, then
    // the resulting recovered abstract state will be the valid
    // initial value
    // `AbstractMultiLogState::initialize(log_capacities)`.
    pub fn write_setup_metadata_to_all_regions<PMRegions: PersistentMemoryRegions>(
        pm_regions: &mut PMRegions,
        region_sizes: &Vec<u64>,
        Ghost(log_capacities): Ghost<Seq<u64>>,
        multilog_id: u128,
    )
        requires
            old(pm_regions).inv(),
            old(pm_regions)@.len() == region_sizes@.len() == log_capacities.len(),
            1 <= old(pm_regions)@.len() <= u32::MAX,
            forall |i: int| 0 <= i < old(pm_regions)@.len() ==> #[trigger] old(pm_regions)@[i].len() == region_sizes@[i],
            forall |i: int| 0 <= i < old(pm_regions)@.len() ==>
                #[trigger] old(pm_regions)@[i].len() >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE,
            forall |i: int| 0 <= i < old(pm_regions)@.len() ==>
                #[trigger] old(pm_regions)@[i].len() == log_capacities[i] + ABSOLUTE_POS_OF_LOG_AREA,
            old(pm_regions)@.no_outstanding_writes(),
        ensures
            pm_regions.inv(),
            pm_regions.constants() == old(pm_regions).constants(),
            pm_regions@.len() == old(pm_regions)@.len(),
            forall |i: int| 0 <= i < pm_regions@.len() ==> #[trigger] pm_regions@[i].len() == old(pm_regions)@[i].len(),
            pm_regions@.no_outstanding_writes(),
            recover_all(pm_regions@.committed(), multilog_id) == Some(AbstractMultiLogState::initialize(log_capacities)),
            regions_metadata_types_set(pm_regions@),
    {
        // Loop `which_log` from 0 to `region_sizes.len() - 1`, each time
        // setting up the metadata for region `which_log`.

        let ghost old_pm_regions = pm_regions@;
        let num_logs = region_sizes.len() as u32;

        // the first region is set up differently, so we do it outside of the loop to make the loop invariant
        // easier to maintain
        let region_size: u64 = region_sizes[0];
        assert (region_size == pm_regions@[0].len());
        write_setup_metadata_to_first_region(pm_regions, region_size, multilog_id, num_logs);

        for which_log in 1..num_logs
            invariant
                num_logs == pm_regions@.len(),
                pm_regions.inv(),
                pm_regions.constants() == old(pm_regions).constants(),
                pm_regions@.len() == old(pm_regions)@.len() == region_sizes@.len() == log_capacities.len(),
                pm_regions@.len() >= 1,
                pm_regions@.len() <= u32::MAX,
                forall |i: int| 0 <= i < pm_regions@.len() ==> #[trigger] pm_regions@[i].len() == old(pm_regions)@[i].len(),
                forall |i: int| 0 <= i < pm_regions@.len() ==> #[trigger] pm_regions@[i].len() == region_sizes@[i],
                forall |i: int| 0 <= i < pm_regions@.len() ==>
                    #[trigger] pm_regions@[i].len() >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE,
                forall |i: int| 0 <= i < pm_regions@.len() ==> #[trigger] pm_regions@[i].len() ==
                    log_capacities[i] + ABSOLUTE_POS_OF_LOG_AREA,
                forall |i: int| which_log <= i < pm_regions@.len() ==> #[trigger] pm_regions@[i].no_outstanding_writes(),
                // The key invariant is that every region less than `which_log` has been set up correctly.
                forall |i: u32| i < which_log ==>
                    memory_correctly_set_up_on_single_region(#[trigger] pm_regions@[i as int].flush().committed(),
                                                             region_sizes@[i as int], multilog_id, num_logs, i),
                metadata_types_set_in_first_region(pm_regions@[0].flush().committed()),
                forall |i: u32| 1 <= i < which_log ==> metadata_types_set_in_region(#[trigger] pm_regions@[i as int].flush().committed(), false),
        {
            let region_size: u64 = region_sizes[which_log as usize];
            assert (region_size == pm_regions@[which_log as int].len());
            write_setup_metadata_to_single_region(pm_regions, region_size, multilog_id, num_logs, which_log);
        }
        
        proof {
            // First, establish that recovering after a flush will get
            // abstract state
            // `AbstractMultiLogState::initialize(log_capacities)`.

            let flushed_regions = pm_regions@.flush();
            let pm_regions_committed = flushed_regions.committed();
            assert(recover_all(pm_regions_committed, multilog_id)
                   =~= Some(AbstractMultiLogState::initialize(log_capacities))) by {
                assert(forall |i: int| 0 <= i < pm_regions_committed.len() ==>
                       #[trigger] pm_regions_committed[i].len() == pm_regions@[i as int].len());
                assert(forall |i| 0 <= i < pm_regions@.len() ==>
                       #[trigger] pm_regions_committed[i] == pm_regions@[i as int].flush().committed());
                assert(forall |i| 0 <= i < pm_regions_committed.len() ==>
                       extract_log(#[trigger] pm_regions_committed[i], log_capacities[i] as int, 0int, 0int) =~= Seq::<u8>::empty());
            }

            // Second, establish that the flush we're about to do
            // won't change regions' lengths.
            assert(forall |i| 0 <= i < pm_regions@.len() ==> pm_regions@[i].len() == #[trigger] flushed_regions[i].len());
            
            // Finally, help Verus establish that the metadata types are set for all regions
            lemma_metadata_types_set_flush_committed(pm_regions@, false);
            assert(regions_metadata_types_set(pm_regions@.flush()));
        }

        pm_regions.flush()
    }

}

================
File: ./storage_node/src/multilog/mod.rs
================

pub mod append_v;
pub mod inv_v;
pub mod layout_v;
pub mod multilogimpl_t;
pub mod multilogimpl_v;
pub mod multilogspec_t;
pub mod setup_v;
pub mod start_v;

================
File: ./storage_node/src/multilog/multilogspec_t.rs
================

//! This file contains the trusted specification for an abstract
//! multilog, which has type `AbstractMultiLogState`.
//!
//! Although the verifier is run on this file, it needs to be
//! carefully read and audited to be confident of the correctness of
//! this specification for the multilog implementation.
//!
//! An `AbstractMultiLogState` has the following operations:
//!
//! `initialize(capacities: Seq<u64>) -> AbstractMultiLogState`
//!
//! This static function creates an initial multilog with the given
//! capacities.
//!
//! `num_logs(self) -> nat`
//!
//! This method returns the number of logs in the multilog.
//!
//! `tentatively_append(self, which_log: int, bytes_to_append: Seq<u8>) -> Self`
//!
//! This method tentatively appends the given bytes to the end of the
//! log in the multilog with the given index.
//!
//! `commit(self) -> Self`
//!
//! This method commits all tentative appends atomically across all
//! logs in the multilog.
//!
//! `read(self, which_log: int, pos: int, len: int) -> Seq<u8>`
//!
//! This method reads a certain number of bytes from one of the logs
//! at a certain logical position.
//!
//! `drop_pending_appends(self) -> Self`
//!
//! This method drops all pending appends. It's not meant to be
//! explicitly invoked by clients; it's a model of what clients should
//! consider to have happened during a crash.

use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {
    
    // An `AbstractLogState` is an abstraction of a single log, of
    // which an abstract multilog is composed. Its fields are:
    //
    // `head` -- the logical position of the first accessible byte
    // in the log
    //
    // `log` -- the accessible bytes in the log, logically starting
    // at position `head`
    //
    // `pending` -- the bytes tentatively appended past the end of the
    // log, which will not become part of the log unless committed
    // and which will be discarded on a crash
    //
    // `capacity` -- the maximum length of the `log` field
    #[verifier::ext_equal]
    pub struct AbstractLogState {
        pub head: int,
        pub log: Seq<u8>,
        pub pending: Seq<u8>,
        pub capacity: int,
    }

    impl AbstractLogState {

        // This is the specification for the initial state of an
        // abstract log.
        pub open spec fn initialize(capacity: int) -> Self {
            Self {
                head: 0int,
                log: Seq::<u8>::empty(),
                pending: Seq::<u8>::empty(),
                capacity: capacity
            }
        }

        // This is the specification for what it means to tentatively
        // append to a log. It appends the given bytes to the
        // `pending` field.
        pub open spec fn tentatively_append(self, bytes: Seq<u8>) -> Self {
            Self { pending: self.pending + bytes, ..self }
        }

        // This is the specification for what it means to commit a
        // log.  It adds all pending bytes to the log and clears the
        // pending bytes.
        pub open spec fn commit(self) -> Self {
            Self { log: self.log + self.pending, pending: Seq::<u8>::empty(), ..self }
        }

        // This is the specification for what it means to advance the
        // head to a given new value `new_value`.
        pub open spec fn advance_head(self, new_head: int) -> Self
        {
            let new_log = self.log.subrange(new_head - self.head, self.log.len() as int);
            Self { head: new_head, log: new_log, ..self }
        }

        // This is the specification for what it means to read `len`
        // bytes from a certain virtual position `pos` in the abstract
        // log.
        pub open spec fn read(self, pos: int, len: int) -> Seq<u8>
        {
            self.log.subrange(pos - self.head, pos - self.head + len)
        }

        // This is the specification for what it means to drop pending
        // appends. (This isn't a user-invokable operation; it's what
        // happens on a crash.)
        pub open spec fn drop_pending_appends(self) -> Self
        {
            Self { pending: Seq::<u8>::empty(), ..self }
        }
    }
    
    // An `AbstractMultiLogState` is an abstraction of a collection of
    // logs that can be atomically collectively appended to. It
    // consists of a sequence of logs of type `AbstractLogState`.
    #[verifier::ext_equal]
    pub struct AbstractMultiLogState {
        pub states: Seq<AbstractLogState>
    }

    #[verifier::ext_equal]
    impl AbstractMultiLogState {

        // This is the specification for the number of logs in a
        // multilog.
        pub open spec fn num_logs(self) -> nat {
            self.states.len()
        }

        // This is the specification for indexing into the sequence of
        // logs and getting the one at the given index `which_log`. Naming
        // it `spec_index` means it will be used whenever a term `s[w]` is
        // used in a specification where `s` is an `AbstractMultiLogState`.
        pub open spec fn spec_index(self, which_log: int) -> AbstractLogState {
            self.states[which_log]
        }

        // This is the specification for the initial state of an
        // abstract multilog.
        pub open spec fn initialize(capacities: Seq<u64>) -> Self {
            Self {
                states: Seq::<AbstractLogState>::new(capacities.len(),
                    |i| AbstractLogState::initialize(capacities[i] as int))
            }
        }

        // This is the specification for the operation of tentatively
        // appending to an abstract multilog.
        pub open spec fn tentatively_append(self, which_log: int, bytes_to_append: Seq<u8>) -> Self {
            Self {
                states: self.states.update(which_log, self.states[which_log].tentatively_append(bytes_to_append))
            }
        }

        // This is the specification for the operation of committing
        // all pending appends to an abstract multilog. It atomically
        // commits all such pending appends at once.
        pub open spec fn commit(self) -> Self {
            Self {
                states: self.states.map(|_idx, s: AbstractLogState| s.commit())
            }
        }

        // This is the specification for the operation of advancing
        // the head of one of the logs in a multilog.
        pub open spec fn advance_head(self, which_log: int, new_head: int) -> Self {
            Self {
                states: self.states.update(which_log, self.states[which_log].advance_head(new_head))
            }
        }

        // This is the specification for what it means to read `len`
        // bytes from a certain virtual position `pos` in the log
        // with a certain index `which_log`:
        pub open spec fn read(self, which_log: int, pos: int, len: int) -> Seq<u8>
        {
            self.states[which_log].read(pos, len)
        }

        // This is the specification for the operation of dropping all
        // pending appends to a multilog.
        pub open spec fn drop_pending_appends(self) -> Self {
            Self {
                states: self.states.map(|_idx, s: AbstractLogState| s.drop_pending_appends())
            }
        }
    }

}

================
File: ./storage_node/src/multilog/inv_v.rs
================

//! This file contains functions describing invariants of a
//! `UntrustedMultiLogImpl`, as well as lemmas about those invariants.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.
//!

use crate::log::inv_v::{lemma_auto_smaller_range_of_seq_is_subrange, lemma_active_metadata_bytes_equal_implies_metadata_types_set};
use crate::multilog::layout_v::*;
use crate::multilog::multilogimpl_v::LogInfo;
use crate::multilog::multilogspec_t::{AbstractLogState, AbstractMultiLogState};
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmemutil_v::*;
use crate::pmem::pmcopy_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {

    broadcast use pmcopy_axioms;

    // This trivial function indicating whether a given log index is
    // valid is used for triggering numerous `forall` invariants.
    pub open spec fn is_valid_log_index(which_log: u32, num_logs: u32) -> bool
    {
        which_log < num_logs
    }

    // This invariant says that there are no outstanding writes to any
    // part of the metadata subregion of any persistent-memory region.
    // It's temporarily violated in the middle of various operations,
    // of course, but it's always restored before finishing an
    // operation.
    pub open spec fn no_outstanding_writes_to_metadata(
        pm_regions_view: PersistentMemoryRegionsView,
        num_logs: u32,
    ) -> bool
    {
        forall |which_log: u32| #[trigger] is_valid_log_index(which_log, num_logs) ==>
           pm_regions_view[which_log as int].no_outstanding_writes_in_range(ABSOLUTE_POS_OF_GLOBAL_METADATA as int,
                                                                          ABSOLUTE_POS_OF_LOG_AREA as int)
    }

    pub proof fn lemma_no_outstanding_writes_to_metadata_implies_no_outstanding_writes_to_active_metadata(
        pm_regions_view: PersistentMemoryRegionsView, 
        num_logs: u32,
        cdb: bool
    )
        requires 
            no_outstanding_writes_to_metadata(pm_regions_view, num_logs),
            num_logs == pm_regions_view.len(),
            deserialize_and_check_log_cdb(pm_regions_view[0].committed()) is Some,
            cdb == deserialize_and_check_log_cdb(pm_regions_view[0].committed()).unwrap(),
        ensures 
            no_outstanding_writes_to_active_metadata(pm_regions_view, cdb)
    {
        assert(forall |i: u32| is_valid_log_index(i, num_logs) ==> {
            let pm_region_view = #[trigger] pm_regions_view[i as int];
            &&& pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int)
            &&& pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, 
                ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of() + u64::spec_size_of())
            &&& !cdb ==> pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, 
                ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of() + u64::spec_size_of())
        });
    }

    pub open spec fn active_metadata_is_equal(
        pm1: PersistentMemoryRegionsView,
        pm2: PersistentMemoryRegionsView
    ) -> bool
    {
        let cdb1 = deserialize_and_check_log_cdb(pm1[0].committed());
        let cdb2 = deserialize_and_check_log_cdb(pm2[0].committed());
        &&& cdb1 is Some
        &&& cdb2 is Some 
        &&& cdb1 == cdb2
        &&& pm1.len() == pm2.len()
        &&& forall |i: int| #![auto] 0 <= i < pm1.len() ==> active_metadata_is_equal_in_region(pm1[i], pm2[i], cdb1.unwrap())
    }

    pub open spec fn active_metadata_is_equal_in_region(
        pm_region_view1: PersistentMemoryRegionView,
        pm_region_view2: PersistentMemoryRegionView,
        cdb: bool
    ) -> bool 
    {
        let pm_bytes1 = pm_region_view1.committed();
        let pm_bytes2 = pm_region_view2.committed();
        active_metadata_bytes_are_equal(pm_bytes1, pm_bytes2, cdb)
    }

    pub open spec fn active_metadata_bytes_are_equal(
        pm_bytes1: Seq<u8>,
        pm_bytes2: Seq<u8>,
        cdb: bool
    ) -> bool {
        &&& pm_bytes1.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) ==
                pm_bytes2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) 
        &&& {
            if cdb {
                extract_bytes(pm_bytes1, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                    extract_bytes(pm_bytes2, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of() + u64::spec_size_of()) 
            } else {
                extract_bytes(pm_bytes1, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                    extract_bytes(pm_bytes2, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of() + u64::spec_size_of()) 
            }
        }
    }

    // This invariant is similar to no_outstanding_writes_to_metadata, except that it allows outstanding writes
    // to the inactive log metadata region.
    pub open spec fn no_outstanding_writes_to_active_metadata(
        pm_regions_view: PersistentMemoryRegionsView,
        cdb: bool
    ) -> bool 
    {
        forall |i: int| #![auto] 0 <= i < pm_regions_view.len() ==> 
            no_outstanding_writes_to_active_metadata_in_region(pm_regions_view[i], cdb)
    }
    
    pub open spec fn no_outstanding_writes_to_active_metadata_in_region(
        pm_region_view: PersistentMemoryRegionView,
        cdb: bool,
    ) -> bool 
    {
        // Note that we include the active log metadata's CRC in the region
        let active_log_metadata_result = if cdb {
            pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int,
                ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of() + u64::spec_size_of())
        } else {
            pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int,
                ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of() + u64::spec_size_of())
        };
        &&& pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_GLOBAL_METADATA as int,
                                                        ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int)
        &&& active_log_metadata_result
    }

    // This invariant says that there are no outstanding writes to the
    // CDB area of persistent memory, and that the committed contents
    // of that area correspond to the given boolean `cdb`.
    pub open spec fn memory_matches_cdb(pm_regions_view: PersistentMemoryRegionsView, cdb: bool) -> bool
    {
        &&& pm_regions_view.no_outstanding_writes_in_range(0int, ABSOLUTE_POS_OF_LOG_CDB as int,
                                                         ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of())
        &&& extract_and_parse_log_cdb(pm_regions_view[0].committed()) == Some(cdb)
    }

    pub open spec fn memory_matches_deserialized_cdb(pm_regions_view: PersistentMemoryRegionsView, cdb: bool) -> bool
    {
        &&& pm_regions_view.no_outstanding_writes_in_range(0int, ABSOLUTE_POS_OF_LOG_CDB as int,
            ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of())
        &&& deserialize_and_check_log_cdb(pm_regions_view[0].committed()) == Some(cdb)
    }

    // This invariant says that there are no outstanding writes to the
    // activate metadata subregion of the persistent-memory region
    // (i.e., everything but the log area and the log metadata
    // corresponding to `!cdb`). It also says that that metadata is
    // consistent with the log information in `info` and various other
    // in-memory variables given in parameters. The parameters to this
    // function are:
    //
    // `pm_region_view` -- the current view of the persistent memory region
    //
    // `multilog_id` -- the GUID of the multilog
    //
    // `num_logs` -- the number of logs in the multilog
    //
    // `which_log` -- which of the multilog's logs `pm_region_view` stores
    //
    // `cdb` -- the current boolean value of the corruption-detection
    // boolean
    //
    // `info` -- various variables describing information about this
    // log
    pub open spec fn metadata_consistent_with_info(
        pm_region_view: PersistentMemoryRegionView,
        multilog_id: u128,
        num_logs: u32,
        which_log: u32,
        cdb: bool,
        info: LogInfo,
    ) -> bool
    {
        let mem = pm_region_view.committed();
        let global_metadata = deserialize_global_metadata(mem);
        let global_crc = deserialize_global_crc(mem);
        let region_metadata = deserialize_region_metadata(mem);
        let region_crc = deserialize_region_crc(mem);
        let log_metadata = deserialize_log_metadata(mem, cdb);
        let log_crc = deserialize_log_crc(mem, cdb);

        // No outstanding writes to global metadata, region metadata, or the log metadata CDB
        &&& pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_GLOBAL_METADATA as int,
                                                        ABSOLUTE_POS_OF_LOG_CDB as int)
        // Also, no outstanding writes to the log metadata corresponding to the active log metadata CDB
        &&& pm_region_view.no_outstanding_writes_in_range(get_log_metadata_pos(cdb) as int,
                                                        get_log_crc_end(cdb) as int)

        // All the CRCs match
        &&& global_crc == global_metadata.spec_crc()
        &&& region_crc == region_metadata.spec_crc()
        &&& log_crc == log_metadata.spec_crc()

        // Various fields are valid and match the parameters to this function
        &&& global_metadata.program_guid == MULTILOG_PROGRAM_GUID
        &&& global_metadata.version_number == MULTILOG_PROGRAM_VERSION_NUMBER
        &&& global_metadata.length_of_region_metadata == RegionMetadata::spec_size_of()
        &&& region_metadata.region_size == mem.len()
        &&& region_metadata.multilog_id == multilog_id
        &&& region_metadata.num_logs == num_logs
        &&& region_metadata.which_log == which_log
        &&& region_metadata.log_area_len == info.log_area_len
        &&& log_metadata.head == info.head
        &&& log_metadata.log_length == info.log_length

        // The memory region is large enough to hold the entirety of the log area
        &&& mem.len() >= ABSOLUTE_POS_OF_LOG_AREA + info.log_area_len
    }

    // This lemma proves that, if all regions are consistent wrt a new CDB, and then we
    // write and flush that CDB, the regions stay consistent with info.
    pub proof fn lemma_each_metadata_consistent_with_info_after_cdb_update(
        old_pm_region_view: PersistentMemoryRegionsView,
        new_pm_region_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        num_logs: u32,
        new_cdb_bytes: Seq<u8>,
        new_cdb: bool,
        infos: Seq<LogInfo>,
    )
        requires
            new_cdb == false ==> new_cdb_bytes == CDB_FALSE.spec_to_bytes(),
            new_cdb == true ==> new_cdb_bytes == CDB_TRUE.spec_to_bytes(),
            new_cdb_bytes.len() == u64::spec_size_of(),
            old_pm_region_view.no_outstanding_writes(),
            new_pm_region_view.no_outstanding_writes(),
            num_logs > 0,
            new_pm_region_view == old_pm_region_view.write(0int, ABSOLUTE_POS_OF_LOG_CDB as int, new_cdb_bytes).flush(),
            each_metadata_consistent_with_info(old_pm_region_view, multilog_id, num_logs, new_cdb, infos),
        ensures
            each_metadata_consistent_with_info(new_pm_region_view, multilog_id, num_logs, new_cdb, infos),
    {
        // The bytes in non-updated regions are unchanged and remain consistent after updating the CDB.
        assert(forall |w: u32| 1 <= w && #[trigger] is_valid_log_index(w, num_logs) ==>
            old_pm_region_view[w as int].committed() =~= new_pm_region_view[w as int].committed()
        );
        assert(forall |w: u32| 1 <= w && #[trigger] is_valid_log_index(w, num_logs) ==>
            metadata_consistent_with_info(new_pm_region_view[w as int], multilog_id, num_logs, w, new_cdb, infos[w as int])
        );

        // The 0th old region (where the CDB is stored) is consistent with the new CDB; this follows from
        // the precondition.
        assert(is_valid_log_index(0, num_logs));
        assert(metadata_consistent_with_info(old_pm_region_view[0int], multilog_id, num_logs, 0, new_cdb, infos[0int]));

        // The metadata in the updated region is also consistent
        assert(metadata_consistent_with_info(new_pm_region_view[0int], multilog_id, num_logs, 0, new_cdb, infos[0int])) by {
            let old_mem = old_pm_region_view[0int].committed();
            let new_mem = new_pm_region_view[0int].committed();
            lemma_establish_extract_bytes_equivalence(old_mem, new_mem);
        }
    }

    // This invariant says that `metadata_consistent_with_info` holds
    // for each region of the given persistent memory regions view
    // `pm_regions_view`.
    pub open spec fn each_metadata_consistent_with_info(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        num_logs: u32,
        cdb: bool,
        infos: Seq<LogInfo>,
    ) -> bool
    {
        &&& pm_regions_view.regions.len() == infos.len() == num_logs > 0
        &&& forall |which_log: u32| #[trigger] is_valid_log_index(which_log, num_logs) ==> {
            let w = which_log as int;
            metadata_consistent_with_info(pm_regions_view[w], multilog_id, num_logs, which_log, cdb, infos[w])
        }
    }

    // This invariant says that the log area of the given
    // persistent-memory region view is consistent with both the log
    // information `info` and the abstract log state `state`. Also,
    // `info` satisfies certain invariant properties and is consistent
    // with `state`.
    //
    // This means three things for every relative log position
    // `pos` and its corresponding persistent-memory byte `pmb`:
    //
    // 1) If `0 <= pos < log_length`, then `pmb` has no outstanding
    // writes and its committed content is the byte in the abstract
    // log at position `pos`. This is critical so that recovery will
    // recover to the right abstract log.
    //
    // 2) If `log_length <= pos < log_plus_pending_length`, then `pmb`
    // may or may not have outstanding writes. But when/if it gets
    // flushed, its content will be the byte in the abstract pending
    // appends at position `pos - log_length`. This is useful so that,
    // when a commit is requested, a flush is all that's needed to
    // durably write the pending appends.
    //
    // 3) If `log_plus_pending_length <= pos < log_area_len`, then
    // `pmb` has no outstanding writes because it's past the pending
    // tail. This is useful so that, if there are further pending
    // appends, they can be written into this part of the log area.
    pub open spec fn info_consistent_with_log_area(
        pm_region_view: PersistentMemoryRegionView,
        info: LogInfo,
        state: AbstractLogState,
    ) -> bool
    {
        // `info` satisfies certain invariant properties
        &&& info.log_area_len >= MIN_LOG_AREA_SIZE
        &&& info.log_length <= info.log_plus_pending_length <= info.log_area_len
        &&& info.head_log_area_offset == info.head as int % info.log_area_len as int
        &&& info.head + info.log_plus_pending_length <= u128::MAX

        // `info` and `state` are consistent with each other
        &&& state.log.len() == info.log_length
        &&& state.pending.len() == info.log_plus_pending_length - info.log_length
        &&& state.head == info.head
        &&& state.capacity == info.log_area_len

        // The log area is consistent with `info` and `state`
        &&& forall |pos_relative_to_head: int| {
                let addr = ABSOLUTE_POS_OF_LOG_AREA +
                    #[trigger] relative_log_pos_to_log_area_offset(pos_relative_to_head,
                                                                   info.head_log_area_offset as int,
                                                                   info.log_area_len as int);
                let pmb = pm_region_view.state[addr];
                &&& 0 <= pos_relative_to_head < info.log_length ==> {
                      &&& pmb.state_at_last_flush == state.log[pos_relative_to_head]
                      &&& pmb.outstanding_write.is_none()
                   }
                &&& info.log_length <= pos_relative_to_head < info.log_plus_pending_length ==>
                       pmb.flush_byte() == state.pending[pos_relative_to_head - info.log_length]
                &&& info.log_plus_pending_length <= pos_relative_to_head < info.log_area_len ==>
                       pmb.outstanding_write.is_none()
            }
    }

    // This invariant says that `info_consistent_with_log_area` holds
    // for all logs in the multilog.
    pub open spec fn each_info_consistent_with_log_area(
        pm_regions_view: PersistentMemoryRegionsView,
        num_logs: u32,
        infos: Seq<LogInfo>,
        state: AbstractMultiLogState,
    ) -> bool
    {
        &&& pm_regions_view.regions.len() == infos.len() == state.num_logs() == num_logs > 0
        &&& forall |which_log: u32| #[trigger] is_valid_log_index(which_log, num_logs) ==> {
           let w = which_log as int;
           info_consistent_with_log_area(pm_regions_view[w], infos[w], state[w])
        }
    }

    pub open spec fn regions_metadata_types_set(pm_regions: PersistentMemoryRegionsView) -> bool 
    {
        let mems = pm_regions.committed();
        metadata_types_set(mems)
    }

    pub open spec fn metadata_types_set(mems: Seq<Seq<u8>>) -> bool 
    {
        &&& metadata_types_set_in_first_region(mems[0])
        &&& {
            let cdb = deserialize_and_check_log_cdb(mems[0]);
            &&& cdb is Some 
            &&& forall |i: int| #![auto] 1 <= i < mems.len() ==> metadata_types_set_in_region(mems[i], cdb.unwrap())
        }
    }

    pub open spec fn metadata_types_set_in_first_region(mem: Seq<u8>) -> bool 
    {
        &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()))
        &&& {
            let cdb = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()));
            &&& cdb == CDB_TRUE || cdb == CDB_FALSE 
            &&& cdb == CDB_TRUE ==> metadata_types_set_in_region(mem, true)
            &&& cdb == CDB_FALSE ==> metadata_types_set_in_region(mem, false)
        }
    }

    pub open spec fn metadata_types_set_in_region(mem: Seq<u8>, cdb: bool) -> bool
    {
        &&& GlobalMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()))
        &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()))
        &&& {
            let metadata = GlobalMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()));
            let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()));
            crc == spec_crc_u64(metadata.spec_to_bytes())
        }
        &&& RegionMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of()))
        &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of()))
        &&& {
            let metadata = RegionMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of()));
            let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of()));
            crc == spec_crc_u64(metadata.spec_to_bytes())
        }
        &&& if cdb {
               &&& LogMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of()))
               &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE as int, u64::spec_size_of()))
               &&& {
                   let metadata = LogMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of()));
                   let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE as int, u64::spec_size_of()));
                   crc == spec_crc_u64(metadata.spec_to_bytes())
               }
           }
           else {
               &&& LogMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of()))
               &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of()))
               &&& {
                   let metadata = LogMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of()));
                   let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of()));
                   crc == spec_crc_u64(metadata.spec_to_bytes())
               }
           }
    }

    pub open spec fn inactive_metadata_types_set_in_region(mem: Seq<u8>, cdb: bool) -> bool 
    {
        &&& if cdb {
            &&& LogMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of()))
            &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of()))
            &&& {
                let metadata = LogMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of()));
                let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of()));
                crc == spec_crc_u64(metadata.spec_to_bytes())
            }
        }
        else {
            &&& LogMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of()))
            &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE as int, u64::spec_size_of()))
            &&& {
                let metadata = LogMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of()));
                let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE as int, u64::spec_size_of()));
                crc == spec_crc_u64(metadata.spec_to_bytes())
            }
        }
    }

    // This lemma helps us establish that metadata types are set in the a region even when we obtain 
    // a view of its bytes in different ways.
    pub proof fn lemma_metadata_types_set_flush_committed(
        pm_regions_view: PersistentMemoryRegionsView,
        cdb: bool
    )
        ensures 
            forall |i: int| #![auto] {
                &&& 0 < i < pm_regions_view.len() 
                &&& metadata_types_set_in_region(pm_regions_view[i].flush().committed(), cdb) 
            } ==> metadata_types_set_in_region(pm_regions_view.flush().committed()[i], cdb)
    {} 

    pub proof fn lemma_metadata_set_after_crash_in_region(
        pm_region_view: PersistentMemoryRegionView,
        cdb: bool 
    )
        requires 
            no_outstanding_writes_to_active_metadata_in_region(pm_region_view, cdb),
            metadata_types_set_in_region(pm_region_view.committed(), cdb),
            ABSOLUTE_POS_OF_GLOBAL_METADATA < ABSOLUTE_POS_OF_LOG_AREA < pm_region_view.len()
        ensures 
            forall |s| #![auto] {
                pm_region_view.can_crash_as(s) 
            } ==> metadata_types_set_in_region(s, cdb),
    {
        let pm_bytes = pm_region_view.committed();
        lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(pm_region_view);

        assert forall |s| #![auto] {
            &&& pm_region_view.can_crash_as(s) 
        } implies metadata_types_set_in_region(s, cdb) by {
            lemma_establish_extract_bytes_equivalence(s, pm_region_view.committed());
        }
    }

    pub proof fn lemma_metadata_set_after_crash_in_first_region(
        pm_regions_view: PersistentMemoryRegionsView,
        cdb: bool 
    )
        requires 
            no_outstanding_writes_to_active_metadata_in_region(pm_regions_view[0], cdb),
            metadata_types_set_in_first_region(pm_regions_view[0].committed()),
            deserialize_and_check_log_cdb(pm_regions_view[0].committed()) is Some,
            cdb == deserialize_and_check_log_cdb(pm_regions_view[0].committed()).unwrap(),
        ensures 
            forall |s| #![auto] {
                &&& pm_regions_view[0].can_crash_as(s) 
                &&& s.len() >= ABSOLUTE_POS_OF_LOG_AREA
            } ==> {
                let cdb2 = deserialize_and_check_log_cdb(s);
                &&& cdb2 is Some
                &&& cdb2.unwrap() == cdb
                &&& metadata_types_set_in_first_region(s)
            }
    {
        let pm_bytes = pm_regions_view[0].committed();
        lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(pm_regions_view[0]);

        assert forall |s| #![auto] {
            &&& pm_regions_view[0].can_crash_as(s) 
            &&& s.len() >= ABSOLUTE_POS_OF_LOG_AREA
        } implies {
            let cdb2 = deserialize_and_check_log_cdb(s);
            &&& cdb2 is Some
            &&& cdb == cdb2.unwrap()
            &&& metadata_types_set_in_first_region(s)
        } by {
            lemma_establish_extract_bytes_equivalence(s, pm_bytes);
            assert(pm_regions_view[0].no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_CDB as int, ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of()));
            assert(extract_bytes(pm_bytes, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()) =~= 
                extract_bytes(s, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()));
        }
    }

    pub proof fn lemma_metadata_set_after_crash(
        pm_regions_view: PersistentMemoryRegionsView,
        cdb: bool
    ) 
        requires 
            no_outstanding_writes_to_active_metadata(pm_regions_view, cdb),
            regions_metadata_types_set(pm_regions_view),
            memory_matches_deserialized_cdb(pm_regions_view, cdb),
            // is this just the same as above...?
            deserialize_and_check_log_cdb(pm_regions_view[0].committed()) is Some,
            cdb == deserialize_and_check_log_cdb(pm_regions_view[0].committed()).unwrap(),
            pm_regions_view.len() > 0,
            forall |i: int| 0 <= i < pm_regions_view.len() ==> 0 <= ABSOLUTE_POS_OF_GLOBAL_METADATA < ABSOLUTE_POS_OF_LOG_AREA < #[trigger] pm_regions_view[i].len()
        ensures 
            forall |s| #![auto] {
                &&& pm_regions_view.can_crash_as(s) 
            } ==> metadata_types_set(s),
    {

        assert(metadata_types_set(pm_regions_view.committed()));
        // Z3 cannot always tell that pm_regions_view[i].committed() == pm_regions_view.committed()[i]; we have to help it out here
        assert forall |i: int| #![auto] 1 <= i < pm_regions_view.len() implies metadata_types_set_in_region(pm_regions_view[i].committed(), cdb) by {
            assert(metadata_types_set_in_region(pm_regions_view.committed()[i], cdb));
        };

        assert forall |s| #![auto] pm_regions_view.can_crash_as(s) implies metadata_types_set(s)
        by {
            assert forall |s| #![auto] pm_regions_view[0].can_crash_as(s) implies {
                let cdb2 = deserialize_and_check_log_cdb(s);
                &&& cdb2 is Some
                &&& cdb2.unwrap() == cdb
                &&& metadata_types_set_in_first_region(s) 
            } by {
                lemma_metadata_set_after_crash_in_first_region(pm_regions_view, cdb);
            }

            assert forall |i, s| #![auto] {
                &&& 1 <= i < pm_regions_view.len()
                &&& pm_regions_view[i].can_crash_as(s)
            } implies metadata_types_set_in_region(s, cdb)
            by {
                lemma_metadata_set_after_crash_in_region(pm_regions_view[i], cdb);
            }
    
        }
    }

    // This lemma proves that, for any address in the log area of the
    // given persistent memory view, it corresponds to a specific
    // logical position in the abstract log relative to the head. That
    // logical position `pos` satisfies `0 <= pos < log_area_len`.
    //
    // It's useful to call this lemma because it takes facts that
    // trigger `pm_region_view.state[addr]` and turns them into facts
    // that trigger `relative_log_pos_to_log_area_offset`. That's the
    // trigger used in `info_consistent_with_log_area` and
    // `each_info_consistent_with_log_area`.
    pub proof fn lemma_addresses_in_log_area_correspond_to_relative_log_positions(
        pm_region_view: PersistentMemoryRegionView,
        info: LogInfo
    )
        requires
            pm_region_view.len() >= ABSOLUTE_POS_OF_LOG_AREA + info.log_area_len,
            info.head_log_area_offset < info.log_area_len,
            info.log_area_len > 0,
        ensures
            forall |addr: int| #![trigger pm_region_view.state[addr]]
                ABSOLUTE_POS_OF_LOG_AREA <= addr < ABSOLUTE_POS_OF_LOG_AREA + info.log_area_len ==> {
                    let log_area_offset = addr - ABSOLUTE_POS_OF_LOG_AREA;
                    let pos_relative_to_head =
                        if log_area_offset >= info.head_log_area_offset {
                            log_area_offset - info.head_log_area_offset
                        }
                        else {
                            log_area_offset - info.head_log_area_offset + info.log_area_len
                        };
                    &&& 0 <= pos_relative_to_head < info.log_area_len
                    &&& addr == ABSOLUTE_POS_OF_LOG_AREA +
                              relative_log_pos_to_log_area_offset(pos_relative_to_head, info.head_log_area_offset as int,
                                                                  info.log_area_len as int)
                }
    {
    }

    // This lemma proves that, if various invariants hold for the
    // given persistent-memory view `pm_region_view` and abstract log state
    // `state`, and if that view can crash as contents `mem`, then
    // recovery on `mem` will produce `state.drop_pending_appends()`.
    //
    // `pm_region_view` -- the view of this persistent-memory region
    // `mem` -- a possible memory contents that `pm_region_view` can crash as
    // `multilog_id` -- the ID of the multilog
    // `num_logs` -- the number of logs
    // `which_log` -- which log this region stores
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract log state
    proof fn lemma_invariants_imply_crash_recover_for_one_log(
        pm_region_view: PersistentMemoryRegionView,
        mem: Seq<u8>,
        multilog_id: u128,
        num_logs: u32,
        which_log: u32,
        cdb: bool,
        info: LogInfo,
        state: AbstractLogState,
    )
        requires
            pm_region_view.can_crash_as(mem),
            metadata_consistent_with_info(pm_region_view, multilog_id, num_logs, which_log, cdb, info),
            info_consistent_with_log_area(pm_region_view, info, state),
        ensures
            recover_abstract_log_from_region_given_cdb(mem, multilog_id, num_logs as int, which_log as int, cdb) ==
               Some(state.drop_pending_appends())
    {
        // For the metadata, we observe that:
        //
        // (1) there are no outstanding writes, so the crashed-into
        //     state `mem` must match the committed state
        //     `pm_region_view.committed()`, and
        // (2) wherever the crashed-into state matches the committed
        //     state on a per-byte basis, any `extract_bytes` results
        //     will also match.
        //
        // Therefore, since the metadata in
        // `pm_region_view.committed()` matches `state` (per the
        // invariants), the metadata in `mem` must also match `state`.

        lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(pm_region_view);
        lemma_establish_extract_bytes_equivalence(mem, pm_region_view.committed());

        // The tricky part is showing that the result of `extract_log` will produce the desired result.
        // Use `=~=` to ask Z3 to prove this equivalence by proving it holds on each byte.

        assert(extract_log(mem, info.log_area_len as int, info.head as int, info.log_length as int) =~=
               state.drop_pending_appends().log);
    }

    // This lemma proves that, if various invariants hold for the
    // given persistent-memory regions view `pm_regions_view` and
    // abstract multilog state `state`, and if that view can crash as
    // contents `mem`, then recovery on `mem` will produce
    // `state.drop_pending_appends()`.
    //
    // `pm_regions_view` -- the persistent memory regions view
    // `mems` -- a possible memory contents that `pm_regions_view` can crash as
    // `multilog_id` -- the ID of the multilog
    // `num_logs` -- the number of logs
    // `cdb` -- the current value of the corruption-detecting boolean
    // `infos` -- the log information
    // `state` -- the abstract multilog state
    proof fn lemma_invariants_imply_crash_recover(
        pm_regions_view: PersistentMemoryRegionsView,
        mems: Seq<Seq<u8>>,
        multilog_id: u128,
        num_logs: u32,
        cdb: bool,
        infos: Seq<LogInfo>,
        state: AbstractMultiLogState,
    )
        requires
            pm_regions_view.can_crash_as(mems),
            memory_matches_deserialized_cdb(pm_regions_view, cdb),
            each_metadata_consistent_with_info(pm_regions_view, multilog_id, num_logs, cdb, infos),
            each_info_consistent_with_log_area(pm_regions_view, num_logs, infos, state),
        ensures
            recover_all(mems, multilog_id) == Some(state.drop_pending_appends())
    {
        // For the CDB, we observe that:
        //
        // (1) there are no outstanding writes, so the crashed-into
        // state `mems[0]` must match the committed state
        // `pm_regions_view.committed()[0]`, and
        //
        // (2) wherever the crashed-into state matches the committed
        // state on a per-byte basis, any `extract_bytes` results will
        // also match.
        //
        // Therefore, since the metadata in `pm_regions_view.committed()[0]`
        // matches `cdb` (per the invariants), the metadata in
        // `mems[0]` must also match `cdb`.

        assert (recover_cdb(mems[0]) == Some(cdb)) by {
            assert(is_valid_log_index(0, num_logs)); // This triggers various `forall`s in the invariants
            lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(pm_regions_view[0]);
            lemma_establish_extract_bytes_equivalence(mems[0], pm_regions_view.committed()[0]);
        }

        // Use `lemma_invariants_imply_crash_recover_for_one_log` on
        // each region to establish that recovery works on all the
        // regions.

        assert forall |which_log: u32| is_valid_log_index(which_log, num_logs) implies
                recover_abstract_log_from_region_given_cdb(
                    #[trigger] mems[which_log as int], multilog_id, mems.len() as int, which_log as int, cdb) ==
                Some(state[which_log as int].drop_pending_appends()) by {
            let w = which_log as int;
            lemma_invariants_imply_crash_recover_for_one_log(pm_regions_view[w], mems[w], multilog_id,
                                                             num_logs, which_log, cdb, infos[w], state[w]);
        }

        // Finally, get Z3 to see the equivalence of the recovery
        // result and the desired abstract state by asking it (with
        // `=~=`) to prove that they're piecewise equivalent.

        assert(recover_all(mems, multilog_id) =~= Some(state.drop_pending_appends()));
    }

    // This exported lemma proves that, if various invariants hold for
    // the given persistent memory regions view `pm_regions_view` and
    // abstract multilog state `state`, then for any contents `mem`
    // the view can recover into, recovery on `mem` will produce
    // `state.drop_pending_appends()`.
    //
    // `pm_regions_view` -- the persistent memory regions view
    // `multilog_id` -- the ID of the multilog
    // `num_logs` -- the number of logs
    // `cdb` -- the current value of the corruption-detecting boolean
    // `infos` -- the log information
    // `state` -- the abstract multilog state
    pub proof fn lemma_invariants_imply_crash_recover_forall(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        num_logs: u32,
        cdb: bool,
        infos: Seq<LogInfo>,
        state: AbstractMultiLogState,
    )
        requires
            memory_matches_deserialized_cdb(pm_regions_view, cdb),
            each_metadata_consistent_with_info(pm_regions_view, multilog_id, num_logs, cdb, infos),
            each_info_consistent_with_log_area(pm_regions_view, num_logs, infos, state),
        ensures
            forall |mem| pm_regions_view.can_crash_as(mem) ==>
                recover_all(mem, multilog_id) == Some(state.drop_pending_appends())
    {
        assert forall |mem| pm_regions_view.can_crash_as(mem) implies recover_all(mem, multilog_id) ==
                   Some(state.drop_pending_appends()) by
        {
            lemma_invariants_imply_crash_recover(pm_regions_view, mem, multilog_id, num_logs, cdb, infos, state);
        }
    }

    // This lemma establishes that, if one updates the inactive
    // log metadata in a region, this will maintain various
    // invariants. The "inactive" log metadata is the
    // metadata corresponding to the negation of the current
    // corruption-detecting boolean.
    //
    // `pm_regions_view` -- the persistent memory regions view
    // `multilog_id` -- the ID of the multilog
    // `num_logs` -- the number of logs
    // `cdb` -- the current value of the corruption-detecting boolean
    // `infos` -- the log information
    // `state` -- the abstract multilog state
    // `which_log` -- region on which the inactive level-3 metadata will be overwritten
    // `bytes_to_write` -- bytes to be written to the inactive log metadata area
    pub proof fn lemma_updating_inactive_metadata_maintains_invariants(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        num_logs: u32,
        cdb: bool,
        infos: Seq<LogInfo>,
        state: AbstractMultiLogState,
        which_log: u32,
        bytes_to_write: Seq<u8>,
    )
        requires
            memory_matches_deserialized_cdb(pm_regions_view, cdb),
            each_metadata_consistent_with_info(pm_regions_view, multilog_id, num_logs, cdb, infos),
            each_info_consistent_with_log_area(pm_regions_view, num_logs, infos, state),
            is_valid_log_index(which_log, num_logs),
            bytes_to_write.len() == LogMetadata::spec_size_of(),
       ensures
            ({
                let pm_regions_view2 = pm_regions_view.write(which_log as int, get_log_metadata_pos(!cdb) as int,
                                                             bytes_to_write);
                &&& memory_matches_deserialized_cdb(pm_regions_view2, cdb)
                &&& each_metadata_consistent_with_info(pm_regions_view2, multilog_id, num_logs, cdb, infos)
                &&& each_info_consistent_with_log_area(pm_regions_view2, num_logs, infos, state)
            })
    {
        let pm_regions_view2 = pm_regions_view.write(which_log as int, get_log_metadata_pos(!cdb) as int,
                                                     bytes_to_write);
        let w = which_log as int;

        assert(memory_matches_deserialized_cdb(pm_regions_view2, cdb)) by {
            assert(is_valid_log_index(0, num_logs)); // This triggers various `forall`s in invariants.
            assert(extract_log_cdb(pm_regions_view2[0].committed()) =~=
                   extract_log_cdb(pm_regions_view[0].committed()));
        }

        // To show that all the metadata still matches even after the
        // write, observe that everywhere the bytes match, any call to
        // `extract_bytes` will also match.

        assert(each_metadata_consistent_with_info(pm_regions_view2, multilog_id, num_logs, cdb, infos)) by {
            lemma_establish_extract_bytes_equivalence(pm_regions_view[w].committed(), pm_regions_view2[w].committed());
        }
    }

    // This lemma establishes that, if one updates the inactive
    // log metadata in a region, this will maintain various
    // invariants. The "inactive" log metadata is the
    // metadata corresponding to the negation of the current
    // corruption-detecting boolean.
    //
    // `pm_regions_view` -- the persistent memory regions view
    // `multilog_id` -- the ID of the multilog
    // `num_logs` -- the number of logs
    // `cdb` -- the current value of the corruption-detecting boolean
    // `infos` -- the log information
    // `state` -- the abstract multilog state
    // `which_log` -- region on which the inactive log metadata will be overwritten
    // `bytes_to_write` -- bytes to be written to the inactive log metadata area
    pub proof fn lemma_updating_inactive_crc_maintains_invariants(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        num_logs: u32,
        cdb: bool,
        infos: Seq<LogInfo>,
        state: AbstractMultiLogState,
        which_log: u32,
        bytes_to_write: Seq<u8>,
    )
        requires
            memory_matches_deserialized_cdb(pm_regions_view, cdb),
            each_metadata_consistent_with_info(pm_regions_view, multilog_id, num_logs, cdb, infos),
            each_info_consistent_with_log_area(pm_regions_view, num_logs, infos, state),
            is_valid_log_index(which_log, num_logs),
            bytes_to_write.len() == u64::spec_size_of(),
        ensures
            ({
                let pm_regions_view2 = pm_regions_view.write(
                    which_log as int,
                    get_log_metadata_pos(!cdb) + LogMetadata::spec_size_of(),
                    bytes_to_write
                );
                &&& memory_matches_deserialized_cdb(pm_regions_view2, cdb)
                &&& each_metadata_consistent_with_info(pm_regions_view2, multilog_id, num_logs, cdb, infos)
                &&& each_info_consistent_with_log_area(pm_regions_view2, num_logs, infos, state)
            })
    {
        let pm_regions_view2 = pm_regions_view.write(
            which_log as int,
            get_log_metadata_pos(!cdb) + LogMetadata::spec_size_of(),
            bytes_to_write
        );
        let w = which_log as int;

        assert(memory_matches_deserialized_cdb(pm_regions_view2, cdb)) by {
            assert(is_valid_log_index(0, num_logs)); // This triggers various `forall`s in invariants.
            assert(extract_log_cdb(pm_regions_view2[0].committed()) =~=
                   extract_log_cdb(pm_regions_view[0].committed()));
        }

        // To show that all the metadata still matches even after the
        // write, observe that everywhere the bytes match, any call to
        // `extract_bytes` will also match.

        assert(each_metadata_consistent_with_info(pm_regions_view2, multilog_id, num_logs, cdb, infos)) by {
            lemma_establish_extract_bytes_equivalence(pm_regions_view[w].committed(), pm_regions_view2[w].committed());
        }
    }

    // This lemma establishes that, if one flushes persistent memory,
    // this will maintain various invariants.
    //
    // `pm_regions_view` -- the persistent memory regions view
    // `multilog_id` -- the ID of the multilog
    // `num_logs` -- the number of logs
    // `cdb` -- the current value of the corruption-detecting boolean
    // `infos` -- the log information
    // `state` -- the abstract multilog state
    pub proof fn lemma_flushing_metadata_maintains_invariants(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        num_logs: u32,
        cdb: bool,
        infos: Seq<LogInfo>,
        state: AbstractMultiLogState,
    )
        requires
            memory_matches_deserialized_cdb(pm_regions_view, cdb),
            each_metadata_consistent_with_info(pm_regions_view,  multilog_id, num_logs, cdb, infos),
            each_info_consistent_with_log_area(pm_regions_view, num_logs, infos, state),
       ensures
            ({
                let pm_regions_view2 = pm_regions_view.flush();
                &&& memory_matches_deserialized_cdb(pm_regions_view2, cdb)
                &&& each_metadata_consistent_with_info(pm_regions_view2, multilog_id, num_logs, cdb, infos)
                &&& each_info_consistent_with_log_area(pm_regions_view2, num_logs, infos, state)
            })
    {
        let pm_regions_view2 = pm_regions_view.flush();

        assert(memory_matches_deserialized_cdb(pm_regions_view2, cdb)) by {
            assert(is_valid_log_index(0, num_logs)); // This triggers various `forall`s in invariants.
            assert(extract_log_cdb(pm_regions_view2[0].committed()) =~=
                   extract_log_cdb(pm_regions_view[0].committed()));
        }

        // To show that all the metadata still matches even after the
        // flush, observe that everywhere the bytes match, any call to
        // `extract_bytes` will also match.

        assert forall |which_log: u32| #[trigger] is_valid_log_index(which_log, num_logs) implies {
            metadata_consistent_with_info(pm_regions_view2[which_log as int], multilog_id, num_logs, which_log, cdb,
                                          infos[which_log as int])
        } by {
            lemma_establish_extract_bytes_equivalence(pm_regions_view[which_log as int].committed(),
                                                      pm_regions_view2[which_log as int].committed());
        }
    }

    // This lemma proves that if the active metadata is the same in each log between two PersistentMemoryRegions,
    // and one set of logs has its metadata types set, then the other also has its metadata types set.
    // 
    // `pm1` -- the multilog that has metadata types set
    // `pm2` -- a multilog with equal active metadata to pm1
    // `cdb` -- the current CDB of pm1 (and pm2)
    pub proof fn lemma_regions_metadata_matches_implies_metadata_types_set(
        pm1: PersistentMemoryRegionsView,
        pm2: PersistentMemoryRegionsView,
        cdb: bool
    )
        requires 
            no_outstanding_writes_to_active_metadata(pm1, cdb),
            no_outstanding_writes_to_active_metadata(pm2, cdb),
            metadata_types_set(pm1.committed()),
            memory_matches_deserialized_cdb(pm1, cdb),
            active_metadata_is_equal(pm1, pm2),
            pm1.len() == pm2.len(),
            pm1.len() > 0,
            forall |i: int| #![auto] 0 <= i < pm1.len() ==> pm1[i].len() == pm2[i].len(),
            forall |i: int| #![auto] 0 <= i < pm1.len() ==> ABSOLUTE_POS_OF_LOG_AREA < pm1[i].len(),
        ensures 
            metadata_types_set(pm2.committed())
    {
        let log_metadata_pos = get_log_metadata_pos(cdb);
        
        lemma_auto_smaller_range_of_seq_is_subrange(pm1[0].committed());
        lemma_auto_smaller_range_of_seq_is_subrange(pm2[0].committed());
        
        assert(metadata_types_set_in_first_region(pm2[0].committed()));
        assert forall |i: int| #![auto] 1 <= i < pm1.len() implies metadata_types_set_in_region(pm2.committed()[i], cdb) by {
            lemma_establish_extract_bytes_equivalence(pm1.committed()[i], pm2.committed()[i]);
            lemma_auto_smaller_range_of_seq_is_subrange(pm1.committed()[i]);
            lemma_auto_smaller_range_of_seq_is_subrange(pm2.committed()[i]);
            assert(pm1[i].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) == 
                pm2[i].committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));
            assert(extract_bytes(pm1.committed()[i], ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()) ==
                extract_bytes(pm2.committed()[i], ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()));
            assert(extract_bytes(pm1.committed()[i], ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()) ==
                extract_bytes(pm2.committed()[i], ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()));
        }
    }

    // This lemma proves that metadata types are set after updating the logs' CDB if we have properly
    // set the inactive metadata beforehand.
    //
    // `old_pm_regions_view` -- the initial state of PM
    // `new_pm_regions_view` -- the same PM state, but with its CDB updated
    // `log_id` -- the ID of the multilog
    // `new_cdb_bytes` -- a byte representation of the new CDB value. 
    // `old_cdb` -- the current CDB, as a boolean, of `old_pm_regions_view``
    pub proof fn lemma_metadata_types_set_after_cdb_update(
        old_pm_regions_view: PersistentMemoryRegionsView,
        new_pm_regions_view: PersistentMemoryRegionsView,
        log_id: u128,
        new_cdb_bytes: Seq<u8>,
        old_cdb: bool,
    )
        requires 
            old_pm_regions_view.no_outstanding_writes(),
            new_pm_regions_view.no_outstanding_writes(),
            old_pm_regions_view.len() == new_pm_regions_view.len(),
            new_cdb_bytes == CDB_FALSE.spec_to_bytes() || new_cdb_bytes == CDB_TRUE.spec_to_bytes(),
            old_pm_regions_view.len() > 0,
            deserialize_and_check_log_cdb(old_pm_regions_view.committed()[0]) is Some,
            deserialize_and_check_log_cdb(old_pm_regions_view.committed()[0]).unwrap() == old_cdb,
            old_cdb ==> new_cdb_bytes == CDB_FALSE.spec_to_bytes(),
            !old_cdb ==> new_cdb_bytes == CDB_TRUE.spec_to_bytes(),
            new_pm_regions_view == old_pm_regions_view.write(0, ABSOLUTE_POS_OF_LOG_CDB as int, new_cdb_bytes).flush(),
            metadata_types_set(old_pm_regions_view.committed()),
            forall |i: int| #![auto] 0 <= i < old_pm_regions_view.len() ==> old_pm_regions_view[i].len() == new_pm_regions_view[i].len(),
            forall |i: int| #![auto] 0 <= i < old_pm_regions_view.len() ==> ABSOLUTE_POS_OF_LOG_AREA < old_pm_regions_view[i].len(),
            forall |i: int| #![auto] 0 <= i < old_pm_regions_view.len() ==> inactive_metadata_types_set_in_region(old_pm_regions_view.committed()[i], old_cdb),
        ensures 
            metadata_types_set(new_pm_regions_view.committed())
    {
        lemma_establish_extract_bytes_equivalence(old_pm_regions_view.committed()[0], new_pm_regions_view.committed()[0]);

        // The CDB has been updated in log 0, so its type is set
        assert(extract_bytes(new_pm_regions_view.committed()[0], ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()) =~= new_cdb_bytes);

        let new_cdb = deserialize_and_check_log_cdb(new_pm_regions_view.committed()[0]).unwrap();
        let log_metadata_pos = get_log_metadata_pos(new_cdb);

        assert forall |i: int| #![auto] 1 <= i < old_pm_regions_view.len() implies {
            metadata_types_set_in_region(new_pm_regions_view.committed()[i], new_cdb)
        } by {
            lemma_establish_extract_bytes_equivalence(old_pm_regions_view.committed()[i], new_pm_regions_view.committed()[i]);
        }

        assume(false);
    }

    // This lemma proves that if there are no outstanding writes to active metadata, and metadata types are set,
    // then metadata types remain set if the persistent memory regions are flushed.
    // 
    // `pm_regions_view` -- a PM state with metadata types set and no outstanding writes to active metadata
    // `cdb` -- the current CDB of `pm_regions_view`
    pub proof fn lemma_no_outstanding_writes_to_active_metadata_implies_metadata_types_set_after_flush(
        pm_regions_view: PersistentMemoryRegionsView,
        cdb: bool,
    ) 
        requires 
            deserialize_and_check_log_cdb(pm_regions_view.committed()[0]) is Some,
            cdb == deserialize_and_check_log_cdb(pm_regions_view.committed()[0]).unwrap(),
            no_outstanding_writes_to_active_metadata(pm_regions_view, cdb),
            metadata_types_set(pm_regions_view.committed()),
            pm_regions_view.len() > 0,
            forall |i: int| #![auto] 0 <= i < pm_regions_view.len() ==> pm_regions_view[i].len() > ABSOLUTE_POS_OF_LOG_AREA
        ensures 
            metadata_types_set(pm_regions_view.flush().committed()),
    {
        assert(pm_regions_view.len() == pm_regions_view.committed().len());
        
        assert(metadata_types_set_in_first_region(pm_regions_view.committed()[0]));

        let first_region_committed = pm_regions_view.committed()[0];
        let first_region_flushed = pm_regions_view.flush().committed()[0];
        lemma_establish_extract_bytes_equivalence(first_region_committed, first_region_flushed);

        assert(metadata_types_set_in_first_region(pm_regions_view.flush().committed()[0]));

        assert forall |i: int| #![auto] 0 <= i < pm_regions_view.len() implies 
            metadata_types_set_in_region(pm_regions_view.flush().committed()[i], cdb) 
        by {
            let committed = pm_regions_view.committed()[i];
            let flushed = pm_regions_view.flush().committed()[i];
            lemma_establish_extract_bytes_equivalence(committed, flushed);
        }
        
    }

    // This lemma proves that active metadata remains the same after writing to inactive metadata.
    //
    // `wrpm_regions_old` -- an initial PM state
    // `wrpm_regions_new` -- the same PM state after a write to bytes in inactive metadata of one of the logs
    // `which_log` -- which log was written to 
    // `addr` -- the address that was written to; must be within the inactive metadata of the specified log
    // `bytes_to_write` -- the bytes that were written to `addr`
    // `cdb` -- the current CDB of `wrpm_regions_old` (and `wrpm_regions_new`)
    pub proof fn lemma_write_to_inactive_metadata_implies_active_metadata_stays_equal(
        wrpm_regions_old: PersistentMemoryRegionsView,
        wrpm_regions_new: PersistentMemoryRegionsView,
        which_log: int,
        addr: int,
        bytes_to_write: Seq<u8>,
        cdb: bool,
    )
        requires 
            wrpm_regions_new == wrpm_regions_old.write(which_log, addr, bytes_to_write),
            0 <= which_log < wrpm_regions_old.len(),
            metadata_types_set(wrpm_regions_old.committed()),
            ({
                let unused_metadata_pos = get_log_metadata_pos(!cdb);
                unused_metadata_pos <= addr < addr + bytes_to_write.len() <= unused_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()
            }),
            no_outstanding_writes_to_active_metadata(wrpm_regions_old, cdb),
            no_outstanding_writes_to_active_metadata(wrpm_regions_new, cdb),
            wrpm_regions_new.len() == wrpm_regions_old.len(),
            forall |i: int| #![auto] 0 <= i < wrpm_regions_new.len() ==> {
                &&& wrpm_regions_new[i].len() == wrpm_regions_old[i].len()
                &&& wrpm_regions_new[i].len() > ABSOLUTE_POS_OF_LOG_AREA
                &&& wrpm_regions_old[i].len() > ABSOLUTE_POS_OF_LOG_AREA
            },
            deserialize_and_check_log_cdb(wrpm_regions_old[0].committed()) is Some,
            deserialize_and_check_log_cdb(wrpm_regions_old[0].committed()).unwrap() == cdb,
        ensures 
            metadata_types_set(wrpm_regions_new.committed()),
            active_metadata_is_equal(wrpm_regions_new, wrpm_regions_old)
    {
        assert(forall |i: int| #![auto] 0 <= i < wrpm_regions_new.len() && i != which_log ==> 
                        wrpm_regions_old[i] == wrpm_regions_new[i]); 
        assert(forall |i: int| #![auto] 0 <= i < wrpm_regions_new.len() && i != which_log ==> 
            active_metadata_is_equal_in_region(wrpm_regions_old[i], wrpm_regions_new[i], cdb));

        let cur_old = wrpm_regions_old[which_log].committed();
        let cur_new = wrpm_regions_new[which_log].committed();
        assert(cur_old.len() == wrpm_regions_old[which_log].len());
        assert(cur_new.len() == wrpm_regions_new[which_log].len());

        lemma_auto_smaller_range_of_seq_is_subrange(cur_old);
        lemma_auto_smaller_range_of_seq_is_subrange(cur_new);

        assert(cur_old.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) == 
                cur_new.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));

        let old_cdb = deserialize_and_check_log_cdb(wrpm_regions_old[0].committed());
        let log_metadata_pos = get_log_metadata_pos(old_cdb.unwrap());

        assert(extract_bytes(cur_old, log_metadata_pos as int, LogMetadata::spec_size_of() + u64::spec_size_of()) == 
            extract_bytes(cur_new, log_metadata_pos as int, LogMetadata::spec_size_of() + u64::spec_size_of()));

        assert(active_metadata_is_equal_in_region(wrpm_regions_old[which_log], wrpm_regions_new[which_log], cdb));
        lemma_regions_metadata_matches_implies_metadata_types_set(wrpm_regions_old, wrpm_regions_new, cdb);
        assert(metadata_types_set(wrpm_regions_new.committed()));
    }
}

================
File: ./storage_node/src/multilog/multilogimpl_t.rs
================

//! This file contains the trusted implementation of a `MultiLogImpl`.
//! Although the verifier is run on this file, it needs to be
//! carefully read and audited to be confident of the correctness of
//! this multilog implementation.
//!
//! Fortunately, it delegates most of its work to an untrusted struct
//! `UntrustedMultiLogImpl`, which doesn't need to be read or audited.
//! It forces the `UntrustedMultiLogImpl` to satisfy certain
//! postconditions, and also places restrictions on what
//! `UntrustedMultiLogImpl` can do to persistent memory. These
//! restrictions ensure that even if the system or process crashes in
//! the middle of an operation, the system will still recover to a
//! consistent state.
//!
//! It requires `UntrustedMultiLogImpl` to implement routines that do the
//! various multilog operations like read and commit.
//!
//! It also requires `UntrustedMultiLogImpl` to provide a function
//! `UntrustedMultiLogImpl::recover`, which specifies what its `start`
//! routine will do to recover after a crash. It requires its `start`
//! routine to satisfy that specification. It also uses it to limit
//! how `UntrustedMultiLogImpl` writes to memory: It can only perform
//! updates that, if incompletely performed before a crash, still
//! leave the system in a valid state. The `recover` function takes a
//! second parameter, the `multilog_id` which is passed to the start
//! routine.
//!
//! It also requires `UntrustedMultiLogImpl` to provide a function `view`
//! that converts the current state into an abstract log. It requires that
//! performing a certain operation on the `UntrustedMultiLogImpl` causes a
//! corresponding update to its abstract view. For instance, calling
//! the `u.commit()` method should cause the resulting `u.view()` to
//! become `old(u).view().commit()`.
//!
//! It also permits `UntrustedMultiLogImpl` to provide a function `inv`
//! that encodes any invariants `UntrustedMultiLogImpl` wants maintained
//! across invocations of its functions. This implementation will then
//! guarantee that `inv` holds on any call to an `UntrustedMultiLogImpl`
//! method, and demand that the method preserve that invariant.

use std::fmt::Write;

use crate::multilog::multilogimpl_v::UntrustedMultiLogImpl;
use crate::multilog::multilogspec_t::AbstractMultiLogState;
use crate::pmem::pmemspec_t::*;
use crate::pmem::wrpm_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use deps_hack::rand::Rng;

verus! {

    // This is the specification that `MultiLogImpl` provides for data
    // bytes it reads. It says that those bytes are correct unless
    // there was corruption on the persistent memory between the last
    // write and this read.
    pub open spec fn read_correct_modulo_corruption(bytes: Seq<u8>, true_bytes: Seq<u8>,
                                                    impervious_to_corruption: bool) -> bool
    {
        if impervious_to_corruption {
            // If the region is impervious to corruption, the bytes read
            // must match the true bytes, i.e., the bytes last written.

            bytes == true_bytes
        }
        else {
            // Otherwise, there must exist a sequence of distinct
            // addresses `addrs` such that the nth byte of `bytes` is
            // a possibly corrupted version of the nth byte of
            // `true_bytes` read from the nth address in `addrs`.  We
            // don't require the sequence of addresses to be
            // contiguous because the data might not be contiguous on
            // disk (e.g., if it wrapped around the log area).

            exists |addrs: Seq<int>| {
                &&& all_elements_unique(addrs)
                &&& #[trigger] maybe_corrupted(bytes, true_bytes, addrs)
            }
        }
    }

    // This specification function indicates whether a given view of
    // memory can only crash in a way that, after recovery, leads to a
    // certain abstract state.
    pub open spec fn can_only_crash_as_state(
        pm_regions_view: PersistentMemoryRegionsView,
        multilog_id: u128,
        state: AbstractMultiLogState,
    ) -> bool
    {
        forall |s| #[trigger] pm_regions_view.can_crash_as(s) ==>
            UntrustedMultiLogImpl::recover(s, multilog_id) == Some(state)
    }

    // A `TrustedMultiLogPermission` is the type of a tracked object
    // indicating permission to update memory. It restricts updates so
    // that if a crash happens, the resulting memory `mem` satisfies
    // `is_state_allowable(mem)`.
    //
    // The struct is defined in this file, and it has a non-public
    // field, so the only code that can create one is in this file.
    // So untrusted code in other files can't create one, and we can
    // rely on it to restrict access to persistent memory.
    #[allow(dead_code)]
    pub struct TrustedMultiLogPermission {
        ghost is_state_allowable: spec_fn(Seq<Seq<u8>>) -> bool
    }

    impl CheckPermission<Seq<Seq<u8>>> for TrustedMultiLogPermission {
        closed spec fn check_permission(&self, state: Seq<Seq<u8>>) -> bool {
            (self.is_state_allowable)(state)
        }
    }

    impl TrustedMultiLogPermission {

        // This is one of two constructors for `TrustedMultiLogPermission`.
        // It conveys permission to do any update as long as a
        // subsequent crash and recovery can only lead to given
        // abstract state `state`.
        proof fn new_one_possibility(multilog_id: u128, state: AbstractMultiLogState) -> (tracked perm: Self)
            ensures
                forall |s| #[trigger] perm.check_permission(s) <==>
                    UntrustedMultiLogImpl::recover(s, multilog_id) == Some(state)
        {
            Self {
                is_state_allowable: |s| UntrustedMultiLogImpl::recover(s, multilog_id) == Some(state)
            }
        }

        // This is the second of two constructors for
        // `TrustedMultiLogPermission`.  It conveys permission to do any
        // update as long as a subsequent crash and recovery can only
        // lead to one of two given abstract states `state1` and
        // `state2`.
        proof fn new_two_possibilities(
            multilog_id: u128,
            state1: AbstractMultiLogState,
            state2: AbstractMultiLogState
        ) -> (tracked perm: Self)
            ensures
                forall |s| #[trigger] perm.check_permission(s) <==> {
                    ||| UntrustedMultiLogImpl::recover(s, multilog_id) == Some(state1)
                    ||| UntrustedMultiLogImpl::recover(s, multilog_id) == Some(state2)
                }
        {
            Self {
                is_state_allowable: |s| {
                    ||| UntrustedMultiLogImpl::recover(s, multilog_id) == Some(state1)
                    ||| UntrustedMultiLogImpl::recover(s, multilog_id) == Some(state2)
                }
            }
        }
    }

    // This enumeration represents the various errors that can be
    // returned from multilog operations. They're self-explanatory.
    #[derive(Debug)]
    pub enum MultiLogErr {
        CantSetupWithFewerThanOneRegion { },
        CantSetupWithMoreThanU32MaxRegions { },
        InsufficientSpaceForSetup { which_log: u32, required_space: u64 },
        StartFailedDueToMultilogIDMismatch { which_log: u32, multilog_id_expected: u128, multilog_id_read: u128 },
        StartFailedDueToRegionSizeMismatch { which_log: u32, region_size_expected: u64, region_size_read: u64 },
        StartFailedDueToProgramVersionNumberUnsupported { which_log: u32, version_number: u64, max_supported: u64 },
        StartFailedDueToInvalidMemoryContents { which_log: u32 },
        CRCMismatch,
        InvalidLogIndex { },
        InsufficientSpaceForAppend { available_space: u64 },
        CantReadBeforeHead { head: u128 },
        CantReadPastTail { tail: u128 },
        CantAdvanceHeadPositionBeforeHead { head: u128 },
        CantAdvanceHeadPositionBeyondTail { tail: u128 },
        PmemErr { err: PmemError }
    }

    // This executable method can be called to compute a random GUID.
    // It uses the external `rand` crate.
    #[verifier::external_body]
    pub exec fn generate_fresh_multilog_id() -> (out: u128)
    {
        deps_hack::rand::thread_rng().gen::<u128>()
    }

    /// A `MultiLogImpl` wraps one `UntrustedMultiLogImpl` and a
    /// collection of persistent memory regions to provide the
    /// executable interface that turns the persistent memory regions
    /// into a set of logs in which any subset of logs can be updated
    /// atomically.
    ///
    /// The `untrusted_log_impl` field is the wrapped
    /// `UntrustedMultiLogImpl`.
    ///
    /// The `multilog_id` field is the multilog ID. It's ghost.
    ///
    /// The `wrpm_regions` field contains the write-restricted persistent
    /// memory. This memory will only allow updates allowed by a
    /// tracked `TrustedMultiLogPermission`. So we can pass `wrpm_regions` to an
    /// untrusted method, along with a restricting
    /// `TrustedMultiLogPermission`, to limit what it's allowed to do.

    pub struct MultiLogImpl<PMRegions: PersistentMemoryRegions> {
        untrusted_log_impl: UntrustedMultiLogImpl,
        multilog_id: Ghost<u128>,
        wrpm_regions: WriteRestrictedPersistentMemoryRegions<TrustedMultiLogPermission, PMRegions>
    }

    impl <PMRegions: PersistentMemoryRegions> MultiLogImpl<PMRegions> {
        // The view of a `MultiLogImpl` is whatever the
        // `UntrustedMultiLogImpl` it wraps says it is.
        pub closed spec fn view(self) -> AbstractMultiLogState
        {
            self.untrusted_log_impl@
        }

        // The constants of a `MultiLogImpl` are whatever the
        // persistent memory it wraps says they are.
        pub closed spec fn constants(&self) -> PersistentMemoryConstants {
            self.wrpm_regions.constants()
        }

        // This is the validity condition that is maintained between
        // calls to methods on `self`.
        //
        // That is, each of the trusted wrappers on untrusted methods
        // below (e.g., `commit`, `advance_head`) can count on `valid`
        // holding because it demands that each untrusted method
        // maintains it.
        //
        // One element of `valid` is that the untrusted `inv` function
        // holds.
        //
        // The other element of `valid` is that the persistent memory,
        // if it crashes and recovers, must represent the current
        // abstract state with pending tentative appends dropped.
        pub closed spec fn valid(self) -> bool {
            &&& self.untrusted_log_impl.inv(&self.wrpm_regions, self.multilog_id@)
            &&& can_only_crash_as_state(self.wrpm_regions@, self.multilog_id@, self@.drop_pending_appends())
        }

        proof fn lemma_valid_implies_wrpm_inv(self)
            requires
                self.valid()
            ensures
                self.wrpm_regions.inv()
        {
            self.untrusted_log_impl.lemma_inv_implies_wrpm_inv(&self.wrpm_regions, self.multilog_id@);
        }

        proof fn lemma_untrusted_log_inv_implies_valid(self)
            requires
                self.untrusted_log_impl.inv(&self.wrpm_regions, self.multilog_id@)
            ensures
                self.valid()
        {
            self.untrusted_log_impl.lemma_inv_implies_can_only_crash_as(&self.wrpm_regions, self.multilog_id@);
        }

        // The `setup` method sets up persistent memory regions `pm_regions`
        // to store an initial empty multilog. It returns a vector
        // listing the capacities of the logs as well as a fresh
        // multilog ID to uniquely identify it. See `README.md` for more
        // documentation.
        pub exec fn setup(pm_regions: &mut PMRegions) -> (result: Result<(Vec<u64>, u128), MultiLogErr>)
            requires
                old(pm_regions).inv(),
            ensures
                pm_regions.inv(),
                pm_regions@.no_outstanding_writes(),
                match result {
                    Ok((log_capacities, multilog_id)) => {
                        let state = AbstractMultiLogState::initialize(log_capacities@);
                        &&& pm_regions@.len() == old(pm_regions)@.len()
                        &&& pm_regions@.len() >= 1
                        &&& pm_regions@.len() <= u32::MAX
                        &&& log_capacities@.len() == pm_regions@.len()
                        &&& forall |i: int| 0 <= i < pm_regions@.len() ==>
                               #[trigger] log_capacities@[i] <= pm_regions@[i].len()
                        &&& forall |i: int| 0 <= i < pm_regions@.len() ==>
                               #[trigger] pm_regions@[i].len() == old(pm_regions)@[i].len()
                        &&& can_only_crash_as_state(pm_regions@, multilog_id, state)
                        &&& UntrustedMultiLogImpl::recover(pm_regions@.committed(), multilog_id) == Some(state)
                        // Required by the `start` function's precondition. Putting this in the
                        // postcond of `setup` ensures that the trusted caller doesn't have to prove it
                        &&& UntrustedMultiLogImpl::recover(pm_regions@.flush().committed(), multilog_id) == Some(state)
                        &&& state == state.drop_pending_appends()
                    },
                    Err(MultiLogErr::InsufficientSpaceForSetup { which_log, required_space }) => {
                        let flushed_regions = old(pm_regions)@.flush();
                        &&& pm_regions@ == flushed_regions
                        &&& pm_regions@[which_log as int].len() < required_space
                    },
                    Err(MultiLogErr::CantSetupWithFewerThanOneRegion { }) => {
                        let flushed_regions = old(pm_regions)@.flush();
                        &&& pm_regions@ == flushed_regions
                        &&& pm_regions@.len() < 1
                    },
                    Err(MultiLogErr::CantSetupWithMoreThanU32MaxRegions { }) => {
                        let flushed_regions = old(pm_regions)@.flush();
                        &&& pm_regions@ == flushed_regions
                        &&& pm_regions@.len() > u32::MAX
                    },
                    _ => false
                }
        {
            let multilog_id = generate_fresh_multilog_id();
            let capacities = UntrustedMultiLogImpl::setup(pm_regions, multilog_id)?;
            Ok((capacities, multilog_id))
        }

        // The `start` method creates an `UntrustedMultiLogImpl` out
        // of a set of persistent memory regions. It's assumed that
        // those regions were initialized with `setup` and then only
        // multilog operations were allowed to mutate them. See
        // `README.md` for more documentation and an example of use.
        pub exec fn start(pm_regions: PMRegions, multilog_id: u128)
                          -> (result: Result<MultiLogImpl<PMRegions>, MultiLogErr>)
            requires
                pm_regions.inv(),
                UntrustedMultiLogImpl::recover(pm_regions@.flush().committed(), multilog_id).is_Some(),
            ensures
                match result {
                    Ok(trusted_log_impl) => {
                        &&& trusted_log_impl.valid()
                        &&& trusted_log_impl.constants() == pm_regions.constants()
                        &&& Some(trusted_log_impl@) == UntrustedMultiLogImpl::recover(pm_regions@.flush().committed(),
                                                                                     multilog_id)
                    },
                    Err(MultiLogErr::CRCMismatch) => !pm_regions.constants().impervious_to_corruption,
                    Err(MultiLogErr::InsufficientSpaceForSetup { which_log, required_space }) => {
                        let flushed_regions = pm_regions@.flush();
                        &&& 0 <= which_log < flushed_regions.len()
                        &&& pm_regions@[which_log as int].len() < required_space
                    },
                    _ => false
                }
        {
            // We allow the untrusted `start` method to update memory
            // as part of its initialization. But, to avoid bugs
            // stemming from crashes in the middle of this routine, we
            // must restrict how it updates memory. We must only let
            // it write such that, if a crash happens in the middle,
            // it doesn't change the persistent state.

            let ghost state = UntrustedMultiLogImpl::recover(pm_regions@.flush().committed(), multilog_id).get_Some_0();
            let mut wrpm_regions = WriteRestrictedPersistentMemoryRegions::new(pm_regions);
            let tracked perm = TrustedMultiLogPermission::new_one_possibility(multilog_id, state);
            let untrusted_log_impl =
                UntrustedMultiLogImpl::start(&mut wrpm_regions, multilog_id, Tracked(&perm), Ghost(state))?;
            Ok(
                MultiLogImpl {
                    untrusted_log_impl,
                    multilog_id:  Ghost(multilog_id),
                    wrpm_regions
                },
            )
        }

        // The `tentatively_append` method tentatively appends
        // `bytes_to_append` to the end of log number `which_log` in
        // the multilog. It's tentative in that crashes will undo the
        // appends, and reads aren't allowed in the tentative part of
        // the log. See `README.md` for more documentation and examples
        // of use.
        pub exec fn tentatively_append(&mut self, which_log: u32, bytes_to_append: &[u8])
                                       -> (result: Result<u128, MultiLogErr>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                self.constants() == old(self).constants(),
                match result {
                    Ok(offset) => {
                        let state = old(self)@[which_log as int];
                        &&& which_log < old(self)@.num_logs()
                        &&& offset == state.head + state.log.len() + state.pending.len()
                        &&& self@ == old(self)@.tentatively_append(which_log as int, bytes_to_append@)
                    },
                    Err(MultiLogErr::InvalidLogIndex { }) => {
                        &&& which_log >= self@.num_logs()
                        &&& self@ == old(self)@
                    },
                    Err(MultiLogErr::InsufficientSpaceForAppend { available_space }) => {
                        &&& self@ == old(self)@
                        &&& which_log < self@.num_logs()
                        &&& available_space < bytes_to_append@.len()
                        &&& {
                               let state = self@[which_log as int];
                               ||| available_space == state.capacity - state.log.len() - state.pending.len()
                               ||| available_space == u128::MAX - state.head - state.log.len() - state.pending.len()
                           }
                    },
                    _ => false
                }
        {
            // For crash safety, we must restrict the untrusted code's
            // writes to persistent memory. We must only let it write
            // such that, if a crash happens in the middle of a write,
            // the view of the persistent state is either the current
            // state or the current state with `bytes_to_append`
            // appended.
            let tracked perm = TrustedMultiLogPermission::new_one_possibility(self.multilog_id@, self@.drop_pending_appends());
            self.untrusted_log_impl.tentatively_append(&mut self.wrpm_regions, which_log, bytes_to_append,
                                                       self.multilog_id, Tracked(&perm))
        }

        // The `commit` method atomically commits all tentative
        // appends that have been done to `self` since the last
        // commit. The commit is atomic in that even if there's a
        // crash in the middle, the recovered-to state either reflects
        // all those tentative appends or none of them. See `README.md`
        // for more documentation and examples of use.
        pub exec fn commit(&mut self) -> (result: Result<(), MultiLogErr>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                self.constants() == old(self).constants(),
                match result {
                    Ok(()) => self@ == old(self)@.commit(),
                    _ => false,
                }
        {
            // For crash safety, we must restrict the untrusted code's
            // writes to persistent memory. We must only let it write
            // such that, if a crash happens in the middle of a write,
            // the view of the persistent state is either the current
            // state or the current state with all uncommitted appends
            // committed.
            let tracked perm = TrustedMultiLogPermission::new_two_possibilities(self.multilog_id@, self@.drop_pending_appends(),
                                                                        self@.commit().drop_pending_appends());
            self.untrusted_log_impl.commit(&mut self.wrpm_regions, self.multilog_id, Tracked(&perm))
        }

        // The `advance_head` method advances the head of log number
        // `which_log` to virtual new head position `new_head`. It
        // doesn't do this tentatively; it completes it durably before
        // returning. However, `advance_head` doesn't commit tentative
        // appends; to do that, you need a separate call to
        // `commit`. See `README.md` for more documentation and examples
        // of use.
        pub exec fn advance_head(&mut self, which_log: u32, new_head: u128) -> (result: Result<(), MultiLogErr>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                self.constants() == old(self).constants(),
                match result {
                    Ok(()) => {
                        let w = which_log as int;
                        &&& which_log < self@.num_logs()
                        &&& old(self)@[w].head <= new_head <= old(self)@[w].head + old(self)@[w].log.len()
                        &&& self@ == old(self)@.advance_head(w, new_head as int)
                    },
                    Err(MultiLogErr::InvalidLogIndex{ }) => {
                        &&& which_log >= self@.num_logs()
                        &&& self@ == old(self)@
                    },
                    Err(MultiLogErr::CantAdvanceHeadPositionBeforeHead { head }) => {
                        &&& self@ == old(self)@
                        &&& which_log < self@.num_logs()
                        &&& head == self@[which_log as int].head
                        &&& new_head < head
                    },
                    Err(MultiLogErr::CantAdvanceHeadPositionBeyondTail { tail }) => {
                        &&& self@ == old(self)@
                        &&& which_log < self@.num_logs()
                        &&& tail == self@[which_log as int].head + self@[which_log as int].log.len()
                        &&& new_head > tail
                    },
                    _ => false,
                }
        {
            // For crash safety, we must restrict the untrusted code's
            // writes to persistent memory. We must only let it write
            // such that, if a crash happens in the middle of a write,
            // the view of the persistent state is either the current
            // state or the current state with the head advanced.
            let tracked perm = TrustedMultiLogPermission::new_two_possibilities(
                self.multilog_id@,
                self@.drop_pending_appends(),
                self@.advance_head(which_log as int, new_head as int).drop_pending_appends()
            );
            self.untrusted_log_impl.advance_head(&mut self.wrpm_regions, which_log, new_head,
                                                 self.multilog_id, Tracked(&perm))
        }

        // The `read` method reads `len` bytes from log number
        // `which_log` starting at virtual position `pos`. It isn't
        // allowed to read earlier than the head or past the committed
        // tail. See `README.md` for more documentation and examples of
        // use.
        pub exec fn read(&self, which_log: u32, pos: u128, len: u64) -> (result: Result<Vec<u8>, MultiLogErr>)
            requires
                self.valid(),
                pos + len <= u128::MAX,
            ensures
                ({
                    let state = self@[which_log as int];
                    let head = state.head;
                    let log = state.log;
                    match result {
                        Ok(bytes) => {
                            let true_bytes = self@.read(which_log as int, pos as int, len as int);
                            &&& which_log < self@.num_logs()
                            &&& pos >= head
                            &&& pos + len <= head + log.len()
                            &&& read_correct_modulo_corruption(bytes@, true_bytes,
                                                             self.constants().impervious_to_corruption)
                        },
                        Err(MultiLogErr::InvalidLogIndex { }) => {
                            which_log >= self@.num_logs()
                        },
                        Err(MultiLogErr::CantReadBeforeHead{ head: head_pos }) => {
                            &&& which_log < self@.num_logs()
                            &&& pos < head
                            &&& head_pos == head
                        },
                        Err(MultiLogErr::CantReadPastTail{ tail }) => {
                            &&& which_log < self@.num_logs()
                            &&& pos + len > tail
                            &&& tail == head + log.len()
                        },
                        _ => false
                    }
                })
        {
            self.untrusted_log_impl.read(&self.wrpm_regions, which_log, pos, len, self.multilog_id)
        }

        // The `get_head_tail_and_capacity` method returns three
        // pieces of metadata about log number `which_log`: the
        // virtual head position, the virtual tail position, and the
        // capacity. The capacity is the maximum number of bytes there
        // can be in the log past the head, including bytes in
        // tentative appends that haven't been committed yet. See
        // `README.md` for more documentation and examples of use.
        pub exec fn get_head_tail_and_capacity(&self, which_log: u32) -> (result: Result<(u128, u128, u64), MultiLogErr>)
            requires
                self.valid()
            ensures
                match result {
                    Ok((result_head, result_tail, result_capacity)) => {
                        let inf_log = self@[which_log as int];
                        &&& which_log < self@.num_logs()
                        &&& result_head == inf_log.head
                        &&& result_tail == inf_log.head + inf_log.log.len()
                        &&& result_capacity == inf_log.capacity
                    },
                    Err(MultiLogErr::InvalidLogIndex{ }) => {
                        which_log >= self@.num_logs()
                    },
                    _ => false
                }
        {
            self.untrusted_log_impl.get_head_tail_and_capacity(&self.wrpm_regions, which_log, self.multilog_id)
        }
    }

}

================
File: ./storage_node/src/log/layout_v.rs
================

//! This file describes the persistent-memory layout used by the
//! log implementation.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.
//!
//! The persistent-memory region used to store a log will have the following layout.
//!
//! Global metadata:   Metadata whose length is constant across all versions and
//!                    the same for each region/log
//! Region metadata:   Metadata that does not change over the course of
//!                    execution.
//! Log metadata:      Metadata that changes as the data changes, so it
//!                    has two versions and a corruption-detecting boolean
//!                    distinguishing which of those two versions is active
//! Log area:          Area where log is written
//!
//! The corruption-detecting boolean dictates which of the two instances of
//! log metadata is used.
//!
//! Global metadata (absolute offsets):
//!   bytes 0..8:     Version number of the program that created this metadata
//!   bytes 8..16:    Length of region metadata, not including CRC
//!   bytes 16..32:   Program GUID for this program  
//!   bytes 32..40:   CRC of the above 32 bytes
//!
//! Region metadata (absolute offsets):
//!   bytes 40..48:   This region's size
//!   bytes 48..56:   Length of log area (LoLA)
//!   bytes 56..72:   Log ID
//!   bytes 72..80:   CRC of the above 32 bytes
//!
//! Log metadata (relative offsets):
//!   bytes 0..8:     Log length
//!   bytes 8..16:    Unused padding bytes
//!   bytes 16..32:   Log head virtual position
//!   bytes 32..40:   CRC of the above 32 bytes
//!
//! Log area (relative offsets):
//!   bytes 0..LoLA:   Byte #n is the one whose virtual log position modulo LoLA is n
//!
//! The log area starts at absolute offset 256 to improve Intel Optane DC PMM performance.
//!
//! The way the corruption-detecting boolean (CDB) detects corruption
//! is as follows. To write a CDB to persistent memory, we store one
//! of two eight-byte values: `CDB_FALSE` or `CDB_TRUE`. These are
//! sufficiently different from one another that each is extremely
//! unlikely to be corrupted to become the other. So, if corruption
//! happens, we can detect it by the fact that something other than
//! `CDB_FALSE` or `CDB_TRUE` was read.
//!

use crate::log::logspec_t::AbstractLogState;
use crate::log::inv_v::*;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmemutil_v::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::traits_t::{size_of, PmSized, ConstPmSized, UnsafeSpecPmSized, PmSafe};
use deps_hack::{PmSafe, PmSized};
use builtin::*;
use builtin_macros::*;
use core::fmt::Debug;
use vstd::bytes::*;
use vstd::prelude::*;

verus! {

    /// Constants

    // These constants describe the absolute or relative positions of
    // various parts of the layout.
    // TODO: clean these up
    pub const ABSOLUTE_POS_OF_GLOBAL_METADATA: u64 = 0;
    pub const RELATIVE_POS_OF_GLOBAL_VERSION_NUMBER: u64 = 0;
    pub const RELATIVE_POS_OF_GLOBAL_LENGTH_OF_REGION_METADATA: u64 = 8;
    pub const RELATIVE_POS_OF_GLOBAL_PROGRAM_GUID: u64 = 16;
    pub const LENGTH_OF_GLOBAL_METADATA: u64 = 32;
    pub const ABSOLUTE_POS_OF_GLOBAL_CRC: u64 = 32;

    pub const ABSOLUTE_POS_OF_REGION_METADATA: u64 = 40;
    pub const RELATIVE_POS_OF_REGION_REGION_SIZE: u64 = 0;
    pub const RELATIVE_POS_OF_REGION_LENGTH_OF_LOG_AREA: u64 = 8;
    pub const RELATIVE_POS_OF_REGION_LOG_ID: u64 = 16;
    pub const LENGTH_OF_REGION_METADATA: u64 = 32;
    pub const ABSOLUTE_POS_OF_REGION_CRC: u64 = 72;

    pub const ABSOLUTE_POS_OF_LOG_CDB: u64 = 80;
    pub const ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE: u64 = 88;
    pub const ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE: u64 = 128;
    pub const RELATIVE_POS_OF_LOG_LOG_LENGTH: u64 = 0;
    pub const RELATIVE_POS_OF_LOG_PADDING: u64 = 8;
    pub const RELATIVE_POS_OF_LOG_HEAD: u64 = 16;
    pub const LENGTH_OF_LOG_METADATA: u64 = 32;
    pub const ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE: u64 = 120;
    pub const ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE: u64 = 160;
    pub const ABSOLUTE_POS_OF_LOG_AREA: u64 = 256;
    pub const MIN_LOG_AREA_SIZE: u64 = 1;

    // This GUID was generated randomly and is meant to describe the
    // multilog program, even if it has future versions.

    pub const LOG_PROGRAM_GUID: u128 = 0x8eecd9dea2de4443903e2acf951380bf;

    // The current version number, and the only one whose contents
    // this program can read, is the following:

    pub const LOG_PROGRAM_VERSION_NUMBER: u64 = 1;

    // These structs represent the different levels of metadata.

    #[repr(C)]
    #[derive(PmSized, PmSafe, Copy, Clone, Default)]
    pub struct GlobalMetadata {
        pub version_number: u64,
        pub length_of_region_metadata: u64,
        pub program_guid: u128,
    }


    impl PmCopy for GlobalMetadata {}

    
    #[repr(C)]
    #[derive(PmSized, PmSafe, Copy, Clone, Default)]
    pub struct RegionMetadata {
        pub region_size: u64,
        pub log_area_len: u64,
        pub log_id: u128,
    }

    impl PmCopy for RegionMetadata {}

    #[repr(C)]
    #[derive(PmSized, PmSafe, Copy, Clone, Default)]
    pub struct LogMetadata {
        pub log_length: u64,
        pub _padding: u64,
        pub head: u128,
    }

    impl PmCopy for LogMetadata {}


    /// Specification functions for extracting metadata from a
    /// persistent-memory region.

    // This function extracts the subsequence of `bytes` that lie
    // between `pos` and `pos + len` inclusive of `pos` but exclusive
    // of `pos + len`.
    pub open spec fn extract_bytes(bytes: Seq<u8>, pos: int, len: int) -> Seq<u8>
    {
        bytes.subrange(pos, pos + len)
    }

    // This function extracts the bytes encoding global metadata from
    // the contents `mem` of a persistent memory region.
    pub open spec fn extract_global_metadata(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of() as int)
    }

    pub open spec fn deserialize_global_metadata(mem: Seq<u8>) -> GlobalMetadata
    {
        let bytes = extract_global_metadata(mem);
        GlobalMetadata::spec_from_bytes(bytes)
    }

    // This function extracts the CRC of the global metadata from the
    // contents `mem` of a persistent memory region.
    pub open spec fn extract_global_crc(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of() as int)
    }

    pub open spec fn deserialize_global_crc(mem: Seq<u8>) -> u64
    {
        let bytes = extract_global_crc(mem);
        u64::spec_from_bytes(bytes)
    }

    // This function extracts the bytes encoding region metadata
    // from the contents `mem` of a persistent memory region.
    pub open spec fn extract_region_metadata(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of() as int)
    }

    pub open spec fn deserialize_region_metadata(mem: Seq<u8>) -> RegionMetadata
    {
        let bytes = extract_region_metadata(mem);
        RegionMetadata::spec_from_bytes(bytes)
    }

    // This function extracts the CRC of the region metadata from the
    // contents `mem` of a persistent memory region.
    pub open spec fn extract_region_crc(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of() as int)
    }

    pub open spec fn deserialize_region_crc(mem: Seq<u8>) -> u64
    {
        let bytes = extract_region_crc(mem);
        u64::spec_from_bytes(bytes)
    }

    // This function extracts the bytes encoding the log metadata's
    // corruption-detecting boolean (i.e., CDB) from the contents
    // `mem` of a persistent memory region.
    pub open spec fn extract_log_cdb(mem: Seq<u8>) -> Seq<u8>
    {
        extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of() as int)
    }

    // This function extracts the log metadata's corruption-detecting boolean
    // (i.e., CDB) from the contents `mem` of a persistent memory
    // region. It returns an Option<bool> with the following meanings:
    //
    // None -- Corruption was detected when reading the CDB
    // Some(true) -- No corruption was detected and the CDB is true
    // Some(false) -- No corruption was detected and the CDB is false
    //
    pub open spec fn extract_and_parse_log_cdb(mem: Seq<u8>) -> Option<bool>
    {
        let log_cdb = extract_log_cdb(mem);
        if spec_u64_from_le_bytes(log_cdb) == CDB_FALSE {
            Some(false)
        }
        else if spec_u64_from_le_bytes(log_cdb) == CDB_TRUE {
            Some(true)
        }
        else {
            None
        }
    }

    pub open spec fn deserialize_log_cdb(mem: Seq<u8>) -> u64
    {
        let bytes = extract_log_cdb(mem);
        u64::spec_from_bytes(bytes)
    }

    pub open spec fn deserialize_and_check_log_cdb(mem: Seq<u8>) -> Option<bool>
    {
        let log_cdb = deserialize_log_cdb(mem);
        if log_cdb == CDB_FALSE {
            Some(false)
        } else if log_cdb == CDB_TRUE {
            Some(true)
        } else {
            None
        }
    }

    // This function computes where the log metadata will be in a
    // persistent-memory region given the current boolean value `cdb`
    // of the corruption-detecting boolean.
    pub open spec fn get_log_metadata_pos(cdb: bool) -> u64
    {
        if cdb { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE } else { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE }
    }
    // This function extracts the log metadata and its CRC from the
    // `bytes` where they're stored.

    // This function computes where the log metadata ends in a
    // persistent-memory region (i.e., the index of the byte just past
    // the end of the log metadata) given the current boolean
    // value `cdb` of the corruption-detecting boolean.
    pub open spec fn get_log_crc_end(cdb: bool) -> u64
    {
        (get_log_metadata_pos(cdb) + LogMetadata::spec_size_of() + u64::spec_size_of()) as u64
    }

    // This function extracts the bytes encoding log metadata from
    // the contents `mem` of a persistent memory region. It needs to
    // know the current boolean value `cdb` of the
    // corruption-detecting boolean because there are two possible
    // places for such metadata.
    pub open spec fn extract_log_metadata(mem: Seq<u8>, cdb: bool) -> Seq<u8>
    {
        let pos = get_log_metadata_pos(cdb);
        extract_bytes(mem, pos as int, LogMetadata::spec_size_of() as int)
    }

    pub open spec fn deserialize_log_metadata(mem: Seq<u8>, cdb: bool) -> LogMetadata
    {
        let bytes = extract_log_metadata(mem, cdb);
        LogMetadata::spec_from_bytes(bytes)
    }

    // This function extracts the CRC of the log metadata from the
    // contents `mem` of a persistent memory region. It needs to know
    // the current boolean value `cdb` of the corruption-detecting
    // boolean because there are two possible places for that CRC.
    pub open spec fn extract_log_crc(mem: Seq<u8>, cdb: bool) -> Seq<u8>
    {
        let pos = if cdb { ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE }
                  else { ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE };
        extract_bytes(mem, pos as int, u64::spec_size_of() as int)
    }

    pub open spec fn deserialize_log_crc(mem: Seq<u8>, cdb: bool) -> u64
    {
        let bytes = extract_log_crc(mem, cdb);
        u64::spec_from_bytes(bytes)
    }

    // This function returns the 4-byte unsigned integer (i.e., u32)
    // encoded at position `pos` in byte sequence `bytes`.
    pub open spec fn parse_u32(bytes: Seq<u8>, pos: int) -> u32
    {
        spec_u32_from_le_bytes(extract_bytes(bytes, pos, 4))
    }

    // This function returns the 8-byte unsigned integer (i.e., u64)
    // encoded at position `pos` in byte sequence `bytes`.
    pub open spec fn parse_u64(bytes: Seq<u8>, pos: int) -> u64
    {
        spec_u64_from_le_bytes(extract_bytes(bytes, pos, 8))
    }

    // This function returns the 16-byte unsigned integer (i.e., u128)
    // encoded at position `pos` in byte sequence `bytes`.
    pub open spec fn parse_u128(bytes: Seq<u8>, pos: int) -> u128
    {
        spec_u128_from_le_bytes(extract_bytes(bytes, pos, 16))
    }

    // This function returns the global metadata encoded as the given
    // bytes `bytes`.
    pub open spec fn parse_global_metadata(bytes: Seq<u8>) -> GlobalMetadata
    {
        let program_guid = parse_u128(bytes, RELATIVE_POS_OF_GLOBAL_PROGRAM_GUID as int);
        let version_number = parse_u64(bytes, RELATIVE_POS_OF_GLOBAL_VERSION_NUMBER as int);
        let length_of_region_metadata = parse_u64(bytes, RELATIVE_POS_OF_GLOBAL_LENGTH_OF_REGION_METADATA as int);
        GlobalMetadata { program_guid, version_number, length_of_region_metadata }
    }

    // This function returns the region metadata encoded as the given
    // bytes `bytes`.
    pub open spec fn parse_region_metadata(bytes: Seq<u8>) -> RegionMetadata
    {
        let region_size = parse_u64(bytes, RELATIVE_POS_OF_REGION_REGION_SIZE as int);
        let log_id = parse_u128(bytes, RELATIVE_POS_OF_REGION_LOG_ID as int);
        let log_area_len = parse_u64(bytes, RELATIVE_POS_OF_REGION_LENGTH_OF_LOG_AREA as int);
        RegionMetadata { region_size, log_id, log_area_len }
    }

    // This function returns the log metadata encoded as the given
    // bytes `bytes`.
    pub open spec fn parse_log_metadata(bytes: Seq<u8>) -> LogMetadata
    {
        let head = parse_u128(bytes, RELATIVE_POS_OF_LOG_HEAD as int);
        let log_length = parse_u64(bytes, RELATIVE_POS_OF_LOG_LOG_LENGTH as int);
        LogMetadata { head, _padding: 0, log_length }
    }

    /// Specification functions for extracting log data from a
    /// persistent-memory region.

    // This function converts a virtual log position (given relative
    // to the virtual log's head) to a memory location (given relative
    // to the beginning of the log area in memory).
    //
    // `pos_relative_to_head` -- the position in the virtual log being
    // asked about, expressed as the number of positions past the
    // virtual head (e.g., if the head is 3 and this is 7, it
    // means position 10 in the virtual log).
    //
    // `head_log_area_offset` -- the offset from the location in the
    // log area in memory containing the head position of the virtual
    // log (e.g., if this is 3, that means the log's head byte is at
    // address ABSOLUTE_POS_OF_LOG_AREA + 3 in the persistent memory
    // region)
    //
    // `log_area_len` -- the length of the log area in memory
    pub open spec fn relative_log_pos_to_log_area_offset(
        pos_relative_to_head: int,
        head_log_area_offset: int,
        log_area_len: int
    ) -> int
    {
        let log_area_offset = head_log_area_offset + pos_relative_to_head;
        if log_area_offset >= log_area_len {
            log_area_offset - log_area_len
        }
        else {
            log_area_offset
        }
    }

    pub open spec fn log_area_offset_to_relative_log_pos(
        log_area_offset: int,
        head_log_area_offset: int,
        log_area_len: int
    ) -> int
    {
        if log_area_offset >= head_log_area_offset {
            log_area_offset - head_log_area_offset
        }
        else {
            log_area_offset - head_log_area_offset + log_area_len
        }
    }

    // This function extracts the virtual log from the contents of a
    // persistent-memory region.
    //
    // `mem` -- the contents of the persistent-memory region
    //
    // `log_area_len` -- the size of the log area in that region
    //
    // `head` -- the virtual log position of the head
    //
    // `log_length` -- the current length of the virtual log past the
    // head
    pub open spec fn extract_log_from_log_area(log_area: Seq<u8>, head: int, log_length: int) -> Seq<u8>
    {
        let head_log_area_offset = head % (log_area.len() as int);
        Seq::<u8>::new(log_length as nat, |pos_relative_to_head: int|
                       log_area[relative_log_pos_to_log_area_offset(pos_relative_to_head, head_log_area_offset,
                                                                    log_area.len() as int)])
    }

    /// Specification functions for recovering data and metadata from
    /// persistent memory after a crash

    // This function specifies how recovery should treat the contents
    // of the log area in the persistent-memory region as an abstract
    // log state. It only deals with data; it assumes the metadata has
    // already been recovered. Relevant aspects of that metadata are
    // passed in as parameters.
    //
    // `log_area` -- the contents of the log area
    //
    // `head` -- the virtual log position of the head
    //
    // `log_length` -- the current length of the virtual log past the
    // head
    //
    // Returns an `Option<AbstractLogState>` with the following
    // meaning:
    //
    // `None` -- the given metadata isn't valid
    // `Some(s)` -- `s` is the abstract state represented in memory
    pub open spec fn recover_log_from_log_area_given_metadata(
        log_area: Seq<u8>,
        head: int,
        log_length: int,
    ) -> Option<AbstractLogState>
    {
        if log_length > log_area.len() || head + log_length > u128::MAX
        {
            None
        }
        else {
            Some(AbstractLogState {
                head,
                log: extract_log_from_log_area(log_area, head, log_length),
                pending: Seq::<u8>::empty(),
                capacity: log_area.len() as int
            })
        }
    }

    // This function specifies how recovery should treat the contents
    // of the persistent-memory region as an abstract log state. It
    // only deals with data; it assumes the metadata has already been
    // recovered. Relevant aspects of that metadata are passed in as
    // parameters.
    //
    // `mem` -- the contents of the persistent-memory region
    //
    // `log_area_len` -- the size of the log area in that region
    //
    // `head` -- the virtual log position of the head
    //
    // `log_length` -- the current length of the virtual log past the
    // head
    //
    // Returns an `Option<AbstractLogState>` with the following
    // meaning:
    //
    // `None` -- the given metadata isn't valid
    // `Some(s)` -- `s` is the abstract state represented in memory
    pub open spec fn recover_log(
        mem: Seq<u8>,
        log_area_len: int,
        head: int,
        log_length: int,
    ) -> Option<AbstractLogState>
    {
        recover_log_from_log_area_given_metadata(
            extract_bytes(mem, ABSOLUTE_POS_OF_LOG_AREA as int, log_area_len), head, log_length
        )
    }

    // This function specifies how recovery should treat the contents
    // of the persistent-memory region as an abstract log state.
    // It assumes the corruption-detecting boolean has already been
    // read and is given by `cdb`.
    //
    // `mem` -- the contents of the persistent-memory region
    //
    // `log_id` -- the GUID associated with the log when it
    // was initialized
    //
    // `cdb` -- what value the corruption-detecting boolean has,
    // according to the metadata in region 0
    //
    // Returns an `Option<AbstractLogState>` with the following
    // meaning:
    //
    // `None` -- the metadata on persistent memory isn't consistent
    // with it having been used as a multilog with the given
    // parameters
    //
    // `Some(s)` -- `s` is the abstract state represented in memory
    pub open spec fn recover_given_cdb(
        mem: Seq<u8>,
        log_id: u128,
        cdb: bool
    ) -> Option<AbstractLogState>
    {
        if mem.len() < ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE {
            // To be valid, the memory's length has to be big enough to store at least
            // `MIN_LOG_AREA_SIZE` in the log area.
            None
        }
        else {
            let global_metadata = deserialize_global_metadata(mem);
            let global_crc = deserialize_global_crc(mem);
            if global_crc != global_metadata.spec_crc() {
                // To be valid, the global metadata CRC has to be a valid CRC of the global metadata
                // encoded as bytes.
                None
            }
            else {
                if global_metadata.program_guid != LOG_PROGRAM_GUID {
                    // To be valid, the global metadata has to refer to this program's GUID.
                    // Otherwise, it wasn't created by this program.
                    None
                }
                else if global_metadata.version_number == 1 {
                    // If this metadata was written by version #1 of this code, then this is how to
                    // interpret it:

                    if global_metadata.length_of_region_metadata != RegionMetadata::spec_size_of() {
                        // To be valid, the global metadata's encoding of the region metadata's
                        // length has to be what we expect. (This version of the code doesn't
                        // support any other length of region metadata.)
                        None
                    }
                    else {
                        let region_metadata = deserialize_region_metadata(mem);
                        let region_crc = deserialize_region_crc(mem);
                        if region_crc != region_metadata.spec_crc() {
                            // To be valid, the region metadata CRC has to be a valid CRC of the region
                            // metadata encoded as bytes.
                            None
                        }
                        else {
                            // To be valid, the region metadata's region size has to match the size of the
                            // region given to us. Also, its metadata has to match what we expect
                            // from the list of regions given to us. Finally, there has to be
                            // sufficient room for the log area.
                            if {
                                ||| region_metadata.region_size != mem.len()
                                ||| region_metadata.log_id != log_id
                                ||| region_metadata.log_area_len < MIN_LOG_AREA_SIZE
                                ||| mem.len() < ABSOLUTE_POS_OF_LOG_AREA + region_metadata.log_area_len
                            } {
                                None
                            }
                            else {
                                let log_metadata = deserialize_log_metadata(mem, cdb);
                                let log_crc = deserialize_log_crc(mem, cdb);
                                if log_crc != log_metadata.spec_crc() {
                                    // To be valid, the log metadata CRC has to be a valid CRC of the
                                    // log metadata encoded as bytes. (This only applies to the
                                    // "active" log metadata, i.e., the log metadata
                                    // corresponding to the current CDB.)
                                    None
                                }
                                else {
                                    recover_log(mem, region_metadata.log_area_len as int, log_metadata.head as int,
                                                log_metadata.log_length as int)
                                }
                            }
                        }
                    }
                }
                else {
                    // This version of the code doesn't know how to parse metadata for any other
                    // versions of this code besides 1. If we reach this point, we're presumably
                    // reading metadata written by a future version of this code, which we can't
                    // interpret.
                    None
                }
            }
        }
    }

    // This function specifies how recovery should recover the
    // corruption-detecting boolean. The input `mem` is the contents
    // of the persistent memory region.
    //
    // Returns an `Option<bool>` with the following meaning:
    //
    // `None` -- the metadata on this region isn't consistent
    // with it having been used as a log
    //
    // `Some(cdb)` -- `cdb` is the corruption-detecting boolean
    pub open spec fn recover_cdb(mem: Seq<u8>) -> Option<bool>
    {
        if mem.len() < ABSOLUTE_POS_OF_REGION_METADATA {
            // If there isn't space in memory to store the global metadata
            // and CRC, then this region clearly isn't a valid log region.
            None
        }
        else {
            let global_metadata = deserialize_global_metadata(mem);
            let global_crc = deserialize_global_crc(mem);
            if global_crc != global_metadata.spec_crc() {
                // To be valid, the global metadata CRC has to be a valid CRC of the global metadata
                // encoded as bytes.
                None
            }
            else {
                if global_metadata.program_guid != LOG_PROGRAM_GUID {
                    // To be valid, the global metadata has to refer to this program's GUID.
                    // Otherwise, it wasn't created by this program.
                    None
                }
                else if global_metadata.version_number == 1 {
                    // If this metadata was written by version #1 of this code, then this is how to
                    // interpret it:

                    if mem.len() < ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of() {
                        // If memory isn't big enough to store the CDB, then this region isn't
                        // valid.
                        None
                    }
                    else {
                        // Extract and parse the log metadata CDB
                        deserialize_and_check_log_cdb(mem)
                    }
                }
                else {
                    // This version of the code doesn't know how to parse metadata for any other
                    // versions of this code besides 1. If we reach this point, we're presumably
                    // reading metadata written by a future version of this code, which we can't
                    // interpret.
                    None
                }
            }
        }
    }

    // This function specifies how recovery should treat the contents
    // of a persistent-memory region as an abstract log state.
    //
    // `mem` -- the contents of the persistent memory region
    //
    // `log_id` -- the GUID associated with the log when it
    // was initialized
    //
    // Returns an `Option<AbstractLogState>` with the following
    // meaning:
    //
    // `None` -- the metadata on persistent memory isn't consistent
    // with it having been used as a log with the given log ID
    //
    // `Some(s)` -- `s` is the abstract state represented in memory
    pub open spec fn recover_state(mem: Seq<u8>, log_id: u128) -> Option<AbstractLogState>
    {
        // To recover, first recover the CDB, then use it to recover the abstract state.
        match recover_cdb(mem) {
            Some(cdb) => recover_given_cdb(mem, log_id, cdb),
            None => None
        }
    }

    /// Useful utility proofs about layout that other files use.

    // This lemma establishes that if a persistent memory region view
    // `pm_region_view` has no outstanding writes, and if its committed byte
    // sequence recovers to abstract state `state`, then any state
    // `pm_region_view` can crash into also recovers that same abstract state.
    pub proof fn lemma_if_no_outstanding_writes_then_can_only_crash_as_state(
        pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        state: AbstractLogState,
    )
        requires
            pm_region_view.no_outstanding_writes(),
            recover_state(pm_region_view.committed(), log_id) == Some(state),
        ensures
            forall |s| #[trigger] pm_region_view.can_crash_as(s) ==> recover_state(s, log_id) == Some(state)
    {
        // This follows trivially from the observation that the only
        // byte sequence `pm_region_view` can crash into is its committed byte
        // sequence. (It has no outstanding writes, so there's nothing
        // else it could crash into.)
        lemma_if_no_outstanding_writes_then_persistent_memory_view_can_only_crash_as_committed(pm_region_view);
    }

    // This lemma establishes that if a persistent memory region's
    // contents `mem` can successfully be recovered from, then it has
    // size large enough to hold at least `MIN_LOG_AREA_SIZE` bytes in
    // its log area.
    pub proof fn lemma_recover_state_successful_implies_region_size_sufficient(mem: Seq<u8>, log_id: u128)
        requires
            recover_state(mem, log_id).is_Some()
        ensures
            mem.len() >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
    {
        let cdb = recover_cdb(mem).get_Some_0();
        let recovered_mem = recover_given_cdb(mem, log_id, cdb);
        assert(recovered_mem.is_Some());
    }

    // This lemma establishes that for any `i` and `n`, if
    //
    // `forall |k| 0 <= k < n ==> mem1[i+k] == mem2[i+k]`
    //
    // holds, then
    //
    // `extract_bytes(mem1, i, n) == mem2.extract_bytes(mem2, i, n)`
    //
    // also holds.
    //
    // This is an obvious fact, so the body of the lemma is
    // empty. Nevertheless, the lemma is useful because it establishes
    // a trigger. Specifically, it hints Z3 that whenever Z3 is
    // thinking about two terms `extract_bytes(mem1, i, n)` and
    // `extract_bytes(mem2, i, n)` where `mem1` and `mem2` are the
    // specific memory byte sequences passed to this lemma, Z3 should
    // also think about this lemma's conclusion. That is, it should
    // try to prove that
    //
    // `forall |k| 0 <= k < n ==> mem1[i+k] == mem2[i+k]`
    //
    // and, whenever it can prove that, conclude that
    //
    // `extract_bytes(mem1, i, n) == mem2.extract_bytes(mem2, i, n)`
    pub proof fn lemma_establish_extract_bytes_equivalence(
        mem1: Seq<u8>,
        mem2: Seq<u8>,
    )
        ensures
            forall |i: int, n: int| extract_bytes(mem1, i, n) =~= extract_bytes(mem2, i, n) ==>
                #[trigger] extract_bytes(mem1, i, n) == #[trigger] extract_bytes(mem2, i, n)
    {
    }

    pub proof fn lemma_same_bytes_same_deserialization<S>(mem1: Seq<u8>, mem2: Seq<u8>)
        where
            S: PmCopy + Sized
        ensures
            forall |i: int, n: int| extract_bytes(mem1, i, n) =~= extract_bytes(mem2, i, n) ==>
                S::spec_from_bytes(#[trigger] extract_bytes(mem1, i, n)) == S::spec_from_bytes(#[trigger] extract_bytes(mem2, i, n))
    {}

    // This lemma establishes that if the given persistent memory
    // region's contents can be recovered to a valid abstract state,
    // then that abstract state is unaffected by
    // `drop_pending_appends`.
    pub proof fn lemma_recovered_state_is_crash_idempotent(mem: Seq<u8>, log_id: u128)
        requires
            recover_state(mem, log_id).is_Some()
        ensures
            ({
                let state = recover_state(mem, log_id).unwrap();
                state == state.drop_pending_appends()
            })
    {
        let state = recover_state(mem, log_id).unwrap();
        assert(state.pending.len() == 0);
        assert(state =~= state.drop_pending_appends());
    }

    pub proof fn lemma_if_only_differences_in_memory_are_inactive_metadata_then_recover_state_matches(
        mem1: Seq<u8>,
        mem2: Seq<u8>,
        log_id: u128,
        cdb: bool,
    )
        requires
            mem1.len() == mem2.len() >= ABSOLUTE_POS_OF_LOG_AREA,
            recover_cdb(mem1) == Some(cdb),
            metadata_types_set(mem1),
            ({
                let unused_metadata_start = if cdb { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE }
                                            else { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE };
                let unused_metadata_end = unused_metadata_start + LogMetadata::spec_size_of() + u64::spec_size_of();
                forall |addr: int| 0 <= addr < mem1.len() && !(unused_metadata_start <= addr < unused_metadata_end)
                    ==> mem1[addr] == mem2[addr]
            }),
        ensures
            recover_cdb(mem2) == Some(cdb),
            recover_state(mem1, log_id) == recover_state(mem2, log_id),
            metadata_types_set(mem2),
    {
        lemma_establish_extract_bytes_equivalence(mem1, mem2);
        assert(recover_state(mem1, log_id) =~= recover_state(mem2, log_id));

        assert(mem1.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) == 
            mem2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));
        if cdb {
            assert(mem1.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of() + u64::spec_size_of()) == 
                mem2.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of() + u64::spec_size_of()));
        } else {
            assert(mem1.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of() + u64::spec_size_of()) == 
                mem2.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of() + u64::spec_size_of()));
        }
        assert(active_metadata_bytes_are_equal(mem1, mem2));
        lemma_active_metadata_bytes_equal_implies_metadata_types_set(mem1, mem2, cdb);
    }
}

================
File: ./storage_node/src/log/append_v.rs
================

//! This file contains lemmas about tentatively appending to a log.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::log::inv_v::*;
use crate::log::layout_v::*;
use crate::log::logimpl_v::LogInfo;
use crate::log::logspec_t::AbstractLogState;
use crate::pmem::pmemspec_t::PersistentMemoryRegionView;
use crate::pmem::subregion_v::*;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {
    // This lemma establishes useful facts about performing a
    // contiguous write to effect a tentative append:
    //
    // 1) The write is permitted because, for each address written to,
    //    there's no outstanding write and it's unreachable during
    //    recovery.
    //
    // 2) It maintains invariants, if `info` and `state` are updated
    //    in a certain way.
    //
    // Parameters:
    //
    // `pm_region_view` -- the view of the persistent memory region
    // before the write
    //
    // `log_id` -- the ID of the log stored on that memory
    //
    // `bytes_to_append` -- what bytes are being tentatively appended
    //
    // `cdb` -- the current corruption-detecting boolean value
    //
    // `prev_info` -- the pre-append `info` value
    //
    // `prev_state` -- the pre-append abstract state
    pub proof fn lemma_tentatively_append(
        pm_region_view: PersistentMemoryRegionView,
        bytes_to_append: Seq<u8>,
        prev_info: LogInfo,
        prev_state: AbstractLogState,
    )
        requires
            pm_region_view.len() == prev_info.log_area_len,
            info_consistent_with_log_area(pm_region_view, prev_info, prev_state),
            ({
                let log_area_len = prev_info.log_area_len;
                let num_bytes = bytes_to_append.len();
                let max_len_without_wrapping = log_area_len -
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int, log_area_len as int);
                &&& 0 < num_bytes <= max_len_without_wrapping // no wrapping is necessary
                &&& prev_info.log_plus_pending_length + num_bytes <= log_area_len
                &&& prev_info.head + prev_info.log_plus_pending_length + num_bytes <= u128::MAX
            })
        ensures
            ({
                let log_area_len = prev_info.log_area_len;
                let num_bytes = bytes_to_append.len();
                // This is how you should update `infos`
                let new_info = prev_info.tentatively_append(num_bytes as u64);
                // This is how you should update `state`
                let new_state = prev_state.tentatively_append(bytes_to_append);
                let write_addr =
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int,
                                                        log_area_len as int);
                let pm_region_view2 = pm_region_view.write(write_addr, bytes_to_append);
                &&& pm_region_view.no_outstanding_writes_in_range(write_addr, write_addr + num_bytes)
                &&& forall |log_area_offset: int| write_addr <= log_area_offset < write_addr + num_bytes ==>
                       log_area_offset_unreachable_during_recovery(prev_info.head_log_area_offset as int,
                                                                   prev_info.log_area_len as int,
                                                                   prev_info.log_length as int,
                                                                   log_area_offset)
                &&& info_consistent_with_log_area(pm_region_view2, new_info, new_state)
            }),
    {
        let new_state = prev_state.tentatively_append(bytes_to_append);

        // We need extensional equality to reason that the old and new
        // abstract states are the same after dropping pending appends.

        assert(new_state.drop_pending_appends() =~= prev_state.drop_pending_appends());

        // To prove that there are no outstanding writes in the range
        // where we plan to write, we need to reason about how
        // addresses in the log area correspond to relative log
        // positions. This is because the invariant talks about
        // relative log positions but we're trying to prove something
        // about addresses in the log area (that there are no
        // outstanding writes to certain of them).

        lemma_addresses_in_log_area_subregion_correspond_to_relative_log_positions(pm_region_view, prev_info);
    }

    // This lemma establishes useful facts about performing two
    // contiguous writes, one at the end of the log area and one at
    // the beginning, to effect a tentative append:
    //
    // 1) Each write is permitted because there are no outstanding writes
    //    to the range of addresses to write to.
    //
    // 2) The pair of writes maintains invariants, if `infos` and
    //    `state` are updated in a certain way.
    //
    // Parameters:
    //
    // `pm_region_view` -- the view of the persistent memory regions
    // before the write
    //
    // `log_id` -- the ID of the log stored on that memory
    //
    // `bytes_to_append` -- what bytes are being tentatively appended
    //
    // `cdb` -- the current corruption-detecting boolean value
    //
    // `prev_info` -- the pre-append `info` value
    //
    // `prev_state` -- the pre-append abstract state
    pub proof fn lemma_tentatively_append_wrapping(
        pm_region_view: PersistentMemoryRegionView,
        bytes_to_append: Seq<u8>,
        prev_info: LogInfo,
        prev_state: AbstractLogState,
    )
        requires
            pm_region_view.len() == prev_info.log_area_len,
            info_consistent_with_log_area(pm_region_view, prev_info, prev_state),
            ({
                let log_area_len = prev_info.log_area_len;
                let num_bytes = bytes_to_append.len();
                let max_len_without_wrapping = log_area_len -
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int, log_area_len as int);
                &&& num_bytes > max_len_without_wrapping // wrapping is required
                &&& prev_info.head + prev_info.log_plus_pending_length + num_bytes <= u128::MAX
                &&& num_bytes <= log_area_len - prev_info.log_plus_pending_length
            }),
        ensures
            ({
                let log_area_len = prev_info.log_area_len;
                let max_len_without_wrapping = log_area_len -
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int, log_area_len as int);
                let new_info = prev_info.tentatively_append(bytes_to_append.len() as u64);
                let new_state = prev_state.tentatively_append(bytes_to_append);
                let bytes_to_append_part1 = bytes_to_append.subrange(0, max_len_without_wrapping as int);
                let bytes_to_append_part2 = bytes_to_append.subrange(max_len_without_wrapping as int,
                                                                     bytes_to_append.len() as int);
                let write_addr =
                    relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                        prev_info.head_log_area_offset as int,
                                                        log_area_len as int);
                let pm_region_view2 = pm_region_view.write(write_addr, bytes_to_append_part1);
                let pm_region_view3 = pm_region_view2.write(0int, bytes_to_append_part2);
                // The first write doesn't conflict with any outstanding writes
                &&& pm_region_view.no_outstanding_writes_in_range(write_addr,
                                                                 write_addr + bytes_to_append_part1.len())
                // The first write is only to log area offsets unreachable during recovery
                &&& forall |log_area_offset: int| write_addr <= log_area_offset < write_addr + bytes_to_append_part1.len() ==>
                       log_area_offset_unreachable_during_recovery(prev_info.head_log_area_offset as int,
                                                                   prev_info.log_area_len as int,
                                                                   prev_info.log_length as int,
                                                                   log_area_offset)
                // The second write also doesn't conflict with any outstanding writes
                &&& pm_region_view2.no_outstanding_writes_in_range(0int, bytes_to_append_part2.len() as int)
                // The second write is also only to log area offsets unreachable during recovery
                &&& forall |log_area_offset: int| 0 <= log_area_offset < bytes_to_append_part2.len() ==>
                       log_area_offset_unreachable_during_recovery(prev_info.head_log_area_offset as int,
                                                                   prev_info.log_area_len as int,
                                                                   prev_info.log_length as int,
                                                                   log_area_offset)
                // After the writes, the log area will be consistent with an updated info and state.
                &&& info_consistent_with_log_area(pm_region_view3, new_info, new_state)
            }),
    {
        let log_area_len = prev_info.log_area_len;
        let max_len_without_wrapping = log_area_len -
            relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                prev_info.head_log_area_offset as int, log_area_len as int);
        let bytes_to_append_part1 = bytes_to_append.subrange(0, max_len_without_wrapping as int);
        let bytes_to_append_part2 = bytes_to_append.subrange(max_len_without_wrapping as int,
                                                             bytes_to_append.len() as int);
        let intermediate_info = LogInfo{
            log_plus_pending_length: (prev_info.log_plus_pending_length + max_len_without_wrapping) as u64,
            ..prev_info
        };
        let intermediate_state = prev_state.tentatively_append(bytes_to_append_part1);
        let new_state = prev_state.tentatively_append(bytes_to_append);
        let write_addr =
            relative_log_pos_to_log_area_offset(prev_info.log_plus_pending_length as int,
                                                prev_info.head_log_area_offset as int,
                                                log_area_len as int);
        let pm_region_view2 = pm_region_view.write(write_addr, bytes_to_append_part1);

        // Invoke `lemma_tentatively_append` on each write.

        lemma_tentatively_append(pm_region_view, bytes_to_append_part1, prev_info, prev_state);
        lemma_tentatively_append(pm_region_view2, bytes_to_append_part2, intermediate_info, intermediate_state);
    }

}

================
File: ./storage_node/src/log/start_v.rs
================

//! This file contains functions for starting to use persistent memory
//! as a log. Such starting is done either after setup or after a
//! crash.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::log::inv_v::*;
use crate::log::layout_v::*;
use crate::log::logimpl_t::LogErr;
use crate::log::logimpl_v::LogInfo;
use crate::log::logspec_t::AbstractLogState;
use crate::pmem::pmemspec_t::{PersistentMemoryRegion, CRC_SIZE, CDB_SIZE, PmemError, spec_crc_bytes};
use crate::pmem::pmemutil_v::{check_cdb, check_crc};
use crate::pmem::pmcopy_t::*;
use crate::pmem::subregion_v::*;
use crate::pmem::traits_t::size_of;
use builtin::*;
use builtin_macros::*;
use vstd::arithmetic::div_mod::*;
use vstd::bytes::*;
use vstd::prelude::*;
use vstd::slice::*;

verus! {

    // This exported function reads the corruption-detecting boolean
    // and returns it.
    //
    // `pm_region` -- the persistent-memory region to read from
    //
    // The result is a `Result<bool, LogErr>` with the following meanings:
    //
    // `Err(LogErr::CRCMismatch)` -- The CDB couldn't be read due
    // to a CRC error.
    //
    // `Ok(b)` -- The CDB could be read and represents the boolean `b`.
    pub fn read_cdb<PMRegion: PersistentMemoryRegion>(pm_region: &PMRegion) -> (result: Result<bool, LogErr>)
        requires
            pm_region.inv(),
            recover_cdb(pm_region@.committed()).is_Some(),
            pm_region@.no_outstanding_writes(),
            metadata_types_set(pm_region@.committed()),
        ensures
            match result {
                Ok(b) => Some(b) == recover_cdb(pm_region@.committed()),
                // To make sure this code doesn't spuriously generate CRC-mismatch errors,
                // it's obligated to prove that it won't generate such an error when
                // the persistent memory is impervious to corruption.
                Err(LogErr::CRCMismatch) => !pm_region.constants().impervious_to_corruption,
                Err(e) => e == LogErr::PmemErr{ err: PmemError::AccessOutOfRange },
            }
    {
        let ghost mem = pm_region@.committed();
        let ghost log_cdb_addrs = Seq::new(u64::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_LOG_CDB + i);

        let ghost true_cdb_bytes = mem.subrange(ABSOLUTE_POS_OF_LOG_CDB as int, ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of());
        let ghost true_cdb = u64::spec_from_bytes(true_cdb_bytes);
        // check_cdb does not require that the true bytes be contiguous, so we need to make Z3 confirm that the 
        // contiguous region we are using as the true value matches the address sequence we pass in.
        assert(true_cdb_bytes == Seq::new(u64::spec_size_of() as nat, |i: int| mem[log_cdb_addrs[i]]));

        let log_cdb = match pm_region.read_aligned::<u64>(ABSOLUTE_POS_OF_LOG_CDB, Ghost(true_cdb)) {
            Ok(log_cdb) => log_cdb,
            Err(e) => return Err(LogErr::PmemErr{ err: e }),
        };
        
        let result = check_cdb(log_cdb, Ghost(true_cdb), Ghost(mem),
                               Ghost(pm_region.constants().impervious_to_corruption),
                               Ghost(log_cdb_addrs));
        match result {
            Some(b) => Ok(b),
            None => Err(LogErr::CRCMismatch)
        }
    }

    // This function reads the log information for a single log from
    // persistent memory.
    //
    // `pm_region` -- the persistent memory region to read from
    //
    // `log_id` -- the GUID of the log
    //
    // `cdb` -- the corruption-detection boolean
    //
    // The result is a `Result<LogInfo, LogErr>` with the following meanings:
    //
    // `Ok(log_info)` -- The information `log_info` has been
    // successfully read.
    //
    // `Err(LogErr::CRCMismatch)` -- The region couldn't be read due
    // to a CRC error when reading data.
    //
    // `Err(LogErr::StartFailedDueToProgramVersionNumberUnsupported)`
    // -- The program version number stored in persistent memory is
    // one that this code doesn't know how to recover from. It was
    // presumably created by a later version of this code.
    //
    // `Err(LogErr::StartFailedDueToLogIDMismatch)` -- The
    // log ID stored in persistent memory doesn't match the one
    // passed to the `start` routine. So the caller of `start` gave
    // the wrong persistent memory region or the wrong ID.
    //
    // `Err(LogErr::StartFailedDueToRegionSizeMismatch)` -- The
    // region size stored in persistent memory doesn't match the size
    // of the region passed to the `start` routine. So the caller of
    // `start` is likely using a persistent memory region that starts
    // in the right place but ends in the wrong place.
    //
    // `Err(LogErr::StartFailedDueToInvalidMemoryContents)` --
    // The region's contents aren't valid, i.e., they're not
    // recoverable to a valid log. The user must have requested to
    // start using the wrong region of persistent memory.
    pub fn read_log_variables<PMRegion: PersistentMemoryRegion>(
        pm_region: &PMRegion,
        log_id: u128,
        cdb: bool,
    ) -> (result: Result<LogInfo, LogErr>)
        requires
            pm_region.inv(),
            pm_region@.no_outstanding_writes(),
            metadata_types_set(pm_region@.committed()),
            cdb == deserialize_and_check_log_cdb(pm_region@.committed()).unwrap(),
        ensures
            ({
                let state = recover_given_cdb(pm_region@.committed(), log_id, cdb);
                match result {
                    Ok(info) => state.is_Some() ==> {
                        &&& metadata_consistent_with_info(pm_region@, log_id, cdb, info)
                        &&& info_consistent_with_log_area_in_region(pm_region@, info, state.unwrap())
                    },
                    Err(LogErr::CRCMismatch) =>
                        state.is_Some() ==> !pm_region.constants().impervious_to_corruption,
                    Err(LogErr::StartFailedDueToInvalidMemoryContents) => {
                        ||| pm_region@.len() < ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
                        ||| state is None
                    },
                    Err(LogErr::StartFailedDueToProgramVersionNumberUnsupported{ version_number, max_supported}) => {
                        &&& state is None 
                        &&& version_number != max_supported
                    },
                    Err(LogErr::StartFailedDueToLogIDMismatch { log_id_expected, log_id_read }) => {
                        &&& state is None 
                        &&& log_id_expected != log_id_read
                    }
                    Err(LogErr::StartFailedDueToRegionSizeMismatch { region_size_expected, region_size_read }) => {
                        &&& state is None 
                        &&& region_size_expected != region_size_read
                    }
                    _ => false,
                }
            })
    {
        let ghost mem = pm_region@.committed();
        let ghost state = recover_given_cdb(pm_region@.committed(), log_id, cdb);

        // Check that the region is at least the minimum required size. If
        // not, indicate invalid memory contents.

        let region_size = pm_region.get_region_size();
        if region_size < ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToInvalidMemoryContents)
        }
        
        // Obtain the true global metadata and CRC. The precondition ensures that a true value exists for each.
        let ghost true_global_metadata = GlobalMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()));
        let ghost true_crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()));

        // Read the global metadata struct and CRC from PM. We still have to prove that they are not corrupted before we can use the metadata.
        let global_metadata = match pm_region.read_aligned::<GlobalMetadata>(ABSOLUTE_POS_OF_GLOBAL_METADATA, Ghost(true_global_metadata)) {
            Ok(global_metadata) => global_metadata,
            Err(e) => {
                assert(false);
                return Err(LogErr::PmemErr { err: e });
            }
        };
        let global_crc = match pm_region.read_aligned::<u64>(ABSOLUTE_POS_OF_GLOBAL_CRC, Ghost(true_crc)) {
            Ok(global_crc) => global_crc,
            Err(e) => {
                assert(false);
                return Err(LogErr::PmemErr { err: e });
            }
        };

        // Check whether the bytes we read are corrupted; if they aren't, we can safely cast the global metadata bytes to a GlobalMetadata
        // and access its fields.
        let ghost global_metadata_addrs = Seq::new(GlobalMetadata::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_GLOBAL_METADATA + i);
        let ghost crc_addrs = Seq::new(u64::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_GLOBAL_CRC + i);
        let ghost true_bytes = Seq::new(GlobalMetadata::spec_size_of()as nat, |i: int| mem[global_metadata_addrs[i] as int]);
        let ghost true_crc_bytes = Seq::new(u64::spec_size_of() as nat, |i: int| mem[crc_addrs[i] as int]);
        assert(true_global_metadata.spec_to_bytes() == true_bytes && true_crc.spec_to_bytes() == true_crc_bytes);

        if !check_crc(global_metadata.as_slice(), global_crc.as_slice(),
                      Ghost(mem), Ghost(pm_region.constants().impervious_to_corruption),
                      Ghost(global_metadata_addrs), 
                      Ghost(crc_addrs)) {
            return Err(LogErr::CRCMismatch);
        }

        let global_metadata = global_metadata.extract_init_val(
            Ghost(true_global_metadata), 
            Ghost(true_bytes), 
            Ghost(pm_region.constants().impervious_to_corruption)
        );

        // Check the global metadata for validity. If it isn't valid,
        // e.g., due to the program GUID not matching, then return an
        // error. Such invalidity can't happen if the persistent
        // memory is recoverable.

        if global_metadata.program_guid != LOG_PROGRAM_GUID {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToInvalidMemoryContents)
        }

        if global_metadata.version_number != LOG_PROGRAM_VERSION_NUMBER {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToProgramVersionNumberUnsupported{
                version_number: global_metadata.version_number,
                max_supported: LOG_PROGRAM_VERSION_NUMBER,
            })
        }

        if global_metadata.length_of_region_metadata != size_of::<RegionMetadata>() as u64 {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToInvalidMemoryContents)
        }

        // Read the region metadata and its CRC, and check that the
        // CRC matches.
        let ghost metadata_addrs = Seq::new(RegionMetadata::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_REGION_METADATA + i);
        let ghost crc_addrs = Seq::new(u64::spec_size_of() as nat, |i: int| ABSOLUTE_POS_OF_REGION_CRC + i);

        let ghost true_region_metadata = RegionMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of()));
        let ghost true_crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of()));

        let region_metadata = match pm_region.read_aligned::<RegionMetadata>(ABSOLUTE_POS_OF_REGION_METADATA, Ghost(true_region_metadata)) {
            Ok(region_metadata) => region_metadata,
            Err(e) => {
                assert(false);
                return Err(LogErr::PmemErr { err: e });
            }
        };
        let region_crc = match pm_region.read_aligned::<u64>(ABSOLUTE_POS_OF_REGION_CRC, Ghost(true_crc)) {
            Ok(region_crc) => region_crc,
            Err(e) => {
                assert(false);
                return Err(LogErr::PmemErr { err: e });
            }
        };

        let ghost true_bytes = Seq::new(metadata_addrs.len(), |i: int| mem[metadata_addrs[i] as int]);
        let ghost true_crc_bytes = Seq::new(crc_addrs.len(), |i: int| mem[crc_addrs[i] as int]);
        assert(true_region_metadata.spec_to_bytes() == true_bytes && true_crc.spec_to_bytes() == true_crc_bytes);

        if !check_crc(region_metadata.as_slice(), region_crc.as_slice(),
                      Ghost(mem), Ghost(pm_region.constants().impervious_to_corruption),
                      Ghost(metadata_addrs),
                      Ghost(crc_addrs)) {
            return Err(LogErr::CRCMismatch);
        }

        assert(true_region_metadata.spec_to_bytes() == true_bytes);
        let region_metadata = region_metadata.extract_init_val(
            Ghost(true_region_metadata),
            Ghost(true_bytes),
            Ghost(pm_region.constants().impervious_to_corruption)
        );

        // Check the region metadata for validity. If it isn't valid,
        // e.g., due to the encoded region size not matching the
        // actual region size, then return an error. Such invalidity
        // can't happen if the persistent memory is recoverable.

        if region_metadata.region_size != region_size {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToRegionSizeMismatch{
                region_size_expected: region_size,
                region_size_read: region_metadata.region_size,
            })
        }

        if region_metadata.log_id != log_id {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToLogIDMismatch{
                log_id_expected: log_id,
                log_id_read: region_metadata.log_id,
            })
        }

        if region_metadata.log_area_len > region_size {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToInvalidMemoryContents)
        }
        if region_size - region_metadata.log_area_len < ABSOLUTE_POS_OF_LOG_AREA {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToInvalidMemoryContents)
        }
        if region_metadata.log_area_len < MIN_LOG_AREA_SIZE {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToInvalidMemoryContents)
        }

        // Read the log metadata and its CRC, and check that the
        // CRC matches. The position where to find the log
        // metadata depend on the CDB.

        let log_metadata_pos = if cdb { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE }
                                    else { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE };
        let log_crc_pos = if cdb { ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE }
                                    else { ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE };
        assert(log_metadata_pos == get_log_metadata_pos(cdb));
        let subregion = PersistentMemorySubregion::new(pm_region, log_metadata_pos,
                                                       Ghost(LogMetadata::spec_size_of() + u64::spec_size_of()));
        let ghost true_log_metadata = LogMetadata::spec_from_bytes(extract_bytes(mem, log_metadata_pos as int, LogMetadata::spec_size_of()));
        let ghost true_crc = u64::spec_from_bytes(extract_bytes(mem, log_crc_pos as int, u64::spec_size_of()));
        let ghost log_metadata_addrs = Seq::new(LogMetadata::spec_size_of() as nat, |i: int| log_metadata_pos + i);
        let ghost crc_addrs = Seq::new(u64::spec_size_of() as nat, |i: int| log_crc_pos + i);
        let ghost true_bytes = Seq::new(log_metadata_addrs.len(), |i: int| mem[log_metadata_addrs[i] as int]);
        let ghost true_crc_bytes = Seq::new(crc_addrs.len(), |i: int| mem[crc_addrs[i] as int]);

        assert(pm_region@.committed().subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of()) == true_bytes);
        let log_metadata = match pm_region.read_aligned::<LogMetadata>(log_metadata_pos, Ghost(true_log_metadata)) {
            Ok(log_metadata) => log_metadata,
            Err(e) => {
                assert(false);
                return Err(LogErr::PmemErr { err: e });
            }
        };
        let log_crc = match pm_region.read_aligned::<u64>(log_crc_pos, Ghost(true_crc)) {
            Ok(log_crc) => log_crc,
            Err(e) => {
                assert(false);
                return Err(LogErr::PmemErr { err: e });
            }
        };

        assert(true_log_metadata.spec_to_bytes() == true_bytes && true_crc.spec_to_bytes() == true_crc_bytes);

        if !check_crc(log_metadata.as_slice(), log_crc.as_slice(), Ghost(mem),
                                   Ghost(pm_region.constants().impervious_to_corruption),
                                    Ghost(log_metadata_addrs), Ghost(crc_addrs)) {
            return Err(LogErr::CRCMismatch);
        }

        let log_metadata = log_metadata.extract_init_val(
            Ghost(true_log_metadata), 
            Ghost(true_bytes),
            Ghost(pm_region.constants().impervious_to_corruption)
        );

        // Check the log metadata for validity. If it isn't valid,
        // e.g., due to the log length being greater than the log area
        // length, then return an error. Such invalidity can't happen
        // if the persistent memory is recoverable.

        let head = log_metadata.head;
        let log_length = log_metadata.log_length;
        if log_length > region_metadata.log_area_len {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToInvalidMemoryContents)
        }
        if log_length as u128 > u128::MAX - head {
            assert(state.is_None()); // This can't happen if the persistent memory is recoverable
            return Err(LogErr::StartFailedDueToInvalidMemoryContents)
        }

        // Compute the offset into the log area where the head of the
        // log is. This is the u128 `head` mod the u64
        // `log_area_len`. To prove that this will fit in a `u64`, we
        // need to invoke a math lemma saying that the result of a
        // modulo operation is always less than the divisor.

        proof { lemma_mod_bound(head as int, region_metadata.log_area_len as int); }
        let head_log_area_offset: u64 = (head % region_metadata.log_area_len as u128) as u64;

        // Return the log info. This necessitates computing the
        // pending tail position relative to the head, but this is
        // easy: It's the same as the log length. This is because,
        // upon recovery, there are no pending appends beyond the tail
        // of the log.

        Ok(LogInfo{
            log_area_len: region_metadata.log_area_len,
            head,
            head_log_area_offset,
            log_length,
            log_plus_pending_length: log_length
        })
    }
}

================
File: ./storage_node/src/log/logimpl_v.rs
================

//! This file contains the implementation of `UntrustedLogImpl`,
//! which implements a provably correct log.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::log::append_v::*;
use crate::log::inv_v::*;
use crate::log::layout_v::*;
use crate::log::logimpl_t::*;
use crate::log::logspec_t::AbstractLogState;
use crate::log::setup_v::write_setup_metadata;
use crate::log::start_v::{read_cdb, read_log_variables};
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmemutil_v::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::subregion_v::*;
use crate::pmem::wrpm_t::*;
use crate::pmem::traits_t::size_of;
use builtin::*;
use builtin_macros::*;
use vstd::arithmetic::div_mod::*;
use vstd::bytes::*;
use vstd::prelude::*;
use vstd::slice::*;

verus! {

    // This structure, `LogInfo`, is used by `UntrustedLogImpl`
    // to store information about a single log. Its fields are:
    //
    // `log_area_len` -- how many bytes are in the log area on
    //     persistent memory
    //
    // `head` -- the logical position of the log's head
    //
    // `head_log_area_offset` -- the offset into the log area
    //     holding the byte at the head position. This is
    //     always equal to `head % log_area_len`, and is
    //     cached in this variable to avoid expensive modulo
    //     operations.
    //
    // `log_length` -- the number of bytes in the log beyond the head
    //
    // `log_plus_pending_length` -- the number of bytes in the log and
    //     the pending appends to the log combined
    pub struct LogInfo {
        pub log_area_len: u64,
        pub head: u128,
        pub head_log_area_offset: u64,
        pub log_length: u64,
        pub log_plus_pending_length: u64,
    }

    impl LogInfo {
        pub open spec fn tentatively_append(self, num_bytes: u64) -> Self
        {
             Self{ log_plus_pending_length: (self.log_plus_pending_length + num_bytes) as u64, ..self }
        }
    }

    // This structure, `UntrustedLogImpl`, implements a
    // log. Its fields are:
    //
    // `num_logs` -- the number of logs in the log
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract view of the log
    pub struct UntrustedLogImpl {
        cdb: bool,
        info: LogInfo,
        state: Ghost<AbstractLogState>,
    }

    impl UntrustedLogImpl
    {
        // This static function specifies how multiple regions'
        // contents should be viewed upon recovery as an abstract
        // log state.
        pub closed spec fn recover(mem: Seq<u8>, log_id: u128) -> Option<AbstractLogState>
        {
            if !metadata_types_set(mem) {
                // If the metadata types aren't properly set up, the log is unrecoverable.
                None
            } else {
                recover_state(mem, log_id)
            }
        }

        // This lemma proves that if the log can be succesfully recovered, then we must have written valid
        // values to its metadata locations.
        pub proof fn lemma_recover_successful_implies_metadata_types_set(mem: Seq<u8>, log_id: u128)
            ensures 
                Self::recover(mem, log_id) is Some ==> metadata_types_set(mem)
        {}

        // This method specifies an invariant on `self` that all
        // `UntrustedLogImpl` methods maintain. It requires this
        // invariant to hold on any method invocation, and ensures it
        // in any method invocation that takes `&mut self`.
        //
        // Most of the conjuncts in this invariant are defined in the
        // file `inv_v.rs`. See that file for detailed explanations.
        pub closed spec fn inv<Perm, PMRegion>(
            &self,
            wrpm_region: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
            log_id: u128,
        ) -> bool
            where
                Perm: CheckPermission<Seq<u8>>,
                PMRegion: PersistentMemoryRegion
        {
            &&& wrpm_region.inv() // whatever the persistent memory regions require as an invariant
            &&& no_outstanding_writes_to_metadata(wrpm_region@)
            &&& memory_matches_deserialized_cdb(wrpm_region@, self.cdb)
            &&& metadata_consistent_with_info(wrpm_region@, log_id, self.cdb, self.info)
            &&& info_consistent_with_log_area_in_region(wrpm_region@, self.info, self.state@)
            &&& can_only_crash_as_state(wrpm_region@, log_id, self.state@.drop_pending_appends())
            &&& metadata_types_set(wrpm_region@.committed())
        }

        pub proof fn lemma_inv_implies_wrpm_inv<Perm, PMRegion>(
            &self,
            wrpm_region: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
            log_id: u128
        )
            where
                Perm: CheckPermission<Seq<u8>>,
                PMRegion: PersistentMemoryRegion
            requires
                self.inv(wrpm_region, log_id)
            ensures
                wrpm_region.inv()
        {}

        pub proof fn lemma_inv_implies_can_only_crash_as<Perm, PMRegion>(
            &self,
            wrpm_region: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
            log_id: u128
        )
            where
                Perm: CheckPermission<Seq<u8>>,
                PMRegion: PersistentMemoryRegion
            requires
                self.inv(wrpm_region, log_id)
            ensures
                can_only_crash_as_state(wrpm_region@, log_id, self@.drop_pending_appends())
        {}

        // This function specifies how to view the in-memory state of
        // `self` as an abstract log state.
        pub closed spec fn view(&self) -> AbstractLogState
        {
            self.state@
        }

        // The `setup` method sets up persistent memory objects `pm_region`
        // to store an initial empty log. It returns the capacity of the log.
        // See `README.md` for more documentation.
        pub exec fn setup<PMRegion>(
            pm_region: &mut PMRegion,
            log_id: u128,
        ) -> (result: Result<u64, LogErr>)
            where
                PMRegion: PersistentMemoryRegion
            requires
                old(pm_region).inv(),
            ensures
                pm_region.inv(),
                pm_region.constants() == old(pm_region).constants(),
                pm_region@.no_outstanding_writes(),
                match result {
                    Ok(log_capacity) => {
                        let state = AbstractLogState::initialize(log_capacity as int);
                        &&& log_capacity@ <= pm_region@.len()
                        &&& pm_region@.len() == old(pm_region)@.len()
                        &&& can_only_crash_as_state(pm_region@, log_id, state)
                        &&& Self::recover(pm_region@.committed(), log_id) == Some(state)
                        &&& Self::recover(pm_region@.flush().committed(), log_id) == Some(state)
                        &&& state == state.drop_pending_appends()
                    },
                    Err(LogErr::InsufficientSpaceForSetup { required_space }) => {
                        &&& pm_region@ == old(pm_region)@.flush()
                        &&& pm_region@.len() < required_space
                    },
                    _ => false
                }
        {
            let ghost original_pm_region = pm_region@;

            // We can't write without proving that there are no
            // outstanding writes where we're writing. So just start
            // out by flushing, so it's clear we can write anywhere.
            //
            // Why can't we write without proving there isn't a
            // conflicting outstanding write, you ask? Two reasons:
            //
            // First, to simplify the specification of how persistent
            // memory behaves, in `pmem::pmemspec_t.rs` we don't specify
            // what happens when there are multiple outstanding writes
            // to the same address. Instead, we just forbid that
            // case.
            //
            // Second, even if we did specify what happened in that
            // case, in this function we have no idea what's already
            // been written. If there were outstanding writes and they
            // got reordered after our writes, the resulting state
            // might be invalid. So we need to flush before writing
            // anything anyway.

            pm_region.flush();

            // Get the list of region sizes and make sure they support
            // storing a log. If not, return an appropriate
            // error.

            let region_size = pm_region.get_region_size();
            if region_size < ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE {
                return Err(LogErr::InsufficientSpaceForSetup{
                    required_space: ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
                });
            }

            // Compute log capacities so we can return them.

            let log_capacity = region_size - ABSOLUTE_POS_OF_LOG_AREA;

            // Write setup metadata.

            write_setup_metadata(pm_region, region_size, Ghost(log_capacity), log_id);

            proof {
                // Prove various postconditions about how we can
                // crash. Specifically, (1) we can only crash as
                // `AbstractLogState::initialize(log_capacities@)`,
                // (2) if we recover after flushing then we get that
                // state, and (3) that state has no pending appends.

                let state = AbstractLogState::initialize(log_capacity as int);
                assert(state =~= state.drop_pending_appends());
                lemma_if_no_outstanding_writes_to_region_then_flush_is_idempotent(pm_region@);
                lemma_if_no_outstanding_writes_then_persistent_memory_view_can_only_crash_as_committed(
                    pm_region@);
            }

            Ok(log_capacity)
        }

        // The `start` static method creates an
        // `UntrustedLogImpl` out of a set of persistent memory
        // regions. It's assumed that those regions were initialized
        // with `setup` and then only `UntrustedLogImpl` methods
        // were allowed to mutate them. See `README.md` for more
        // documentation and an example of its use.
        //
        // This method is passed a write-restricted collection of
        // persistent memory regions `wrpm_region`. This restricts
        // how we can write `wrpm_region`. This is moot, though,
        // because we don't ever write to the memory.
        pub exec fn start<PMRegion>(
            wrpm_region: &mut WriteRestrictedPersistentMemoryRegion<TrustedPermission, PMRegion>,
            log_id: u128,
            Tracked(perm): Tracked<&TrustedPermission>,
            Ghost(state): Ghost<AbstractLogState>,
        ) -> (result: Result<Self, LogErr>)
            where
                PMRegion: PersistentMemoryRegion
            requires
                Self::recover(old(wrpm_region)@.flush().committed(), log_id) == Some(state),
                old(wrpm_region).inv(),
                forall |s| #[trigger] perm.check_permission(s) <==> Self::recover(s, log_id) == Some(state),
            ensures
                wrpm_region.inv(),
                wrpm_region.constants() == old(wrpm_region).constants(),
                match result {
                    Ok(log_impl) => {
                        &&& log_impl.inv(wrpm_region, log_id)
                        &&& log_impl@ == state
                        &&& can_only_crash_as_state(wrpm_region@, log_id, state.drop_pending_appends())
                    },
                    Err(LogErr::CRCMismatch) => !wrpm_region.constants().impervious_to_corruption,
                    Err(e) => e == LogErr::PmemErr{ err: PmemError::AccessOutOfRange },
                }
        {
            // The invariants demand that there are no outstanding
            // writes to various location. To make sure of this, we
            // flush all memory regions.

            wrpm_region.flush();

            // Out of paranoia, we check to make sure that the number
            // of regions is sensible. Both cases are technically
            // precluded by the assumptions about how `start` is
            // invoked, since it's assumed the user invokes `start` on
            // a properly set-up collection of persistent memory
            // regions. We check for them anyway in case that
            // assumption doesn't hold.

            let pm_region = wrpm_region.get_pm_region_ref();

            // First, we read the corruption-detecting boolean and
            // return an error if that fails.

            let cdb = read_cdb(pm_region)?;

            // Second, we read the log variables to store in `info`.
            // If that fails, we return an error.

            let info = read_log_variables(pm_region, log_id, cdb)?;
            proof {
                // We have to prove that we can only crash as the given abstract
                // state with all pending appends dropped. We prove this with two
                // lemmas. The first says that since we've established certain
                // invariants, we can only crash as `state`. The second says that,
                // because this is a recovered state, it's unaffected by dropping
                // all pending appends.

                lemma_invariants_imply_crash_recover_forall(pm_region@, log_id, cdb, info, state);
                lemma_recovered_state_is_crash_idempotent(wrpm_region@.committed(), log_id);

                assert(no_outstanding_writes_to_metadata(pm_region@));
                lemma_metadata_set_after_crash(pm_region@, cdb);
            }
            Ok(Self{ cdb, info, state: Ghost(state) })
        }

        // The `tentatively_append_to_log` method is called by
        // `tentatively_append` to perform writes to the log area.
        // It's passed a `subregion` that frames access to only that
        // log area, and only to offsets within that log area that are
        // unreachable during recovery.
        exec fn tentatively_append_to_log<PMRegion>(
            &self,
            wrpm_region: &mut WriteRestrictedPersistentMemoryRegion<TrustedPermission, PMRegion>,
            subregion: &WriteRestrictedPersistentMemorySubregion,
            bytes_to_append: &[u8],
            Tracked(perm): Tracked<&TrustedPermission>,
        ) -> (result: Result<u128, LogErr>)
            where
                PMRegion: PersistentMemoryRegion
            requires
                bytes_to_append.len() <= self.info.log_area_len - self.info.log_plus_pending_length,
                self.info.head + self.info.log_plus_pending_length + bytes_to_append.len() <= u128::MAX,
                subregion.inv(&*old(wrpm_region), perm),
                subregion.start() == ABSOLUTE_POS_OF_LOG_AREA,
                subregion.len() == self.info.log_area_len,
                info_consistent_with_log_area(subregion.view(&*old(wrpm_region)), self.info, self.state@),
                forall |log_area_offset: int|
                    #[trigger] subregion.is_writable_relative_addr(log_area_offset) <==>
                    log_area_offset_unreachable_during_recovery(self.info.head_log_area_offset as int,
                                                                self.info.log_area_len as int,
                                                                self.info.log_length as int,
                                                                log_area_offset),
            ensures
                subregion.inv(wrpm_region, perm),
                match result {
                    Ok(offset) => {
                        &&& offset == self.info.head + self.info.log_plus_pending_length
                        &&& info_consistent_with_log_area(
                            subregion.view(wrpm_region),
                            self.info.tentatively_append(bytes_to_append.len() as u64),
                            self.state@.tentatively_append(bytes_to_append@)
                        )
                    },
                    Err(LogErr::InsufficientSpaceForAppend { available_space }) => {
                        &&& subregion.view(wrpm_region) == subregion.view(&*old(wrpm_region))
                        &&& available_space < bytes_to_append@.len()
                        &&& {
                               ||| available_space == self@.capacity - self@.log.len() - self@.pending.len()
                               ||| available_space == u128::MAX - self@.head - self@.log.len() - self@.pending.len()
                           }
                    },
                    _ => false
                }
        {
            let info = &self.info;

            // Compute the current logical offset of the end of the
            // log, including any earlier pending appends. This is the
            // offset at which we'll be logically appending, and so is
            // the offset we're expected to return. After all, the
            // caller wants to know what virtual log position they
            // need to use to read this data in the future.

            let old_pending_tail: u128 = info.head + info.log_plus_pending_length as u128;

            // The simple case is that we're being asked to append the
            // empty string. If so, do nothing and return.

            let num_bytes: u64 = bytes_to_append.len() as u64;
            if num_bytes == 0 {
                assert(forall |a: Seq<u8>, b: Seq<u8>| b == Seq::<u8>::empty() ==> a + b == a);
                assert(bytes_to_append@ =~= Seq::<u8>::empty());
                assert(self.info.tentatively_append(bytes_to_append.len() as u64) =~= self.info);
                assert(self.state@.tentatively_append(bytes_to_append@) =~= self.state@);
                assert(info_consistent_with_log_area(
                    subregion.view(wrpm_region),
                    self.info.tentatively_append(bytes_to_append.len() as u64),
                    self.state@.tentatively_append(bytes_to_append@)
                ));
                return Ok(old_pending_tail);
            }

            let ghost state = self.state@;

            // If the number of bytes in the log plus pending appends
            // is at least as many bytes as are beyond the head in the
            // log area, there's obviously enough room to append all
            // the bytes without wrapping. So just write the bytes
            // there.

            if info.log_plus_pending_length >= info.log_area_len - info.head_log_area_offset {

                // We could compute the address to write to with:
                //
                // `write_addr = old_pending_tail % info.log_area_len;`
                //
                // But we can replace the expensive modulo operation above with two subtraction
                // operations as follows. This is somewhat subtle, but we have verification backing
                // us up and proving this optimization correct.

                let write_addr: u64 =
                    info.log_plus_pending_length - (info.log_area_len - info.head_log_area_offset);
                assert(write_addr ==
                       relative_log_pos_to_log_area_offset(info.log_plus_pending_length as int,
                                                           info.head_log_area_offset as int,
                                                           info.log_area_len as int));

                proof {
                    lemma_tentatively_append(subregion.view(wrpm_region), bytes_to_append@, self.info, self.state@);
                }
                subregion.write_relative(wrpm_region, write_addr, bytes_to_append, Tracked(perm));
            }
            else {
                // We could compute the address to write to with:
                //
                // `write_addr = old_pending_tail % info.log_area_len`
                //
                // But we can replace the expensive modulo operation above with an addition
                // operation as follows. This is somewhat subtle, but we have verification backing
                // us up and proving this optimization correct.

                let write_addr: u64 = info.log_plus_pending_length + info.head_log_area_offset;
                assert(write_addr ==
                       relative_log_pos_to_log_area_offset(info.log_plus_pending_length as int,
                                                           info.head_log_area_offset as int,
                                                           info.log_area_len as int));

                // There's limited space beyond the pending bytes in the log area, so as we write
                // the bytes we may have to wrap around the end of the log area. So we must compute
                // how many bytes we can write without having to wrap:

                let max_len_without_wrapping: u64 =
                    info.log_area_len - info.head_log_area_offset - info.log_plus_pending_length;
                assert(max_len_without_wrapping == info.log_area_len -
                       relative_log_pos_to_log_area_offset(info.log_plus_pending_length as int,
                                                           info.head_log_area_offset as int, info.log_area_len as int));

                if num_bytes <= max_len_without_wrapping {

                    // If there's room for all the bytes we need to write, we just need one write.

                    proof {
                        lemma_tentatively_append(subregion.view(wrpm_region), bytes_to_append@,
                                                 self.info, self.state@);
                    }
                    subregion.write_relative(wrpm_region, write_addr, bytes_to_append, Tracked(perm));
                }
                else {

                    // If there isn't room for all the bytes we need to write, we need two writes,
                    // one writing the first `max_len_without_wrapping` bytes to address
                    // `write_addr` and the other writing the remaining bytes to the beginning of
                    // the log area.
                    //
                    // There are a lot of things we have to prove about these writes, like the fact
                    // that they're both permitted by `perm`. We offload those proofs to a lemma in
                    // `append_v.rs` that we invoke here.

                    proof {
                        lemma_tentatively_append_wrapping(subregion.view(wrpm_region),
                                                          bytes_to_append@, self.info, self.state@);
                    }
                    subregion.write_relative(
                        wrpm_region,
                        write_addr,
                        slice_subrange(bytes_to_append, 0, max_len_without_wrapping as usize),
                        Tracked(perm)
                    );
                    subregion.write_relative(
                        wrpm_region,
                        0u64,
                        slice_subrange(bytes_to_append, max_len_without_wrapping as usize, bytes_to_append.len()),
                        Tracked(perm)
                    );
                }
            }

            Ok(old_pending_tail)
        }
        
        // The `tentatively_append` method tentatively appends
        // `bytes_to_append` to the end of the log. It's tentative in
        // that crashes will undo the appends, and reads aren't
        // allowed in the tentative part of the log. See `README.md` for
        // more documentation and examples of its use.
        //
        // This method is passed a write-restricted persistent memory
        // region `wrpm_region`. This restricts how it can write
        // `wrpm_region`. It's only given permission (in `perm`) to
        // write if it can prove that any crash after initiating the
        // write is safe. That is, any such crash must put the memory
        // in a state that recovers as the current abstract state with
        // all pending appends dropped.
        pub exec fn tentatively_append<PMRegion>(
            &mut self,
            wrpm_region: &mut WriteRestrictedPersistentMemoryRegion<TrustedPermission, PMRegion>,
            bytes_to_append: &[u8],
            Ghost(log_id): Ghost<u128>,
            Tracked(perm): Tracked<&TrustedPermission>,
        ) -> (result: Result<u128, LogErr>)
            where
                PMRegion: PersistentMemoryRegion
            requires
                old(self).inv(&*old(wrpm_region), log_id),
                forall |s| #[trigger] perm.check_permission(s) <==>
                    Self::recover(s, log_id) == Some(old(self)@.drop_pending_appends()),
            ensures
                self.inv(wrpm_region, log_id),
                wrpm_region.constants() == old(wrpm_region).constants(),
                can_only_crash_as_state(wrpm_region@, log_id, self@.drop_pending_appends()),
                match result {
                    Ok(offset) => {
                        let state = old(self)@;
                        &&& offset == state.head + state.log.len() + state.pending.len()
                        &&& self@ == old(self)@.tentatively_append(bytes_to_append@)
                    },
                    Err(LogErr::InsufficientSpaceForAppend { available_space }) => {
                        &&& self@ == old(self)@
                        &&& available_space < bytes_to_append@.len()
                        &&& {
                               ||| available_space == self@.capacity - self@.log.len() - self@.pending.len()
                               ||| available_space == u128::MAX - self@.head - self@.log.len() - self@.pending.len()
                           }
                    },
                    _ => false
                },
        {
            // One useful invariant implies that
            // `info.log_plus_pending_length <= info.log_area_len`, so
            // we know we can safely do the following subtraction
            // without underflow.

            let info = &self.info;
            let available_space: u64 = info.log_area_len - info.log_plus_pending_length as u64;

            // Check to make sure we have enough available space, and
            // return an error otherwise. There are two ways we might
            // not have available space. The first is that doing the
            // append would overfill the log area. The second (which
            // will probably never happen) is that doing this append
            // and a subsequent commit would make the logical tail
            // exceed u128::MAX.

            let num_bytes: u64 = bytes_to_append.len() as u64;
            if num_bytes > available_space {
                return Err(LogErr::InsufficientSpaceForAppend{ available_space })
            }
            if num_bytes as u128 > u128::MAX - info.log_plus_pending_length as u128 - info.head {
                return Err(LogErr::InsufficientSpaceForAppend{
                    available_space: (u128::MAX - info.log_plus_pending_length as u128 - info.head) as u64
                })
            }

            // Create a `WriteRestrictedPersistentMemorySubregion` that only provides
            // access to the log area, and that places a simpler
            // restriction on writes: one can only use it to overwrite
            // log addresses not accessed by the recovery view. That
            // is, one can only use it to overwrite parts of the log
            // beyond the current tail.

            let ghost is_writable_absolute_addr_fn =
                |addr: int| log_area_offset_unreachable_during_recovery(self.info.head_log_area_offset as int,
                                                                      self.info.log_area_len as int,
                                                                      self.info.log_length as int,
                                                                      addr - ABSOLUTE_POS_OF_LOG_AREA);
            assert forall |alt_region_view: PersistentMemoryRegionView, crash_state: Seq<u8>| {
                &&& #[trigger] alt_region_view.can_crash_as(crash_state)
                &&& wrpm_region@.len() == alt_region_view.len()
                &&& views_differ_only_where_subregion_allows(wrpm_region@, alt_region_view,
                                                           ABSOLUTE_POS_OF_LOG_AREA as int,
                                                           info.log_area_len as int,
                                                           is_writable_absolute_addr_fn)
            } implies perm.check_permission(crash_state) by {
                lemma_if_view_differs_only_in_log_area_parts_not_accessed_by_recovery_then_recover_state_matches(
                    wrpm_region@, alt_region_view, crash_state, log_id, self.cdb, self.info, self.state@,
                    is_writable_absolute_addr_fn
                );
                assert(wrpm_region@.committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_AREA as int) ==
                    alt_region_view.committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_AREA as int));
                lemma_establish_extract_bytes_equivalence(wrpm_region@.committed(), alt_region_view.committed());
                lemma_header_bytes_equal_implies_active_metadata_bytes_equal(wrpm_region@.committed(), alt_region_view.committed());
                lemma_metadata_matches_implies_metadata_types_set(wrpm_region@, alt_region_view, self.cdb);
                lemma_metadata_set_after_crash(alt_region_view, self.cdb);
            }
            let subregion = WriteRestrictedPersistentMemorySubregion::new(
                wrpm_region, Tracked(perm), ABSOLUTE_POS_OF_LOG_AREA,
                Ghost(self.info.log_area_len as int), Ghost(is_writable_absolute_addr_fn)
            );

            // Call `tentatively_append_to_log` to do the real work of this function,
            // providing it the subregion created above so it doesn't have to think
            // about anything but the log area and so it doesn't have to reason about
            // the overall recovery view to perform writes.
            let ghost old_wrpm_region = wrpm_region@;
            let result = self.tentatively_append_to_log(wrpm_region, &subregion, bytes_to_append, Tracked(perm));

            // We now update our `info` field to reflect the new
            // `log_plus_pending_length` value.

            let num_bytes: u64 = bytes_to_append.len() as u64;
            self.info.log_plus_pending_length = (self.info.log_plus_pending_length + num_bytes) as u64;

            // We update our `state` field to reflect the tentative append.

            self.state = Ghost(self.state@.tentatively_append(bytes_to_append@));

            proof {
                subregion.lemma_reveal_opaque_inv(wrpm_region, perm);
                lemma_establish_extract_bytes_equivalence(subregion.initial_region_view().committed(),
                                                          wrpm_region@.committed());
                assert(views_differ_only_where_subregion_allows(old_wrpm_region, wrpm_region@,
                ABSOLUTE_POS_OF_LOG_AREA as int, self.info.log_area_len as int, is_writable_absolute_addr_fn));
                assert(old_wrpm_region.committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_AREA as int) ==
                    wrpm_region@.committed().subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_AREA as int));
                lemma_header_bytes_equal_implies_active_metadata_bytes_equal(old_wrpm_region.committed(), wrpm_region@.committed());
                lemma_metadata_matches_implies_metadata_types_set(old_wrpm_region, wrpm_region@, self.cdb);
            }

            result
        }

        // This local helper method updates the inactive log metadata
        // on persistent memory to be consistent with `self.info` and
        // `self.state`. It's passed a subregion that gives it permission
        // to do arbitrary writes to the inactive log metadata portion
        // of the persistent memory.
        exec fn update_inactive_log_metadata<PMRegion>(
            &self,
            wrpm_region: &mut WriteRestrictedPersistentMemoryRegion<TrustedPermission, PMRegion>,
            subregion: &WriteRestrictedPersistentMemorySubregion,
            Ghost(log_id): Ghost<u128>,
            Ghost(prev_info): Ghost<LogInfo>,
            Ghost(prev_state): Ghost<AbstractLogState>,
            Tracked(perm): Tracked<&TrustedPermission>,
        )
            where
                PMRegion: PersistentMemoryRegion,
            requires
                subregion.inv(old::<&mut _>(wrpm_region), perm),
                subregion.len() == LogMetadata::spec_size_of() + u64::spec_size_of(),
                subregion.view(old::<&mut _>(wrpm_region)).no_outstanding_writes(),
                forall |addr: int| #[trigger] subregion.is_writable_absolute_addr_fn()(addr),
            ensures
                subregion.inv(wrpm_region, perm),
                ({
                    let state_after_flush = subregion.view(wrpm_region).flush().committed();
                    let log_metadata_bytes = extract_bytes(state_after_flush, 0, LogMetadata::spec_size_of());
                    let log_crc_bytes = extract_bytes(state_after_flush, LogMetadata::spec_size_of(), u64::spec_size_of());
                    let log_metadata = LogMetadata::spec_from_bytes(log_metadata_bytes);
                    let log_crc = u64::spec_from_bytes(log_crc_bytes);
                    let new_metadata = LogMetadata {
                        head: self.info.head,
                        _padding: 0,
                        log_length: self.info.log_length,
                    };
                    let new_crc = new_metadata.spec_crc();

                    &&& log_crc == log_metadata.spec_crc()
                    &&& log_metadata.head == self.info.head
                    &&& log_metadata.log_length == self.info.log_length

                    &&& log_metadata_bytes == new_metadata.spec_to_bytes()
                    &&& log_crc_bytes == new_crc.spec_to_bytes()
                }),
        {
            broadcast use pmcopy_axioms;

            // Encode the log metadata as bytes, and compute the CRC of those bytes

            let info = &self.info;
            let log_metadata = LogMetadata {
                head: info.head,
                _padding: 0,
                log_length: info.log_length
            };
            let log_crc = calculate_crc(&log_metadata);

            assert(log_metadata.spec_to_bytes().len() == LogMetadata::spec_size_of());
            assert(log_crc.spec_to_bytes().len() == u64::spec_size_of());

            // Write the new metadata to the inactive header (without the CRC)
            subregion.serialize_and_write_relative(wrpm_region, 0, &log_metadata, Tracked(perm));
            subregion.serialize_and_write_relative(wrpm_region, size_of::<LogMetadata>() as u64, &log_crc, Tracked(perm));

            // Prove that after the flush, the log metadata will be reflected in the subregion's
            // state.

            proof {
                let state_after_flush = subregion.view(wrpm_region).flush().committed();
                assert(extract_bytes(state_after_flush, 0, LogMetadata::spec_size_of() as int)
                       =~= log_metadata.spec_to_bytes());
                assert(extract_bytes(state_after_flush, LogMetadata::spec_size_of() as int, CRC_SIZE as int)
                       =~= log_crc.spec_to_bytes());
            }
        }

        // This local helper method updates the log metadata on
        // persistent memory to be consistent with `self.info` and
        // `self.state`. It does so in the following steps: (1) update
        // the log metadata corresponding to the inactive CDB; (2)
        // flush; (3) swap the CDB in region #0; (4) flush again.
        //
        // The first of these steps only writes to inactive metadata, i.e.,
        // metadata that's ignored during recovery. So even if a crash
        // happens during or immediately after this call, recovery will be
        // unaffected.
        //
        // Before calling this function, the caller should make sure that
        // `self.info` and `self.state` contain the data that the inactive
        // log metadata should reflect. But, since this function has to
        // reason about crashes, it also needs to know things about the
        // *previous* values of `self.info` and `self.state`, since those
        // are the ones that the active log metadata is consistent with
        // and will stay consistent with until we write the new CDB. These
        // previous values are passed as ghost parameters since they're
        // only needed for proving things.
        //
        // The caller of this function is responsible for making sure that
        // the contents of the log area are compatible with both the old
        // and the new `info` and `state`. However, the log area contents
        // only need to be compatible with the new `info` and `state`
        // after the next flush, since we're going to be doing a flush.
        // This weaker requirement allows a performance optimization: the
        // caller doesn't have to flush before calling this function.
        exec fn update_log_metadata<PMRegion>(
            &mut self,
            wrpm_region: &mut WriteRestrictedPersistentMemoryRegion<TrustedPermission, PMRegion>,
            Ghost(log_id): Ghost<u128>,
            Ghost(prev_info): Ghost<LogInfo>,
            Ghost(prev_state): Ghost<AbstractLogState>,
            Tracked(perm): Tracked<&TrustedPermission>,
        )
            where
                PMRegion: PersistentMemoryRegion
            requires
                old(wrpm_region).inv(),
                memory_matches_deserialized_cdb(old(wrpm_region)@, old(self).cdb),
                no_outstanding_writes_to_metadata(old(wrpm_region)@),
                metadata_consistent_with_info(old(wrpm_region)@, log_id, old(self).cdb, prev_info),
                info_consistent_with_log_area_in_region(old(wrpm_region)@.flush(), old(self).info, old(self).state@),
                info_consistent_with_log_area_in_region(old(wrpm_region)@, prev_info, prev_state),
                old(self).info.log_area_len == prev_info.log_area_len,
                forall |s| {
                          ||| Self::recover(s, log_id) == Some(prev_state.drop_pending_appends())
                          ||| Self::recover(s, log_id) == Some(old(self).state@.drop_pending_appends())
                      } ==> #[trigger] perm.check_permission(s),
                metadata_types_set(old(wrpm_region)@.committed()),
            ensures
                self.inv(wrpm_region, log_id),
                wrpm_region.constants() == old(wrpm_region).constants(),
                self.state == old(self).state,
        {
            broadcast use pmcopy_axioms;

            // Set the `unused_metadata_pos` to be the position corresponding to !self.cdb
            // since we're writing in the inactive part of the metadata.

            let ghost old_wrpm = wrpm_region@;
            let unused_metadata_pos = if self.cdb { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE }
                                      else { ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE };
            assert(unused_metadata_pos == get_log_metadata_pos(!self.cdb));

            // Update the inactive log metadata by creating a
            // subregion and invoking `update_inactive_log_metadata`.
            // The main interesting part of creating the subregion is
            // establishing a condition `condition` such that (1)
            // `condition(crash_state) ==>
            // perm.check_permission(crash_state)` and (2) `condition`
            // is preserved by updating writable addresses within the
            // subregion.

            let ghost is_writable_absolute_addr_fn = |addr: int| true;
            let ghost condition = |mem: Seq<u8>| {
                &&& mem.len() >= ABSOLUTE_POS_OF_LOG_AREA
                &&& recover_cdb(mem) == Some(self.cdb)
                &&& recover_state(mem, log_id) == Some(prev_state.drop_pending_appends())
                &&& metadata_types_set(mem)
            };
            assert forall |s1: Seq<u8>, s2: Seq<u8>| {
                &&& condition(s1)
                &&& s1.len() == s2.len() == wrpm_region@.len()
                &&& #[trigger] memories_differ_only_where_subregion_allows(s1, s2, unused_metadata_pos as int,
                    LogMetadata::spec_size_of() + u64::spec_size_of(), is_writable_absolute_addr_fn)
            } implies condition(s2) by {
                lemma_if_only_differences_in_memory_are_inactive_metadata_then_recover_state_matches(
                    s1, s2, log_id, self.cdb
                );
            }
            assert forall |crash_state: Seq<u8>| wrpm_region@.can_crash_as(crash_state) implies condition(crash_state) by {
                lemma_invariants_imply_crash_recover_forall(wrpm_region@, log_id, self.cdb, prev_info, prev_state);
            }
            let subregion = WriteRestrictedPersistentMemorySubregion::new_with_condition(
                wrpm_region, Tracked(perm), unused_metadata_pos,
                Ghost(LogMetadata::spec_size_of() + u64::spec_size_of()), Ghost(is_writable_absolute_addr_fn),
                Ghost(condition),
            );
            self.update_inactive_log_metadata(wrpm_region, &subregion, Ghost(log_id), Ghost(prev_info),
                                              Ghost(prev_state), Tracked(perm));

            // We've updated the inactive log metadata now, so it's a good time to
            // mention some relevant facts about the consequent state.
            
            proof {
                let mem1 = old_wrpm.committed();
                let mem2 = wrpm_region@.committed();
                subregion.lemma_reveal_opaque_inv(wrpm_region, perm);
                lemma_establish_extract_bytes_equivalence(mem1, mem2);
        
                assert(wrpm_region.inv());
                assert(wrpm_region.constants() == old(wrpm_region).constants());
                assert(unused_metadata_pos == get_log_metadata_pos(!self.cdb));
                assert(memory_matches_deserialized_cdb(wrpm_region@, self.cdb));
                assert(metadata_consistent_with_info(wrpm_region@, log_id, self.cdb, prev_info));
                assert(info_consistent_with_log_area_in_region(wrpm_region@, prev_info, prev_state));
                assert(info_consistent_with_log_area_in_region(wrpm_region@.flush(), self.info, self.state@));
                assert(forall |s| Self::recover(s, log_id) == Some(prev_state.drop_pending_appends()) ==>
                           #[trigger] perm.check_permission(s));
                assert(self.info.log_area_len == prev_info.log_area_len);
                assert(metadata_consistent_with_info(wrpm_region@.flush(), log_id, !self.cdb, self.info)) by {
                    let mem3 = wrpm_region@.flush().committed();
                    lemma_establish_extract_bytes_equivalence(mem1, mem3);
                    assert(extract_bytes(mem3, unused_metadata_pos as int, LogMetadata::spec_size_of())
                           =~= extract_bytes(subregion.view(wrpm_region).flush().committed(), 0,
                                            LogMetadata::spec_size_of() as int));
                    assert(extract_bytes(mem3, unused_metadata_pos + LogMetadata::spec_size_of(), u64::spec_size_of())
                           =~= extract_bytes(subregion.view(wrpm_region).flush().committed(),
                                            LogMetadata::spec_size_of(),  u64::spec_size_of()));
                }

                assert(inactive_metadata_types_set(wrpm_region@.flush().committed())) by {
                    let mem = wrpm_region@.flush().committed();
                    
                    lemma_flushing_metadata_maintains_invariants(wrpm_region@, log_id, self.cdb, prev_info, prev_state);

                    // Construct the new inactive log metadata contents to show that the types for the inactive metadata
                    // and crc are set.
                    let new_metadata = LogMetadata {
                        head: self.info.head,
                        _padding: 0,
                        log_length: self.info.log_length,
                    };
                    let new_crc = new_metadata.spec_crc();

                    let inactive_metadata_pos = get_log_metadata_pos(!self.cdb);
                    assert(extract_bytes(mem, inactive_metadata_pos as int, LogMetadata::spec_size_of()) == new_metadata.spec_to_bytes());
                    assert(extract_bytes(mem, inactive_metadata_pos + LogMetadata::spec_size_of(), u64::spec_size_of()) == new_crc.spec_to_bytes());
                }
            }

            // Prove that after the flush we're about to do, all our
            // invariants will continue to hold (using the still-unchanged
            // CDB and the old metadata, infos, and state).

            proof {
                lemma_flushing_metadata_maintains_invariants(wrpm_region@, log_id, self.cdb, prev_info, prev_state);
            }

            // Next, flush all outstanding writes to memory. This is
            // necessary so that those writes are ordered before the update
            // to the CDB.
            wrpm_region.flush();

            // Next, compute the new encoded CDB to write.
            let new_cdb = if self.cdb { CDB_FALSE } else { CDB_TRUE };
            let ghost new_cdb_bytes = new_cdb.spec_to_bytes();

            // Show that after writing and flushing, the CDB will be !self.cdb

            let ghost pm_region_after_write = wrpm_region@.write(ABSOLUTE_POS_OF_LOG_CDB as int, new_cdb_bytes);
            let ghost flushed_mem_after_write = pm_region_after_write.flush();
            assert(memory_matches_deserialized_cdb(flushed_mem_after_write, !self.cdb)) by {
                let flushed_region = pm_region_after_write.flush();
                lemma_write_reflected_after_flush_committed(wrpm_region@, ABSOLUTE_POS_OF_LOG_CDB as int,
                                                            new_cdb_bytes);
            }

            // Show that after writing and flushing, our invariants will
            // hold for each log if we flip `self.cdb`.

            let ghost pm_region_after_flush = pm_region_after_write.flush();
            assert ({
                &&& metadata_consistent_with_info(pm_region_after_flush, log_id, !self.cdb, self.info)
                &&& info_consistent_with_log_area_in_region(pm_region_after_flush, self.info, self.state@)
                &&& metadata_types_set(pm_region_after_flush.committed())
            }) by {
                lemma_establish_extract_bytes_equivalence(wrpm_region@.committed(),
                                                          pm_region_after_flush.committed());

                lemma_metadata_consistent_with_info_after_cdb_update(
                    wrpm_region@,
                    pm_region_after_flush,
                    log_id,
                    new_cdb_bytes,
                    !self.cdb,
                    self.info
                );
                lemma_metadata_types_set_after_cdb_update(
                    wrpm_region@,
                    pm_region_after_flush,
                    log_id,
                    new_cdb_bytes,
                    self.cdb
                )
            }
            assert(memory_matches_deserialized_cdb(pm_region_after_flush, !self.cdb));

            // Show that if we crash after the write and flush, we recover
            // to an abstract state corresponding to `self.state@` after
            // dropping pending appends.

            proof {
                lemma_invariants_imply_crash_recover_forall(pm_region_after_flush, log_id,
                                                            !self.cdb, self.info, self.state@);
            }

            // Show that if we crash after initiating the write of the CDB,
            // we'll recover to a permissible state. There are two cases:
            //
            // If we crash without any updating, then we'll recover to
            // state `prev_state.drop_pending_appends()` with the current
            // CDB.
            //
            // If we crash after writing, then we'll recover to state
            // `self.state@.drop_pending_appends()` with the flipped CDB.
            //
            // Because we're only writing within the persistence
            // granularity of the persistent memory, a crash in the middle
            // will either leave the persistent memory in the pre-state or
            // the post-state.
            //
            // This means we're allowed to do the write because if we
            // crash, we'll either be in state wrpm_region@.committed() or
            // pm_region_after_write.flush().committed(). In the former
            // case, we'll be in state `prev_state.drop_pending_appends()`
            // and in the latter case, as shown above, we'll be in state
            // `self.state@.drop_pending_appends()`.

            assert forall |crash_bytes| pm_region_after_write.can_crash_as(crash_bytes) implies
                       #[trigger] perm.check_permission(crash_bytes) by {
                lemma_invariants_imply_crash_recover_forall(wrpm_region@, log_id,
                                                            self.cdb, prev_info, prev_state);
                lemma_single_write_crash_effect_on_pm_region_view(wrpm_region@, ABSOLUTE_POS_OF_LOG_CDB as int,
                                                                  new_cdb_bytes);
                if crash_bytes == wrpm_region@.committed() {
                    assert(wrpm_region@.can_crash_as(crash_bytes));
                }
                else {
                    assert(pm_region_after_flush.can_crash_as(crash_bytes));
                }
            }

            // Finally, update the CDB, then flush, then flip `self.cdb`.
            // There's no need to flip `self.cdb` atomically with the write
            // since the flip of `self.cdb` is happening in local
            // non-persistent memory so if we crash it'll be lost anyway.
            // wrpm_region.write(0, ABSOLUTE_POS_OF_LOG_CDB, new_cdb.as_slice(), Tracked(perm));
            wrpm_region.serialize_and_write(ABSOLUTE_POS_OF_LOG_CDB, &new_cdb, Tracked(perm));
            wrpm_region.flush();
            self.cdb = !self.cdb;
        }

        // The `commit` method commits all tentative appends that have been
        // performed since the last one. See `README.md` for more
        // documentation and examples of its use.
        //
        // This method is passed a write-restricted persistent memory
        // region `wrpm_region`. This restricts how it can write
        // `wrpm_region`. It's only given permission (in `perm`) to
        // write if it can prove that any crash after initiating the
        // write is safe. That is, any such crash must put the memory
        // in a state that recovers as either (1) the current abstract
        // state with all pending appends dropped, or (2) the abstract
        // state after all pending appends are committed.
        pub exec fn commit<PMRegion>(
            &mut self,
            wrpm_region: &mut WriteRestrictedPersistentMemoryRegion<TrustedPermission, PMRegion>,
            Ghost(log_id): Ghost<u128>,
            Tracked(perm): Tracked<&TrustedPermission>,
        ) -> (result: Result<(), LogErr>)
            where
                PMRegion: PersistentMemoryRegion
            requires
                old(self).inv(&*old(wrpm_region), log_id),
                forall |s| #[trigger] perm.check_permission(s) <==> {
                    ||| Self::recover(s, log_id) == Some(old(self)@.drop_pending_appends())
                    ||| Self::recover(s, log_id) == Some(old(self)@.commit().drop_pending_appends())
                },
            ensures
                self.inv(wrpm_region, log_id),
                wrpm_region.constants() == old(wrpm_region).constants(),
                can_only_crash_as_state(wrpm_region@, log_id, self@.drop_pending_appends()),
                result is Ok,
                self@ == old(self)@.commit(),
        {
            let ghost prev_info = self.info;
            let ghost prev_state = self.state@;

            self.state = Ghost(self.state@.commit());

            self.info.log_length = self.info.log_plus_pending_length;

            assert(memory_matches_deserialized_cdb(wrpm_region@, self.cdb));
            assert(metadata_consistent_with_info(wrpm_region@, log_id, self.cdb, prev_info));
            assert(info_consistent_with_log_area_in_region(wrpm_region@, prev_info, prev_state));
            assert(self.state@ == prev_state.commit());
            assert(info_consistent_with_log_area_in_region(wrpm_region@.flush(), self.info, self.state@));

            // Update the inactive metadata on all regions and flush, then
            // swap the CDB to its opposite.

            self.update_log_metadata(wrpm_region, Ghost(log_id), Ghost(prev_info),
                                     Ghost(prev_state), Tracked(perm));

            Ok(())
        }

        // This lemma, used by `advance_head`, gives a mathematical
        // proof that one can compute `new_head % log_area_len`
        // using only linear math operations (`+` and `-`).
        proof fn lemma_check_fast_way_to_compute_head_mod_log_area_len(
            info: LogInfo,
            state: AbstractLogState,
            new_head: u128,
        )
            requires
                info.head <= new_head,
                new_head - info.head <= info.log_length as u128,
                info.log_area_len >= MIN_LOG_AREA_SIZE,
                info.log_length <= info.log_plus_pending_length <= info.log_area_len,
                info.head_log_area_offset == info.head as int % info.log_area_len as int,
            ensures
                ({
                    let amount_of_advancement: u64 = (new_head - info.head) as u64;
                    new_head as int % info.log_area_len as int ==
                        if amount_of_advancement < info.log_area_len - info.head_log_area_offset {
                            amount_of_advancement + info.head_log_area_offset
                        }
                        else {
                            amount_of_advancement - (info.log_area_len - info.head_log_area_offset)
                        }
                }),
        {
            let amount_of_advancement: u64 = (new_head - info.head) as u64;
            let new_head_log_area_offset =
                if amount_of_advancement < info.log_area_len - info.head_log_area_offset {
                    amount_of_advancement + info.head_log_area_offset
                }
                else {
                    amount_of_advancement - (info.log_area_len - info.head_log_area_offset)
                };

            let n = info.log_area_len as int;
            let advancement = amount_of_advancement as int;
            let head = info.head as int;
            let head_mod_n = info.head_log_area_offset as int;
            let supposed_new_head_mod_n = new_head_log_area_offset as int;

            // First, observe that `advancement` plus `head` is
            // congruent modulo n to `advancement` plus `head` % n.

            assert((advancement + head) % n == (advancement + head_mod_n) % n) by {
                assert(head == n * (head / n) + head % n) by {
                    lemma_fundamental_div_mod(head, n);
                }
                assert((n * (head / n) + (advancement + head_mod_n)) % n == (advancement + head_mod_n) % n) by {
                    lemma_mod_multiples_vanish(head / n, advancement + head_mod_n, n);
                }
            }

            // Next, observe that `advancement` + `head` % n is
            // congruent modulo n to itself minus n. This is
            // relevant because there are two cases for computing
            // `new_head_mod_log_area_offset`. In one case, it's
            // computed as `advancement` + `head` % n. In the
            // other case, it's that quantity minus n.

            assert((advancement + head % n) % n == (advancement + head_mod_n - n) % n) by {
                lemma_mod_sub_multiples_vanish(advancement + head_mod_n, n);
            }

            // So we know that in either case, `new_head` % n ==
            // `new_head_mod_log_area_offset` % n.

            assert(new_head as int % n == supposed_new_head_mod_n % n);

            // But what we want to prove is that `new_head` % n ==
            // `new_head_mod_log_area_offset`. So we need to show
            // that `new_head_mod_log_area_offset` % n ==
            // `new_head_mod_log_area_offset`.  We can deduce this
            // from the fact that 0 <= `new_head_mod_log_area_offset`
            // < n.

            assert(supposed_new_head_mod_n % n == supposed_new_head_mod_n) by {
                lemma_small_mod(supposed_new_head_mod_n as nat, n as nat);
            }
        }

        // The `advance_head` method advances the head of the log,
        // thereby making more space for appending but making log
        // entries before the new head unavailable for reading. Upon
        // return from this method, the head advancement is durable,
        // i.e., it will survive crashes. See `README.md` for more
        // documentation and examples of its use.
        //
        // This method is passed a write-restricted persistent memory
        // region `wrpm_region`. This restricts how it can write
        // `wrpm_region`. It's only given permission (in `perm`) to
        // write if it can prove that any crash after initiating the
        // write is safe. That is, any such crash must put the memory
        // in a state that recovers as either (1) the current abstract
        // state with all pending appends dropped, or (2) the state
        // after advancing the head and then dropping all pending
        // appends.
        pub exec fn advance_head<PMRegion>(
            &mut self,
            wrpm_region: &mut WriteRestrictedPersistentMemoryRegion<TrustedPermission, PMRegion>,
            new_head: u128,
            Ghost(log_id): Ghost<u128>,
            Tracked(perm): Tracked<&TrustedPermission>,
        ) -> (result: Result<(), LogErr>)
            where
                PMRegion: PersistentMemoryRegion
            requires
                old(self).inv(&*old(wrpm_region), log_id),
                forall |s| #[trigger] perm.check_permission(s) <==> {
                    ||| Self::recover(s, log_id) == Some(old(self)@.drop_pending_appends())
                    ||| Self::recover(s, log_id) ==
                        Some(old(self)@.advance_head(new_head as int).drop_pending_appends())
                },
            ensures
                self.inv(wrpm_region, log_id),
                wrpm_region.constants() == old(wrpm_region).constants(),
                can_only_crash_as_state(wrpm_region@, log_id, self@.drop_pending_appends()),
                match result {
                    Ok(()) => {
                        &&& old(self)@.head <= new_head <= old(self)@.head + old(self)@.log.len()
                        &&& self@ == old(self)@.advance_head(new_head as int)
                    },
                    Err(LogErr::CantAdvanceHeadPositionBeforeHead { head }) => {
                        &&& self@ == old(self)@
                        &&& head == self@.head
                        &&& new_head < head
                    },
                    Err(LogErr::CantAdvanceHeadPositionBeyondTail { tail }) => {
                        &&& self@ == old(self)@
                        &&& tail == self@.head + self@.log.len()
                        &&& new_head > tail
                    },
                    _ => false
                }
        {
            // Even if we return an error code, we still have to prove that
            // upon return the states we can crash into recover into valid
            // abstract states.

            proof {
                lemma_invariants_imply_crash_recover_forall(wrpm_region@, log_id, self.cdb,
                                                            self.info, self.state@);
            }

            // Handle error cases due to improper parameters passed to the
            // function.
            if new_head < self.info.head {
                return Err(LogErr::CantAdvanceHeadPositionBeforeHead{ head: self.info.head })
            }
            if new_head - self.info.head > self.info.log_length as u128 {
                return Err(LogErr::CantAdvanceHeadPositionBeyondTail{
                    tail: self.info.head + self.info.log_length as u128
                })
            }

            // To compute the new head mod n (where n is the log area
            // length), take the old head mod n, add the amount by
            // which the head is advancing, then subtract n if
            // necessary.

            let amount_of_advancement: u64 = (new_head - self.info.head) as u64;
            let new_head_log_area_offset =
                if amount_of_advancement < self.info.log_area_len - self.info.head_log_area_offset {
                    amount_of_advancement + self.info.head_log_area_offset
                }
                else {
                    // To compute `self.info.head_log_area_offset` [the old
                    // head] plus `amount_of_advancement` [the amount
                    // by which the head is advancing] minus
                    // `self.info.log_area_len` [the log area length], we
                    // do it in the following order that guarantees no
                    // overflow/underflow.
                    amount_of_advancement - (self.info.log_area_len - self.info.head_log_area_offset)
                };

            assert(new_head_log_area_offset == new_head as int % self.info.log_area_len as int) by {
                Self::lemma_check_fast_way_to_compute_head_mod_log_area_len(self.info, self.state@, new_head);
            }

            // Update `self.self.info` to reflect the change to the head
            // position. This necessitates updating all the fields
            // except the log area length.

            let ghost prev_info = self.info;
            self.info.head = new_head;
            self.info.head_log_area_offset = new_head_log_area_offset;
            self.info.log_length = self.info.log_length - amount_of_advancement;
            self.info.log_plus_pending_length = self.info.log_plus_pending_length - amount_of_advancement;

            // Update the abstract `self.state` to reflect the head update.

            let ghost prev_state = self.state@;
            self.state = Ghost(self.state@.advance_head(new_head as int));

            // To prove that the log area for log number `which_log` is
            // compatible with the new `self.infos` and `self.state`, we
            // need to reason about how addresses in the log area
            // correspond to relative log positions. That's because the
            // invariants we know about the log area talk about log
            // positions relative to the old head, but we want to know
            // things about log positions relative to the new head. What
            // connects those together is that they both talk about the
            // same addresses in the log area.

            assert (info_consistent_with_log_area_in_region(wrpm_region@.flush(), self.info, self.state@)) by {
                lemma_addresses_in_log_area_correspond_to_relative_log_positions(wrpm_region@, prev_info);
            }

            // Update the inactive metadata on all regions and flush, then
            // swap the CDB to its opposite. We have to update the metadata
            // on all regions, even though we're only advancing the head on
            // one, for the following reason. The only way available to us
            // to update the active metadata is to flip the CDB, but this
            // flips which metadata is active on *all* regions. So we have
            // to update the inactive metadata on all regions.

            self.update_log_metadata(wrpm_region, Ghost(log_id), Ghost(prev_info), Ghost(prev_state),
                                     Tracked(perm));

            Ok(())
        }

        // This local helper method proves that we can read a portion of
        // the abstract log by reading a continuous range of the log area.
        // It requires that the position being read from is correct, and
        // that the read is short enough to not require wrapping around the
        // end of the log area.
        proof fn lemma_read_of_continuous_range(
            &self,
            pm_region_view: PersistentMemoryRegionView,
            log_id: u128,
            pos: int,
            len: int,
            addr: int,
        )
            requires
                len > 0,
                metadata_consistent_with_info(pm_region_view, log_id, self.cdb, self.info),
                info_consistent_with_log_area_in_region(pm_region_view, self.info, self.state@),
                ({
                    let info = self.info;
                    let max_len_without_wrapping = info.log_area_len -
                        relative_log_pos_to_log_area_offset(pos - info.head,
                                                            info.head_log_area_offset as int,
                                                            info.log_area_len as int);
                    &&& pos >= info.head
                    &&& pos + len <= info.head + info.log_length
                    &&& len <= max_len_without_wrapping
                    &&& addr == ABSOLUTE_POS_OF_LOG_AREA +
                           relative_log_pos_to_log_area_offset(pos - info.head as int,
                                                               info.head_log_area_offset as int,
                                                               info.log_area_len as int)
                })
            ensures
                ({
                    let log = self@;
                    &&& pm_region_view.no_outstanding_writes_in_range(addr, addr + len)
                    &&& pm_region_view.committed().subrange(addr, addr + len)
                           == log.log.subrange(pos - log.head, pos + len - log.head)
                })
        {
            let info = self.info;
            let s = self.state@;

            // The key to the proof is that we need to reason about how
            // addresses in the log area correspond to relative log
            // positions. This is because the invariant talks about
            // relative log positions but this lemma is proving things
            // about addresses in the log area.

            lemma_addresses_in_log_area_correspond_to_relative_log_positions(pm_region_view, info);
            assert(pm_region_view.committed().subrange(addr, addr + len) =~=
                   s.log.subrange(pos - s.head, pos + len - s.head));
        }

        // The `read` method reads part of the log, returning a vector
        // containing the read bytes. It doesn't guarantee that those
        // bytes aren't corrupted by persistent memory corruption. See
        // `README.md` for more documentation and examples of its use.
        pub exec fn read<Perm, PMRegion>(
            &self,
            wrpm_region: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
            pos: u128,
            len: u64,
            Ghost(log_id): Ghost<u128>,
        ) -> (result: Result<(Vec<u8>, Ghost<Seq<int>>), LogErr>)
            where
                Perm: CheckPermission<Seq<u8>>,
                PMRegion: PersistentMemoryRegion,
            requires
                self.inv(wrpm_region, log_id),
                pos + len <= u128::MAX
            ensures
                ({
                    let log = self@;
                    match result {
                        Ok((bytes, addrs)) => {
                            let true_bytes = self@.read(pos as int, len as int);
                            &&& pos >= log.head
                            &&& pos + len <= log.head + log.log.len()
                            &&& read_correct_modulo_corruption(bytes@, true_bytes,
                                                              wrpm_region.constants().impervious_to_corruption)
                        },
                        Err(LogErr::CantReadBeforeHead{ head: head_pos }) => {
                            &&& pos < log.head
                            &&& head_pos == log.head
                        },
                        Err(LogErr::CantReadPastTail{ tail }) => {
                            &&& pos + len > log.head + log.log.len()
                            &&& tail == log.head + log.log.len()
                        },
                        _ => false,
                    }
                })
        {
            // Handle error cases due to improper parameters passed to the
            // function.

            let info = &self.info;
            if pos < info.head {
                return Err(LogErr::CantReadBeforeHead{ head: info.head })
            }
            if len > info.log_length { // We have to do this check first to avoid underflow in the next comparison
                return Err(LogErr::CantReadPastTail{ tail: info.head + info.log_length as u128 })
            }
            if pos - info.head > (info.log_length - len) as u128 { // we know `info.log_length - len` can't underflow
                return Err(LogErr::CantReadPastTail{ tail: info.head + info.log_length as u128 })
            }

            let ghost s = self.state@;
            let ghost true_bytes = s.log.subrange(pos - s.head, pos + len - s.head);

            if len == 0 {
                // Case 0: The trivial case where we're being asked to read zero bytes.

                assert (true_bytes =~= Seq::<u8>::empty());
                assert (maybe_corrupted(Seq::<u8>::empty(), true_bytes, Seq::<int>::empty()));
                return Ok((Vec::<u8>::new(), Ghost(Seq::empty())));
            }

            let pm_region = wrpm_region.get_pm_region_ref();

            let log_area_len: u64 = info.log_area_len;
            let relative_pos: u64 = (pos - info.head) as u64;
            if relative_pos >= log_area_len - info.head_log_area_offset {

                // Case 1: The position we're being asked to read appears
                // in the log area before the log head. So the read doesn't
                // need to wrap.
                //
                // We could compute the address to write to with:
                //
                // `write_addr = ABSOLUTE_POS_OF_LOG_AREA + pos % info.log_area_len;`
                //
                // But we can replace the expensive modulo operation above with two subtraction
                // operations as follows. This is somewhat subtle, but we have verification backing
                // us up and proving this optimization correct.

                let addr = ABSOLUTE_POS_OF_LOG_AREA + relative_pos - (info.log_area_len - info.head_log_area_offset);
                proof { self.lemma_read_of_continuous_range(pm_region@, log_id, pos as int,
                                                            len as int, addr as int); }
                let bytes = match pm_region.read_unaligned(addr, len) {
                    Ok(bytes) => bytes,
                    Err(e) => {
                        assert(e == PmemError::AccessOutOfRange);
                        return Err(LogErr::PmemErr{ err: e });
                    }
                };
                return Ok((bytes, Ghost(Seq::new(len as nat, |i: int| i + addr))));
            }

            // The log area wraps past the point we're reading from, so we
            // need to compute the maximum length we can read without
            // wrapping to be able to figure out whether we need to wrap.

            let max_len_without_wrapping: u64 = log_area_len - info.head_log_area_offset - relative_pos;
            assert(max_len_without_wrapping == info.log_area_len -
                   relative_log_pos_to_log_area_offset(pos - info.head,
                                                       info.head_log_area_offset as int, info.log_area_len as int));

            // Whether we need to wrap or not, we know the address where
            // our read should start, so we can compute that and put it in
            // `addr`.
            //
            // We could compute the address to write to with:
            //
            // `write_addr = ABSOLUTE_POS_OF_LOG_AREA + pos % info.log_area_len;`
            //
            // But we can replace the expensive modulo operation above with
            // one addition operation as follows. This is somewhat subtle,
            // but we have verification backing us up and proving this
            // optimization correct.

            let addr: u64 = ABSOLUTE_POS_OF_LOG_AREA + relative_pos + info.head_log_area_offset;
            assert(addr == ABSOLUTE_POS_OF_LOG_AREA +
                   relative_log_pos_to_log_area_offset(pos - info.head,
                                                       info.head_log_area_offset as int,
                                                       info.log_area_len as int));

            if len <= max_len_without_wrapping {

                // Case 2: We're reading few enough bytes that we don't have to wrap.

                proof { self.lemma_read_of_continuous_range(pm_region@, log_id, pos as int,
                                                            len as int, addr as int); }
                let bytes = match pm_region.read_unaligned(addr, len) {
                    Ok(bytes) => bytes,
                    Err(e) => {
                        assert(e == PmemError::AccessOutOfRange);
                        return Err(LogErr::PmemErr{ err: e });
                    }
                };
                return Ok((bytes, Ghost(Seq::new(len as nat, |i: int| i + addr))));
            }

            // Case 3: We're reading enough bytes that we have to wrap.
            // That necessitates doing two contiguous reads, one from the
            // end of the log area and one from the beginning, and
            // concatenating the results.

            proof {
                self.lemma_read_of_continuous_range(pm_region@, log_id, pos as int,
                                                    max_len_without_wrapping as int, addr as int);
            }

            let mut part1 = match pm_region.read_unaligned(addr, max_len_without_wrapping) {
                Ok(part1) => part1,
                Err(e) => {
                    assert(e == PmemError::AccessOutOfRange);
                    return Err(LogErr::PmemErr{ err: e });
                }
            };

            proof {
                self.lemma_read_of_continuous_range(pm_region@, log_id,
                                                    pos + max_len_without_wrapping,
                                                    len - max_len_without_wrapping,
                                                    ABSOLUTE_POS_OF_LOG_AREA as int);
            }

            let mut part2 = match pm_region.read_unaligned(ABSOLUTE_POS_OF_LOG_AREA, len - max_len_without_wrapping) {
                Ok(part2) => part2,
                Err(e) => {
                    assert(e == PmemError::AccessOutOfRange);
                    return Err(LogErr::PmemErr{ err: e });
                }
            };

            // Now, prove that concatenating them produces the correct
            // bytes to return. The subtle thing in this argument is that
            // the bytes are only correct modulo corruption. And the
            // "correct modulo corruption" specification function talks
            // about the concrete addresses the bytes were read from and
            // demands that those addresses all be distinct.

            proof {
                let true_part1 = s.log.subrange(pos - s.head, pos + max_len_without_wrapping - s.head);
                let true_part2 = s.log.subrange(pos + max_len_without_wrapping - s.head, pos + len - s.head);
                let addrs1 = Seq::<int>::new(max_len_without_wrapping as nat, |i: int| i + addr);
                let addrs2 = Seq::<int>::new((len - max_len_without_wrapping) as nat,
                                           |i: int| i + ABSOLUTE_POS_OF_LOG_AREA);
                assert(true_part1 + true_part2 =~= s.log.subrange(pos - s.head, pos + len - s.head));

                if !pm_region.constants().impervious_to_corruption {
                    assert(maybe_corrupted(part1@ + part2@, true_part1 + true_part2, addrs1 + addrs2));
                    assert(all_elements_unique(addrs1 + addrs2));
                }
            }

            // Append the two byte vectors together and return the result.

            part1.append(&mut part2);
            let addrs = Ghost(Seq::<int>::new(max_len_without_wrapping as nat, |i: int| i + addr) + 
                Seq::<int>::new((len - max_len_without_wrapping) as nat, |i: int| i + ABSOLUTE_POS_OF_LOG_AREA));
            Ok((part1, addrs))
        }

        // The `get_head_tail_and_capacity` method returns the head,
        // tail, and capacity of the log. See `README.md` for more
        // documentation and examples of its use.
        #[allow(unused_variables)]
        pub exec fn get_head_tail_and_capacity<Perm, PMRegion>(
            &self,
            wrpm_region: &WriteRestrictedPersistentMemoryRegion<Perm, PMRegion>,
            Ghost(log_id): Ghost<u128>,
        ) -> (result: Result<(u128, u128, u64), LogErr>)
            where
                Perm: CheckPermission<Seq<u8>>,
                PMRegion: PersistentMemoryRegion
            requires
                self.inv(wrpm_region, log_id)
            ensures
                ({
                    let log = self@;
                    match result {
                        Ok((result_head, result_tail, result_capacity)) => {
                            &&& result_head == log.head
                            &&& result_tail == log.head + log.log.len()
                            &&& result_capacity == log.capacity
                        },
                        _ => false
                    }
                })
        {
            // We cache information in `self.info` that lets us easily
            // compute the return values.

            let info = &self.info;
            Ok((info.head, info.head + info.log_length as u128, info.log_area_len))
        }

    }

}

================
File: ./storage_node/src/log/logimpl_t.rs
================

//! This file contains the trusted implementation of a `LogImpl`.
//! Although the verifier is run on this file, it needs to be
//! carefully read and audited to be confident of the correctness of
//! this log implementation.
//!
//! Fortunately, it delegates most of its work to an untrusted struct
//! `UntrustedLogImpl`, which doesn't need to be read or audited. It
//! forces the `UntrustedLogImpl` to satisfy certain postconditions,
//! and also places restrictions on what `UntrustedLogImpl` can do to
//! persistent memory. These restrictions ensure that even if the
//! system or process crashes in the middle of an operation, the
//! system will still recover to a consistent state.
//!
//! It requires `UntrustedLogImpl` to implement routines that do the
//! various log operations like read and commit.
//!
//! It also requires `UntrustedLogImpl` to provide a function
//! `UntrustedLogImpl::recover`, which specifies what its `start`
//! routine will do to recover after a crash. It requires its `start`
//! routine to satisfy that specification. It also uses it to limit
//! how `UntrustedLogImpl` writes to memory: It can only perform
//! updates that, if incompletely performed before a crash, still
//! leave the system in a valid state. The `recover` function takes a
//! second parameter, the `log_id` which is passed to the start
//! routine.
//!
//! It also requires `UntrustedLogImpl` to provide a function `view`
//! that converts the current state into an abstract log. It requires
//! that performing a certain operation on the `UntrustedLogImpl`
//! causes a corresponding update to its abstract view. For instance,
//! calling the `u.commit()` method should cause the resulting
//! `u.view()` to become `old(u).view().commit()`.
//!
//! It also permits `UntrustedLogImpl` to provide a function `inv`
//! that encodes any invariants `UntrustedLogImpl` wants maintained
//! across invocations of its functions. This implementation will then
//! guarantee that `inv` holds on any call to an `UntrustedLogImpl`
//! method, and demand that the method preserve that invariant.

use std::fmt::Write;

use crate::log::logimpl_v::UntrustedLogImpl;
use crate::log::logspec_t::AbstractLogState;
use crate::pmem::pmemspec_t::*;
use crate::pmem::wrpm_t::*;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

use deps_hack::rand::Rng;

verus! {

    // This is the specification that `LogImpl` provides for data
    // bytes it reads. It says that those bytes are correct unless
    // there was corruption on the persistent memory between the last
    // write and this read.
    pub open spec fn read_correct_modulo_corruption(bytes: Seq<u8>, true_bytes: Seq<u8>,
                                                    impervious_to_corruption: bool) -> bool
    {
        if impervious_to_corruption {
            // If the region is impervious to corruption, the bytes read
            // must match the true bytes, i.e., the bytes last written.

            bytes == true_bytes
        }
        else {
            // Otherwise, there must exist a sequence of distinct
            // addresses `addrs` such that the nth byte of `bytes` is
            // a possibly corrupted version of the nth byte of
            // `true_bytes` read from the nth address in `addrs`.  We
            // don't require the sequence of addresses to be
            // contiguous because the data might not be contiguous on
            // disk (e.g., if it wrapped around the log area).

            exists |addrs: Seq<int>| {
                &&& all_elements_unique(addrs)
                &&& #[trigger] maybe_corrupted(bytes, true_bytes, addrs)
            }
        }
    }

    // This specification function indicates whether a given view of
    // memory can only crash in a way that, after recovery, leads to a
    // certain abstract state.
    pub open spec fn can_only_crash_as_state(
        pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        state: AbstractLogState,
    ) -> bool
    {
        forall |s| #[trigger] pm_region_view.can_crash_as(s) ==>
            UntrustedLogImpl::recover(s, log_id) == Some(state)
    }

    // A `TrustedPermission` is the type of a tracked object
    // indicating permission to update memory. It restricts updates so
    // that if a crash happens, the resulting memory `mem` satisfies
    // `is_state_allowable(mem)`.
    //
    // The struct is defined in this file, and it has a non-public
    // field, so the only code that can create one is in this file.
    // So untrusted code in other files can't create one, and we can
    // rely on it to restrict access to persistent memory.
    #[allow(dead_code)]
    pub struct TrustedPermission {
        ghost is_state_allowable: spec_fn(Seq<u8>) -> bool
    }

    impl CheckPermission<Seq<u8>> for TrustedPermission {
        closed spec fn check_permission(&self, state: Seq<u8>) -> bool {
            (self.is_state_allowable)(state)
        }
    }

    impl TrustedPermission {

        // This is one of two constructors for `TrustedPermission`.
        // It conveys permission to do any update as long as a
        // subsequent crash and recovery can only lead to given
        // abstract state `state`.
        proof fn new_one_possibility(log_id: u128, state: AbstractLogState) -> (tracked perm: Self)
            ensures
                forall |s| #[trigger] perm.check_permission(s) <==>
                    UntrustedLogImpl::recover(s, log_id) == Some(state)
        {
            Self {
                is_state_allowable: |s| UntrustedLogImpl::recover(s, log_id) == Some(state)
            }
        }

        // This is the second of two constructors for
        // `TrustedPermission`.  It conveys permission to do any
        // update as long as a subsequent crash and recovery can only
        // lead to one of two given abstract states `state1` and
        // `state2`.
        proof fn new_two_possibilities(
            log_id: u128,
            state1: AbstractLogState,
            state2: AbstractLogState
        ) -> (tracked perm: Self)
            ensures
                forall |s| #[trigger] perm.check_permission(s) <==> {
                    ||| UntrustedLogImpl::recover(s, log_id) == Some(state1)
                    ||| UntrustedLogImpl::recover(s, log_id) == Some(state2)
                }
        {
            Self {
                is_state_allowable: |s| {
                    ||| UntrustedLogImpl::recover(s, log_id) == Some(state1)
                    ||| UntrustedLogImpl::recover(s, log_id) == Some(state2)
                }
            }
        }
    }

    // This enumeration represents the various errors that can be
    // returned from log operations. They're self-explanatory.
    // TODO: make `PmemErr` and `LogErr` handling cleaner
    #[derive(Debug)]
    pub enum LogErr {
        InsufficientSpaceForSetup { required_space: u64 },
        StartFailedDueToLogIDMismatch { log_id_expected: u128, log_id_read: u128 },
        StartFailedDueToRegionSizeMismatch { region_size_expected: u64, region_size_read: u64 },
        StartFailedDueToProgramVersionNumberUnsupported { version_number: u64, max_supported: u64 },
        StartFailedDueToInvalidMemoryContents,
        CRCMismatch,
        InsufficientSpaceForAppend { available_space: u64 },
        CantReadBeforeHead { head: u128 },
        CantReadPastTail { tail: u128 },
        CantAdvanceHeadPositionBeforeHead { head: u128 },
        CantAdvanceHeadPositionBeyondTail { tail: u128 },
        PmemErr { err: PmemError } // janky workaround so that callers can handle PmemErrors as LogErrors
    }

    // This executable method can be called to compute a random GUID.
    // It uses the external `rand` crate.
    #[verifier::external_body]
    pub exec fn generate_fresh_log_id() -> (out: u128)
    {
        deps_hack::rand::thread_rng().gen::<u128>()
    }

    /// A `LogImpl` wraps one `UntrustedLogImpl` and one persistent
    /// memory region to provide the executable interface that turns
    /// the persistent memory region into a log.
    ///
    /// The `untrusted_log_impl` field is the wrapped
    /// `UntrustedLogImpl`.
    ///
    /// The `log_id` field is the log ID. It's ghost.
    ///
    /// The `wrpm_region` field contains the write-restricted persistent
    /// memory. This memory will only allow updates allowed by a
    /// tracked `TrustedPermission`. So we can pass `wrpm_region` to an
    /// untrusted method, along with a restricting
    /// `TrustedPermission`, to limit what it's allowed to do.

    pub struct LogImpl<PMRegion: PersistentMemoryRegion> {
        untrusted_log_impl: UntrustedLogImpl,
        log_id: Ghost<u128>,
        wrpm_region: WriteRestrictedPersistentMemoryRegion<TrustedPermission, PMRegion>
    }

    impl <PMRegion: PersistentMemoryRegion> LogImpl<PMRegion> {
        // The view of a `LogImpl` is whatever the
        // `UntrustedLogImpl` it wraps says it is.
        pub closed spec fn view(self) -> AbstractLogState
        {
            self.untrusted_log_impl@
        }

        // The constants of a `LogImpl` are whatever the
        // persistent memory it wraps says they are.
        pub closed spec fn constants(&self) -> PersistentMemoryConstants {
            self.wrpm_region.constants()
        }

        // This is the validity condition that is maintained between
        // calls to methods on `self`.
        //
        // That is, each of the trusted wrappers on untrusted methods
        // below (e.g., `commit`, `advance_head`) can count on `valid`
        // holding because it demands that each untrusted method
        // maintains it.
        //
        // One element of `valid` is that the untrusted `inv` function
        // holds.
        //
        // The other element of `valid` is that the persistent memory,
        // if it crashes and recovers, must represent the current
        // abstract state with pending tentative appends dropped.
        pub closed spec fn valid(self) -> bool {
            &&& self.untrusted_log_impl.inv(&self.wrpm_region, self.log_id@)
            &&& can_only_crash_as_state(self.wrpm_region@, self.log_id@, self@.drop_pending_appends())
        }

        // The `setup` method sets up persistent memory regions
        // `pm_region` to store an initial empty log. It returns a
        // vector listing the capacity of the log as well as a
        // fresh log ID to uniquely identify it. See `README.md`
        // for more documentation.
        pub exec fn setup(pm_region: &mut PMRegion) -> (result: Result<(u64, u128), LogErr>)
            requires
                old(pm_region).inv(),
            ensures
                pm_region.inv(),
                pm_region@.no_outstanding_writes(),
                match result {
                    Ok((log_capacity, log_id)) => {
                        let state = AbstractLogState::initialize(log_capacity as int);
                        &&& log_capacity <= pm_region@.len()
                        &&& pm_region@.len() == old(pm_region)@.len()
                        &&& can_only_crash_as_state(pm_region@, log_id, state)
                        &&& UntrustedLogImpl::recover(pm_region@.committed(), log_id) == Some(state)
                        // Required by the `start` function's precondition. Putting this in the
                        // postcond of `setup` ensures that the trusted caller doesn't have to prove it
                        &&& UntrustedLogImpl::recover(pm_region@.flush().committed(), log_id) == Some(state)
                        &&& state == state.drop_pending_appends()
                    },
                    Err(LogErr::InsufficientSpaceForSetup { required_space }) => {
                        &&& pm_region@ == old(pm_region)@.flush()
                        &&& pm_region@.len() < required_space
                    },
                    _ => false
                }
        {
            let log_id = generate_fresh_log_id();
            let capacities = UntrustedLogImpl::setup(pm_region, log_id)?;
            Ok((capacities, log_id))
        }

        // The `start` method creates an `UntrustedLogImpl` out of a
        // persistent memory region. It's assumed that the region was
        // initialized with `setup` and then only log operations were
        // allowed to mutate them. See `README.md` for more
        // documentation and an example of use.
        pub exec fn start(pm_region: PMRegion, log_id: u128) -> (result: Result<LogImpl<PMRegion>, LogErr>)
            requires
                pm_region.inv(),
                UntrustedLogImpl::recover(pm_region@.flush().committed(), log_id).is_Some(),
            ensures
                match result {
                    Ok(trusted_log_impl) => {
                        &&& trusted_log_impl.valid()
                        &&& trusted_log_impl.constants() == pm_region.constants()
                        &&& Some(trusted_log_impl@) == UntrustedLogImpl::recover(pm_region@.flush().committed(),
                                                                               log_id)
                    },
                    Err(LogErr::CRCMismatch) => !pm_region.constants().impervious_to_corruption,
                    Err(e) => e == LogErr::PmemErr{ err: PmemError::AccessOutOfRange },
                }
        {
            // We allow the untrusted `start` method to update memory
            // as part of its initialization. But, to avoid bugs
            // stemming from crashes in the middle of this routine, we
            // must restrict how it updates memory. We must only let
            // it write such that, if a crash happens in the middle,
            // it doesn't change the persistent state.

            let ghost state = UntrustedLogImpl::recover(pm_region@.flush().committed(), log_id).get_Some_0();
            let mut wrpm_region = WriteRestrictedPersistentMemoryRegion::new(pm_region);
            let tracked perm = TrustedPermission::new_one_possibility(log_id, state);
            let untrusted_log_impl =
                UntrustedLogImpl::start(&mut wrpm_region, log_id, Tracked(&perm), Ghost(state))?;
            Ok(
                LogImpl {
                    untrusted_log_impl,
                    log_id:  Ghost(log_id),
                    wrpm_region
                },
            )
        }

        // The `tentatively_append` method tentatively appends
        // `bytes_to_append` to the end of the log. It's tentative in
        // that crashes will undo the appends, and reads aren't
        // allowed in the tentative part of the log. See `README.md` for
        // more documentation and examples of use.
        pub exec fn tentatively_append(&mut self, bytes_to_append: &[u8]) -> (result: Result<u128, LogErr>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                self.constants() == old(self).constants(),
                match result {
                    Ok(offset) => {
                        let state = old(self)@;
                        &&& offset == state.head + state.log.len() + state.pending.len()
                        &&& self@ == old(self)@.tentatively_append(bytes_to_append@)
                    },
                    Err(LogErr::InsufficientSpaceForAppend { available_space }) => {
                        &&& self@ == old(self)@
                        &&& available_space < bytes_to_append@.len()
                        &&& {
                               ||| available_space == self@.capacity - self@.log.len() - self@.pending.len()
                               ||| available_space == u128::MAX - self@.head - self@.log.len() - self@.pending.len()
                           }
                    },
                    _ => false
                }
        {
            // For crash safety, we must restrict the untrusted code's
            // writes to persistent memory. We must only let it write
            // such that, if a crash happens in the middle of a write,
            // the view of the persistent state is either the current
            // state or the current state with `bytes_to_append`
            // appended.
            let tracked perm = TrustedPermission::new_one_possibility(self.log_id@, self@.drop_pending_appends());
            self.untrusted_log_impl.tentatively_append(&mut self.wrpm_region, bytes_to_append,
                                                       self.log_id, Tracked(&perm))
        }

        // The `commit` method atomically commits all tentative
        // appends that have been done to `self` since the last
        // commit. The commit is atomic in that even if there's a
        // crash in the middle, the recovered-to state either reflects
        // all those tentative appends or none of them. See `README.md`
        // for more documentation and examples of use.
        pub exec fn commit(&mut self) -> (result: Result<(), LogErr>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                self.constants() == old(self).constants(),
                match result {
                    Ok(()) => self@ == old(self)@.commit(),
                    _ => false
                }
        {
            // For crash safety, we must restrict the untrusted code's
            // writes to persistent memory. We must only let it write
            // such that, if a crash happens in the middle of a write,
            // the view of the persistent state is either the current
            // state or the current state with all uncommitted appends
            // committed.
            let tracked perm = TrustedPermission::new_two_possibilities(self.log_id@, self@.drop_pending_appends(),
                                                                        self@.commit().drop_pending_appends());
            self.untrusted_log_impl.commit(&mut self.wrpm_region, self.log_id, Tracked(&perm))
        }

        // The `advance_head` method advances the head of the log to
        // virtual new head position `new_head`. It doesn't do this
        // tentatively; it completes it durably before returning.
        // However, `advance_head` doesn't commit tentative appends;
        // to do that, you need a separate call to `commit`. See
        // `README.md` for more documentation and examples of use.
        pub exec fn advance_head(&mut self, new_head: u128) -> (result: Result<(), LogErr>)
            requires
                old(self).valid(),
            ensures
                self.valid(),
                self.constants() == old(self).constants(),
                match result {
                    Ok(()) => {
                        let state = old(self)@;
                        &&& state.head <= new_head <= state.head + state.log.len()
                        &&& self@ == old(self)@.advance_head(new_head as int)
                    },
                    Err(LogErr::CantAdvanceHeadPositionBeforeHead { head }) => {
                        &&& self@ == old(self)@
                        &&& head == self@.head
                        &&& new_head < head
                    },
                    Err(LogErr::CantAdvanceHeadPositionBeyondTail { tail }) => {
                        &&& self@ == old(self)@
                        &&& tail == self@.head + self@.log.len()
                        &&& new_head > tail
                    },
                    _ => false,
                }
        {
            // For crash safety, we must restrict the untrusted code's
            // writes to persistent memory. We must only let it write
            // such that, if a crash happens in the middle of a write,
            // the view of the persistent state is either the current
            // state or the current state with the head advanced.
            let tracked perm = TrustedPermission::new_two_possibilities(
                self.log_id@,
                self@.drop_pending_appends(),
                self@.advance_head(new_head as int).drop_pending_appends()
            );
            self.untrusted_log_impl.advance_head(&mut self.wrpm_region, new_head,
                                                 self.log_id, Tracked(&perm))
        }

        // The `read` method reads `len` bytes from the log starting
        // at virtual position `pos`. It isn't allowed to read earlier
        // than the head or past the committed tail. See `README.md` for
        // more documentation and examples of use.
        pub exec fn read(&self, pos: u128, len: u64) -> (result: Result<Vec<u8>, LogErr>)
            requires
                self.valid(),
                pos + len <= u128::MAX,
            ensures
                ({
                    let state = self@;
                    let head = state.head;
                    let log = state.log;
                    match result {
                        Ok(bytes) => {
                            let true_bytes = self@.read(pos as int, len as int);
                            &&& pos >= head
                            &&& pos + len <= head + log.len()
                            &&& read_correct_modulo_corruption(bytes@, true_bytes,
                                                             self.constants().impervious_to_corruption)
                        },
                        Err(LogErr::CantReadBeforeHead{ head: head_pos }) => {
                            &&& pos < head
                            &&& head_pos == head
                        },
                        Err(LogErr::CantReadPastTail{ tail }) => {
                            &&& pos + len > tail
                            &&& tail == head + log.len()
                        },
                        Err(e) => e == LogErr::PmemErr{ err: PmemError::AccessOutOfRange },
                    }
                })
        {
            let (bytes, addrs) = self.untrusted_log_impl.read(&self.wrpm_region, pos, len, self.log_id)?;
            Ok(bytes)
        }

        // The `get_head_tail_and_capacity` method returns three
        // pieces of metadata about the log: the virtual head
        // position, the virtual tail position, and the capacity. The
        // capacity is the maximum number of bytes there can be in the
        // log past the head, including bytes in tentative appends
        // that haven't been committed yet. See `README.md` for more
        // documentation and examples of use.
        pub exec fn get_head_tail_and_capacity(&self) -> (result: Result<(u128, u128, u64), LogErr>)
            requires
                self.valid()
            ensures
                match result {
                    Ok((result_head, result_tail, result_capacity)) => {
                        &&& result_head == self@.head
                        &&& result_tail == self@.head + self@.log.len()
                        &&& result_capacity == self@.capacity
                    },
                    _ => false
                }
        {
            self.untrusted_log_impl.get_head_tail_and_capacity(&self.wrpm_region, self.log_id)
        }
    }

}

================
File: ./storage_node/src/log/README.md
================

# Log:  A proven-correct log using persistent memory

## Overview

A `LogImpl` implements a log that can be atomically appended to.
The log is stored on a persistent memory region. The
implementation handles crash consistency, ensuring that even if
the process or machine crashes, the log acts like a log across
the crashes.

The implementation handles bit corruption on the persistent
memory, but only for the implementation's internal metadata. If
you want to be sure that *data* you wrote to the log isn't
corrupted when you read it back out, you should include your own
corruption-detecting information like a CRC. You can be confident
that the implementation read your data from the same place on
persistent memory where it was stored, but the data still might
have gotten corrupted on that memory.

## Using the library

To create a `LogImpl`, you need an object satisfying the
`PersistentMemoryRegion` trait, i.e., one implementing a
persistent-memory region. To set up a persistent memory object to
store an initial empty multilog, you call `LogImpl::setup`.
For instance, here's code to first create a
`PersistentMemoryRegion` out of volatile memory mocking
persistent memory, then to set up a log on them.

```
let region_size = 1024;
if let Ok(mut pm_region) =
    VolatileMemoryMockingPersistentMemoryRegion::new_mock_only_for_use_in_testing(region_size) {
   if let Ok(capacity, log_id) = LogImpl::setup(&mut pm_region) {
       println!("Log {} is set up with capacity {}", log_id, capacity);
   }
}
```

Remember that such setup is only intended to be run a single time.
Once you've set up a log, you shouldn't set it up again as
that will clear its state.

Once you've set up a log, you can start using it. A log is only
intended to be used by one process at a time. But if the process
or the machine crashes, it's fine to start using it again.
Indeed, that's the whole point: the most interesting verified
property of this code is that the log acts consistently like
a log even across crashes.

To start using a log, run `LogImpl::start`, passing the
persistent-memory region and the log ID, as in the following
example:

```
match LogImpl::start(pm_region, log_id) {
    Ok(mut log) => ...,
    Err(e) => ...,
}
```

To use a log, you can do five operations: tentatively append,
commit, read, advance head, and get information. Here's a
description of them all:

To tentatively append some data to the end of the log, use
`LogImpl::tentatively_append`. For example, the following
code tentatively appends [30, 42, 100] to the log:

```
let mut v: Vec<u8> = Vec::<u8>::new();
v.push(30); v.push(42); v.push(100);
if let Ok(pos) = log.tentatively_append(v.as_slice()) {
    if let Ok((head, tail, capacity)) = log.get_head_tail_and_capacity(0) {
        assert(head == 0);
        assert(tail == 0); // it's only tentative, so tail unchanged
    }
}
```

(That example also shows an example of the get-information call.)
Note that tentatively appending doesn't actually append to the
log, so the tail doesn't go up. It puts the append operation
inside of the current append transaction, which will be aborted
if the machine crashes before you commit the transaction. It will
also be aborted if the process accessing the log via a `LogImpl`
crashes. You can tentatively append to the log multiple times
before committing.

To atomically commit all tentative appends in the current append
transaction, use `LogImpl::commit`, as in the following example:

```
if let Ok(_) = log.commit() {
    if let Ok((head, tail, capacity)) = multilog.get_head_tail_and_capacity() {
        assert(head == 0);
        assert(tail == 3);
    }
}
```

This, unlike `tentatively_append`, advances logs' tails.

Commit is atomic even with respect to crashes, so it logically
does all the tentative appends at once. So even if the machine
crashes in the middle of the commit operation, or if the process
that called `commit` crashes in the middle of the commit
operation, either all tentative appends will happen or none of
them will.

If a crash occurs in the middle of a commit but the tentative
appends aren't performed, those tentative appends are dropped and
a fresh empty append transaction is started. The same happens if
the crash occurs before you call `commit`.

Once you have data committed in the log, you can read it using
`LogImpl::read`, as in the following example:

```
if let Ok((bytes)) = log.read(1, 2) {
    assert(bytes.len() == 2);
    assert(pm_region.constants().impervious_to_corruption ==> bytes[0] == 42);
}
```

Note, as discussed before, that the bytes returned might be
corruptions of the data you appended, since the implementation of
the log only checks for corruption of its own internal metadata.
If you want to use the bytes, we suggest including a CRC and
checking that CRC after any read.

If the memory storing the log is getting too full, you'll need to
advance the log's head with `LogImpl::advance_head`. This doesn't
affect the logical contents of the log or the positions of bytes
in the log, but does restrict you from reading earlier than the
new head. So, for instance, if you advance the head from 0 to
1000, you can still read byte #1042 by reading from position #1042.
You just can't read byte #42 anymore without getting an error
return value.

The advance-head operation is not tentative, so it doesn't need a
commit. By the time the operation returns, it will have
definitively happened, and that head advancement will survive a
subsequent crash.

The advance-head operation is atomic. That is, either the head
advances or it doesn't. Even if there's an intermediate crash,
the log will either be in a state where the advance-head happened
or a state where it didn't happen.

Here's an example of a call to `advance_head`:

```
if let Ok(()) = log.advance_head(0, 2) {
    if let Ok((head, tail, capacity)) = log.get_head_tail_and_capacity() {
        assert(head == 2);
        assert(tail == 3);
    }
    if let Ok((bytes)) = log.read(2, 1) {
        assert(pm_region.constants().impervious_to_corruption ==> bytes[0] == 100);
    }
    let e = multilog.read(0, 1);
    assert(e == Result::<Vec<u8>, LogErr>::Err(LogErr::CantReadBeforeHead{head: 2}));
}
```

## Code organization

The code is organized into the following files. Files ending in
`_t.rs` are trusted specifications (e.g., for the semantics of
each log operation) that must be audited and read to understand
the semantics being proven. Files ending in `_v.rs` are verified
and untrusted and so do not have to be read to have confidence in
the correctness of the code.

<!-- * `lib.rs` packages this crate as a library -->
* `logspec_t.rs` specifies the correct behavior of an abstract log,
  e.g., what should happen during a call to `tentatively_append`
* `logimpl_t.rs` implements `LogImpl`, the main type used by
  clients of this library
* `logimpl_v.rs` implements `UntrustedLogImpl`, verified for
  correctness and invoked by `LogImpl` methods
* `inv_v.rs` provides invariants of the log code and proofs about those
  invariants
* `layout_v.rs` provides constants, functions, and proofs about how the
  log implementation lays out its metadata and data in persistent memory
* `append_v.rs` provides helper lemmas used by the log code to
  prove that tentative appends are done correctly
* `setup_v.rs` implements subroutines called when the log code is
  setting up a collection of persistent memory regions to act as a multilog
* `start_v.rs` implements subroutines called when the log code is
  starting up, either immediately after setup or to recover after a crash

## Example

Here's an example of a program that uses a `LogImpl`:

```
fn test_log_on_memory_mapped_file() -> Option<()>
{
    let region_size = 1024;

    // Create the memory out of a single file.
    let file_name = vstd::string::new_strlit("test_log");
    #[cfg(target_os = "windows")]
    let mut pm_region = FileBackedPersistentMemoryRegion::new(
        &file_name, MemoryMappedFileMediaType::SSD,
        region_size,
        FileCloseBehavior::TestingSoDeleteOnClose
    ).ok()?;
    #[cfg(target_os = "linux")]
    let mut pm_region = FileBackedPersistentMemoryRegion::new(
        &file_name,
        region_size,
        PersistentMemoryCheck::DontCheckForPersistentMemory,
    ).ok()?;

    // Set up the memory region to contain a log. The capacity will be less than
    // the file size because a few bytes are needed for metadata.
    let (capacity, log_id) = LogImpl::setup(&mut pm_region).ok()?;
    runtime_assert(capacity <= 1024);

    // Start accessing the log.
    let mut log = LogImpl::start(pm_region, log_id).ok()?;

    // Tentatively append [30, 42, 100] to the log.
    let mut v: Vec<u8> = Vec::<u8>::new();
    v.push(30); v.push(42); v.push(100);
    let pos = log.tentatively_append(v.as_slice()).ok()?;
    runtime_assert(pos == 0);

    // Note that a tentative append doesn't actually advance the tail. That
    // doesn't happen until the next commit.
    let (head, tail, _capacity) = log.get_head_tail_and_capacity().ok()?;
    runtime_assert(head == 0);
    runtime_assert(tail == 0);

    // Now commit the tentative appends. This causes the log to have tail 3.
    if log.commit().is_err() {
        runtime_assert(false); // can't fail
    }
    match log.get_head_tail_and_capacity() {
        Ok((head, tail, capacity)) => {
            runtime_assert(head == 0);
            runtime_assert(tail == 3);
        },
        _ => runtime_assert(false) // can't fail
    }

    // We read the 2 bytes starting at position 1 of the log. We should
    // read bytes [42, 100]. This is only guaranteed if the memory
    // wasn't corrupted.
    if let Ok(bytes) = log.read(1, 2) {
        runtime_assert(bytes.len() == 2);
        assert(log.constants().impervious_to_corruption ==> bytes[0] == 42);
    }

    // We now advance the head of the log to position 2. This causes the
    // head to become 2 and the tail stays at 3.
    match log.advance_head(2) {
        Ok(()) => runtime_assert(true),
        _ => runtime_assert(false) // can't fail
    }
    match log.get_head_tail_and_capacity() {
        Ok((head, tail, capacity)) => {
            runtime_assert(head == 2);
            runtime_assert(tail == 3);
        },
        _ => runtime_assert(false) // can't fail
    }

    // If we read from position 2 of the log, we get the same thing we
    // would have gotten before the advance-head operation.
    if let Ok(bytes) = log.read(2, 1) {
        assert(log.constants().impervious_to_corruption ==> bytes[0] == 100);
    }

    // But if we try to read from position 0, we get an
    // error because we're not allowed to read from before the head.
    match log.read(0, 1) {
        Err(LogErr::CantReadBeforeHead{head}) => runtime_assert(head == 2),
        _ => runtime_assert(false) // can't succeed, and can't fail with any other error
    }
    Some(())
}
```

================
File: ./storage_node/src/log/setup_v.rs
================

//! This file contains functions for setting up persistent memory
//! regions for use in a multilog.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.

use crate::log::inv_v::*;
use crate::log::layout_v::*;
use crate::log::logimpl_t::LogErr;
use crate::log::logspec_t::AbstractLogState;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::pmemutil_v::*;
use crate::pmem::traits_t::size_of;
use builtin::*;
use builtin_macros::*;
use vstd::bytes::*;
use vstd::prelude::*;

verus! {

    // This function evaluates whether memory was correctly set up on
    // a region. It's a helpful specification function for use in
    // later functions in this file.
    //
    // Because answering this question depends on knowing various
    // metadata about this region and the log it's part of, the
    // function needs various input parameters for that. Its
    // parameters are:
    //
    // `mem` -- the contents of memory for this region
    // `region_size` -- how big this region is
    // `log_id` -- the GUID of the log it's being used for
    spec fn memory_correctly_set_up_on_region(
        mem: Seq<u8>,
        region_size: u64,
        log_id: u128,
    ) -> bool
    {
        let global_crc = deserialize_global_crc(mem);
        let global_metadata = deserialize_global_metadata(mem);
        let region_crc = deserialize_region_crc(mem);
        let region_metadata = deserialize_region_metadata(mem);
        let log_cdb = deserialize_and_check_log_cdb(mem);
        let log_metadata = deserialize_log_metadata(mem, false);
        let log_crc = deserialize_log_crc(mem, false);
        &&& mem.len() >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE
        &&& mem.len() == region_size
        &&& global_crc == global_metadata.spec_crc()
        &&& region_crc == region_metadata.spec_crc()
        &&& log_crc == log_metadata.spec_crc()
        &&& global_metadata.program_guid == LOG_PROGRAM_GUID
        &&& global_metadata.version_number == LOG_PROGRAM_VERSION_NUMBER
        &&& global_metadata.length_of_region_metadata == RegionMetadata::spec_size_of()
        &&& region_metadata.region_size == region_size
        &&& region_metadata.log_id == log_id
        &&& region_metadata.log_area_len == region_size - ABSOLUTE_POS_OF_LOG_AREA
        &&& log_cdb == Some(false)
        &&& log_metadata.head == 0
        &&& log_metadata.log_length == 0
    }

    // This executable function sets up a single region for use in a
    // log. To do so, it needs various metadata about this region
    // and the log it's for, so it needs some parameters:
    //
    // `region_size`: how big this region is
    // `log_id`: the GUID of the log it's being used for
    //
    // It also needs the parameter `pm_region` that gives the
    // persistent memory region for us to write to.
    //
    // The main postcondition is:
    //
    // `memory_correctly_set_up_on_single_region(pm_regions@[which_log as int].flush().committed(),
    //                                           region_size, log_id, num_logs, which_log)`
    //
    // This means that, after the next flush, the memory in this
    // region will have been set up correctly. (This function doesn't
    // do the flush, for efficiency. That way we only need one flush
    // operation to flush all regions.)
    fn write_setup_metadata_to_region<PMRegion: PersistentMemoryRegion>(
        pm_region: &mut PMRegion,
        region_size: u64,
        log_id: u128,
    )
        requires
            old(pm_region).inv(),
            old(pm_region)@.no_outstanding_writes(),
            old(pm_region)@.len() == region_size,
            region_size >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE,
        ensures
            pm_region.inv(),
            pm_region.constants() == old(pm_region).constants(),
            memory_correctly_set_up_on_region(
                pm_region@.flush().committed(), // it'll be correct after the next flush
                region_size, log_id),
            metadata_types_set(pm_region@.flush().committed()),
    {
        broadcast use pmcopy_axioms;

        // Initialize global metadata and compute its CRC
        let global_metadata = GlobalMetadata {
            program_guid: LOG_PROGRAM_GUID,
            version_number: LOG_PROGRAM_VERSION_NUMBER,
            length_of_region_metadata: size_of::<RegionMetadata>() as u64,
        };
        let global_crc = calculate_crc(&global_metadata);

        // Initialize region metadata and compute its CRC
        let region_metadata = RegionMetadata {
            region_size,
            log_id,
            log_area_len: region_size - ABSOLUTE_POS_OF_LOG_AREA,
        };
        let region_crc = calculate_crc(&region_metadata);

        // Obtain the initial CDB value
        let cdb = CDB_FALSE;

        // Initialize log metadata and compute its CRC
        let log_metadata = LogMetadata {
            head: 0,
            _padding: 0,
            log_length: 0
        };
        let log_crc = calculate_crc(&log_metadata);

        assert(pm_region@.no_outstanding_writes());
        // Write all metadata structures and their CRCs to memory
        pm_region.serialize_and_write(ABSOLUTE_POS_OF_GLOBAL_METADATA, &global_metadata);
        pm_region.serialize_and_write(ABSOLUTE_POS_OF_GLOBAL_CRC, &global_crc);
        pm_region.serialize_and_write(ABSOLUTE_POS_OF_REGION_METADATA, &region_metadata);
        pm_region.serialize_and_write(ABSOLUTE_POS_OF_REGION_CRC, &region_crc);
        pm_region.serialize_and_write(ABSOLUTE_POS_OF_LOG_CDB, &cdb);
        pm_region.serialize_and_write(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE, &log_metadata);
        pm_region.serialize_and_write(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + size_of::<LogMetadata>() as u64, &log_crc);

        proof {
            // We want to prove that if we parse the result of
            // flushing memory, we get the desired metadata.

            // Prove that if we extract pieces of the flushed memory,
            // we get the little-endian encodings of the desired
            // metadata. By using the `=~=` operator, we get Z3 to
            // prove this by reasoning about per-byte equivalence.
            let mem = pm_region@.flush().committed();
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of())
                   =~= global_metadata.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of())
                   =~= global_crc.spec_to_bytes());

            assert(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of())
                   =~= region_metadata.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of())
                   =~= region_crc.spec_to_bytes());

            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of())
                   =~= CDB_FALSE.spec_to_bytes());
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of())
                   =~= log_metadata.spec_to_bytes());
            assert (extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of())
                    =~= log_crc.spec_to_bytes());

            // Asserting these two postconditions here helps Verus finish out the proof.
            assert(memory_correctly_set_up_on_region(mem, region_size, log_id));
            assert(metadata_types_set(mem));
        }
    }

    // This exported executable function writes to persistent memory
    // all the metadata necessary to set up a log. To do so, it
    // needs some parameters:
    //
    // `region_size`: how big the region is
    //
    // `log_capacity`: what its capacity will be.
    // Note that this parameter is ghost because it's only needed to
    // establish the postcondition described below.
    //
    // `log_id`: the GUID of the log it's being used for
    //
    // It also needs the parameter `pm_region` that gives the
    // persistent memory region for us to write to.
    //
    // The main postcondition is:
    //
    // ```
    // recover_state(pm_region@.committed(), log_id) ==
    //     Some(AbstractLogState::initialize(log_capacity))
    // ```
    //
    // This means that if the recovery routine runs afterward, then
    // the resulting recovered abstract state will be the valid
    // initial value
    // `AbstractLogState::initialize(log_capacity)`.
    pub fn write_setup_metadata<PMRegion: PersistentMemoryRegion>(
        pm_region: &mut PMRegion,
        region_size: u64,
        Ghost(log_capacity): Ghost<u64>,
        log_id: u128,
    )
        requires
            old(pm_region).inv(),
            old(pm_region)@.len() == region_size,
            old(pm_region)@.len() >= ABSOLUTE_POS_OF_LOG_AREA + MIN_LOG_AREA_SIZE,
            old(pm_region)@.len() == log_capacity + ABSOLUTE_POS_OF_LOG_AREA,
            old(pm_region)@.no_outstanding_writes(),
        ensures
            pm_region.inv(),
            pm_region.constants() == old(pm_region).constants(),
            pm_region@.len() == old(pm_region)@.len(),
            pm_region@.no_outstanding_writes(),
            recover_state(pm_region@.committed(), log_id) == Some(AbstractLogState::initialize(log_capacity as int)),
            metadata_types_set(pm_region@.committed()),
    {
        write_setup_metadata_to_region(pm_region, region_size, log_id);

        proof {
            // First, establish that recovering after a flush will get
            // abstract state
            // `AbstractLogState::initialize(log_capacity)`.

            let flushed_region = pm_region@.flush();
            let pm_region_committed = flushed_region.committed();
            assert(recover_state(pm_region_committed, log_id)
                   =~= Some(AbstractLogState::initialize(log_capacity as int))) by {
                assert(pm_region_committed.len() == pm_region@.len());
                assert(pm_region_committed == pm_region@.flush().committed());
                assert(recover_log(pm_region_committed, log_capacity as int, 0int, 0int) =~=
                       Some(AbstractLogState::initialize(log_capacity as int)));
            }

            // Second, establish that the flush we're about to do
            // won't change regions' lengths.
            assert(pm_region@.len() == flushed_region.len());
        }

        pm_region.flush()
    }

}

================
File: ./storage_node/src/log/mod.rs
================

pub mod append_v;
pub mod inv_v;
pub mod layout_v;
pub mod logimpl_t;
pub mod logimpl_v;
pub mod logspec_t;
pub mod setup_v;
pub mod start_v;
================
File: ./storage_node/src/log/logspec_t.rs
================

//! This file contains the trusted specification for an abstract
//! log, which has type `AbstractLogState`.
//!
//! Although the verifier is run on this file, it needs to be
//! carefully read and audited to be confident of the correctness of
//! this specification for the multilog implementation.
//!
//! An `AbstractLogState` has the following operations:
//!
//! `initialize(capacity: u64) -> AbstractLogState`
//!
//! This static function creates an initial log with the given
//! capacities.
//!
//! `tentatively_append(self, bytes_to_append: Seq<u8>) -> Self`
//!
//! This method tentatively appends the given bytes to the end of the
//! log.
//!
//! `commit(self) -> Self`
//!
//! This method commits all outstanding tentative appends atomically.
//!
//! `read(self, pos: int, len: int) -> Seq<u8>`
//!
//! This method reads a certain number of bytes from the log at a
//! certain logical position.
//!
//! `drop_pending_appends(self) -> Self`
//!
//! This method drops all pending appends. It's not meant to be
//! explicitly invoked by clients; it's a model of what clients should
//! consider to have happened during a crash.

use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {
    
    // An `AbstractLogState` is an abstraction of a single log. Its
    // fields are:
    //
    // `head` -- the logical position of the first accessible byte
    // in the log
    //
    // `log` -- the accessible bytes in the log, logically starting
    // at position `head`
    //
    // `pending` -- the bytes tentatively appended past the end of the
    // log, which will not become part of the log unless committed
    // and which will be discarded on a crash
    //
    // `capacity` -- the maximum length of the `log` field
    #[verifier::ext_equal]
    pub struct AbstractLogState {
        pub head: int,
        pub log: Seq<u8>,
        pub pending: Seq<u8>,
        pub capacity: int,
    }

    impl AbstractLogState {

        // This is the specification for the initial state of an
        // abstract log.
        pub open spec fn initialize(capacity: int) -> Self {
            Self {
                head: 0int,
                log: Seq::<u8>::empty(),
                pending: Seq::<u8>::empty(),
                capacity: capacity
            }
        }

        // This is the specification for what it means to tentatively
        // append to a log. It appends the given bytes to the
        // `pending` field.
        pub open spec fn tentatively_append(self, bytes: Seq<u8>) -> Self {
            Self { pending: self.pending + bytes, ..self }
        }

        // This is the specification for what it means to commit a
        // log.  It adds all pending bytes to the log and clears the
        // pending bytes.
        pub open spec fn commit(self) -> Self {
            Self { log: self.log + self.pending, pending: Seq::<u8>::empty(), ..self }
        }

        // This is the specification for what it means to advance the
        // head to a given new value `new_value`.
        pub open spec fn advance_head(self, new_head: int) -> Self
        {
            let new_log = self.log.subrange(new_head - self.head, self.log.len() as int);
            Self { head: new_head, log: new_log, ..self }
        }

        // This is the specification for what it means to read `len`
        // bytes from a certain virtual position `pos` in the abstract
        // log.
        pub open spec fn read(self, pos: int, len: int) -> Seq<u8>
        {
            self.log.subrange(pos - self.head, pos - self.head + len)
        }

        // This is the specification for what it means to drop pending
        // appends. (This isn't a user-invokable operation; it's what
        // happens on a crash.)
        pub open spec fn drop_pending_appends(self) -> Self
        {
            Self { pending: Seq::<u8>::empty(), ..self }
        }
    }

}

================
File: ./storage_node/src/log/inv_v.rs
================

//! This file contains functions describing invariants of a
//! `UntrustedLogImpl`, as well as lemmas about those invariants.
//!
//! The code in this file is verified and untrusted (as indicated by
//! the `_v.rs` suffix), so you don't have to read it to be confident
//! of the system's correctness.
//!
use crate::log::layout_v::*;
use crate::log::logimpl_v::LogInfo;
use crate::log::logspec_t::AbstractLogState;
use crate::pmem::pmemspec_t::*;
use crate::pmem::pmemutil_v::*;
use crate::pmem::pmcopy_t::*;
use crate::pmem::subregion_v::*;
use builtin::*;
use builtin_macros::*;
use vstd::prelude::*;

verus! {

    // This invariant says that there are no outstanding writes to any
    // part of the metadata subregion of the persistent-memory region.
    // It's temporarily violated in the middle of various operations,
    // of course, but it's always restored before finishing an
    // operation.
    pub open spec fn no_outstanding_writes_to_metadata(
        pm_region_view: PersistentMemoryRegionView,
    ) -> bool
    {
        pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_GLOBAL_METADATA as int,
                                                      ABSOLUTE_POS_OF_LOG_AREA as int)
    }

    // This invariant is similar to no_outstanding_writes_to_metadata, except that it allows outstanding writes
    // to the inactive log metadata region.
    pub open spec fn no_outstanding_writes_to_active_metadata(
        pm_region_view: PersistentMemoryRegionView,
        cdb: bool,
    ) -> bool 
    {
        // Note that we include the active log metadata's CRC in the region
        let active_log_metadata_result = if cdb {
            pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int,
                ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of() + u64::spec_size_of())
        } else {
            pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int,
                ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of() + u64::spec_size_of())
        };
        &&& pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_GLOBAL_METADATA as int,
                                                        ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int)
        &&& active_log_metadata_result
    }

    pub open spec fn active_metadata_is_equal(
        pm_region_view1: PersistentMemoryRegionView,
        pm_region_view2: PersistentMemoryRegionView,
    ) -> bool 
    {
        let pm_bytes1 = pm_region_view1.committed();
        let pm_bytes2 = pm_region_view2.committed();
        active_metadata_bytes_are_equal(pm_bytes1, pm_bytes2)
    }

    pub open spec fn active_metadata_bytes_are_equal(
        pm_bytes1: Seq<u8>,
        pm_bytes2: Seq<u8>,
    ) -> bool {
        let cdb1 = deserialize_and_check_log_cdb(pm_bytes1);
        let cdb2 = deserialize_and_check_log_cdb(pm_bytes2);

        &&& cdb1.is_Some()
        &&& cdb2.is_Some()
        &&& cdb1 == cdb2 
        &&& pm_bytes1.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) ==
                pm_bytes2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) 
        &&& {
            if cdb1.unwrap() {
                pm_bytes1.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                    pm_bytes2.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of() + u64::spec_size_of()) 
            } else {
                pm_bytes1.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                    pm_bytes2.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of() + u64::spec_size_of()) 
            }
        }
    }

    pub open spec fn inactive_metadata_types_set(mem: Seq<u8>) -> bool 
    {
        &&& u64::bytes_parseable(mem.subrange(ABSOLUTE_POS_OF_LOG_CDB as int, ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of()))
        &&& {
            let cdb = u64::spec_from_bytes(mem.subrange(ABSOLUTE_POS_OF_LOG_CDB as int, ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of()));
            &&& cdb == CDB_TRUE || cdb == CDB_FALSE 
            &&& if cdb == CDB_TRUE {
                &&& LogMetadata::bytes_parseable(mem.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of()))
                &&& u64::bytes_parseable(mem.subrange(ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE + u64::spec_size_of()))
                &&& {
                    let crc = u64::spec_from_bytes(mem.subrange(ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE + u64::spec_size_of()));
                    let metadata = LogMetadata::spec_from_bytes(mem.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE + LogMetadata::spec_size_of()));
                    crc == spec_crc_u64(metadata.spec_to_bytes())
                }
            }
            else {
                &&& LogMetadata::bytes_parseable(mem.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of()))
                &&& u64::bytes_parseable(mem.subrange(ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE as int, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE + u64::spec_size_of()))
                &&& {
                    let crc = u64::spec_from_bytes(mem.subrange(ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE as int, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE + u64::spec_size_of()));
                    let metadata = LogMetadata::spec_from_bytes(mem.subrange(ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE + LogMetadata::spec_size_of()));
                    crc == spec_crc_u64(metadata.spec_to_bytes())
                }
            }
        }
    }

    // This invariant says that there are no outstanding writes to the
    // CDB area of persistent memory, and that the committed contents
    // of that area correspond to the given boolean `cdb`.
    pub open spec fn memory_matches_cdb(pm_region_view: PersistentMemoryRegionView, cdb: bool) -> bool
    {
        &&& pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_CDB as int,
                                                        ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of())
        &&& extract_and_parse_log_cdb(pm_region_view.committed()) == Some(cdb)
    }

    pub open spec fn memory_matches_deserialized_cdb(pm_region_view: PersistentMemoryRegionView, cdb: bool) -> bool
    {
        &&& pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_LOG_CDB as int,
                                                        ABSOLUTE_POS_OF_LOG_CDB + u64::spec_size_of())
        &&& deserialize_and_check_log_cdb(pm_region_view.committed()) == Some(cdb)
    }

    // This invariant says that there are no outstanding writes to the
    // activate metadata subregion of the persistent-memory region
    // (i.e., everything but the log area and the log metadata
    // corresponding to `!cdb`). It also says that that metadata is
    // consistent with the log information in `info` and various other
    // in-memory variables given in parameters. The parameters to this
    // function are:
    //
    // `pm_region_view` -- the current view of the persistent memory region
    //
    // `log_id` -- the GUID of the log
    //
    // `cdb` -- the current boolean value of the corruption-detection
    // boolean
    //
    // `info` -- various variables describing information about this
    // log
    pub open spec fn metadata_consistent_with_info(
        pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        cdb: bool,
        info: LogInfo,
    ) -> bool
    {
        let mem = pm_region_view.committed();
        let global_metadata = deserialize_global_metadata(mem);
        let global_crc = deserialize_global_crc(mem);
        let region_metadata = deserialize_region_metadata(mem);
        let region_crc = deserialize_region_crc(mem);
        let log_metadata = deserialize_log_metadata(mem, cdb);
        let log_crc = deserialize_log_crc(mem, cdb);

        // No outstanding writes to global metadata, region metadata, or the log metadata CDB
        &&& pm_region_view.no_outstanding_writes_in_range(ABSOLUTE_POS_OF_GLOBAL_METADATA as int,
                                                        ABSOLUTE_POS_OF_LOG_CDB as int)
        // Also, no outstanding writes to the log metadata corresponding to the active log metadata CDB
        &&& pm_region_view.no_outstanding_writes_in_range(get_log_metadata_pos(cdb) as int,
                                                        get_log_crc_end(cdb) as int)

        // All the CRCs match
        &&& global_crc == global_metadata.spec_crc()
        &&& region_crc == region_metadata.spec_crc()
        &&& log_crc == log_metadata.spec_crc()

        // Various fields are valid and match the parameters to this function
        &&& global_metadata.program_guid == LOG_PROGRAM_GUID
        &&& global_metadata.version_number == LOG_PROGRAM_VERSION_NUMBER
        &&& global_metadata.length_of_region_metadata == RegionMetadata::spec_size_of()
        &&& region_metadata.region_size == mem.len()
        &&& region_metadata.log_id == log_id
        &&& region_metadata.log_area_len == info.log_area_len
        &&& log_metadata.head == info.head
        &&& log_metadata.log_length == info.log_length

        // The memory region is large enough to hold the entirety of the log area
        &&& mem.len() >= ABSOLUTE_POS_OF_LOG_AREA + info.log_area_len
    }

    // This lemma proves that, if all regions are consistent wrt a new CDB, and then we
    // write and flush that CDB, the regions stay consistent with info.
    pub proof fn lemma_metadata_consistent_with_info_after_cdb_update(
        old_pm_region_view: PersistentMemoryRegionView,
        new_pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        new_cdb_bytes: Seq<u8>,
        new_cdb: bool,
        info: LogInfo,
    )
        requires
            new_cdb == false ==> new_cdb_bytes == CDB_FALSE.spec_to_bytes(),
            new_cdb == true ==> new_cdb_bytes == CDB_TRUE.spec_to_bytes(),
            new_cdb_bytes.len() == u64::spec_size_of(),
            old_pm_region_view.no_outstanding_writes(),
            new_pm_region_view.no_outstanding_writes(),
            new_pm_region_view =~= old_pm_region_view.write(ABSOLUTE_POS_OF_LOG_CDB as int, new_cdb_bytes).flush(),
            metadata_consistent_with_info(old_pm_region_view, log_id, new_cdb, info),
        ensures
            metadata_consistent_with_info(new_pm_region_view, log_id, new_cdb, info),
    {
        assert(metadata_consistent_with_info(new_pm_region_view, log_id, new_cdb, info)) by {
            let old_mem = old_pm_region_view.committed();
            let new_mem = new_pm_region_view.committed();
            lemma_establish_extract_bytes_equivalence(old_mem, new_mem);
        }
    }

    // This invariant says that the log area of the given
    // persistent-memory region view is consistent with both the log
    // information `info` and the abstract log state `state`. Also,
    // `info` satisfies certain invariant properties and is consistent
    // with `state`.
    //
    // This means three things for every relative log position
    // `pos` and its corresponding persistent-memory byte `pmb`:
    //
    // 1) If `0 <= pos < log_length`, then `pmb` has no outstanding
    // writes and its committed content is the byte in the abstract
    // log at position `pos`. This is critical so that recovery will
    // recover to the right abstract log.
    //
    // 2) If `log_length <= pos < log_plus_pending_length`, then `pmb`
    // may or may not have outstanding writes. But when/if it gets
    // flushed, its content will be the byte in the abstract pending
    // appends at position `pos - log_length`. This is useful so that,
    // when a commit is requested, a flush is all that's needed to
    // durably write the pending appends.
    //
    // 3) If `log_plus_pending_length <= pos < log_area_len`, then
    // `pmb` has no outstanding writes because it's past the pending
    // tail. This is useful so that, if there are further pending
    // appends, they can be written into this part of the log area.
    pub open spec fn info_consistent_with_log_area(
        log_area_view: PersistentMemoryRegionView,
        info: LogInfo,
        state: AbstractLogState,
    ) -> bool
    {
        // `info` satisfies certain invariant properties
        &&& info.log_area_len >= MIN_LOG_AREA_SIZE
        &&& info.log_length <= info.log_plus_pending_length <= info.log_area_len
        &&& info.head_log_area_offset == info.head as int % info.log_area_len as int
        &&& info.head + info.log_plus_pending_length <= u128::MAX

        // `info` and `state` are consistent with each other
        &&& state.log.len() == info.log_length
        &&& state.pending.len() == info.log_plus_pending_length - info.log_length
        &&& state.head == info.head
        &&& state.capacity == info.log_area_len

        // The log area is consistent with `info` and `state`
        &&& forall |pos_relative_to_head: int| {
                let log_area_offset =
                    #[trigger] relative_log_pos_to_log_area_offset(pos_relative_to_head,
                                                                   info.head_log_area_offset as int,
                                                                   info.log_area_len as int);
                let pmb = log_area_view.state[log_area_offset];
                &&& 0 <= pos_relative_to_head < info.log_length ==> {
                      &&& pmb.state_at_last_flush == state.log[pos_relative_to_head]
                      &&& pmb.outstanding_write.is_none()
                   }
                &&& info.log_length <= pos_relative_to_head < info.log_plus_pending_length ==>
                       pmb.flush_byte() == state.pending[pos_relative_to_head - info.log_length]
                &&& info.log_plus_pending_length <= pos_relative_to_head < info.log_area_len ==>
                       pmb.outstanding_write.is_none()
            }
    }

    pub open spec fn info_consistent_with_log_area_in_region(
        pm_region_view: PersistentMemoryRegionView,
        info: LogInfo,
        state: AbstractLogState,
    ) -> bool
    {
        &&& pm_region_view.len() >= ABSOLUTE_POS_OF_LOG_AREA + info.log_area_len
        &&& info_consistent_with_log_area(
               get_subregion_view(pm_region_view, ABSOLUTE_POS_OF_LOG_AREA as int, info.log_area_len as int),
               info,
               state
           )
    }

    pub open spec fn metadata_types_set(mem: Seq<u8>) -> bool 
    {
        &&& GlobalMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()))
        &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()))
        &&& {
            let metadata = GlobalMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()));
            let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()));
            crc == spec_crc_u64(metadata.spec_to_bytes())
        }
        &&& RegionMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of()))
        &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of()))
        &&& {
            let metadata = RegionMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of()));
            let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of()));
            crc == spec_crc_u64(metadata.spec_to_bytes())
        }
        &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()))
        &&& {
            let cdb = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()));
            &&& cdb == CDB_TRUE || cdb == CDB_FALSE 
            &&& if cdb == CDB_TRUE {
                &&& LogMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of()))
                &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE as int, u64::spec_size_of()))
                &&& {
                    let metadata = LogMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of()));
                    let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_TRUE as int, u64::spec_size_of()));
                    crc == spec_crc_u64(metadata.spec_to_bytes())
                }
            }
            else {
                &&& LogMetadata::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of()))
                &&& u64::bytes_parseable(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of()))
                &&& {
                    let metadata = LogMetadata::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of()));
                    let crc = u64::spec_from_bytes(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CRC_FOR_CDB_FALSE as int, u64::spec_size_of()));
                    crc == spec_crc_u64(metadata.spec_to_bytes())
                }
            }
        }
    }

    pub proof fn lemma_addresses_in_log_area_subregion_correspond_to_relative_log_positions(
        pm_region_view: PersistentMemoryRegionView,
        info: LogInfo
    )
        requires
            pm_region_view.len() == info.log_area_len,
            info.head_log_area_offset < info.log_area_len,
            info.log_area_len > 0,
        ensures
            forall |log_area_offset: int| #![trigger pm_region_view.state[log_area_offset]]
                0 <= log_area_offset < info.log_area_len ==> {
                    let pos_relative_to_head =
                        if log_area_offset >= info.head_log_area_offset {
                            log_area_offset - info.head_log_area_offset
                        }
                        else {
                            log_area_offset - info.head_log_area_offset + info.log_area_len
                        };
                    &&& 0 <= pos_relative_to_head < info.log_area_len
                    &&& log_area_offset ==
                           relative_log_pos_to_log_area_offset(pos_relative_to_head, info.head_log_area_offset as int,
                                                               info.log_area_len as int)
                }
    {
    }

    // This lemma proves that, for any address in the log area of the
    // given persistent memory view, it corresponds to a specific
    // logical position in the abstract log relative to the head. That
    // logical position `pos` satisfies `0 <= pos < log_area_len`.
    //
    // It's useful to call this lemma because it takes facts that
    // trigger `pm_region_view.state[addr]` and turns them into facts
    // that trigger `relative_log_pos_to_log_area_offset`. That's the
    // trigger used in `info_consistent_with_log_area_in_region`.
    pub proof fn lemma_addresses_in_log_area_correspond_to_relative_log_positions(
        pm_region_view: PersistentMemoryRegionView,
        info: LogInfo
    )
        requires
            pm_region_view.len() >= ABSOLUTE_POS_OF_LOG_AREA + info.log_area_len,
            info.head_log_area_offset < info.log_area_len,
            info.log_area_len > 0,
        ensures
            forall |addr: int| #![trigger pm_region_view.state[addr]]
                ABSOLUTE_POS_OF_LOG_AREA <= addr < ABSOLUTE_POS_OF_LOG_AREA + info.log_area_len ==> {
                    let log_area_offset = addr - ABSOLUTE_POS_OF_LOG_AREA;
                    let pos_relative_to_head =
                        if log_area_offset >= info.head_log_area_offset {
                            log_area_offset - info.head_log_area_offset
                        }
                        else {
                            log_area_offset - info.head_log_area_offset + info.log_area_len
                        };
                    &&& 0 <= pos_relative_to_head < info.log_area_len
                    &&& addr == ABSOLUTE_POS_OF_LOG_AREA +
                              relative_log_pos_to_log_area_offset(pos_relative_to_head,
                                                                  info.head_log_area_offset as int,
                                                                  info.log_area_len as int)
                }
    {
    }

    // This lemma proves that, if various invariants hold for the
    // given persistent-memory view `pm_region_view` and abstract log state
    // `state`, and if that view can crash as contents `mem`, then
    // recovery on `mem` will produce `state.drop_pending_appends()`.
    //
    // `pm_region_view` -- the view of this persistent-memory region
    // `mem` -- a possible memory contents that `pm_region_view` can crash as
    // `log_id` -- the ID of the log
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract log state
    proof fn lemma_invariants_imply_crash_recover_for_one_log(
        pm_region_view: PersistentMemoryRegionView,
        mem: Seq<u8>,
        log_id: u128,
        cdb: bool,
        info: LogInfo,
        state: AbstractLogState,
    )
        requires
            pm_region_view.can_crash_as(mem),
            metadata_consistent_with_info(pm_region_view, log_id, cdb, info),
            info_consistent_with_log_area_in_region(pm_region_view, info, state),
        ensures
            recover_given_cdb(mem, log_id, cdb) == Some(state.drop_pending_appends())
    {
        // For the metadata, we observe that:
        //
        // (1) there are no outstanding writes, so the crashed-into
        //     state `mem` must match the committed state
        //     `pm_region_view.committed()`, and
        // (2) wherever the crashed-into state matches the committed
        //     state on a per-byte basis, any `extract_bytes` results
        //     will also match.
        //
        // Therefore, since the metadata in
        // `pm_region_view.committed()` matches `state` (per the
        // invariants), the metadata in `mem` must also match `state`.

        lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(pm_region_view);
        lemma_establish_extract_bytes_equivalence(mem, pm_region_view.committed());

        // The tricky part is showing that the result of `extract_log` will produce the desired result.
        // Use `=~=` to ask Z3 to prove this equivalence by proving it holds on each byte.

        let log_view = get_subregion_view(pm_region_view, ABSOLUTE_POS_OF_LOG_AREA as int, info.log_area_len as int);
        lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(log_view);
        assert(recover_log_from_log_area_given_metadata(log_view.committed(), info.head as int, info.log_length as int)
               =~= Some(state.drop_pending_appends()));
        assert(recover_log(mem, info.log_area_len as int, info.head as int, info.log_length as int)
               =~= Some(state.drop_pending_appends()));
    }

    // This lemma proves that, if various invariants hold for the
    // given persistent-memory region view `pm_region_view` and
    // abstract log state `state`, and if that view can crash as
    // contents `mem`, then recovery on `mem` will produce
    // `state.drop_pending_appends()`.
    //
    // `pm_region_view` -- the persistent memory region view
    // `mem` -- a possible memory contents that `pm_region_view` can crash as
    // `log_id` -- the ID of the log
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract multilog state
    proof fn lemma_invariants_imply_crash_recover(
        pm_region_view: PersistentMemoryRegionView,
        mem: Seq<u8>,
        log_id: u128,
        cdb: bool,
        info: LogInfo,
        state: AbstractLogState,
    )
        requires
            pm_region_view.can_crash_as(mem),
            memory_matches_deserialized_cdb(pm_region_view, cdb),
            metadata_consistent_with_info(pm_region_view, log_id, cdb, info),
            info_consistent_with_log_area_in_region(pm_region_view, info, state),
            metadata_types_set(pm_region_view.committed()),
        ensures
            recover_cdb(mem) == Some(cdb),
            recover_state(mem, log_id) == Some(state.drop_pending_appends()),
            metadata_types_set(mem),
    {
        // For the CDB, we observe that:
        //
        // (1) there are no outstanding writes, so the crashed-into
        // state `mem` must match the committed state
        // `pm_region_view.committed()`, and
        //
        // (2) wherever the crashed-into state matches the committed
        // state on a per-byte basis, any `extract_bytes` results will
        // also match.
        //
        // Therefore, since the metadata in `pm_region_view.committed()`
        // matches `cdb` (per the invariants), the metadata in
        // `mem` must also match `cdb`.

        assert (recover_cdb(mem) == Some(cdb)) by {
            lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(pm_region_view);
            lemma_establish_extract_bytes_equivalence(mem, pm_region_view.committed());
        }

        // Use `lemma_invariants_imply_crash_recover_for_one_log` on
        // each region to establish that recovery works on all the
        // regions.

        assert(recover_given_cdb(mem, log_id, cdb) == Some(state.drop_pending_appends())) by {
            lemma_invariants_imply_crash_recover_for_one_log(pm_region_view, mem, log_id, cdb, info, state);
        }

        // Get Z3 to see the equivalence of the recovery
        // result and the desired abstract state by asking it (with
        // `=~=`) to prove that they're piecewise equivalent.

        assert(recover_state(mem, log_id) =~= Some(state.drop_pending_appends()));

        // Finally, invoke the lemma that proves that metadata types 
        // are still set in crash states

        lemma_metadata_set_after_crash(pm_region_view, cdb);
    }

    // This exported lemma proves that, if various invariants hold for
    // the given persistent memory region view `pm_region_view` and
    // abstract log state `state`, then for any contents `mem`
    // the view can recover into, recovery on `mem` will produce
    // `state.drop_pending_appends()`.
    //
    // `pm_region_view` -- the persistent memory region view
    // `log_id` -- the ID of the log
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract log state
    pub proof fn lemma_invariants_imply_crash_recover_forall(
        pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        cdb: bool,
        info: LogInfo,
        state: AbstractLogState,
    )
        requires
            memory_matches_deserialized_cdb(pm_region_view, cdb),
            metadata_consistent_with_info(pm_region_view, log_id, cdb, info),
            info_consistent_with_log_area_in_region(pm_region_view, info, state),
            metadata_types_set(pm_region_view.committed()),
        ensures
            forall |mem| #[trigger] pm_region_view.can_crash_as(mem) ==> {
                &&& recover_cdb(mem) == Some(cdb)
                &&& recover_state(mem, log_id) == Some(state.drop_pending_appends())
                &&& metadata_types_set(mem)
            }
    {
        assert forall |mem| #[trigger] pm_region_view.can_crash_as(mem) implies {
                   &&& recover_cdb(mem) == Some(cdb)
                   &&& recover_state(mem, log_id) == Some(state.drop_pending_appends())
                   &&& metadata_types_set(mem)
               } by
        {
            lemma_invariants_imply_crash_recover(pm_region_view, mem, log_id, cdb, info, state);
        }
    }

    // This lemma establishes that, if one updates the inactive
    // log metadata in a region, this will maintain various
    // invariants. The "inactive" log metadata is the
    // metadata corresponding to the negation of the current
    // corruption-detecting boolean.
    //
    // `pm_region_view` -- the persistent memory region view
    // `log_id` -- the ID of the log
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract log state
    // `bytes_to_write` -- bytes to be written to the inactive log metadata area
    pub proof fn lemma_updating_inactive_metadata_maintains_invariants(
        pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        cdb: bool,
        info: LogInfo,
        state: AbstractLogState,
        bytes_to_write: Seq<u8>,
    )
        requires
            memory_matches_deserialized_cdb(pm_region_view, cdb),
            metadata_consistent_with_info(pm_region_view, log_id, cdb, info),
            info_consistent_with_log_area_in_region(pm_region_view, info, state),
            bytes_to_write.len() == LogMetadata::spec_size_of(),
            metadata_types_set(pm_region_view.committed())
       ensures
            ({
                let pm_region_view2 = pm_region_view.write(get_log_metadata_pos(!cdb) as int, bytes_to_write);
                &&& memory_matches_deserialized_cdb(pm_region_view2, cdb)
                &&& metadata_consistent_with_info(pm_region_view2, log_id, cdb, info)
                &&& info_consistent_with_log_area_in_region(pm_region_view2, info, state)
                &&& metadata_types_set(pm_region_view2.committed())
            })
    {
        let pm_region_view2 = pm_region_view.write(get_log_metadata_pos(!cdb) as int, bytes_to_write);

        assert(memory_matches_deserialized_cdb(pm_region_view2, cdb)) by {
            assert(extract_log_cdb(pm_region_view2.committed()) =~=
                   extract_log_cdb(pm_region_view.committed()));
        }

        // To show that all the metadata still matches even after the
        // write, observe that everywhere the bytes match, any call to
        // `extract_bytes` will also match.

        lemma_establish_extract_bytes_equivalence(pm_region_view.committed(), pm_region_view2.committed());

        let mem = pm_region_view.committed();
        let global_metadata = deserialize_global_metadata(mem);
        let global_crc = deserialize_global_crc(mem);
        let region_metadata = deserialize_region_metadata(mem);
        let region_crc = deserialize_region_crc(mem);
        let log_metadata = deserialize_log_metadata(mem, cdb);
        let log_crc = deserialize_log_crc(mem, cdb);

        let mem2 = pm_region_view2.committed();
        let global_metadata2 = deserialize_global_metadata(mem2);
        let global_crc2 = deserialize_global_crc(mem2);
        let region_metadata2 = deserialize_region_metadata(mem2);
        let region_crc2 = deserialize_region_crc(mem2);
        let log_metadata2 = deserialize_log_metadata(mem2, cdb);
        let log_crc2 = deserialize_log_crc(mem2, cdb);

        let global_metadata_bytes1 = extract_bytes(mem, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of() as int);
        let global_metadata_bytes2 = extract_bytes(mem2, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of() as int);
    
        assert(metadata_consistent_with_info(pm_region_view2, log_id, cdb, info)) by {
            lemma_establish_extract_bytes_equivalence(pm_region_view.committed(), pm_region_view2.committed());
        }

        assert(mem.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) == 
            (mem2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int)));
        if cdb {
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of() + u64::spec_size_of()) == 
                extract_bytes(mem2, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of() + u64::spec_size_of()));
        } else {
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                extract_bytes(mem2, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of() + u64::spec_size_of()));
        }
        assert(active_metadata_bytes_are_equal(mem, mem2));
        lemma_metadata_matches_implies_metadata_types_set(pm_region_view, pm_region_view2, cdb);
    }

    // This lemma establishes that, if one updates the inactive
    // log metadata in a region, this will maintain various
    // invariants. The "inactive" log metadata is the
    // metadata corresponding to the negation of the current
    // corruption-detecting boolean.
    //
    // `pm_region_view` -- the persistent memory region view
    // `log_id` -- the ID of the log
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract log state
    // `bytes_to_write` -- bytes to be written to the inactive log metadata area
    pub proof fn lemma_updating_inactive_crc_maintains_invariants(
        pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        cdb: bool,
        info: LogInfo,
        state: AbstractLogState,
        bytes_to_write: Seq<u8>,
    )
        requires
            memory_matches_deserialized_cdb(pm_region_view, cdb),
            metadata_consistent_with_info(pm_region_view, log_id, cdb, info),
            info_consistent_with_log_area_in_region(pm_region_view, info, state),
            bytes_to_write.len() == u64::spec_size_of(),
            metadata_types_set(pm_region_view.committed()),
        ensures
            ({
                let pm_region_view2 = pm_region_view.write(
                    get_log_metadata_pos(!cdb) + LogMetadata::spec_size_of(),
                    bytes_to_write
                );
                &&& memory_matches_deserialized_cdb(pm_region_view2, cdb)
                &&& metadata_consistent_with_info(pm_region_view2, log_id, cdb, info)
                &&& info_consistent_with_log_area_in_region(pm_region_view2, info, state)
                &&& metadata_types_set(pm_region_view2.flush().committed())
            })
    {
        let pm_region_view2 = pm_region_view.write(
            get_log_metadata_pos(!cdb) + LogMetadata::spec_size_of(),
            bytes_to_write
        );

        assert(memory_matches_deserialized_cdb(pm_region_view2, cdb)) by {
            assert(extract_log_cdb(pm_region_view2.committed()) =~=
                   extract_log_cdb(pm_region_view.committed()));
        }

        let mem = pm_region_view.committed();
        let mem2 = pm_region_view2.flush().committed();

        assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()) ==
               extract_bytes(mem2, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()));

        assert(mem.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) ==
                mem2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));
        if cdb {
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                extract_bytes(mem2, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_TRUE as int, LogMetadata::spec_size_of() + u64::spec_size_of()));
        } else {
            assert(extract_bytes(mem, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of() + u64::spec_size_of()) ==
                extract_bytes(mem2, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int, LogMetadata::spec_size_of() + u64::spec_size_of()));
        }

        // To show that all the metadata still matches even after the
        // write, observe that everywhere the bytes match, any call to
        // `extract_bytes` will also match.

        assert(metadata_consistent_with_info(pm_region_view2, log_id, cdb, info)) by {
            lemma_establish_extract_bytes_equivalence(pm_region_view.committed(), pm_region_view2.committed());
        }

        lemma_metadata_matches_implies_metadata_types_set(pm_region_view, pm_region_view2.flush(), cdb);
    }

    // This lemma establishes that, if one flushes persistent memory,
    // this will maintain various invariants.
    //
    // `pm_region_view` -- the persistent memory region view
    // `log_id` -- the ID of the log
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract log state
    pub proof fn lemma_flushing_metadata_maintains_invariants(
        pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        cdb: bool,
        info: LogInfo,
        state: AbstractLogState,
    )
        requires
            memory_matches_deserialized_cdb(pm_region_view, cdb),
            metadata_consistent_with_info(pm_region_view,  log_id, cdb, info),
            info_consistent_with_log_area_in_region(pm_region_view, info, state),
            metadata_types_set(pm_region_view.committed()),
       ensures
            ({
                let pm_region_view2 = pm_region_view.flush();
                &&& memory_matches_deserialized_cdb(pm_region_view2, cdb)
                &&& metadata_consistent_with_info(pm_region_view2, log_id, cdb, info)
                &&& info_consistent_with_log_area_in_region(pm_region_view2, info, state)
                &&& metadata_types_set(pm_region_view2.committed())
            })
    {
        let pm_region_view2 = pm_region_view.flush();

        assert(memory_matches_deserialized_cdb(pm_region_view2, cdb)) by {
            assert(extract_log_cdb(pm_region_view2.committed()) =~=
                   extract_log_cdb(pm_region_view.committed()));
        }

        // To show that all the metadata still matches even after the
        // flush, observe that everywhere the bytes match, any call to
        // `extract_bytes` will also match.

        assert(metadata_consistent_with_info(pm_region_view2, log_id, cdb, info)) by {
            lemma_establish_extract_bytes_equivalence(pm_region_view.committed(),
                                                      pm_region_view2.committed());
        }

        // Prove that the bytes in the active metadata are unchanged after the flush, so 
        // the metadata types are still set.
        
        assert(active_metadata_is_equal(pm_region_view, pm_region_view2)) by {
            let mem1 = pm_region_view.committed();
            let mem2 = pm_region_view2.committed();
            let log_metadata_pos = get_log_metadata_pos(cdb);

            assert(deserialize_and_check_log_cdb(mem1) == deserialize_and_check_log_cdb(mem2));
            assert(mem1.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) == 
                mem2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int));
            assert(extract_bytes(mem1, log_metadata_pos as int, LogMetadata::spec_size_of() + u64::spec_size_of()) == 
                extract_bytes(mem2, log_metadata_pos as int, LogMetadata::spec_size_of() + u64::spec_size_of()));
        }
        lemma_metadata_matches_implies_metadata_types_set(pm_region_view, pm_region_view2, cdb);
    }

    // This predicate describes whether a given log area offset is
    // unreachable during recovery (because it's beyond the tail).
    //
    // Its parameters are:
    // `head_log_area_offset` -- the the log offset where the log head is
    // `log_area_len` -- the length of the log area
    // `log_length` -- the length of the abstract log
    // `log_area_offset` -- the log area offet being asked about
    pub open spec fn log_area_offset_unreachable_during_recovery(
        head_log_area_offset: int,
        log_area_len: int,
        log_length: int,
        log_area_offset: int,
    ) -> bool
    {
        log_area_offset_to_relative_log_pos(log_area_offset, head_log_area_offset, log_area_len) >= log_length
    }

    // This lemma establishes that if:
    //
    // 1) two views `v1` and `v2` only differ in unreachable parts of
    // the log area (one that satisfy
    // `log_area_offset_unreachable_during_recovery`),
    //
    // 2) view `v1` satisfies certain invariant properties,
    //
    // 3) view `v2` can crash into state `crash_state`,
    //
    // then `crash_state` recovers to the same abstract state as
    // `v1.committed()`. This is useful to know for the following
    // reason. `v1` can obviously crash as `v1.committed()`. So, if we
    // know that all possible crash states of `v1` recover to a valid
    // state then we know `crash_state` recovers to a valid state.
    //
    // The parameters to this function are:
    //
    // `v1` and `v2` -- the two views
    // `crash_state` -- the state that `v2` can crash into
    // `log_id` -- the ID of the log
    // `cdb` -- the current value of the corruption-detecting boolean
    // `info` -- the log information
    // `state` -- the abstract log state
    // `is_writable_absolute_addr` -- a spec predicate describing
    // which absolute addresses in the log area may differ between
    // `v1` and `v2`.
    pub proof fn lemma_if_view_differs_only_in_log_area_parts_not_accessed_by_recovery_then_recover_state_matches(
        v1: PersistentMemoryRegionView,
        v2: PersistentMemoryRegionView,
        crash_state: Seq<u8>,
        log_id: u128,
        cdb: bool,
        info: LogInfo,
        state: AbstractLogState,
        is_writable_absolute_addr: spec_fn(int) -> bool,
    )
        requires
            no_outstanding_writes_to_metadata(v1),
            memory_matches_deserialized_cdb(v1, cdb),
            metadata_consistent_with_info(v1, log_id, cdb, info),
            info_consistent_with_log_area_in_region(v1, info, state),
            ABSOLUTE_POS_OF_LOG_AREA + info.log_area_len <= v1.len(),
            v2.can_crash_as(crash_state),
            v1.len() == v2.len(),
            forall |addr: int| #[trigger] is_writable_absolute_addr(addr) <==> 
                  log_area_offset_unreachable_during_recovery(info.head_log_area_offset as int,
                                                              info.log_area_len as int,
                                                              info.log_length as int,
                                                              addr - ABSOLUTE_POS_OF_LOG_AREA),
            views_differ_only_where_subregion_allows(v1, v2, ABSOLUTE_POS_OF_LOG_AREA as int,
                                                     info.log_area_len as int, is_writable_absolute_addr),
        ensures
            v1.can_crash_as(v1.committed()),
            recover_state(crash_state, log_id) == recover_state(v1.committed(), log_id),
    {
        lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(v2);
        lemma_establish_extract_bytes_equivalence(crash_state, v1.committed());
        assert(recover_state(crash_state, log_id) =~= recover_state(v1.committed(), log_id));
    }

    // This lemma proves that if the log metadata has been properly set up and there are no outstanding writes to 
    // metadata, then the metadata_types_set invariant holds after any crash. This is useful when proving the invariant
    // after an update that does not touch metadata.
    pub proof fn lemma_metadata_set_after_crash(
        pm_region_view: PersistentMemoryRegionView,
        cdb: bool
    )
        requires 
            no_outstanding_writes_to_active_metadata(pm_region_view, cdb),
            metadata_types_set(pm_region_view.committed()),
            memory_matches_deserialized_cdb(pm_region_view, cdb),
        ensures 
            forall |s| #![auto] {
                &&& pm_region_view.can_crash_as(s) 
                &&& 0 <= ABSOLUTE_POS_OF_GLOBAL_METADATA < ABSOLUTE_POS_OF_LOG_AREA < s.len()
            } ==> metadata_types_set(s),
    {
        let pm_bytes = pm_region_view.committed();
        assert(cdb == deserialize_and_check_log_cdb(pm_bytes).unwrap());

        lemma_wherever_no_outstanding_writes_persistent_memory_view_can_only_crash_as_committed(pm_region_view);

        assert forall |s| {
            &&& pm_region_view.can_crash_as(s) 
            &&& 0 <= ABSOLUTE_POS_OF_GLOBAL_METADATA < ABSOLUTE_POS_OF_LOG_AREA < s.len()
        } implies {
            let s_cdb = deserialize_and_check_log_cdb(s).unwrap();
            &&& deserialize_global_metadata(s) == deserialize_global_metadata(pm_bytes)
            &&& deserialize_global_crc(s) == deserialize_global_crc(pm_bytes)
            &&& deserialize_region_metadata(s) == deserialize_region_metadata(pm_bytes)
            &&& deserialize_region_crc(s) == deserialize_region_crc(pm_bytes)
            &&& s_cdb == cdb 
            &&& if s_cdb {
                   &&& deserialize_log_metadata(s, true) == deserialize_log_metadata(pm_bytes, true)
                   &&& deserialize_log_crc(s, true) == deserialize_log_crc(pm_bytes, true)
               }
               else {
                   &&& deserialize_log_metadata(s, false) == deserialize_log_metadata(pm_bytes, false)
                   &&& deserialize_log_crc(s, false) == deserialize_log_crc(pm_bytes, false)
               }
        } by {
            lemma_establish_extract_bytes_equivalence(s, pm_region_view.committed());
        }

        assert forall |s| #![auto] {
            &&& pm_region_view.can_crash_as(s) 
            &&& 0 <= ABSOLUTE_POS_OF_GLOBAL_METADATA < ABSOLUTE_POS_OF_LOG_AREA < s.len()
        } implies metadata_types_set(s) by {
            lemma_establish_extract_bytes_equivalence(s, pm_region_view.committed());
        }
    }

    // This lemma proves that if we two PM states have the same bytes in the log header and no outstanding writes in that region,
    // and one of the states has metadata types set, then the other also has metadata types set. This is useful for proving 
    // that the metadata types invariant holds when appending to the log.
    pub proof fn lemma_metadata_matches_implies_metadata_types_set(
        pm1: PersistentMemoryRegionView,
        pm2: PersistentMemoryRegionView,
        cdb: bool
    )
        requires 
            no_outstanding_writes_to_active_metadata(pm1, cdb),
            no_outstanding_writes_to_active_metadata(pm2, cdb),
            metadata_types_set(pm1.committed()),
            memory_matches_deserialized_cdb(pm1, cdb),
            0 < ABSOLUTE_POS_OF_LOG_AREA < pm1.committed().len(),
            0 < ABSOLUTE_POS_OF_LOG_AREA < pm2.committed().len(),
            active_metadata_is_equal(pm1, pm2),
            pm1.len() == pm2.len()
        ensures 
            metadata_types_set(pm2.committed())
    {
        lemma_active_metadata_bytes_equal_implies_metadata_types_set(pm1.committed(), pm2.committed(), cdb);
    }

    // This lemma proves that if two sequences have equal active metadata bytes and one has its metadata types set,
    // then the other sequence also has its metadata types set.
    pub proof fn lemma_active_metadata_bytes_equal_implies_metadata_types_set(
        mem1: Seq<u8>,
        mem2: Seq<u8>,
        cdb: bool
    )
        requires 
            ABSOLUTE_POS_OF_LOG_AREA <= mem1.len(),
            ABSOLUTE_POS_OF_LOG_AREA <= mem2.len(),
            active_metadata_bytes_are_equal(mem1, mem2),
            ({
                let cdb1 = deserialize_and_check_log_cdb(mem1);
                let cdb2 = deserialize_and_check_log_cdb(mem2);
                let log_metadata_pos = get_log_metadata_pos(cdb);
                &&& cdb1 is Some 
                &&& cdb2 is Some 
                &&& cdb ==> cdb1.unwrap() && cdb2.unwrap()
                &&& !cdb ==> !cdb1.unwrap() && !cdb2.unwrap()
            }),
            metadata_types_set(mem1)
        ensures 
            metadata_types_set(mem2),
    {
        lemma_establish_extract_bytes_equivalence(mem1, mem2);

        // This lemma automatically establishes the relationship between subranges of subranges from the same sequence, 
        // so knowing that the assertions below cover subranges of larger, equal subranges is enough to establish equality
        // (but we have to assert it explicitly to hit the triggers)
        lemma_auto_smaller_range_of_seq_is_subrange(mem1);

        // First, establish that the immutable parts and the CDB are the same between both byte sequences.
        let mem1_without_log_metadata = mem1.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int);
        let mem2_without_log_metadata = mem2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int);
        assert(extract_bytes(mem1, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()) == 
            extract_bytes(mem2, ABSOLUTE_POS_OF_GLOBAL_METADATA as int, GlobalMetadata::spec_size_of()));
        assert(extract_bytes(mem1, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()) == 
            extract_bytes(mem2, ABSOLUTE_POS_OF_GLOBAL_CRC as int, u64::spec_size_of()));
        assert(extract_bytes(mem1, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of()) == 
            extract_bytes(mem2, ABSOLUTE_POS_OF_REGION_METADATA as int, RegionMetadata::spec_size_of()));
        assert(extract_bytes(mem1, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of()) == 
            extract_bytes(mem2, ABSOLUTE_POS_OF_REGION_CRC as int, u64::spec_size_of()));
        assert(extract_bytes(mem1, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()) == 
            extract_bytes(mem2, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()));

        // Next, establish that the types are set in the active metadata
        let log_metadata_pos = get_log_metadata_pos(cdb);
        assert(extract_bytes(mem1, log_metadata_pos as int, LogMetadata::spec_size_of()) == 
            extract_bytes(mem2, log_metadata_pos as int, LogMetadata::spec_size_of()));
        assert(extract_bytes(mem1, log_metadata_pos + LogMetadata::spec_size_of(), u64::spec_size_of()) ==
            extract_bytes(mem2, log_metadata_pos + LogMetadata::spec_size_of(), u64::spec_size_of()));
    }

    pub proof fn lemma_auto_smaller_range_of_seq_is_subrange(mem1: Seq<u8>)
        ensures 
            forall |i: int, j, k: int, l: int| 0 <= i <= k <= l <= j <= mem1.len() ==> mem1.subrange(i, j).subrange(k - i, l - i) == mem1.subrange(k, l) 
    {
        assert forall |i: int, j, k: int, l: int| 0 <= i <= k <= l <= j <= mem1.len() implies mem1.subrange(i, j).subrange(k - i, l - i) == mem1.subrange(k, l) by {
            lemma_smaller_range_of_seq_is_subrange(mem1, i, j, k, l);
        }
    }

    pub proof fn lemma_smaller_range_of_seq_is_subrange(mem1: Seq<u8>, i: int, j: int, k: int, l: int)
        requires 
            0 <= i <= k <= l <= j <= mem1.len()
        ensures 
            mem1.subrange(i, j).subrange(k - i, l - i) == mem1.subrange(k, l) 
    {
        assert(mem1.subrange(k, l) == mem1.subrange(i + k - i, i + l - i));
        assert(mem1.subrange(i, j).subrange(k - i, l - i) == mem1.subrange(i + k - i, i + l - i));
    }

    pub proof fn lemma_header_bytes_equal_implies_active_metadata_bytes_equal(mem1: Seq<u8>, mem2: Seq<u8>)
        requires 
            ABSOLUTE_POS_OF_LOG_AREA <= mem1.len(),
            ABSOLUTE_POS_OF_LOG_AREA <= mem2.len(),
            mem1.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_AREA as int) =~= 
                mem2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_AREA as int),
            deserialize_and_check_log_cdb(mem1) is Some,
        ensures 
            active_metadata_bytes_are_equal(mem1, mem2)
    {
        lemma_establish_extract_bytes_equivalence(mem1, mem2);

        lemma_auto_smaller_range_of_seq_is_subrange(mem1);

        let cdb = deserialize_and_check_log_cdb(mem1).unwrap();
        let log_metadata_pos = get_log_metadata_pos(cdb);

        assert(mem1.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) ==
            mem2.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_METADATA_FOR_CDB_FALSE as int) );
        assert(mem1.subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()) == 
            mem2.subrange(log_metadata_pos as int, log_metadata_pos + LogMetadata::spec_size_of() + u64::spec_size_of()));
    }

    pub proof fn lemma_metadata_types_set_after_cdb_update(
        old_pm_region_view: PersistentMemoryRegionView,
        new_pm_region_view: PersistentMemoryRegionView,
        log_id: u128,
        new_cdb_bytes: Seq<u8>,
        old_cdb: bool,
    )
        requires 
            old_pm_region_view.no_outstanding_writes(),
            new_pm_region_view.no_outstanding_writes(),
            old_pm_region_view.len() >= ABSOLUTE_POS_OF_LOG_AREA,
            old_pm_region_view.len() == new_pm_region_view.len(),
            new_cdb_bytes == CDB_FALSE.spec_to_bytes() || new_cdb_bytes == CDB_TRUE.spec_to_bytes(),
            old_cdb ==> new_cdb_bytes == CDB_FALSE.spec_to_bytes(),
            !old_cdb ==> new_cdb_bytes == CDB_TRUE.spec_to_bytes(),
            new_pm_region_view =~= old_pm_region_view.write(ABSOLUTE_POS_OF_LOG_CDB as int, new_cdb_bytes).flush(),
            metadata_types_set(old_pm_region_view.committed()),
            inactive_metadata_types_set(old_pm_region_view.committed())
        ensures 
            metadata_types_set(new_pm_region_view.committed())
    {
        broadcast use pmcopy_axioms;

        let old_mem = old_pm_region_view.committed();
        let new_mem = new_pm_region_view.committed();
        lemma_auto_smaller_range_of_seq_is_subrange(old_mem);
        lemma_auto_smaller_range_of_seq_is_subrange(new_mem);
        
        // Immutable metadata has not changed
        assert(old_mem.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_CDB as int) =~=
            new_mem.subrange(ABSOLUTE_POS_OF_GLOBAL_METADATA as int, ABSOLUTE_POS_OF_LOG_CDB as int));

        // We updated the CDB -- its type is still set, since new_cdb_bytes corresponds to a serialization of a valid CDB value
        assert(extract_bytes(new_mem, ABSOLUTE_POS_OF_LOG_CDB as int, u64::spec_size_of()) == new_cdb_bytes);

        let new_cdb = deserialize_and_check_log_cdb(new_mem).unwrap();
        let active_metadata_pos = get_log_metadata_pos(new_cdb);
        // The bytes in the new active position are the same in both byte sequences, and they had their metadata types set in the old view,
        // so types are also set in the new view, and the postcondition holds.
        assert(extract_bytes(new_mem, active_metadata_pos as int, LogMetadata::spec_size_of() + u64::spec_size_of()) == 
            extract_bytes(old_mem, active_metadata_pos as int, LogMetadata::spec_size_of() + u64::spec_size_of()));
    }
}

================
File: ./LICENSE.txt
================

    MIT License

    Copyright (c) Microsoft Corporation.

    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the "Software"), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE

================
File: ./SECURITY.md
================

<!-- BEGIN MICROSOFT SECURITY.MD V0.0.8 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/opensource/security/definition), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/opensource/security/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/opensource/security/pgpkey).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://aka.ms/opensource/security/msrc). 

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
  * Full paths of source file(s) related to the manifestation of the issue
  * The location of the affected source code (tag/branch/commit or direct URL)
  * Any special configuration required to reproduce the issue
  * Step-by-step instructions to reproduce the issue
  * Proof-of-concept or exploit code (if possible)
  * Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/opensource/security/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/opensource/security/cvd).

<!-- END MICROSOFT SECURITY.MD BLOCK -->

